{
  "courses": [
    {
      "title": "PySpark & AWS: Master Big Data With PySpark and AWS",
      "url": "https://www.udemy.com/course/pyspark-aws-master-big-data-with-pyspark-and-aws/",
      "bio": "Mastering AWS & PySpark: Spark, PySpark, AWS, Spark Ecosystem, Hadoop, and Spark Applications [AWS, Hadoop, Pyspark]",
      "objectives": [
        "● The introduction and importance of Big Data.",
        "● Practical explanation and live coding with PySpark.",
        "● Spark applications",
        "● Spark EcoSystem",
        "● Spark Architecture",
        "● Hadoop EcoSystem",
        "● Hadoop Architecture",
        "● PySpark RDDs",
        "● PySpark RDD transformations",
        "● PySpark RDD actions",
        "● PySpark DataFrames",
        "● PySpark DataFrames transformations",
        "● PySpark DataFrames actions",
        "● Collaborative filtering in PySpark",
        "● Spark Streaming",
        "● ETL Pipeline",
        "● CDC and Replication on Going"
      ],
      "course_content": {
        "Introduction": [
          "Why Big Data",
          "Applications of PySpark",
          "Introduction to Instructor",
          "Introduction to Course",
          "Projects Overview",
          "Request for Your Honest Review",
          "Links for the Course's Materials and Codes",
          "Practice Test # 01"
        ],
        "01-Introduction to Hadoop, Spark EcoSystems and Architectures": [
          "Links for the Course's Materials and Codes",
          "Why Spark",
          "Hadoop EcoSystem",
          "Spark Architecture and EcoSystem",
          "DataBricks SignUp",
          "Create DataBricks Notebook",
          "Download Spark and Dependencies",
          "Java Setup on Window",
          "Windows Setup Python Spark Hadoop",
          "Runing Spark on Window",
          "Java Download on MAC",
          "Installing JDK on MAC",
          "Setting Java Home on MAC",
          "Java check on MAC",
          "Installing Python on MAC",
          "Setup Spark on MAC",
          "Which of the following statement is True",
          "Which of the following is not a part of spark ecosystem?",
          "Practice Test # 02"
        ],
        "Spark RDDs": [
          "Links for the Course's Materials and Codes",
          "Spark RDDs",
          "Creating Spark RDD",
          "Running Spark Code Locally",
          "RDD stands for:",
          "RDD is created by using:",
          "RDD Map (Lambda)",
          "RDD Map (Simple Function)",
          "Quiz (Map)",
          "Solution 1 (Map)",
          "Solution 2 (Map)",
          "RDD FlatMap",
          "RDD Filter",
          "Quiz (Filter)",
          "Solution (Filter)",
          "RDD Distinct",
          "RDD GroupByKey",
          "RDD ReduceByKey",
          "Quiz (Word Count)",
          "Solution (Word Count)",
          "RDD (Count and CountByValue)",
          "RDD (saveAsTextFile)",
          "RDD (Partition)",
          "Finding Average-1",
          "Finding Average-2",
          "Quiz (Average)",
          "Solution (Average)",
          "Finding Min and Max",
          "Quiz (Min and Max)",
          "Solution (Min and Max)",
          "Project Overview",
          "Total Students",
          "Total Marks by Male and Female Student",
          "Total Passed and Failed Students",
          "Total Enrollments per Course",
          "Total Marks per Course",
          "Average marks per Course",
          "Finding Minimum and Maximum marks",
          "Average Age of Male and Female Students"
        ],
        "Spark DFs": [
          "Links for the Course's Materials and Codes",
          "Introduction to Spark DFs",
          "Creating Spark DFs",
          "DF stands for:",
          "DF is created by using:",
          "Spark Infer Schema",
          "Spark Provide Schema",
          "Create DF from Rdd",
          "Rectifying the Error",
          "Select DF Colums",
          "Spark DF withColumn",
          "Spark DF withColumnRenamed and Alias",
          "Spark DF Filter rows",
          "Quiz (select, withColumn, filter)",
          "Solution (select, withColumn, filter)",
          "Spark DF (Count, Distinct, Duplicate)",
          "Quiz (Distinct, Duplicate)",
          "Solution (Distinct, Duplicate)",
          "Spark DF (sort, orderBy)",
          "Quiz (sort, orderBy)",
          "Solution (sort, orderBy)",
          "Spark DF (Group By)",
          "Spark DF (Group By - Multiple Columns and Aggregations)",
          "Spark DF (Group By -Visualization)",
          "Spark DF (Group By - Filtering)",
          "Quiz (Group By)",
          "Solution (Group By)",
          "Quiz (Word Count)",
          "Solution (Word Count)",
          "Spark DF (UDFs)",
          "Quiz (UDFs)",
          "Solution (UDFs)",
          "Solution (Cache and Presist)",
          "Spark DF (DF to RDD)",
          "Spark DF (Spark SQL)",
          "Spark DF (Write DF)",
          "Project Overview",
          "Project (Count and Select)",
          "Project (Group By)",
          "Project (Group By, Aggregations and Order By)",
          "Project (Filtering)",
          "Project (UDF and WithColumn)",
          "Project (Write)"
        ],
        "Collaborative filtering": [
          "Links for the Course's Materials and Codes",
          "Collaborative filtering",
          "Utility Matrix",
          "Explicit and Implicit Ratings",
          "Expected Results",
          "Dataset",
          "Joining Dataframes",
          "Train and Test Data",
          "ALS model",
          "Hyperparameter tuning and cross validation",
          "Best model and evaluate predictions",
          "Recommendations"
        ],
        "Spark Streaming": [
          "Links for the Course's Materials and Codes",
          "Introduction to Spark Streaming",
          "Spark Streaming with RDD",
          "Spark streaming is used to:",
          "Spark Streaming Context",
          "Spark Streaming Reading Data",
          "Spark Streaming Cluster Restart",
          "Spark Streaming RDD Transformations",
          "Which statement is true about SparkContext and StreamingContext:",
          "Spark Streaming DF",
          "Spark Streaming Display",
          "Spark Streaming DF Aggregations"
        ],
        "ETL Pipeline": [
          "Links for the Course's Materials and Codes",
          "Introduction to ETL",
          "We can perform ETL using PySpark:",
          "ETL stands for:",
          "ETL pipeline Flow",
          "Data set",
          "Extracting Data",
          "Transforming Data",
          "Loading data (Creating RDS-I)",
          "Load data (Creating RDS-II)",
          "RDS Networking",
          "Downloading Postgres",
          "Installing Postgres",
          "Connect to RDS thorugh PgAdmin",
          "Loading Data"
        ],
        "Project - Change Data Capture / Replication On Going": [
          "Links for the Course's Materials and Codes",
          "Introduction to Project",
          "Project Architecture",
          "In this project we are going to implement:",
          "The cloud service DMS will be used to:",
          "Creating RDS MySql instance",
          "Creating S3 Bucket",
          "Creating DMS Source Endpoint",
          "Creating DMS Destination Endpoint",
          "Creating DMS Instance",
          "MySql WorkBench",
          "Connecting with RDS and Dumping Data",
          "Quering RDS",
          "DMS Full Load",
          "DMS Replication Ongoing",
          "Stoping Instances",
          "Glue Job (Full Load)",
          "Glue Job (Change Capture)",
          "Glue Job (CDC)",
          "Creating Lambda Function and Adding Trigger",
          "Checking Trigger",
          "Getting S3 file name in Lambda",
          "Creating Glue Job",
          "Adding Invoke for Glue Job",
          "Testing Invoke",
          "Writing Glue Shell Job",
          "Full Load Pipeline",
          "Change Data Capture Pipeline"
        ],
        "Chatbots Development with Amazon Lex": [
          "Links for the Course's Materials and Codes",
          "Fundamentals of AWS for Chatbots: Lex Bot Overview",
          "Fundamentals of AWS for Chatbots: Benifits of Amazon Lex",
          "Fundamentals of AWS for Chatbots: Framework of Lex",
          "Chatbot Development with AWS Lex and AWS Lambda: Module Overview",
          "Chatbot Development with AWS Lex and AWS Lambda: Chatbot Steps",
          "Chatbot Development with AWS Lex and AWS Lambda: AWS Lambda Steps",
          "Chatbot Development with AWS Lex and AWS Lambda: Twilio and Website",
          "Chatbot Development with AWS Lex and AWS Lambda: Response Cards",
          "Chatbot Development with AWS Lex and AWS Lambda: Start Developing Chatbot",
          "Chatbot Development with AWS Lex and AWS Lambda: Intent Utterance and Slot",
          "Chatbot Development with AWS Lex and AWS Lambda: Making Utterances",
          "Chatbot Development with AWS Lex and AWS Lambda: Generic Utterance with Slots",
          "Chatbot Development with AWS Lex and AWS Lambda: Adding Custom Slots",
          "Chatbot Development with AWS Lex and AWS Lambda: Build and Test",
          "Chatbot Development with AWS Lex and AWS Lambda: Visual Builder",
          "Chatbot Development with AWS Lex and AWS Lambda: Lambda Introduction",
          "Chatbot Development with AWS Lex and AWS Lambda: Interconnection",
          "Chatbot Development with AWS Lex and AWS Lambda: Starting Lambda Code",
          "Chatbot Development with AWS Lex and AWS Lambda: Session state Dialog Hook and Dialog Action",
          "Chatbot Development with AWS Lex and AWS Lambda: Completing Lambda Function",
          "Chatbot Development with AWS Lex and AWS Lambda: Testing our Chatbot",
          "Chatbot Development with AWS Lex and AWS Lambda: Chatbot Deployment on Whatsapp with Twilio",
          "Chatbot Development with AWS Lex and AWS Lambda: Integration with Boto",
          "Chatbot Development with AWS Lex and AWS Lambda: Responses with Boto",
          "Chatbot Development with AWS Lex and AWS Lambda: Chatbot on Website",
          "Chatbot Development with AWS Lex and AWS Lambda: Response Cards for User Experience",
          "Chatbot Development with AWS Lex and AWS Lambda: Complete Chatbot with Response Cards"
        ]
      },
      "requirements": [
        "● Prior knowledge of Python.",
        "● An elementary understanding of programming.",
        "● A willingness to learn and practice."
      ],
      "description": "Comprehensive Course Description:\nThe hottest buzzwords in the Big Data analytics industry are Python and Apache Spark. PySpark supports the collaboration of Python and Apache Spark. In this course, you’ll start right from the basics and proceed to the advanced levels of data analysis. From cleaning data to building features and implementing machine learning (ML) models, you’ll learn how to execute end-to-end workflows using PySpark.\n\n\nRight through the course, you’ll be using PySpark for performing data analysis. You’ll explore Spark RDDs, Dataframes, and a bit of Spark SQL queries. Also, you’ll explore the transformations and actions that can be performed on the data using Spark RDDs and dataframes. You’ll also explore the ecosystem of Spark and Hadoop and their underlying architecture. You’ll use the Databricks environment for running the Spark scripts and explore it as well.\n\n\nFinally, you’ll have a taste of Spark with AWS cloud. You’ll see how we can leverage AWS storages, databases, computations, and how Spark can communicate with different AWS services and get its required data.\n\n\nHow Is This Course Different?\nIn this Learning by Doing course, every theoretical explanation is followed by practical implementation.\n\n\nThe course ‘PySpark & AWS: Master Big Data With PySpark and AWS’ is crafted to reflect the most in-demand workplace skills. This course will help you understand all the essential concepts and methodologies with regards to PySpark. The course is:\n• Easy to understand.\n• Expressive.\n• Exhaustive.\n• Practical with live coding.\n• Rich with the state of the art and latest knowledge of this field.\n\n\nAs this course is a detailed compilation of all the basics, it will motivate you to make quick progress and experience much more than what you have learned. At the end of each concept, you will be assigned Homework/tasks/activities/quizzes along with solutions. This is to evaluate and promote your learning based on the previous concepts and methods you have learned. Most of these activities will be coding-based, as the aim is to get you up and running with implementations.\nHigh-quality video content, in-depth course material, evaluating questions, detailed course notes, and informative handouts are some of the perks of this course. You can approach our friendly team in case of any course-related queries, and we assure you of a fast response.\n\n\nThe course tutorials are divided into 140+ brief videos. You’ll learn the concepts and methodologies of PySpark and AWS along with a lot of practical implementation. The total runtime of the HD videos is around 16 hours.\n\n\nWhy Should You Learn PySpark and AWS?\nPySpark is the Python library that makes the magic happen.\nPySpark is worth learning because of the huge demand for Spark professionals and the high salaries they command. The usage of PySpark in Big Data processing is increasing at a rapid pace compared to other Big Data tools.\nAWS, launched in 2006, is the fastest-growing public cloud. The right time to cash in on cloud computing skills—AWS skills, to be precise—is now.\n\n\nCourse Content:\nThe all-inclusive course consists of the following topics:\n1. Introduction:\na. Why Big Data?\nb. Applications of PySpark\nc. Introduction to the Instructor\nd. Introduction to the Course\ne. Projects Overview\n2. Introduction to Hadoop, Spark EcoSystems, and Architectures:\na. Hadoop EcoSystem\nb. Spark EcoSystem\nc. Hadoop Architecture\nd. Spark Architecture\ne. PySpark Databricks setup\nf. PySpark local setup\n\n\n3. Spark RDDs:\na. Introduction to PySpark RDDs\nb. Understanding underlying Partitions\nc. RDD transformations\nd. RDD actions\ne. Creating Spark RDD\nf. Running Spark Code Locally\ng. RDD Map (Lambda)\nh. RDD Map (Simple Function)\ni. RDD FlatMap\nj. RDD Filter\nk. RDD Distinct\nl. RDD GroupByKey\nm. RDD ReduceByKey\nn. RDD (Count and CountByValue)\no. RDD (saveAsTextFile)\np. RDD (Partition)\nq. Finding Average\nr. Finding Min and Max\ns. Mini project on student data set analysis\nt. Total Marks by Male and Female Student\nu. Total Passed and Failed Students\nv. Total Enrollments per Course\nw. Total Marks per Course\nx. Average marks per Course\ny. Finding Minimum and Maximum marks\nz. Average Age of Male and Female Students\n4. Spark DFs:\na. Introduction to PySpark DFs\nb. Understanding underlying RDDs\nc. DFs transformations\nd. DFs actions\ne. Creating Spark DFs\nf. Spark Infer Schema\ng. Spark Provide Schema\nh. Create DF from RDD\ni. Select DF Columns\nj. Spark DF with Column\nk. Spark DF with Column Renamed and Alias\nl. Spark DF Filter rows\nm. Spark DF (Count, Distinct, Duplicate)\nn. Spark DF (sort, order By)\no. Spark DF (Group By)\np. Spark DF (UDFs)\nq. Spark DF (DF to RDD)\nr. Spark DF (Spark SQL)\ns. Spark DF (Write DF)\nt. Mini project on Employees data set analysis\nu. Project Overview\nv. Project (Count and Select)\nw. Project (Group By)\nx. Project (Group By, Aggregations, and Order By)\ny. Project (Filtering)\nz. Project (UDF and With Column)\naa. Project (Write)\n5. Collaborative filtering:\na. Understanding collaborative filtering\nb. Developing recommendation system using ALS model\nc. Utility Matrix\nd. Explicit and Implicit Ratings\ne. Expected Results\nf. Dataset\ng. Joining Dataframes\nh. Train and Test Data\ni. ALS model\nj. Hyperparameter tuning and cross-validation\nk. Best model and evaluate predictions\nl. Recommendations\n\n\n6. Spark Streaming:\na. Understanding the difference between batch and streaming analysis.\nb. Hands-on with spark streaming through word count example\nc. Spark Streaming with RDD\nd. Spark Streaming Context\ne. Spark Streaming Reading Data\nf. Spark Streaming Cluster Restart\ng. Spark Streaming RDD Transformations\nh. Spark Streaming DF\ni. Spark Streaming Display\nj. Spark Streaming DF Aggregations\n7. ETL Pipeline\na. Understanding the ETL\nb. ETL pipeline Flow\nc. Data set\nd. Extracting Data\ne. Transforming Data\nf. Loading data (Creating RDS)\ng. Load data (Creating RDS)\nh. RDS Networking\ni. Downloading Postgres\nj. Installing Postgres\nk. Connect to RDS through PgAdmin\nl. Loading Data\n8. Project – Change Data Capture / Replication On Going\na. Introduction to Project\nb. Project Architecture\nc. Creating RDS MySql Instance\nd. Creating S3 Bucket\ne. Creating DMS Source Endpoint\nf. Creating DMS Destination Endpoint\ng. Creating DMS Instance\nh. MySql WorkBench\ni. Connecting with RDS and Dumping Data\nj. Querying RDS\nk. DMS Full Load\nl. DMS Replication Ongoing\nm. Stoping Instances\nn. Glue Job (Full Load)\no. Glue Job (Change Capture)\np. Glue Job (CDC)\nq. Creating Lambda Function and Adding Trigger\nr. Checking Trigger\ns. Getting S3 file name in Lambda\nt. Creating Glue Job\nu. Adding Invoke for Glue Job\nv. Testing Invoke\nw. Writing Glue Shell Job\nx. Full Load Pipeline\ny. Change Data Capture Pipeline\n\n\nAfter the successful completion of this course, you will be able to:\n● Relate the concepts and practicals of Spark and AWS with real-world problems\n● Implement any project that requires PySpark knowledge from scratch\n● Know the theory and practical aspects of PySpark and AWS\n\n\nWho this course is for:\n● People who are beginners and know absolutely nothing about PySpark and AWS\n● People who want to develop intelligent solutions\n● People who want to learn PySpark and AWS\n● People who love to learn the theoretical concepts first before implementing them using Python\n● People who want to learn PySpark along with its implementation in realistic projects\n● Big Data Scientists\n● Big Data Engineers\n\n\nEnroll in this comprehensive PySpark and AWS course now to master the essential skills in Big Data analytics, data processing, and cloud computing.\nWhether you're a beginner or looking to expand your knowledge, this course offers a hands-on learning experience with practical projects. Don't miss this opportunity to advance your career and tackle real-world challenges in the world of data analytics and cloud computing. Join us today and start your journey towards becoming a Big Data expert with PySpark and AWS!\n\n\nList of keywords:\n\n\nBig Data analytics\nData analysis\nData cleaning\nMachine learning (ML)\nSpark RDDs\nDataframes\nSpark SQL queries\nSpark ecosystem\nHadoop\nDatabricks\nAWS cloud\nSpark scripts\nAWS services\nPySpark and AWS collaboration\nPySpark tutorial\nPySpark hands-on\nPySpark projects\nSpark architecture\nHadoop ecosystem\nPySpark Databricks setup\nSpark local setup\nSpark RDD transformations\nSpark RDD actions\nSpark DF transformations\nSpark DF actions\nSpark Infer Schema\nSpark Provide Schema\nSpark DF Filter rows\nSpark DF (Count, Distinct, Duplicate)\nSpark DF (sort, order By)\nSpark DF (Group By)\nSpark DF (UDFs)\nSpark DF (Spark SQL)\nCollaborative filtering\nRecommendation system\nALS model\nSpark Streaming\nETL pipeline\nChange Data Capture (CDC)\nReplication\nAWS Glue Job\nLambda Function\nRDS\nS3 Bucket\nMySql Instance\nData Migration Service (DMS)\nPgAdmin\nSpark Shell Job\nFull Load Pipeline\nChange Data Capture Pipeline",
      "target_audience": [
        "● People who are beginners and know absolutely nothing about PySpark and AWS.",
        "● People who want to develop intelligent solutions.",
        "● People who want to learn PySpark and AWS.",
        "● People who love to learn the theoretical concepts first before implementing them using Python.",
        "● People who want to learn PySpark along with its implementation in realistic projects.",
        "● Big Data Scientists.",
        "● Big Data Engineers."
      ]
    },
    {
      "title": "Spatial Analysis & Geospatial Data Science in Python",
      "url": "https://www.udemy.com/course/spatial-data-science-in-python/",
      "bio": "Learn how to process and visualize geospatial data and perform spatial analysis using Python.",
      "objectives": [
        "The course introduces you to the most essential Geopython Libraries",
        "Perform Spatial Data analysis with Python",
        "Learn the essentials of Geopy,Plotly Library, the workhorse of Geospatial data science in Python.",
        "Learn how to visualize Geospatial data in Python (static and interactive maps)",
        "Learn how to pre-process geospatial data.",
        "Perform Geocoding on Data"
      ],
      "course_content": {
        "Introduction": [
          "About Courses benefits",
          "Utilize this GOLDEN oppurtunity , QnA section !!",
          "How to follow this course-Must Watch",
          "Installation of Anaconda Navigator",
          "Quick Summary of Jupyter Notebook"
        ],
        "Introduction to Life-Cycle of Spatial Analysis Project": [
          "First Stage : Business Understanding in Real World",
          "Second Stage : What is ETL (Extract ,Transform,Load) Pipeline ?",
          "Third Stage : EDA ( Exploratory Data Analysis ) !",
          "Fourth Stage : Conclusions and Dashboarding !"
        ],
        "--------------- Project 1->> Intro to Zomato use-case --------------": [
          "Overview of Problem Statement",
          "Datasets & Resources"
        ],
        "Perform Data Pre-Processing on Zomato": [
          "Lets Read Data For Spatial Analysis",
          "Performing basic Data Pre-Processing",
          "Extract Latitudes & Longitudes of Data",
          "How to write Structured Queries to extract Latutudes & Longitudes"
        ],
        "Zomato Spatial Analysis": [
          "HeatMap of Restaurants Density",
          "Performing Marker Cluster Analysis !",
          "Plotting all the markers of places of Bangalore !"
        ],
        "Data Cleaning in Zomato": [
          "Dealing with missing values"
        ],
        "Advanced Geospatial Analysis": [
          "Analysing Most highest rated restaurants !"
        ],
        "--------------------Project 2-->> Global Warming Analysis": [
          "Datasets & Resources",
          "Analyse Average Temperature of country",
          "Analyse Existence of Global Warming",
          "Visualise Average Temperature in Each Season",
          "Perform Spatial analysis on Average Temperature of USA States",
          "Visualise Average temperature of major Indian Cities",
          "Perform Spatial Analysis on Average temperature of major Indian Cities"
        ],
        "--------------------Project 3 -->> Intro to Covid-19 Project--------------------": [
          "Datasets & Resources",
          "Reading Data of Covid-19"
        ],
        "Spatial Analysis on Covid-19": [
          "Idea behind Choropleth Maps",
          "Choropleth Map for Confirmed Covid-19 Cases",
          "Choropleth Map of Particular continent",
          "Idea behind Geographical Scatter plot",
          "Geographical Scatter plot for Confirmed Covid-19 Cases",
          "Plotting of Recovery using Choropleth & Geo Scatter plot",
          "Plotting of Deaths using Choropleth & Geo Scatter plot"
        ]
      },
      "requirements": [
        "No GIS knowledge is required. We will give breif theoretical explanation as well as its practical implementation"
      ],
      "description": "This is the first course that gives hands-on Spatial Data Science / Spatial Analysis Projects using Python..\n\n\nStudent Testimonials:\nExcellent course. The narration and presentation of the course are beautiful and simple. The contents contain very clear and interesting explanations. I love this course. Spatial Data Science course in python is entertaining, it contains very clear explanations, every minute you learn something new, it is really exciting. Thank you very much for this wonderful course - Leonardo Guevara\n\n\nGreat course about Spatial analysis using Python! Simple to understand and packed with practical knowledge and real-world examples. Thanks Shan Singh for creating this course! - Gabriel Borja\n\n\nI was longing to get a kick start on geospatial analysis. This has given me a very clear explanation which I will put into practise - Geoffrey Mogonchi\n\n\n\n\nGeospatial data science is a subset of data science that focuses on spatial data and its unique techniques. In this, we are going to perform spatial analysis and trying to find insights from spatial data. In this course, we lay the foundation for a career in Geospatial Data Science. You will get hands-on Geopy, Plotly , Folium etc.. the workhorse of Geospatial data science Python libraries.\n\n\nThe topics covered in this course widely touch on some of the most used spatial technique in Geospatial data science. We will be learning how to read spatial data , manipulate and process spatial data using Pandas and how to perform spatial operations.. A large portion of the course deals with spatial Visuals like Choropleth, Geographical Scatter plot, Geographical Heatmap, Markers, Geographical HeatMap.. Each video contains a summary of the topic and a walkthrough with code examples in layman terms that will help you learn more effectively.\n\n\n\n\nWho this course is for:\nStudents who want to become Data Scientist by show-case these Projects on his/her Resume\nStudents who like to take their first steps in the Geospatial data science career\nPython users who are interested in Spatial Data Science\nGIS users who are new to python and Jupyter notebooks for Geographic data analysis",
      "target_audience": [
        "One who is curious about DataScience"
      ]
    },
    {
      "title": "NLP and Text mining with python(for absolute beginners only)",
      "url": "https://www.udemy.com/course/natural-language-processing-made-easy-using-python/",
      "bio": "Learn Natural Language Processing using Python from experts with hands on examples and practice sessions.",
      "objectives": [
        "After the completion of this course, you will have good understanding of NLP.",
        "You will approach algorthms to solve real world NLP problems.",
        "You will be able to implement sentiment analysis",
        "You will be able to perform document classification"
      ],
      "course_content": {
        "Introduction to NLP and Text Mining": [
          "Introduction to Text Mining",
          "Need for Preparation Before Text Mining",
          "Getting Ready for Data Preparation",
          "Reading Text Data as Corpus",
          "Text Data Cleaning Stages",
          "Tokenizing",
          "Stop Words",
          "Stemming and Lemmatizing",
          "Final Cleaning using Regular Expressions",
          "LAB_Data Cleaning Case Study on News Data",
          "Document Term Matrix",
          "LAB_ Document Term Matrix",
          "NLP and Text Mining Basics Conclusion"
        ],
        "Sentiment Analysis": [
          "Introduction to Sentiment Analysis",
          "Sentiment Analysis Algorithms",
          "LAB Case Study_ Importing Movies Review Data",
          "LAB Case Study Creating DTM",
          "LAB Case Study Refining DTM",
          "Understanding Bayes Theorem P1",
          "Understanding Bayes Theorem P2",
          "Bayes Theorem to Naive Bayes Model",
          "LAB Building and Validating Sentiment Analysis Model",
          "Sentiment Analysis Conclusion"
        ],
        "Document Categorisation": [
          "Introduction to Document Categorisation",
          "Document categorisation using Naive Bays model",
          "LAB Importing data and Preprocessing",
          "LAB Preparing NLTK Friendly Feature Set",
          "LAB Model Building and Exploring Results",
          "LAB Multiple Document Categorisation",
          "SVM Model for Text Classification",
          "Understanding SVM Hyper Parameters to improve model",
          "LAB Tuning SVM Hyper Parameters to Improve Model",
          "Document Categorisation Conclusion"
        ]
      },
      "requirements": [
        "Be able to understand the Python syntax and familiar with basics of Data Science.",
        "Prerequisite course : Machine Learning Made Easy : Beginner to Expert using Python",
        "Knowledge of Text Processing will be advantageous.",
        "Knowledge of ML Algorithms"
      ],
      "description": "Want to know how NLP algorithms work and how people apply it to solve data science problems? You are looking at right course!\n\n\nThis course has been created, designed and assembled by professional Data Scientist who have worked in this field for nearly a decade. We can help you to understand the NLP while keeping you grounded to the implementation on real and data science problems.\n\n\nWe are sure that you will have fun while learning from our tried and tested structure of course to keep you interested in what coming next.\nHere is how the course is going to work:\nSession 1: Get introduced to NLP and text mining basics, NLTK package and learn how to prepare unstructured data for further processing.\nSession 2: Lets you understand sentimental analysis using a case study and a practice session.\nSession 3: Teaches you document categorization using various machine learning algorithms.\n\n\nFeatures:\nFully packed with LAB Sessions. One to learn from and one to do it by yourself.\nCourse includes Python code, Datasets, ipython notebook and other supporting material at the beginning of each section for you to download and use on your own.",
      "target_audience": [
        "Data Scientists who are curious about NLP.",
        "Data Analysts working on text data and want to get some insight using NLP.",
        "Students who want to learn NLP."
      ]
    },
    {
      "title": "Data Analysis with Polars",
      "url": "https://www.udemy.com/course/data-analysis-with-polars/",
      "bio": "\"A thorough introduction to Polars\" - Ritchie Vink, creator of Polars - over 3,000 learners to date!",
      "objectives": [
        "Taking advantage of parallel and optimised analysis with Polars",
        "Working with larger-than-memory data",
        "Using Polars expressions for analysis that is easy to read and write",
        "Loading data from a wide variety of data sources",
        "Combining data from different datasets using fast joins operations",
        "Grouping and parallel aggregations",
        "Deriving insight from time series",
        "Preparing data for machine learning pipelines",
        "Visualising data with Matplotlib, Seaborn, Altair & Plotly",
        "Using Polars with Scikit-learn"
      ],
      "course_content": {
        "Up and running with Polars": [
          "Course introduction",
          "Why use Polars instead of Pandas?",
          "How can you make best use of the course materials?",
          "Course materials",
          "Polars quickstart",
          "Lazy mode: Introducing lazy mode",
          "Lazy mode: evaluating queries",
          "Introduction to Data types",
          "Series and DataFrame",
          "Converting to and from Pandas & Numpy",
          "Visualisation",
          "Lazy mode"
        ],
        "Filtering rows": [
          "Filtering rows I: Filtering rows with square brackets",
          "Filtering rows 2: Using `filter` and the Expression API",
          "Filtering rows 3: using `filter` in lazy mode",
          "Filtering rows based on values from another DataFrame",
          "Filtering rows"
        ],
        "Selecting columns and transforming dataframes": [
          "Selecting columns 1: using square brackets",
          "Selecting columns 2: using select and expressions",
          "Selecting columns 3: choosing multiple columns",
          "Selecting columns 4: transforming and adding columns",
          "Selecting columns 5: Transforming and adding multiple columns",
          "Selecting columns 6: Adding a column based on a condition or mapping",
          "Sorting and fast-track algorithms",
          "Transforming a DataFrame",
          "Iterating through a DataFrame",
          "Selecting columns",
          "Adding new columns",
          "Adding a new column"
        ],
        "Data types and missing values": [
          "Missing values",
          "Replacing missing values",
          "Replacing missing values with expressions",
          "Missing values",
          "Numerical dtypes and precision",
          "Introducing categorical data",
          "Categoricals and the string cache",
          "Introduction to nested dtypes: List, Struct and Object",
          "List dtype 1: Creating and transforming List columns",
          "List dtype 2: using expressions on List columns",
          "Text transformation",
          "Nested dtypes"
        ],
        "Grouping and aggregation": [
          "Statistics",
          "Value counts",
          "Groupby 1: Key concepts",
          "Groupby 2: Iterating and group values",
          "Counting values",
          "Working with GroupBy and groups",
          "Grouping and aggregations",
          "Quantiles and histograms",
          "Introduction to group operations with over()",
          "Pivot & melt"
        ],
        "Combining dataframes": [
          "Concatenating DataFrames",
          "Concatenating DataFrames",
          "Joins",
          "Joins on string, categorical and enum columns",
          "Filtering a DataFrame by another DataFrame",
          "Inequality joins"
        ],
        "Input/Output": [
          "Read a single CSV file",
          "Excel files",
          "Read JSON and serialize a DataFrame",
          "CSV files 3: reading larger-than-memory CSV files in batches",
          "CSV files 4: streaming larger-than-memory datasets",
          "Parquet files 1: single Parquet files",
          "Reading from a database"
        ],
        "Time series analysis": [
          "Introduction to time series dtypes",
          "Time zones",
          "Time zones quiz",
          "Parsing datetime strings",
          "Adjusting datetimes",
          "Parsing and adjusting datetimes quiz",
          "Extracting datetime components",
          "Filtering time series",
          "Temporal groupby - introduction to groupby_dynamic",
          "Controlling the `groupby_dynamic` window"
        ],
        "Nested dtypes": [
          "Visualisations with Plotly",
          "Visualisations with Matplotlib",
          "Visualisations with Seaborn"
        ]
      },
      "requirements": [
        "Computer with Windows/Linux/MacOS and a python installation"
      ],
      "description": "In this course I show you how to take advantage of Polars - the fast-growing open source dataframe library that is becoming the go-to dataframe library for data scientists in python. I am a Polars contributor with a focus on making Polars accessible to new users and I keep this course up-to-date with new releases of Polars - updated to version 1.30.0\n\n\n\"A thorough introduction to Polars\" - Ritchie Vink, creator of Polars\n\n\n\"Thank you for your great work with this course - I've optimized some code thanks to it already!\" Maiia Bocharova\n\n\nThe course is for data scientists who have some familiarity with a dataframe library like Pandas but who want to move to Polars because it is easier to write and faster to run. The core materials are Jupyter notebooks that examine each topic in depth. Each notebook comes with a set of exercises to help you develop your understanding of the core concepts. For many key topics this course is the only source of documentation for learners and comes from my time examining the Polars source code.\n\n\nAn important note about videos: this is a primarily a notebook course and not a video course. Not all of the lectures have videos and some of the videos may have components that are not up-to-date. Why? Because the Polars API has changed too often to allow me to keep videos up-to-date. Instead I focus on keeping the notebooks up-to-date with an extensive automated testing system that alerts me to changes in the API. I release an updated version of the course about twice a month in response to changes in Polars.\n\n\nThe course introduces the syntax of Polars and shows you the many ways that Polars allows you to produce queries that are easy to read and write. However, the course also delves deeper to help you understand and exploit the algorithms that drive the outstanding performance of Polars.\n\n\nBy the end of the course you will have optimised ways to:\nload and transform your data from CSV, Excel, Parquet, cloud storage or a database\nrun your analysis in parallel\nunderstand optimal patterns for building queries\nwork with larger-than-memory datasets\ncarry out aggregations on your data\ncombine your datasets with joins and concatenations\nwork with nested dtypes including lists and structs\noptimise the speed and memory usage of your queries\nwork with string and categorical data\nvisualise your outputs with Matplotlib, Seaborn, Plotly, hvPlot & Altair\nprepare your data for machine learning pipelines with sklearn",
      "target_audience": [
        "Data scientists with no familiarity with Polars and want to get up and running",
        "Data scientists with some familiarity with Polars but want a deeper understanding",
        "Pandas or other dataframe library users"
      ]
    },
    {
      "title": "Data Science for Professionals",
      "url": "https://www.udemy.com/course/data-science-for-professionals/",
      "bio": "It's time to leave spreadsheets behind...",
      "objectives": [
        "Students will be able to analyze, manipulate, explore, illustrate, and report data in ways that will set them far apart from those who use spreadsheets and other traditional Office products."
      ],
      "course_content": {
        "Introduction": [
          "Course Goals and the Data Science Process",
          "Why Use R?",
          "A Quick Overview of the R Language"
        ],
        "Setup": [
          "Downloading and Installing R",
          "RStudio and Project Setup"
        ],
        "R Essentials - Data Objects": [
          "Section Overview",
          "Vectors - Part 1",
          "Getting Help with R",
          "Vectors - Part 2",
          "Vectors - Part 3",
          "Vectors - Part 4",
          "Matrices",
          "Data Frames",
          "Lists",
          "Data Object Recap"
        ],
        "R Essentials - Functions and Loops": [
          "Loops and IF Statements",
          "Custom Functions"
        ],
        "R Essentials - Putting it all Together!": [
          "The Challenge",
          "The Solution"
        ],
        "Data Gymnastics": [
          "Tidy Data",
          "Tidying our Data with tidyr",
          "Data Manipulation with dplyr - Part 1",
          "Data Manipulation with dplyr - Part 2",
          "Data Manipulation with dplyr - Part 3"
        ],
        "Data Visualization": [
          "Making Graphics Easy with ggplot2 - Part 1",
          "Making Graphics Easy with ggplot2 - Part 2"
        ],
        "Modelling and Machine Learning": [
          "What is Machine Learning?",
          "Training and Testing",
          "Inference Trees and Random Forests",
          "Conclusion to the HR Attrition Problem"
        ],
        "Advanced Reporting with R": [
          "RMarkdown and Git Version Control",
          "Shiny Web Apps"
        ],
        "Keep in Touch!": [
          "Thanks for purchasing!"
        ]
      },
      "requirements": [
        "No prior coding knowledge required"
      ],
      "description": "What is it?\n\nData Science for Professionals is simply the best way to gain a in-depth and practical skill set in data science. Through a combination of theory and hands-on practice, course participants will gain a solid grasp of how to manage, manipulate, and visualize data in R - the world's most popular data science language.\n\n\nWho should take this course?\nThis course is for professionals who are tired of using spreadsheets for analysis and have a serious interest in learning how to use code to improve the quality and efficiency of their work. At the end of this course, participants will have a developed a solid foundation of the fundamentals of the R language. Participants will have also gained a perspective on the modern data science landscape and how they can use R not only to better analyze data, but also to better manage projects, create interactive presentations, and collaborate with other teams. Whether it's spreadsheets, text documents, or slides, anyone who analyzes, reports, or presents data will benefit from a knowledge of data science programming.\n\n\nWho should NOT take this course?\nWhile this course covers examples of machine learning in later lectures, this is not a machine learning or a statistics-focused course. The course does go through examples of how to use code to deploy and assess different types of models, including machine learning algorithms, but it does so from a coding perspective and not a statistics perspective. The reason is that the math behind most machine learning algorithms merits a course entirely on its own. There are many courses out there that make dubious claims of easy mastery of machine learning and deep learning algorithms - this is not one of those courses.\n\n\nA Different kind of data science course\nThis course is different from most other courses in several ways:\nWe use very large, real-world examples to guide our learning process. This allows us to tie-together the various aspects of data science in a more intuitive, easy-to-retain manner.\nWe encounter and deal-with various challenges and bugs that arise from imperfect data. Most courses use ideal datasets in their examples, but these are not common in the real-world, and solving data-related issues is usually the most difficult and time-consuming part of data science.\nWe are focused on your long-term success. Our downloadable course code is filled with notes and guidance aimed at making the transition from learning-to-applying as smooth as possible.",
      "target_audience": [
        "Anyone who collects, analyses, reports, or presents data. So pretty much everyone...",
        "Anyone who is tired of spreadsheets. Again, pretty much everyone...",
        "Anyone who wants to add a lot of value to their skillset and is willing to invest a few hours per week."
      ]
    },
    {
      "title": "Machine Learning, Data Science & AI Engineering with Python",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/",
      "bio": "Complete hands-on deep learning, AI engineering and Generative AI tutorial with data science, Tensorflow, GPT, OpenAI",
      "objectives": [
        "Build generative AI systems with OpenAI, RAG, and LLM Agents",
        "Build artificial neural networks with Tensorflow and Keras",
        "Implement machine learning at massive scale with Apache Spark's MLLib",
        "Classify images, data, and sentiments using deep learning",
        "Make predictions using linear regression, polynomial regression, and multivariate regression",
        "Data Visualization with MatPlotLib and Seaborn",
        "Understand reinforcement learning - and how to build a Pac-Man bot",
        "Classify data using K-Means clustering, Support Vector Machines (SVM), KNN, Decision Trees, Naive Bayes, and PCA",
        "Use train/test and K-Fold cross validation to choose and tune your models",
        "Build a movie recommender system using item-based and user-based collaborative filtering",
        "Clean your input data to remove outliers",
        "Design and evaluate A/B tests using T-Tests and P-Values"
      ],
      "course_content": {
        "Getting Started": [
          "Introduction",
          "Udemy 101: Getting the Most From This Course",
          "Important note",
          "Installation: Getting Started",
          "[Activity] WINDOWS: Installing and Using Anaconda & Course Materials",
          "[Activity] MAC: Installing and Using Anaconda & Course Materials",
          "[Activity] LINUX: Installing and Using Anaconda & Course Materials",
          "Python Basics, Part 1 [Optional]",
          "[Activity] Python Basics, Part 2 [Optional]",
          "[Activity] Python Basics, Part 3 [Optional]",
          "[Activity] Python Basics, Part 4 [Optional]",
          "Introducing the Pandas Library [Optional]"
        ],
        "Statistics and Probability Refresher, and Python Practice": [
          "Types of Data (Numerical, Categorical, Ordinal)",
          "Mean, Median, Mode",
          "[Activity] Using mean, median, and mode in Python",
          "[Activity] Variation and Standard Deviation",
          "Probability Density Function; Probability Mass Function",
          "Common Data Distributions (Normal, Binomial, Poisson, etc)",
          "[Activity] Percentiles and Moments",
          "[Activity] A Crash Course in matplotlib",
          "[Activity] Advanced Visualization with Seaborn",
          "[Activity] Covariance and Correlation",
          "[Exercise] Conditional Probability",
          "Exercise Solution: Conditional Probability of Purchase by Age",
          "Bayes' Theorem"
        ],
        "Predictive Models": [
          "[Activity] Linear Regression",
          "[Activity] Polynomial Regression",
          "[Activity] Multiple Regression, and Predicting Car Prices",
          "Multi-Level Models"
        ],
        "Machine Learning with Python": [
          "Supervised vs. Unsupervised Learning, and Train/Test",
          "[Activity] Using Train/Test to Prevent Overfitting a Polynomial Regression",
          "Bayesian Methods: Concepts",
          "[Activity] Implementing a Spam Classifier with Naive Bayes",
          "K-Means Clustering",
          "[Activity] Clustering people based on income and age",
          "Measuring Entropy",
          "[Activity] WINDOWS: Installing Graphviz",
          "[Activity] MAC: Installing Graphviz",
          "[Activity] LINUX: Installing Graphviz",
          "Decision Trees: Concepts",
          "[Activity] Decision Trees: Predicting Hiring Decisions",
          "Ensemble Learning",
          "[Activity] XGBoost",
          "Support Vector Machines (SVM) Overview",
          "[Activity] Using SVM to cluster people using scikit-learn"
        ],
        "Recommender Systems": [
          "User-Based Collaborative Filtering",
          "Item-Based Collaborative Filtering",
          "[Activity] Finding Movie Similarities using Cosine Similarity",
          "[Activity] Improving the Results on Movie Similarities",
          "[Activity] Making Movie Recommendations with Item-Based Collaborative Filtering",
          "[Exercise] Improve the recommender's results"
        ],
        "More Data Mining and Machine Learning Techniques": [
          "K-Nearest-Neighbors: Concepts",
          "[Activity] Using KNN to predict a rating for a movie",
          "Dimensionality Reduction; Principal Component Analysis (PCA)",
          "[Activity] PCA Example with the Iris data set",
          "Data Warehousing Overview: ETL and ELT",
          "Reinforcement Learning",
          "[Activity] Reinforcement Learning & Q-Learning with Gym",
          "Understanding a Confusion Matrix",
          "Measuring Classifiers (Precision, Recall, F1, ROC, AUC)"
        ],
        "Dealing with Real-World Data": [
          "Bias/Variance Tradeoff",
          "[Activity] K-Fold Cross-Validation to avoid overfitting",
          "Data Cleaning and Normalization",
          "[Activity] Cleaning web log data",
          "Normalizing numerical data",
          "[Activity] Detecting outliers",
          "Feature Engineering and the Curse of Dimensionality",
          "Imputation Techniques for Missing Data",
          "Handling Unbalanced Data: Oversampling, Undersampling, and SMOTE",
          "Binning, Transforming, Encoding, Scaling, and Shuffling"
        ],
        "Apache Spark: Machine Learning on Big Data": [
          "Warning about Java 21+ and Spark 3!",
          "Spark installation notes for MacOS and Linux users",
          "[Activity] Installing Spark",
          "Spark Introduction",
          "Spark and the Resilient Distributed Dataset (RDD)",
          "Introducing MLLib",
          "Introduction to Decision Trees in Spark",
          "[Activity] K-Means Clustering in Spark",
          "TF / IDF",
          "[Activity] Searching Wikipedia with Spark",
          "[Activity] Using the Spark DataFrame API for MLLib"
        ],
        "Experimental Design / ML in the Real World": [
          "Deploying Models to Real-Time Systems",
          "A/B Testing Concepts",
          "T-Tests and P-Values",
          "[Activity] Hands-on with T-Tests",
          "Determining How Long to Run an Experiment",
          "A/B Test Gotchas"
        ],
        "Deep Learning and Neural Networks": [
          "Deep Learning Pre-Requisites",
          "The History of Artificial Neural Networks",
          "[Activity] Deep Learning in the Tensorflow Playground",
          "Deep Learning Details",
          "Introducing Tensorflow",
          "[Activity] Using Tensorflow, Part 1",
          "[Activity] Using Tensorflow, Part 2",
          "[Activity] Introducing Keras",
          "[Activity] Using Keras to Predict Political Affiliations",
          "Convolutional Neural Networks (CNN's)",
          "[Activity] Using CNN's for handwriting recognition",
          "Recurrent Neural Networks (RNN's)",
          "[Activity] Using a RNN for sentiment analysis",
          "Tuning Neural Networks: Learning Rate and Batch Size Hyperparameters",
          "Deep Learning Regularization with Dropout and Early Stopping",
          "The Ethics of Deep Learning"
        ]
      },
      "requirements": [
        "You'll need a desktop computer (Windows, Mac, or Linux) capable of running Anaconda 3 or newer. The course will walk you through installing the necessary free software.",
        "Some prior coding or scripting experience is required.",
        "At least high school level math skills will be required."
      ],
      "description": "Master Machine Learning & AI Engineering — From Data Analytics to Agentic AI Solutions\nLaunch your career in AI with a comprehensive, hands-on course that takes you from beginner to advanced. Learn Python, data science, classical machine learning, and the latest in AI engineering—including generative AI, transformers, and LLM agents / agentic AI.\nWhy This Course?\nLearn by Doing\nWith over 145 lectures and 21+ hours of video content, this course is built around practical Python projects and real-world use cases—not just theory.\nBuilt for the Real World\nLearn how companies like Google, Amazon, and OpenAI use AI to drive innovation. Our curriculum is based on skills in demand from leading tech employers.\nNo Experience? No Problem\nStart from scratch with beginner-friendly lessons in Python and statistics. By the end, you’ll be building intelligent systems with cutting-edge AI tools.\nA Structured Path from Beginner to AI Engineer\n1. Programming Foundations\nStart with a crash course in Python, designed for beginners. You’ll learn the language fundamentals needed for data science and AI work.\n2. Data Science and Statistics\nBuild a solid foundation in data analysis, visualization, descriptive and inferential statistics, and feature engineering—essential skills for working with real-world datasets.\n3. Classical Machine Learning\nExplore supervised and unsupervised learning, including linear regression, decision trees, SVMs, clustering, ensemble models, and reinforcement learning.\n4. Deep Learning with TensorFlow and Keras\nUnderstand neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs), using real code examples and exercises.\n5. Advanced AI Engineering and Generative AI\nGo beyond traditional ML to learn the latest AI tools and techniques:\nTransformers and self-attention mechanisms\nGPT, ChatGPT, and the OpenAI API\nFine-tuning foundation models\nAdvanced Retrieval-Augmented Generation (RAG)\nLangChain and LLM agents\nDesigning and building multi-agent systems with the OpenAI Agents SDK\nReal-world GenAI projects and deployment strategies\n6. Big Data and Apache Spark\nLearn how to scale machine learning to large datasets using Spark, and apply ML techniques on distributed computing clusters.\nDesigned for Career Growth\nWhether you're a programmer looking to pivot into AI or a tech professional seeking to expand your skills, this course delivers a complete, industry-relevant education. Concepts are explained clearly, in plain English, with a focus on applying what you learn.\nWhat Students Are Saying\n\"I started doing your course... and it was pivotal in helping me transition into a role where I now solve corporate problems using AI. Your course demystified how to succeed in corporate AI research, making you the most impressive instructor in ML I've encountered.\"\n— Kanad Basu, PhD\nEnroll Today and Build Your Future in AI\nJoin thousands of learners who have used this course to land jobs, lead projects, and build real AI applications. Stay ahead in one of the fastest-growing fields in tech.\nStart your journey today—from Python beginner to AI engineer.",
      "target_audience": [
        "Software developers or programmers who want to transition into the lucrative data science and machine learning career path will learn a lot from this course.",
        "Technologists curious about how deep learning really works",
        "Data analysts in the finance or other non-tech industries who want to transition into the tech industry can use this course to learn how to analyze data using code instead of tools. But, you'll need some prior experience in coding or scripting to be successful.",
        "If you have no prior coding or scripting experience, you should NOT take this course - yet. Go take an introductory Python course first."
      ]
    },
    {
      "title": "MLOps Bootcamp: Mastering AI Operations for Success - AIOps",
      "url": "https://www.udemy.com/course/mlops-bootcamp-mastering-ai-operations-for-success-aiops/",
      "bio": "Unlock success in AI Operations with our MLOps Bootcamp – mastering tools,techniques, AIOps for cutting-edge expertise",
      "objectives": [
        "Develop a solid foundation in Python, tailored for MLOps applications.",
        "Streamline Machine Learning processes using Python's powerful capabilities.",
        "Leverage Python for effective data manipulation and analysis in Data Science.",
        "Understand how Python enhances the entire data science lifecycle.",
        "Master version control using Git for collaborative development.",
        "Learn to manage and track changes efficiently within MLOps projects.",
        "Dive into the art of packaging Machine Learning models for easy deployment.",
        "Ensure models are reproducible and deployable in diverse environments.",
        "Effectively manage and track Machine Learning experiments using MLflow.",
        "Utilize MLflow for enhanced experiment tracking and management.",
        "Acquire essential skills in YAML for MLOps configuration and deployment.",
        "Gain practical experience in writing and interpreting YAML files.",
        "Explore Docker and its role in containerizing Machine Learning applications.",
        "Understand the advantages of containerization for efficient MLOps.",
        "Develop Machine Learning applications with FastAPI for efficient and scalable deployments.",
        "Explore Streamlit and Flask for creating interactive web applications.",
        "Implement Continuous Integration and Continuous Deployment pipelines for MLOps.",
        "Automate development, testing, and deployment of ML models.",
        "Gain a solid understanding of the Linux operating system.",
        "Explore how Linux is essential for both DevOps and Data Scientists in MLOps.",
        "Dive into Jenkins, an open-source automation server.",
        "Learn to set up and configure Jenkins for automating MLOps workflows.",
        "Develop insights into effective monitoring and debugging strategies for MLOps.",
        "Utilize tools and techniques to identify and address issues in ML systems.",
        "Set up continuous monitoring for MLOps using Prometheus and Grafana",
        "Enhance observability in Machine Learning applications.",
        "Extend Docker skills by mastering Docker Compose.",
        "Learn to deploy multi-container applications seamlessly.",
        "Explore tools and strategies for ongoing performance monitoring in MLOps.",
        "Proactively address issues in production ML systems.",
        "Utilize WhyLogs for efficient monitoring and logging of ML data.",
        "Enhance the observability and traceability of ML systems.",
        "Understand crucial steps for maintaining and updating ML models in a production environment.",
        "Implement best practices for ensuring the long-term success of deployed ML systems."
      ],
      "course_content": {},
      "requirements": [
        "Familiarity with programming concepts is preferred, but we cover in our course as well",
        "Some knowledge of data manipulation and analysis will be beneficial.",
        "Basic understanding of version control concepts, preferably with Git - will be beneficial",
        "Enthusiasm for the intersection of Machine Learning and DevOps practices.",
        "Participants should have access to a computer with a stable internet connection for viewing video content and engaging in practical exercises."
      ],
      "description": "Welcome to our extensive MLOps Bootcamp (AI Ops Bootcamp), a transformative learning journey designed to equip you with the skills and knowledge essential for success in the dynamic field of Machine Learning Operations (MLOps). This comprehensive program covers a diverse range of topics, from Python and Data Science fundamentals to advanced Machine Learning workflows, Git essentials, Docker for Machine Learning, CI/CD pipelines, and beyond.\nCurriculum Overview:\n1. Python for MLOps:\nDive into the fundamentals of Python tailored specifically for MLOps.\nExplore Python's role in streamlining and enhancing Machine Learning processes.\nDevelop proficiency in leveraging Python for effective MLOps practices.\n2. Python for Data Science:\nUncover the power of Python in the context of Data Science.\nLearn essential data manipulation and analysis techniques using Python.\nUnderstand how Python enhances the entire data science lifecycle.\n3. Git and GitHub Fundamentals:\nMaster the essentials of version control with Git.\nUnderstand how GitHub facilitates collaborative development in MLOps.\nLearn to manage and track changes effectively within MLOps projects.\n4. Packaging the ML Models:\nDelve into the art of packaging Machine Learning models.\nExplore different packaging techniques and their implications.\nEnsure your ML models are easily deployable and reproducible.\n5. MLflow - Manage ML Experiments:\nLearn to effectively manage and track Machine Learning experiments.\nUnderstand the features and benefits of MLflow for experiment tracking and management.\nImplement MLflow in your MLOps projects for enhanced experimentation.\n6. Crash Course on YAML:\nAcquire a solid foundation in YAML, a key configuration language.\nLearn how YAML is used in MLOps for configuration and deployment.\nGain practical skills in writing and interpreting YAML files.\n7. Docker for Machine Learning:\nExplore Docker and its role in containerizing Machine Learning applications.\nUnderstand the advantages of containerization for MLOps.\nLearn to build and deploy Docker containers for Machine Learning projects.\n8. Build MLApps using FastAPI:\nDive into FastAPI, a modern, fast web framework for building APIs.\nLearn to develop ML applications using FastAPI for efficient and scalable deployments.\nImplement best practices for building robust MLApps.\n9. Build MLApps using Streamlit:\nExplore Streamlit, a powerful framework for creating interactive web applications.\nDevelop hands-on experience in building MLApps with Streamlit.\nUnderstand how Streamlit enhances the user interface for Machine Learning applications.\n10. Build MLApps using Flask:\nGain proficiency in Flask, a popular web framework for Python.\nLearn to build and deploy Machine Learning applications using Flask.\nUnderstand the integration of Flask with MLOps workflows.\n11. CI/CD for Machine Learning:\nExplore Continuous Integration and Continuous Deployment (CI/CD) pipelines in the context of MLOps.\nImplement automation to streamline the development, testing, and deployment of ML models.\nLearn to build robust CI/CD workflows for Machine Learning projects.\n12. Linux Operating System for DevOps and Data Scientists:\nUnderstand the fundamentals of the Linux operating system.\nExplore how Linux is essential for both DevOps and Data Scientists in MLOps.\nGain practical skills in working with Linux for MLOps tasks.\n13. Working with Github Actions:\nDive into Github Actions\nLearn to set up and configure Github actions for automating MLOps workflows.\nUnderstand how Github Actions enhances the efficiency of continuous integration and deployment in MLOps.\n14. Monitoring and Debugging of ML System:\nGain insights into effective monitoring and debugging strategies for MLOps.\nLearn tools and techniques to identify and address issues in Machine Learning systems.\nImplement best practices for maintaining the health and performance of ML systems.\n15. Continuous Monitoring with Prometheus:\nExplore Prometheus, an open-source monitoring and alerting toolkit.\nLearn to set up continuous monitoring for MLOps using Prometheus.\nUnderstand how Prometheus enhances observability in Machine Learning applications.\n16. Deploy Applications with Docker Compose:\nExtend your Docker skills by mastering Docker Compose.\nLearn to deploy multi-container applications seamlessly using Docker Compose.\nUnderstand how Docker Compose enhances the deployment of complex MLOps architectures.\n17. Continuous Monitoring of Machine Learning Application:\nDive into continuous monitoring practices specifically tailored for Machine Learning applications.\nExplore tools and strategies to ensure ongoing performance monitoring in MLOps.\nImplement solutions for proactively addressing issues in production ML systems.\n18. Monitor the ML System with WhyLogs:\nExplore WhyLogs, a data logging library for Machine Learning.\nLearn how WhyLogs facilitates efficient monitoring and logging of ML data.\nImplement WhyLogs to enhance the observability and traceability of your ML system.\n19. Post Productionizing ML Models:\nUnderstand the crucial steps involved in post-productionizing Machine Learning models.\nExplore strategies for maintaining and updating ML models in a production environment.\nGain insights into best practices for ensuring the long-term success of deployed ML systems.\nConclusion:\nEmbark on this comprehensive MLOps Bootcamp to transform your skills and elevate your proficiency in the dynamic and ever-evolving field of Machine Learning Operations. Whether you are a seasoned professional or just starting your journey in MLOps, this program provides the knowledge, tools, and practical experience needed to succeed in implementing robust and efficient Machine Learning workflows. Join us and become a master of MLOps, ready to tackle the challenges of the modern AI landscape with confidence and expertise.",
      "target_audience": [
        "Data scientists seeking to extend their skills into the operational aspects of deploying and maintaining machine learning models.",
        "Software developers interested in mastering the tools and practices for integrating machine learning into real-world applications.",
        "DevOps professionals aiming to specialize in MLOps and enhance their proficiency in deploying and managing machine learning systems.",
        "Data engineers looking to broaden their skill set by incorporating MLOps practices into data pipelines.",
        "IT professionals wanting to understand the integration of machine learning models within operational workflows.",
        "Individuals passionate about the latest advancements in technology and eager to explore the practical aspects of MLOps.",
        "Entrepreneurs and business professionals seeking to understand how MLOps can drive innovation and competitive advantage in their organizations.",
        "Students and researchers in the fields of computer science, data science, and related disciplines looking to expand their knowledge in MLOps.",
        "Individuals transitioning into roles that involve machine learning operations and deployment.",
        "Enthusiasts who are keen to explore the convergence of machine learning and operations, regardless of their current role or background."
      ]
    },
    {
      "title": "Machine Learning Practical Workout | 8 Real-World Projects",
      "url": "https://www.udemy.com/course/deep-learning-machine-learning-practical/",
      "bio": "Build 8 Practical Projects and Go from Zero to Hero in Deep/Machine Learning, Artificial Neural Networks",
      "objectives": [
        "Deep Learning Practical Applications",
        "Machine Learning Practical Applications",
        "How to use ARTIFICIAL NEURAL NETWORKS to predict car sales",
        "How to use DEEP NEURAL NETWORKS for image classification",
        "How to use LE-NET DEEP NETWORK to classify Traffic Signs",
        "How to apply TRANSFER LEARNING for CNN image classification",
        "How to use PROPHET TIME SERIES to predict crime",
        "How to use PROPHET TIME SERIES to predict market conditions",
        "How to develop NATURAL LANGUAGE PROCESSING MODEL to analyze Reviews",
        "How to apply NATURAL LANGUAGE PROCESSING to develop spam filder",
        "How to use USER-BASED COLLABORATIVE FILTERING to develop recommender system"
      ],
      "course_content": {
        "INTRODUCTION TO THE COURSE [QUICK WIN IN FIRST 10-12 MINS]": [
          "Welcome Message",
          "Updates on Udemy Reviews",
          "Course overview",
          "EXTRA: Learning Path",
          "ML vs. DL vs. AI",
          "ML Deep Dive",
          "Download Course Materials",
          "EXTRA: ML vs DL vs AI",
          "EXTRA: 5 Benefits of Jupyter Notebook"
        ],
        "ANACONDA AND JUPYTER INSTALLATION": [
          "Download and Set up Anaconda",
          "What is Jupyter Notebook",
          "Install Tensorflow",
          "How to run a Jupyter Notebook"
        ],
        "PROJECT #1: ARTIFICIAL NEURAL NETWORKS - CAR SALES PREDICTION": [
          "Introduction",
          "Theory Part 1",
          "Theory Part 2",
          "Theory Part 3",
          "Theory Part 4",
          "Theory Part 5",
          "Project Overview",
          "Import Data",
          "Data Visualization Cleaning",
          "Model Training 1",
          "Model Training 2",
          "Model Evaluation"
        ],
        "PROJECT #2: DEEP NEURAL NETWORKS - CIFAR-10 CLASSIFICATION": [
          "Introduction",
          "Theory Part 1",
          "Theory Part 2",
          "Theory Part 3",
          "Theory Part 4",
          "Problem Statement",
          "Data Vizualization",
          "Data Preparation",
          "Model Training Part 1",
          "Model Training Part 2",
          "Model Evaluation",
          "Save the Model",
          "Image Augmentation Part 1",
          "Image augmentation Part 2"
        ],
        "PROJECT #3: PROPHET TIME SERIES - CHICAGO CRIME RATE": [
          "Introduction",
          "Project Overview",
          "Import Dataset",
          "Data Vizualization",
          "Prepare the Data",
          "Make Predictions"
        ],
        "PROJECT #4: PROPHET TIME SERIES - AVOCADO MARKET": [
          "Introduction",
          "Load Avocado Data",
          "Explore Dataset",
          "Make Predictions Part 1",
          "Make Predictions Part 2 (Region Specific)",
          "Make Prediction Part 2.1"
        ],
        "PROJECT #5: LE-NET DEEP NETWORK - TRAFFIC SIGN CLASSIFICATION": [
          "Introduction",
          "Project Overview",
          "Load Data",
          "Data Exploration",
          "Data Normalization",
          "Model Training",
          "Model Evaluation"
        ],
        "PROJECT #6: NATURAL LANGUAGE PROCESSING - E-MAIL SPAM FILTER": [
          "Introduction",
          "Naive Bayes Theory Part 1",
          "Naive Bayes Theory Part 2",
          "Spam Project Overview",
          "Visualize Dataset",
          "Count Vectorizer",
          "Model Training Part 1",
          "Model Training Part 2",
          "Testing"
        ],
        "PROJECT #7: NATURAL LANGUAGE PROCESSING - YELP REVIEWS": [
          "Introduction",
          "Theory",
          "Project Overview",
          "Load Dataset",
          "Visualize Dataset Part 1",
          "Visualize Dataset Part 2",
          "Exercise #1",
          "Exercise #2",
          "Exercise #3",
          "Apply NLP to Data",
          "Apply Count Vectorizer to Data",
          "Model Training Part 1",
          "Model Training Part 2",
          "Model Evaluation Part 1",
          "Model Evaluation Part 2"
        ],
        "PROJECT #8: USER-BASED COLLABORATIVE FILTERING - MOVIE RECOMMENDER SYSTEM": [
          "Introduction",
          "Theory",
          "Project Overview",
          "Import Movie Dataset",
          "Visualize Dataset",
          "Collaborative Filter One Movie",
          "Full Movie Recomendation"
        ]
      },
      "requirements": [
        "Deep Learning and Machine Learning basics",
        "PC with Internet connetion"
      ],
      "description": "\"Deep Learning and Machine Learning are one of the hottest tech fields to be in right now! The field is exploding with opportunities and career prospects. Machine/Deep Learning techniques are widely used in several sectors nowadays such as banking, healthcare, transportation and technology.\n\n\nMachine learning is the study of algorithms that teach computers to learn from experience. Through experience (i.e.: more training data), computers can continuously improve their performance. Deep Learning is a subset of Machine learning that utilizes multi-layer Artificial Neural Networks. Deep Learning is inspired by the human brain and mimics the operation of biological neurons. A hierarchical, deep artificial neural network is formed by connecting multiple artificial neurons in a layered fashion. The more hidden layers added to the network, the more “deep” the network will be, the more complex nonlinear relationships that can be modeled. Deep learning is widely used in self-driving cars, face and speech recognition, and healthcare applications.\n\n\nThe purpose of this course is to provide students with knowledge of key aspects of deep and machine learning techniques in a practical, easy and fun way. The course provides students with practical hands-on experience in training deep and machine learning models using real-world dataset. This course covers several technique in a practical manner, the projects include but not limited to:\n\n\n(1) Train Deep Learning techniques to perform image classification tasks.\n(2) Develop prediction models to forecast future events such as future commodity prices using state of the art Facebook Prophet Time series.\n(3) Develop Natural Language Processing Models to analyze customer reviews and identify spam/ham messages.\n(4) Develop recommender systems such as Amazon and Netflix movie recommender systems.\n\n\nThe course is targeted towards students wanting to gain a fundamental understanding of Deep and machine learning models. Basic knowledge of programming is recommended. However, these topics will be extensively covered during early course lectures; therefore, the course has no prerequisites, and is open to any student with basic programming knowledge. Students who enroll in this course will master deep and machine learning models and can directly apply these skills to solve real world challenging problems.\"",
      "target_audience": [
        "Data Scientists who want to apply their knowledge on Real World Case Studies",
        "Deep Learning practitioners who want to get more Practical Assigmetns",
        "Machine Learning Enthusiasts who look to add more projects to their Portfolio"
      ]
    },
    {
      "title": "Mathematical Foundations of Machine Learning",
      "url": "https://www.udemy.com/course/machine-learning-data-science-foundations-masterclass/",
      "bio": "Essential Linear Algebra and Calculus Hands-On in NumPy, TensorFlow, and PyTorch",
      "objectives": [
        "Understand the fundamentals of linear algebra and calculus, critical mathematical subjects underlying all of machine learning and data science",
        "Manipulate tensors using all three of the most important Python tensor libraries: NumPy, TensorFlow, and PyTorch",
        "How to apply all of the essential vector and matrix operations for machine learning and data science",
        "Reduce the dimensionality of complex data to the most informative elements with eigenvectors, SVD, and PCA",
        "Solve for unknowns with both simple techniques (e.g., elimination) and advanced techniques (e.g., pseudoinversion)",
        "Appreciate how calculus works, from first principles, via interactive code demos in Python",
        "Intimately understand advanced differentiation rules like the chain rule",
        "Compute the partial derivatives of machine-learning cost functions by hand as well as with TensorFlow and PyTorch",
        "Grasp exactly what gradients are and appreciate why they are essential for enabling ML via gradient descent",
        "Use integral calculus to determine the area under any given curve",
        "Be able to more intimately grasp the details of cutting-edge machine learning papers",
        "Develop an understanding of what’s going on beneath the hood of machine learning algorithms, including those used for deep learning"
      ],
      "course_content": {
        "Data Structures for Linear Algebra": [
          "Introduction",
          "What Linear Algebra Is",
          "Plotting a System of Linear Equations",
          "Linear Algebra Exercise",
          "Tensors",
          "Scalars",
          "Vectors and Vector Transposition",
          "Norms and Unit Vectors",
          "Basis, Orthogonal, and Orthonormal Vectors",
          "Matrix Tensors",
          "Generic Tensor Notation",
          "Exercises on Algebra Data Structures",
          "Learning Paths"
        ],
        "Tensor Operations": [
          "Segment Intro",
          "Tensor Transposition",
          "Basic Tensor Arithmetic, incl. the Hadamard Product",
          "Tensor Reduction",
          "The Dot Product",
          "Exercises on Tensor Operations",
          "Solving Linear Systems with Substitution",
          "Solving Linear Systems with Elimination",
          "Visualizing Linear Systems"
        ],
        "Matrix Properties": [
          "Segment Intro",
          "The Frobenius Norm",
          "Matrix Multiplication",
          "Symmetric and Identity Matrices",
          "Matrix Multiplication Exercises",
          "Matrix Inversion",
          "Diagonal Matrices",
          "Orthogonal Matrices",
          "Orthogonal Matrix Exercises"
        ],
        "Eigenvectors and Eigenvalues": [
          "Segment Intro",
          "Applying Matrices",
          "Affine Transformations",
          "Eigenvectors and Eigenvalues",
          "Matrix Determinants",
          "Determinants of Larger Matrices",
          "Determinant Exercises",
          "Determinants and Eigenvalues",
          "Eigendecomposition",
          "Eigenvector and Eigenvalue Applications"
        ],
        "Matrix Operations for Machine Learning": [
          "Segment Intro",
          "Singular Value Decomposition",
          "Data Compression with SVD",
          "The Moore-Penrose Pseudoinverse",
          "Regression with the Pseudoinverse",
          "The Trace Operator",
          "Principal Component Analysis (PCA)",
          "Resources for Further Study of Linear Algebra"
        ],
        "Limits": [
          "Segment Intro",
          "Intro to Differential Calculus",
          "Intro to Integral Calculus",
          "The Method of Exhaustion",
          "Calculus of the Infinitesimals",
          "Calculus Applications",
          "Calculating Limits",
          "Exercises on Limits"
        ],
        "Derivatives and Differentiation": [
          "Segment Intro",
          "The Delta Method",
          "How Derivatives Arise from Limits",
          "Derivative Notation",
          "The Derivative of a Constant",
          "The Power Rule",
          "The Constant Multiple Rule",
          "The Sum Rule",
          "Exercises on Derivative Rules",
          "The Product Rule",
          "The Quotient Rule",
          "The Chain Rule",
          "Advanced Exercises on Derivative Rules",
          "The Power Rule on a Function Chain"
        ],
        "Automatic Differentiation": [
          "Segment Intro",
          "What Automatic Differentiation Is",
          "Autodiff with PyTorch",
          "Autodiff with TensorFlow",
          "The Line Equation as a Tensor Graph",
          "Machine Learning with Autodiff"
        ],
        "Partial Derivative Calculus": [
          "Segment Intro",
          "What Partial Derivatives Are",
          "Partial Derivative Exercises",
          "Calculating Partial Derivatives with Autodiff",
          "Advanced Partial Derivatives",
          "Advanced Partial-Derivative Exercises",
          "Partial Derivative Notation",
          "The Chain Rule for Partial Derivatives",
          "Exercises on the Multivariate Chain Rule",
          "Point-by-Point Regression",
          "The Gradient of Quadratic Cost",
          "Descending the Gradient of Cost",
          "The Gradient of Mean Squared Error",
          "Backpropagation",
          "Higher-Order Partial Derivatives",
          "Exercise on Higher-Order Partial Derivatives"
        ],
        "Integral Calculus": [
          "Segment Intro",
          "Binary Classification",
          "The Confusion Matrix",
          "The Receiver-Operating Characteristic (ROC) Curve",
          "What Integral Calculus Is",
          "The Integral Calculus Rules",
          "Indefinite Integral Exercises",
          "Definite Integrals",
          "Numeric Integration with Python",
          "Definite Integral Exercise",
          "Finding the Area Under the ROC Curve",
          "Resources for the Further Study of Calculus",
          "Congratulations!"
        ]
      },
      "requirements": [
        "All code demos will be in Python so experience with it or another object-oriented programming language would be helpful for following along with the hands-on examples.",
        "Familiarity with secondary school-level mathematics will make the class easier to follow along with. If you are comfortable dealing with quantitative information — such as understanding charts and rearranging simple equations — then you should be well-prepared to follow along with all of the mathematics."
      ],
      "description": "Mathematics forms the core of data science and machine learning. Thus, to be the best data scientist you can be, you must have a working understanding of the most relevant math.\nGetting started in data science is easy thanks to high-level libraries like Scikit-learn and Keras. But understanding the math behind the algorithms in these libraries opens an infinite number of possibilities up to you. From identifying modeling issues to inventing new and more powerful solutions, understanding the math behind it all can dramatically increase the impact you can make over the course of your career.\nLed by deep learning guru Dr. Jon Krohn, this course provides a firm grasp of the mathematics — namely linear algebra and calculus — that underlies machine learning algorithms and data science models.\n\n\nCourse Sections\nLinear Algebra Data Structures\nTensor Operations\nMatrix Properties\nEigenvectors and Eigenvalues\nMatrix Operations for Machine Learning\nLimits\nDerivatives and Differentiation\nAutomatic Differentiation\nPartial-Derivative Calculus\nIntegral Calculus\nThroughout each of the sections, you'll find plenty of hands-on assignments, Python code demos, and practical exercises to get your math game in top form!\nThis Mathematical Foundations of Machine Learning course is complete, but in the future, we intend on adding extra content from related subjects beyond math, namely: probability, statistics, data structures, algorithms, and optimization. Enrollment now includes free, unlimited access to all of this future course content — over 25 hours in total.\n\n\nAre you ready to become an outstanding data scientist? See you in the classroom.",
      "target_audience": [
        "You use high-level software libraries (e.g., scikit-learn, Keras, TensorFlow) to train or deploy machine learning algorithms, and would now like to understand the fundamentals underlying the abstractions, enabling you to expand your capabilities",
        "You’re a software developer who would like to develop a firm foundation for the deployment of machine learning algorithms into production systems",
        "You’re a data scientist who would like to reinforce your understanding of the subjects at the core of your professional discipline",
        "You’re a data analyst or A.I. enthusiast who would like to become a data scientist or data/ML engineer, and so you’re keen to deeply understand the field you’re entering from the ground up (very wise of you!)"
      ]
    },
    {
      "title": "Azure Synapse Analytics For Data Engineers -Hands On Project",
      "url": "https://www.udemy.com/course/azure-synapse-analytics-for-data-engineers/",
      "bio": "Hands on Project for Data Engineers using all the services available in Azure Synapse Analytics [DP-203, DP-500]",
      "objectives": [
        "You will learn how to build a real world project using Azure Synapse Analytics. This course has been taught using real world data from NYC Taxi Trips data",
        "You will acquire professional level data engineering skills in Azure Synapse Analytics",
        "You will learn how to create SQL scripts and Spark notebooks in Azure Synapse Analytics",
        "You will learn how to create dedicated SQL pools and spark pools in Azure Synapse Analytics",
        "You will learn how to enable synapse link and enable analytic store in Cosmos DB",
        "You will learn how to ingest and transform data Serverless SQL Pool and Spark Pool",
        "You will learn how to load data into dedicated SQL Pool",
        "You will learn how to serve data to Power BI from Serverless SQL Pool and Dedicated SQL Pool",
        "You will learn how to execute scripts and notebooks using Synapse Pipelines and Triggers",
        "You will learn how to do operational reporting from the data stored in Cosmos DB using Azure Synapse Analytics",
        "You will learn how to build reports in Power BI for the data stored in Azure Synapse Analytics"
      ],
      "course_content": {
        "Introduction": [
          "Course Introduction",
          "Course Materials Download",
          "Useful Links"
        ],
        "Azure Subscription (Optional)": [
          "Creating Azure Account",
          "Azure Portal Overview"
        ],
        "Azure Synapse Analytics Overview": [
          "Introduction to Azure Synapse Analytics",
          "History of Data Warehouse/ Data Lakes",
          "Emergence of Azure Synapse Analytics",
          "Create Azure Synapse Analytics Workspace",
          "Azure Synapse Analytics Workspace Overview",
          "Azure Synapse Studio Overview",
          "Data Hub Overview",
          "Develop Hub Overview",
          "Integrate Hub Overview",
          "Monitor Hub Overview",
          "Manage Hub Overview"
        ],
        "NYC Taxi Project Overview": [
          "Section Overview",
          "NYC Taxi Data Source Overview",
          "NYC Taxi Data Files Overview",
          "Upload NYC Taxi Data to Data Lake",
          "Project Requirements Overview",
          "Solution Architecture Overview"
        ],
        "Serverless SQL Pool - Overview": [
          "Section Overview",
          "Introduction to Serverless SQL Pool",
          "Serverless SQL Pool Cost Control",
          "Connect from Azure Data Studio to Serverless SQL Pool (Optional)"
        ],
        "Serverless SQL Pool - Query CSV": [
          "Section Overview",
          "OPENROWSET Function Overview",
          "Query Taxi Zone File (CSV File)",
          "Specify Data Types",
          "Specify Collation",
          "Query Subset of Columns",
          "Debugging & Identifying Errors",
          "Use External Data Source",
          "Query Calendar File (CSV File) - Assignment",
          "Query Vendor File (Quoted and Escaped Columns)",
          "Query Trip Type File (TSV File) - Assignment"
        ],
        "Serverless SQL Pool - Query JSON": [
          "Section Overview",
          "Query Payment Type (Single Line JSON) - JSON_VALUE Function",
          "Query Payment Type (Single Line JSON) - OPENJSON Function",
          "Query JSON Array",
          "Query Standard JSON",
          "Query Multi Line JSON (Assignment)"
        ],
        "Serverless SQL Pool - Query Folders & Multiple Files": [
          "Query Folders and Subfolders",
          "File Metadata Functions"
        ],
        "Serverless SQL Pool - Query Columnar Formats": [
          "Query Single Parquet File",
          "Query Folders and Sub Folders (Assignment)",
          "Query Delta files"
        ],
        "Serverless SQL Pool - Data Discovery": [
          "Data Discovery Overview",
          "Identify Duplicates",
          "Data Quality Checks",
          "Joining Files",
          "Transform Data",
          "Data Discovery Assignment"
        ]
      },
      "requirements": [
        "All the code and step-by-step instructions are provided, but the skills below will greatly benefit your journey",
        "Basic SQL knowledge will be required",
        "Basic Python programming experience will be required",
        "Knowledge of cloud fundamentals will be beneficial, but not necessary",
        "Azure subscription will be required, If you don't have one we will create a free account in the course"
      ],
      "description": "Welcome!\nI am looking forward to helping you with learning one of the in-demand data engineering tools in the cloud, Azure Synapse Analytics! This course has been taught with implementing a data engineering solution using Azure Synapse Analytics for a real world project of analysing and reporting on NYC Taxi trips data.\nThis is like no other course in Udemy for Azure Synapse Analytics. Once you have completed the course including all the assignments, I strongly believe that you will be in a position to start a real world data engineering project on your own and also proficient on Azure Synapse Analytics.  The primary focus of the course is Azure Synapse Analytics, but it also covers the relevant concepts and connectivity to the other technologies mentioned.\nThe course follows a logical progression of a real world project implementation with technical concepts being explained and the scripts and notebooks being built at the same time. Even though this course is not specifically designed to teach you the skills required for passing the exams Azure Data Engineer Associate Certification [DP-203] or Designing and Implementing Enterprise-Scale Analytics Solutions Using Microsoft Azure and Microsoft Power BI [DP-500], it can greatly help you get most of the necessary skills required for the exams.\nI value your time as much as I do mine. So, I have designed this course to be fast-paced and to the point. Also, the course has been taught with simple English and no jargons. I start the course from basics and by the end of the course you will be proficient in the technologies used.\nCurrently the course teaches you the following\nAzure Synapse Analytics Architecture\nServerless SQL Pool\nSpark Pool\nDedicated SQL Pool\nSynapse Pipelines\nSynapse Link for Cosmos DB / Hybrid Transactional and Analytical Processing (HTAP) capability\nPower BI Integration with Azure Synapse Analytics\nAzure Data Lake Storage Gen2 integration with Azure Synapse Analytics\nProject using NYC Taxi Trips data using the above technologies\nPlease note that the following are not currently covered\nData Flows\nAdvanced concepts around Dedicated SQL Pool\nSpark Programming\nSQL Fundamentals",
      "target_audience": [
        "University students looking for a career in Data Engineering",
        "IT developers working on other disciplines trying to move to Data Engineering",
        "Data Engineers/ Data Warehouse Developers currently working on on-premises technologies, or other cloud platforms such as AWS or GCP who want to learn Azure Data Technologies",
        "Data Architects looking to gain an understanding about Azure Data Engineering stack"
      ]
    },
    {
      "title": "DP-100 Azure Data Scientist Associate Complete Exam Guide",
      "url": "https://www.udemy.com/course/dp-100-azure-data-scientist-associate-complete-exam-guide/",
      "bio": "Complete DP-100 Azure Machine Learning training guide to prepare you for DP-100, with practice exams on DP 100 Azure ML",
      "objectives": [
        "Everything you need to pass the DP-100 exam and receive the Azure Data Scientist Associate certification",
        "All learning objectives found in the DP-100 curriculum, through video lectures, demos, applications, and practice exams",
        "Learn and master Azure Machine Learning, a service by Microsoft that enables anyone to build, deploy, and manage Data Science and Machine Learning solutions",
        "Create a predictive service based on a model that you create, with a full end-to-end walkthrough",
        "Design and prepare a machine learning solution",
        "Explore data and train models",
        "Prepare a model for deployment",
        "Deploy and retrain a model"
      ],
      "course_content": {
        "Introduction": [
          "What is DP-100?",
          "Course tips",
          "What are the objectives of this course?",
          "Course roadmap",
          "Learning objectives",
          "Instructor overview",
          "Ways to reach out",
          "Keys to success",
          "Leave a rating",
          "Watch in 1080p"
        ],
        "Environment Setup": [
          "Create an Azure account",
          "Cost management in Azure",
          "Reference material",
          "Resources and prerequisites",
          "Helpful advice from students"
        ],
        "LO1: Design and prepare a machine learning solution (20–25%)": [
          "1-1-1 Determine the appropriate compute specifications",
          "1-1-2 Model deployment requirements",
          "1-1-3 Choice to development approach to build or train a model",
          "1-2-1 Create an Azure Machine Learning workspace",
          "1-2-1 Walkthrough of workspace",
          "1-2-1 Resources created by ML workspace",
          "1-2-1 How to access Azure ML tools",
          "1-2-1 Create a compute instance",
          "1-2-1 Run python SDK import statements",
          "1-2-1 Stopping compute instance",
          "1-3-1 Create Azure Data resources",
          "1-3-2 Create and register a datastore",
          "1-3-2 Example of transfering files to datastore",
          "1-3-3 Create a data asset",
          "1-3-3 Register a data asset through SDK",
          "1-3-3 Register and consume data assets through SDK"
        ],
        "LO2: Explore data and train models (35–40%)": [
          "2-1-1 Load and transform data",
          "2-1-2 Analyze data using Azure Data Explorer 1",
          "2-1-2 Analyze data using Azure Data Explorer 2",
          "2-1-2 Use profile mechanics to explore data",
          "2-2-1 Create a training pipeline introduction",
          "2-2-2 Consume data assets into the designer",
          "2-2-3 Use data preparation components in designer",
          "2-2-3 Training model and scoring components in designer",
          "2-2-3 Evaluating trained model components in designer",
          "2-2-3 Evaluation results defined",
          "2-2-4 Context and use-case for custom code components",
          "2-2-4 Adding custom python code in custom components in designer",
          "2-3-1 Automated ML introduction",
          "2-3-1 Automated ML regression and tabular data example 1",
          "2-3-1 Automated ML regression and tabular data example 2",
          "2-3-1 Automated ML regression and tabular data example 3",
          "2-3-2 Automated ML natural language processing NLP example",
          "2-3-4 Training options in Automated ML, including preprocessing and algorithms",
          "2-4-1 Develop code using a compute instance",
          "2-4-2 Consume data in a notebook",
          "2-4-3 How to run an experiment",
          "2-4-4 2-4-5 Evaluate and train a model using Python SDK 1",
          "2-4-4 2-4-5 Evaluate and train a model using Python SDK 2",
          "2-4-4 2-4-5 Evaluate and train a model using Python SDK 3",
          "2-4-4 2-4-5 Run experiments and measure impact on evaluation metrics"
        ],
        "LO3: Prepare a model for deployment (20–25%)": [
          "3-1-1 Introduction to model training scripts",
          "3-1-1 3-1-3 3-1-4 3-1-6 3-1-7 Run model training script end-to-end 1",
          "3-1-1 3-1-3 3-1-4 3-1-6 3-1-7 Run model training script end-to-end 2",
          "3-1-1 3-1-3 3-1-4 3-1-6 3-1-7 Run model training script end-to-end 3",
          "3-1-1 3-1-3 3-1-4 3-1-6 3-1-7 Run model training script end-to-end 4",
          "3-1-1 3-1-3 3-1-4 3-1-6 3-1-7 Run model training script end-to-end 5",
          "3-1-8 3-1-2 Configure compute and set up script parameters set up",
          "3-1-8 3-1-2 Using script parameters",
          "3-1-8 3-1-2 Cycling through script parameters",
          "3-1-8 3-1-2 Testing different script parameters",
          "3-1-8 3-1-2 Configure compute for a job run",
          "3-1-8 3-1-2 Adding compute to an environment",
          "3-1-8 3-1-2 Deleting a compute through Python SDK",
          "3-2-1 Introduction to pipelines",
          "3-2-1 Pipeline context",
          "3-2-1 Create a prepare data step in pipeline",
          "3-2-1 Create a train model step in pipeline",
          "3-2-1 Fix errors in pipeline",
          "3-2-1 Create a pipeline run script",
          "3-2-2 Pass data between steps in pipeline",
          "3-2-3 Run the pipeline",
          "3-2-3 Other ways to run the pipeline",
          "3-2-3 Publishing the endpoint",
          "3-2-3 Create a pipeline endpoint",
          "3-2-3 Call an endpoint 1",
          "3-2-3 Call an endpoint 2",
          "3-2-4 Monitor pipeline runs"
        ],
        "LO4: Deploy and retrain a model (10–15%)": [
          "4-1-1 4-1-3 4-1-5 Introduction to deploying model",
          "4-1-1 4-1-3 4-1-5 Create a model to be deployed",
          "4-1-1 4-1-3 4-1-5 Configure model for a real-time deployment",
          "4-1-1 4-1-3 4-1-5 Removing the dependent variable",
          "4-1-1 4-1-3 4-1-5 Deploy a model to a real-time endpoint",
          "4-1-1 4-1-3 4-1-5 Test a real-time deployed service",
          "4-1-1 4-1-3 4-1-5 Consume the deployed model in endpoint",
          "4-1-1 4-1-3 4-1-5 Make modifications to deployed model",
          "4-1-1 4-1-3 4-1-5 Redeploy a model"
        ],
        "Practice Exams": [
          "Practice Exam 1",
          "Practice Exam 2"
        ],
        "Conclusion": [
          "Congratulations",
          "Conclusion and next steps",
          "Ways to reach out",
          "Certificate"
        ],
        "Bonus": [
          "Bonus"
        ]
      },
      "requirements": [
        "Some familiarity with data science and machine learning concepts (like regressions, training and validation, hyperparameters) is helpful but not necessary",
        "Some familiarity with Python language and data science libraries (Pandas, sklearn, etc.) is helpful but not necessary",
        "Computer with web browser and internet",
        "No Azure experience necessary, come hungry to learn!"
      ],
      "description": "Do you want to quickly build, deploy, and scale Data Science and Machine Learning solutions, without knowing any in-depth code, worrying about containers / endpoints, or coding data pipelines?\n\n\nDo you want to learn and master Azure Machine Learning, an enterprise-grade service by Microsoft that gives you tools for the end-to-end machine learning lifecycle?\n\n\nDo you want to build, deploy, and manage high quality models faster and with confidence?\n\n\nDo you want to be certified from Microsoft, so that you can put it on your Resume/CV and showcase to potential employers that you know how to deploy Data Science solutions using Azure Machine Learning?\n\n\nDo you want to pass the Microsoft DP 100 on the first try, and want one single complete resource that has everything you need for the DP-100 certification?\n\n\nThen this is the course for you. Learn from over 15 hours of instructional content with video lectures, demos, real-life applications, and practice exams, with the only complete guide to everything you need to know to pass the DP-100 exam and receive your certification.\n\n\nThis course gives you all the training you need to pass - with detailed lectures, demos, and practice questions for each of the 62 learning objectives within the DP100 curriculum. This course gives you the structure you need to succeed - we go through each learning objective in sequential order, so that you are never lost.\n\n\nThis course is also for those students who want to learn Azure Machine Learning, and its underlying services. Along with the training required to pass the DP 100 certification, students master this tool.\n\n\nDP-100 Designing and Implementing a Data Science Solution on Azure and Azure Data Scientist Associate certification is also called DP-100, DP100, and DP 100 certifications, and so these are used interchangeably.\n\n\nWhat is the DP-100?\nThe DP-100 is a certification exam offered by Microsoft, that enables you to receive the Azure Data Scientist Associate certification. The exam covers how to design, build, and deploy a Machine Learning solution using Azure Machine Learning. The certification enables you to proves to employers and clients that you can build and operationalize Machine Learning and Data Science solutions and understand the core capabilities of the Azure Machine Learning. The exam format varies, but is most often 40-60 questions within about 2 hours. DP-100 is also referred to as DP 100 or DP100.\n\n\nWhat is this course all about?\nThe purpose of this course is to prepare you for the DP 100 Designing and Implementing a Data Science Solution on Azure exam, so that you can pass it on the first try. It offers you dedicated video lectures, walk through demos, real-life applications, and practice exams to maximize your chances of success. This course covers 100% of the 62 learning objectives in Microsoft's curriculum, and trains you to receive the certification on the first try.\n\n\nWhat does the DP-100 cover?\nThe DP-100 covers how to use Azure Machine Learning to design and implement a Data Science and Machine Learning solution. Specifically, it covers how to design and prepare a Machine Learning solution, how to explore data and train models, how to prepare a model for deployment, and how to deploy and retrain a model. The curriculum covers everything about Azure Machine Learning Studio, in both the designer (no-code) workflow, the Automated ML workflow, and the coding (Python SDK, Notebooks) workflow.\n\n\nWhat are the prerequisites of taking the DP-100?\nCandidates should have subject matter expertise in applying data science and ML to implement and run ML workloads.\n\n\nWhat is Azure Machine Learning?\nAzure Machine Learning (or Azure ML for short) is a service from Microsoft to create, validate, and deploy Machine Learning and Data Science solutions. It covers everything you would need, from data preparation, to model training and validation, to endpoint model management, and monitoring / model management. It makes it easier for anyone to deploy Data Science and Machine Learning solutions, especially if you are not familiar with Data Science algorithms, container management, compute monitoring, etc. - it does that all for you. Azure Machine Learning lets Data Scientists focus on what matters most, and automates the rest. It gives the power of Data Science and Machine Learning to anyone.\n\n\nWhy is Azure Machine Learning so important?\nAzure Machine Learning is Microsoft’s way to democratize Machine Learning and Data Science to the everyday user.\n\n\nWhy should you get the DP-100 certification?\nThe DP-100 certification from Microsoft is a recognized way to prove that you understand and can use Azure Machine Learning to build business critical machine learning models at scale. You can use the knowledge you learn in the DP-100 course to create impact in your organization, but deploying predictive services and endpoints. You can add it to your resume to significantly boost your chances of employment. Most employers even cover the cost of the training and exam because of the value that this certification provides. In certain countries, you can even receive ACE college credits.\n\n\nWhy choose this course?\nComplete guide - this is the 100% complete, start to finish zero to hero training guide to passing the DP 100 exam. It includes lectures, demos, study guides, practice exams, and more. It is the only resource that you will ever need to ace the exam. It contains over 15 hours of instructional content!\nFull coverage - we go through Microsoft's curriculum one-by-one and cover all aspects of each of the 62 different learning objectives. This means no surprises on the exam, and it ensures that you are best prepared to pass the DP-100 exam on the first try.\nStructured to succeed - the course mirrors Microsoft's DP 100 curriculum exactly. Each of the 62 learning objectives has a combination of a PDF study guide, full video lectures, full video walk through demos, and application.\nInstructional and applicable - we not only go through important concepts, but also apply them as we are building our application so that we can solidify them. This is not only a walkthrough of the all the features and theoretical concepts, but a DP-100 course that actually builds applications with you\nPractice exams - this course contains practice exams with questions that exactly mirror the types of questions found on the DP-100 exam. Use them to validate your knowledge and find weaker areas where you need to review.\nStep by step - each learning objective within Microsoft's DP 100 curriculum is covered in order, step by step. This ensures that you never get lost in the course.\nTeacher response - if there's anything else you would like to learn, or if there's something you cannot figure out, I'm here for you! Look at the ways to reach out video\nCommunity - when you enroll in this course, you join a DP100 community full of learners just like you\nMaster a new tool - Learn Azure Machine Learning, from basic no-code designer tools to fully customized code deployments using Python SDK\n\n\nCourse overview theory\nThe course follows exactly according to DP100 curriculum, based on 62 learning objectives (LO) that Microsoft has defined. Everything in this course is made to maximize your chances of passing the exam. For each learning objective, the course offers a combination of guided lecture videos, walk-through demos, and application. We then end with practice exams.\n\n\nCourse overview\nIntroduction - learn about the DP 100 exam and how best to succeed\nEnvironment Setup - set up an Azure account so you can follow along, and review the curriculum\nLO1: Design and prepare a machine learning solution (20–25%)\nLO2: Explore data and train models (35–40%)\nLO3: Prepare a model for deployment (20–25%)\nLO4: Deploy and retrain a model (10–15%)\nPractice Exams - practice what you have learned to validate your knowledge\nConclusion - take your exam, earn your certification, and next steps\n\n\nIcons by Freepik / Flaticon. Music by Bensound.",
      "target_audience": [
        "Business Analysts who want to build, test, and deploy models quickly, especially if they want to create Data Science solutions and predictive services",
        "Students who want to pass the DP-100 exam and receive the Azure Data Scientist Associate certification",
        "Individuals who want to receive a formal certificate from Microsoft for their progress and achievement (useful for moving upwards and getting hired)",
        "Users who want to create and deploy Data Science and Machine Learning solutions with no-code",
        "Data Scientists who want a more streamlined approach to creating, deploying, and managing Machine Learning solutions and services",
        "Data Scientists and Machine Learning Engineers who want to focus on what matters most, and want to automate the rest (algorithm and hyperparameter tuning, endpoint containerization)",
        "Anyone who wants to learn Azure Machine Learning, a tool for building ML model services, from the most basic to the most advanced",
        "Students who want to make a career in Data Science and Machine Learning"
      ]
    },
    {
      "title": "Cutting-Edge AI: Deep Reinforcement Learning in Python",
      "url": "https://www.udemy.com/course/cutting-edge-artificial-intelligence/",
      "bio": "Apply deep learning to artificial intelligence and reinforcement learning using evolution strategies, A2C, and DDPG",
      "objectives": [
        "Understand a cutting-edge implementation of the A2C algorithm (OpenAI Baselines)",
        "Understand and implement Evolution Strategies (ES) for AI",
        "Understand and implement DDPG (Deep Deterministic Policy Gradient)",
        "Understand important foundations for OpenAI ChatGPT, GPT-4"
      ],
      "course_content": {
        "Welcome": [
          "Introduction",
          "Outline",
          "Where to get the code"
        ],
        "Review of Fundamental Reinforcement Learning Concepts": [
          "Review Section Introduction",
          "The Explore-Exploit Dilemma",
          "Markov Decision Processes (MDPs)",
          "Monte Carlo Methods",
          "Temporal Difference Learning (TD)",
          "OpenAI Gym Warmup",
          "Review Section Summary",
          "Suggestion Box"
        ],
        "A2C (Advantage Actor-Critic)": [
          "A2C Section Introduction",
          "A2C Theory (part 1)",
          "A2C Theory (part 2)",
          "A2C Theory (part 3)",
          "A2C Demo",
          "A2C Code - Rough Sketch",
          "Multiple Processes",
          "Environment Wrappers",
          "Convolutional Neural Network",
          "A2C",
          "A2C Section Summary"
        ],
        "DDPG (Deep Deterministic Policy Gradient)": [
          "DDPG Section Introduction",
          "Deep Q-Learning (DQN) Review",
          "DDPG Theory",
          "MuJoCo",
          "DDPG Code (part 1)",
          "DDPG Code (part 2)",
          "DDPG Section Summary"
        ],
        "ES (Evolution Strategies)": [
          "ES Section Introduction",
          "ES Theory",
          "Notes on Evolution Strategies",
          "ES for Optimizing a Function",
          "ES for Supervised Learning",
          "Flappy Bird",
          "ES for Flappy Bird in Code",
          "ES for MuJoCo in Code",
          "ES Section Summary"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (FAQ by Student Request)": [
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Python 2 vs Python 3"
        ],
        "Effective Learning Strategies for Machine Learning (FAQ by Student Request)": [
          "How to Succeed in this Course (Long Version)",
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "Machine Learning and AI Prerequisite Roadmap (pt 1)",
          "Machine Learning and AI Prerequisite Roadmap (pt 2)"
        ],
        "Appendix / FAQ Finale": [
          "BONUS"
        ]
      },
      "requirements": [
        "Know the basics of MDPs (Markov Decision Processes) and Reinforcement Learning",
        "Helpful to have seen my first two Reinforcement Learning courses",
        "Know how to build a convolutional neural network in Tensorflow"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT and GPT-4 really work? In this course, you will learn the foundations of these groundbreaking applications.\n\n\nWelcome to Cutting-Edge AI!\n\n\nThis is technically Deep Learning in Python part 11 of my deep learning series, and my 3rd reinforcement learning course.\nDeep Reinforcement Learning is actually the combination of 2 topics: Reinforcement Learning and Deep Learning (Neural Networks).\nWhile both of these have been around for quite some time, it’s only been recently that Deep Learning has really taken off, and along with it, Reinforcement Learning.\nThe maturation of deep learning has propelled advances in reinforcement learning, which has been around since the 1980s, although some aspects of it, such as the Bellman equation, have been for much longer.\n\n\nRecently, these advances have allowed us to showcase just how powerful reinforcement learning can be.\nWe’ve seen how AlphaZero can master the game of Go using only self-play.\nThis is just a few years after the original AlphaGo already beat a world champion in Go.\n\n\nWe’ve seen real-world robots learn how to walk, and even recover after being kicked over, despite only being trained using simulation.\nSimulation is nice because it doesn’t require actual hardware, which is expensive. If your agent falls down, no real damage is done.\n\n\nWe’ve seen real-world robots learn hand dexterity, which is no small feat.\nWalking is one thing, but that involves coarse movements. Hand dexterity is complex - you have many degrees of freedom and many of the forces involved are extremely subtle.\nImagine using your foot to do something you usually do with your hand, and you immediately understand why this would be difficult.\n\n\nLast but not least - video games.\nEven just considering the past few months, we’ve seen some amazing developments. AIs are now beating professional players in CS:GO and Dota 2.\n\n\nSo what makes this course different from the first two?\nNow that we know deep learning works with reinforcement learning, the question becomes: how do we improve these algorithms?\nThis course is going to show you a few different ways: including the powerful A2C (Advantage Actor-Critic) algorithm, the DDPG (Deep Deterministic Policy Gradient) algorithm, and evolution strategies.\nEvolution strategies is a new and fresh take on reinforcement learning, that kind of throws away all the old theory in favor of a more \"black box\" approach, inspired by biological evolution.\n\n\nWhat’s also great about this new course is the variety of environments we get to look at.\nFirst, we’re going to look at the classic Atari environments. These are important because they show that reinforcement learning agents can learn based on images alone.\nSecond, we’re going to look at MuJoCo, which is a physics simulator. This is the first step to building a robot that can navigate the real-world and understand physics - we first have to show it can work with simulated physics.\nFinally, we’re going to look at Flappy Bird, everyone’s favorite mobile game just a few years ago.\n\n\nThanks for reading, and I’ll see you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested prerequisites:\nCalculus\nProbability\nObject-oriented programming\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations\nLinear regression\nGradient descent\nKnow how to build a convolutional neural network (CNN) in TensorFlow\nMarkov Decision Proccesses (MDPs)\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students and professionals who want to apply Reinforcement Learning to their work and projects",
        "Anyone who wants to learn cutting-edge Artificial Intelligence and Reinforcement Learning algorithms"
      ]
    },
    {
      "title": "Machine Learning & Deep Learning in Python & R",
      "url": "https://www.udemy.com/course/data_science_a_to_z/",
      "bio": "Covers Regression, Decision Trees, SVM, Neural Networks, CNN, Time Series Forecasting and more using both Python & R",
      "objectives": [
        "Learn how to solve real life problem using the Machine learning techniques",
        "Machine Learning models such as Linear Regression, Logistic Regression, KNN etc.",
        "Advanced Machine Learning models such as Decision trees, XGBoost, Random Forest, SVM etc.",
        "Understanding of basics of statistics and concepts of Machine Learning",
        "How to do basic statistical operations and run ML models in Python",
        "In-depth knowledge of data collection and data preprocessing for Machine Learning problem",
        "How to convert business problem into a Machine learning problem"
      ],
      "course_content": {},
      "requirements": [
        "Students will need to install Anaconda software but we have a separate lecture to guide you install the same"
      ],
      "description": "You're looking for a complete Machine Learning and Deep Learning course that can help you launch a flourishing career in the field of Data Science, Machine Learning, Python, R or Deep Learning, right?\nYou've found the right Machine Learning course!\nAfter completing this course you will be able to:\n· Confidently build predictive Machine Learning and Deep Learning models using R, Python to solve business problems and create business strategy\n· Answer Machine Learning, Deep Learning, R, Python related interview questions\n· Participate and perform in online Data Analytics and Data Science competitions such as Kaggle competitions\nCheck out the table of contents below to see what all Machine Learning and Deep Learning models you are going to learn.\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning and deep learning concepts in Real world problems of business, this course will give you a solid base for that by teaching you the most popular techniques of machine learning and deep learning. You will also get exposure to data science and data analysis tools like R and Python.\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear regression. It also focuses Machine Learning and Deep Learning techniques in R and Python.\nMost courses only focus on teaching how to run the data analysis but we believe that what happens before and after running data analysis is even more important i.e. before running data analysis it is very important that you have the right data and do some pre-processing on it. And after running data analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business. Here comes the importance of machine learning and deep learning. Knowledge on data analysis tools like R, Python play an important role in these fields of Machine Learning and Deep Learning.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course. We have an in-depth knowledge on Machine Learning and Deep Learning techniques using data science and data analysis tools R, Python.\nWe are also the creators of some of the most popular online courses - with over 600,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message. We aim at providing best quality training on data science, machine learning, deep learning using R and Python through this machine learning course.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts on data science, machine learning, deep learning using R and Python. Each section contains a practice assignment for you to practically implement your learning on data science, machine learning, deep learning using R and Python.\nTable of Contents\nSection 1 - Python basic\nThis section gets you started with Python.\nThis section will help you set up the python and Jupyter environment on your system and it'll teach you how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn. Python basics will lay foundation for gaining further knowledge on data science, machine learning and deep learning.\nSection 2 - R basic\nThis section will help you set up the R and R studio on your system and it'll teach you how to perform some basic operations in R. Similar to Python basics, R basics will lay foundation for gaining further knowledge on data science, machine learning and deep learning.\nSection 3 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics then graphical representations to describe the data and then a lecture on measures of center like mean median and mode and lastly measures of dispersion like range and standard deviation. This part of the course is instrumental in gaining knowledge data science, machine learning and deep learning in the later part of the course.\nSection 4 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\nSection 5 - Data Preprocessing\nIn this section you will learn what actions you need to take step by step to get the data and then prepare it for the analysis these steps are very important. We start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bivariate analysis then we cover topics like outlier treatment, missing value imputation, variable transformation and correlation.\nSection 6 - Regression Model\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you understand where the concept is coming from and how it is important. But even if you don't understand it, it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results, what are other variations to the ordinary least squared method and how do we finally interpret the result to find out the answer to a business problem.\nSection 7 - Classification Models\nThis section starts with Logistic regression and then covers Linear Discriminant Analysis and K-Nearest Neighbors.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you\nunderstand where the concept is coming from and how it is important. But even if you don't understand\nit, it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models performance using confusion matrix, how categorical variables in the independent variables dataset are interpreted in the results, test-train split and how do we finally interpret the result to find out the answer to a business problem.\nSection 8 - Decision trees\nIn this section, we will start with the basic theory of decision tree then we will create and plot a simple Regression decision tree. Then we will expand our knowledge of regression Decision tree to classification trees, we will also learn how to create a classification tree in Python and R\nSection 9 - Ensemble technique\nIn this section, we will start our discussion about advanced ensemble techniques for Decision trees. Ensembles techniques are used to improve the stability and accuracy of machine learning algorithms. We will discuss Random Forest, Bagging, Gradient Boosting, AdaBoost and XGBoost.\nSection 10 - Support Vector Machines\nSVM's are unique models and stand out in terms of their concept. In this section, we will discussion about support vector classifiers and support vector machines.\nSection 11 - ANN Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nSection 12 - Creating ANN model in Python and R\nIn this part you will learn how to create ANN models in Python and R.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. Lastly we learn how to save and restore models.\nWe also understand the importance of libraries such as Keras and TensorFlow in this part.\nSection 13 - CNN Theoretical Concepts\nIn this part you will learn about convolutional and pooling layers which are the building blocks of CNN models.\nIn this section, we will start with the basic theory of convolutional layer, stride, filters and feature maps. We also explain how gray-scale images are different from colored images. Lastly we discuss pooling layer which bring computational efficiency in our model.\nSection 14 - Creating CNN model in Python and R\nIn this part you will learn how to create CNN models in Python and R.\nWe will take the same problem of recognizing fashion objects and apply CNN model to it. We will compare the performance of our CNN model with our ANN model and notice that the accuracy increases by 9-10% when we use CNN. However, this is not the end of it. We can further improve accuracy by using certain techniques which we explore in the next part.\nSection 15 - End-to-End Image Recognition project in Python and R\nIn this section we build a complete image recognition project on colored images.\nWe take a Kaggle image recognition competition and build CNN model to solve it. With a simple model we achieve nearly 70% accuracy on test set. Then we learn concepts like Data Augmentation and Transfer Learning which help us improve accuracy level from 70% to nearly 97% (as good as the winners of that competition).\nSection 16 - Pre-processing Time Series Data\nIn this section, you will learn how to visualize time series, perform feature engineering, do re-sampling of data, and various other tools to analyze and prepare the data for models\nSection 17 - Time Series Forecasting\nIn this section, you will learn common time series models such as Auto-regression (AR), Moving Average (MA), ARMA, ARIMA, SARIMA and SARIMAX.\nBy the end of this course, your confidence in creating a Machine Learning or Deep Learning model in Python and R will soar. You'll have a thorough understanding of how to use ML/ DL models to create predictive models and solve real world business problems.\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhy use Python for Machine Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Machine Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nMachine Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhy use R for Machine Learning?\nUnderstanding R is one of the valuable skills needed for a career in Machine Learning. Below are some reasons why you should learn Machine learning in R\n1. It’s a popular language for Machine Learning at top tech firms. Almost all of them hire data scientists who use R. Facebook, for example, uses R to do behavioral analysis with user post data. Google uses R to assess ad effectiveness and make economic forecasts. And by the way, it’s not just tech firms: R is in use at analysis and consulting firms, banks and other financial institutions, academic institutions and research labs, and pretty much everywhere else data needs analyzing and visualizing.\n2. Learning the data science basics is arguably easier in R. R has a big advantage: it was designed specifically with data manipulation and analysis in mind.\n3. Amazing packages that make your life easier. Because R was designed with statistical analysis in mind, it has a fantastic ecosystem of packages and other resources that are great for data science.\n4. Robust, growing community of data scientists and statisticians. As the field of data science has exploded, R has exploded with it, becoming one of the fastest-growing languages in the world (as measured by StackOverflow). That means it’s easy to find answers to questions and community guidance as you work your way through projects in R.\n5. Put another tool in your toolkit. No one language is going to be the right tool for every job. Adding R to your repertoire will make some projects easier – and of course, it’ll also make you a more flexible and marketable employee when you’re looking for jobs in data science.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience"
      ]
    },
    {
      "title": "The Complete Python Course for Machine Learning Engineers",
      "url": "https://www.udemy.com/course/the-complete-python-course-for-machine-learning-engineers/",
      "bio": "Two Courses Now Included for the Price of One",
      "objectives": [
        "You'll learn everything you need to know about Python for authoring basic machine learning models.",
        "You'll work through hands on labs that will test the skills you learned in the lessons.",
        "You'll learn all the Python vernacular you need to take you skills to the next level.",
        "You'll build a basic Deep Neural Network in Python line by line.",
        "You'll use Scikit-Learn to Build a Traditional Machine Learning Model",
        "You'll understand why Python has become the Gold Standard in the Machine Learning Space."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Is this Course for You?",
          "What is Python?",
          "Why Applied Machine Learning is Mostly Python",
          "Python Skills Evaluation",
          "Installing Python on Windows (Anaconda Distribution)",
          "Lab: Installing Python with Anaconda",
          "Lab: Connecting to Python",
          "Jupyter Notebook Anatomy - Menu Bar",
          "Jupyter Notebook Anatomy - Toolbar",
          "Lab: Code and Markup",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 1"
        ],
        "Variables and Operators": [
          "The Comment in Python",
          "What's a Variable?",
          "Naming Variables",
          "Lab: Variables in Python",
          "The Assignment Operator",
          "Operators in Python",
          "Lab: Operators Notebook",
          "Data Types in Python",
          "String Formatting with the % Operator",
          "Type Casting in Python: Integers and Floating Points",
          "Type Casting in Python: Strings",
          "Lab: Casting Int and Float",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 2"
        ],
        "Advanced Data Types": [
          "Lists",
          "Indexing Lists",
          "Modifying Items in Lists",
          "Slicing Lists",
          "Modifying Lists with Operators",
          "Removing an Item from a List",
          "Lab: Lists",
          "Tuples",
          "Dictionaries",
          "Accessing Dictionary Elements",
          "Using Functions to Access Elements",
          "Modifying Dictionaries",
          "Lab: Dictionaries",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 3"
        ],
        "Control Flow": [
          "Conditional Statements",
          "Else/If Statement",
          "Lab: If Statement",
          "The For Loop",
          "Looping and the Dictionary",
          "Lab: Looping in Python",
          "While Loop",
          "The Break",
          "Continue Statement",
          "Lab: More Looping",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 4"
        ],
        "Functions and Modules": [
          "What's a Function?",
          "User Defined Functions",
          "Lab: Working with Functions",
          "Variable Scope",
          "Default Parameter Values",
          "Variable Length Argument Lists",
          "Importing Modules",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 5"
        ],
        "Working with Files": [
          "Download Simple Text File",
          "Open and Read Text Files",
          "Reading Text Files with a For Loop",
          "Using Buffer Size to Open and Read Text Files",
          "Lab: Working with Text Files",
          "Summary"
        ],
        "Basic Object Oriented Programming": [
          "What is Object Oriented Programming?",
          "The Class",
          "Lab: Defining a Class in Python",
          "Classes, Objects and Instances",
          "Encapsulation",
          "Inheritance",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 7"
        ],
        "Pandas": [
          "Data Wrangling Defined",
          "What is Pandas",
          "Loading our Dataset",
          "Data Types",
          "Columns, Rows and Cells",
          "Lab: Massaging Data in Pandas",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 8"
        ],
        "SciKit-Learn": [
          "Download Raw Titanic Data Set",
          "What is Sci-kit Learn?",
          "Data Exploration in Pandas",
          "Data Wrangling in Pandas",
          "The X and y Axis",
          "Train, Test and Split",
          "Importing our Classifier",
          "Fit and Predict",
          "Lab: SciKit-Learn",
          "Summary",
          "Quiz",
          "Common Interview Questions - Section 9"
        ],
        "Keras": [
          "Installing Keras (Anaconda Distribution for Windows)",
          "Download Cleansed BikeBuyer Data Set",
          "What is Keras?",
          "Import, Create and Load",
          "Data Wrangling",
          "Defining the Target Variable and Training Data",
          "Model Definition",
          "Model Compilation",
          "Lab: Keras",
          "Summary",
          "Quiz"
        ]
      },
      "requirements": [
        "A basic understanding of programming",
        "Desire to learn Python"
      ],
      "description": "UPDATE: 8/26/2021 - I've added a second course. The second course included with this one is called The Fast Track Introduction to Python for Machine Learning Engineers.\nI've added a second course to this one that will focus more on the machine learning aspect and less on core Python.  There will be some overlap but that's actually good news. The more you see something the easier it will be to remember.\nI hope the two courses a compliment one another.\nCOURSE REVIEWS\nThis is the best hands-on online class I have ever taken. Very clear instructions. - Donato\nI took a few of your courses and you are an amazing teacher. Your courses have brought me up to speed on how to create databases and how to interact and handle Data Engineers and Data Scientists. I will be forever grateful.  -Tony\nBy taking this course my perception has changed and now data science for me is more about data wrangling. Thank you, Mike:)  -Archit\nI have now finished the first one, The complete python course and I have found it extremely structured and clear. I really thank you for your efforts in making these videos. I will now move on to Pandas. I am also looking out for jobs in order to start my career in this exciting field. - Gurukiran A\nI am really thankful to the instructor for creating such a nice and interactive course, thanks again. - Arun\nThis course does a good job in introducing machine learning in Python. - Vivek\nLesson are small, interactive, to the point and knowledge base. -Sanjay\nNice course on python programming & intro to libraries. - Sindhura\nYes. Accurate match for immediate requirements. Thank you! Looking forward to continuation courses. - Gregory\nThis course was very informative. Taught me about the open source models on which Machine Learning can be practiced. Kudos to the author. Great Job!!! - Mehar\nPerfect explaining and perfect length, not too long explanations - Henrik\nI loved the short format of the course. While that is a great thing there are some area which could have been a little longer. Overall, a very good course. - Raymond\n\nReally well done !!!!! With this course the programming language itself can be learnt unrelated to any computational task. - Giovanni\nThe hands on examples made learning very easy. I learned a lot about Python and Machine Learning at the same time. I would totally recommend for beginners. - Lumi\nSimple and easy to understand! - Pavan\nClear and easy to follow and understand the topic. - Dennis\nCOURSE OVERVIEW\nWelcome to The Complete Course for Machine Learning Engineers.\nThis series of courses is the only real world path to attaining a job as a machine learning engineer.  Machine learning engineers don't build models every day.\nIf you want to work in the real world then focus on learning Python. That's what this course is... Python!!!\nThis is the first course in a series of courses designed to prepare you for a real-world career as a machine learning engineer.\nI'll keep this updated and list only the courses that are live.  Here is a list of the courses that can be taken right now.  Please take them in order.  The knowledge builds from course to course.\nThe Complete Python Course for Machine Learning Engineers (This one)\nData Wrangling in Pandas for Machine Learning Engineers\nData Visualization in Python for Machine Learning Engineers\nSciKit-Learn in Python for Machine Learning Engineers (NEW)\nIn this course we are going to learn Python using a lab integrated approach. Programming is something you have to do in order to master it. You can't read about Python and expect to learn it.\nIf you take this course from start to finish you'll know the core foundations of Python, you'll understand the very basics of data cleansing and lastly you'll build a traditional machine learning model and a deep learning model.\nWhile the course is centered on learning the basics of Python you'll get to see how data cleansing is applied to a data set and how a traditional machine learning model and a deep learning model are built.\nThis course is an applied course on machine learning. Here' are a few items you'll learn:\nPython basics from A-Z\nLab integrated. Please don't just watch. Learning is an interactive event.  Go over every lab in detail.\nReal world Interviews Questions\nData Wrangling overview. What is it? Pay attention to the basics, it's what you'll be doing most of your time.\nBuild a basic model build in SciKit-Learn. We call these traditional models to distinguish them from deep learning models.\nBuild a basic Keras model. Keras is becoming the go to Python library for building deep learning models.\nIf you're new to programming or machine learning you might ask, why would I want to learn Python? Python has become the gold standard for building machine learning models in the applied space. The term \"applied\" simply means the real world.\n\nMachine learning is a type of artificial intelligence (AI) that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The key part of that definition is “without being explicitly programmed.”\n\nIf you're interested in working as a machine learning engineer, data engineer or data scientist then you'll have to know Python. The good news is that Python is a high level language. That means it was designed with ease of learning in mind. It's very user friendly and has a lot of applications outside of the ones we are interested in.\nIn The Complete Course for Machine Learning Engineers we are going to start with the basics. You'll learn how to install Python all the way through building a simple deep learning model using the skills you've learned.\nAs you learn Python you'll be completing labs that will build on what you've learned in the previous lesson so please don't skip any.\n*Five Reasons to take this Course.*\n1) You Want to be a Machine Learning Engineer\nIt's one of the most sought after careers in the world. The growth potential career wise is second to none. You want the freedom to move anywhere you'd like. You want to be compensated for your efforts. You want to be able to work remotely. The list of benefits goes on. Without a solid understanding of Python you'll have a hard time of securing a position as a machine learning engineer.\n2) The Google Certified Data Engineer\nGoogle is always ahead of the game. If you were to look back at a timeline of their accomplishments in the data space you might believe they have a crystal ball. They've been a decade ahead of everyone.  Now, they are the first and the only cloud vendor to have a data engineering certification. With their track record I'll go with Google.  You can't become a data engineer without learning Python.\n3) The Growth of Data is Insane\nNinety percent of all the world's data has been created in the last two years. Business around the world generate approximately 450 billion transactions a day. The amount of data collected by all organizations is approximately 2.5 exabytes a day. That number doubles every month.  Almost all real world machine learning is supervised. That means you point your machine learning models at clean tabular data. Python has libraries that are specific to data cleansing.\n4) Machine Learning in Plain English\nMachine learning is one of the hottest careers on the planet and understanding the basics is required to attaining a job as a data engineer.  Google expects data engineers and their machine learning engineers to be able to build machine learning models. In this course, you'll learn enough Python to be able to build a deep learning model.\n5) You want to be ahead of the Curve\nThe data engineer and machine learning engineer roles are fairly new.  While you’re learning, building your skills and becoming certified you are also the first to be part of this burgeoning field.  You know that the first to be certified means the first to be hired and first to receive the top compensation package.\nThanks for interest in The Complete Python Course for Machine Learning Engineers\nSee you in the course!!",
      "target_audience": [
        "If you want to become a machine learning engineer then this course is for you.",
        "If you want to learn the basics of Python then this courses is for.",
        "If you want something beyond the typical lecture style course then this course is for you."
      ]
    },
    {
      "title": "LLM Mastery: Hands-on Code, Align and Master LLMs",
      "url": "https://www.udemy.com/course/llm-mastery-hands-on-code-align-and-master-llms/",
      "bio": "Code LLMs and alignment from scratch with Python and PyTorch. And explore generative AI and deep learning using Origami",
      "objectives": [
        "Code and train an LLM from scratch, line by line, understanding every concept in detail",
        "Understand and analyze in depth an advanced LLM architecture based on the Llama system",
        "Code and train an alignment process from scratch, to align an LLM with a preferred form of interaction",
        "Understand in great depth key concepts like the Attention Mechanisms, the Cross Entropy Loss, the way neural networks learn and many more",
        "Explore in depth insights about deep learning and neural networks through the use of Origami",
        "Visual understanding: Dive deep into the core of an LLM and visually explore how its attention matrices evolve throughout the training process.",
        "In addition to the coding, every section includes in-depth explanations of key concepts related to these architectures and generative AI"
      ],
      "course_content": {
        "Introduction to Generative AI": [
          "Welcome to the course",
          "Why we will start by introducing Generative AI concepts",
          "Introducing myself",
          "Generative modelling, Evolution of Generative AI and Overview of applications",
          "Building Blocks of Machine Creativity: Machine Learning Foundations for Gen AI",
          "Architectures of Machine Imagination, from GANs to Diffusion and beyond",
          "Machine Creativity Meets Real-World Impact - Applications of Generative AI",
          "The Ethics of Machine Creativity: Challenges and Considerations in Generative AI",
          "Worlds Reimagined: Visions of the Future with Generative AI",
          "Summary and closing thoughts about this intro of GenAI",
          "Review Quiz"
        ],
        "Coding a small LLM from scratch, understanding all the key concepts involved": [
          "Welcome to this section",
          "Where to do the coding - Intro",
          "Where to do the coding - Details",
          "Dealing with challenges, and reminder about coding options",
          "Setting up the coding environment",
          "How Jupyter Notebooks work",
          "Importing the necessary libraries",
          "Setting up our base files",
          "Setting up the parameters of the architecture",
          "Exploring the crucial hyperparameters",
          "Key parameters for an effective training process",
          "Introducing Logging",
          "Setting up logging",
          "Setting up the tokenizer and related functionality",
          "Splitting our data and creating our get batch function",
          "The Transformer Architecture",
          "Declaring the top layers of the LLM",
          "The forward function of the LLM",
          "The Cross Entropy Loss with Pytorch",
          "The Cross Entropy Loss recreated manually",
          "From Information to Cross-Entropy - Deep Dive",
          "Completing and verifying the manual cross entropy loss",
          "Generating new samples - Intro",
          "Creating the functionality to generate new samples",
          "Testing the sample generation functionality",
          "Coding the blocks of the LLM architecture",
          "Communication plus Computation",
          "Providing computational power to the LLM",
          "The Multi Head Attention Mechanism",
          "Attention is all you need",
          "Coding and understanding the attention head",
          "Understanding attention - deep manual dive",
          "Review and debugging example",
          "Evaluating the performance with more precision",
          "Setting up the Optimizer and Scheduler",
          "Loading checkpoints for Inference or to restart trainings",
          "Loading and testing a pre-trained checkpoint",
          "Coding the learning process - Intro",
          "The training loop",
          "Training our LLM",
          "Keeping in mind the scale of our LLM",
          "Training the tokenizer",
          "Encoding our dataset with the tokenizer",
          "Conclusions and what comes next"
        ],
        "Into the AI Matrix, exploring advanced visualizations of the core of your LLM": [
          "Into the AI matrix: Introducing our journey to the core of LLMs",
          "Advanced visualizations of LLM cores: a trailer",
          "Deep dive: attention matrices at the start of LLM training",
          "Tracking peak attention between a specific pair of tokens",
          "Tracking peak attention between semantically meaningful connections",
          "Exploring a set of semantic and structural connections through the training",
          "Visualizing the capture of local sequential dependencies",
          "The importance of understanding the attention matrix",
          "Long-term evolution of attention pattern dynamics"
        ],
        "Understanding the code and concepts of an Advanced LLM": [
          "Welcome to a deep dive through an advanced LLM architecture",
          "Setting up a new environment and hosting the support files",
          "Declaring the main parameters of the model",
          "Main structure and loss calculation",
          "Advanced generation using extra parameters",
          "The main blocks of the architecture",
          "Analyzing the computational layers of the LLM",
          "An efficient attention implementation, part 1",
          "An efficient attention implementation, part 2",
          "Exploring rotary positional embeddings and other supporting functions",
          "Analyzing the inference code",
          "Preparing to run inference on the cloud and locally",
          "Inference on non-aligned vs aligned versions of the model",
          "Further reflections on the inference results"
        ],
        "Coding an alignment process from scratch, understanding all the key concepts": [
          "The importance of alignment",
          "The pretraining and alignment datasets",
          "Importing the necessary libraries",
          "Setting up the parameters for the alignment process",
          "Setting up the chat template for the tokenizing process",
          "Filtering our alignment dataset",
          "Pre-processing and Tokenizing the alignment dataset",
          "Debugging and completing the pre-processing function",
          "Splitting the alignment data and creating our dataloaders",
          "Setting up the model and optimizer for the alignment training process",
          "Setting up our scheduler function",
          "Coding the training loop of the alignment process",
          "Coding the alignment loss calculation - part 1",
          "Understanding how we will favor aligned responses - Deep Dive",
          "Coding the alignment loss calculation - part 2",
          "Coding the alignment loss calculation - part 3",
          "Adding logging, checkpoint saving and launching the training",
          "Training and testing the alignment, analyzing and expanding the stats",
          "Adding new code to calculate more precise training and validation losses",
          "Comparing training and validation charts - Deep Dive",
          "Alignment wrap-up",
          "The path towards alignment",
          "Congrats, summary, and what's next"
        ],
        "Origami + AI: Learning key insights about neural networks and AI with Origami": [
          "Welcome to this original origami based journey to the core of AI",
          "In Search of the Magical Mappings of Creativity, using Origami!",
          "The Search for the Perfect Mapping: datasets and dimensionality",
          "From Linearity to Complexity: Neural Networks and the Nonlinearities of Life",
          "Bending the Rules: Non-Linear transformations and the key to complexity",
          "Not Too Tight, Not Too Loose - Finding the perfect fit",
          "How increasing the dimensionality impacts the latent complexity of the network",
          "The Power of Depth: Creating Sophisticated Mappings with AI networks",
          "From high dimensional manifolds to dynamic and ever changing latent spaces",
          "Advanced digital representations of the latent complexity of neural networks",
          "Visualizing the Journey: Loss Landscapes and the Search for Optimal Weights",
          "Example of the dynamic Loss Landscape of a generative adversarial network",
          "Lucy - Real Time Visualization of the changing weights of a neural network",
          "Charting the hidden depths: a recap of our transformative latent space journey",
          "Summary of the last sections",
          "Review Quiz"
        ],
        "Activating the Generative Model of your own mind": [
          "Introducing our final journey",
          "A guided visualization experience to exercise the generative model in your head",
          "Intro to the journey to the center of the neuron",
          "The container, the salty ocean and the 150000 cortical columns",
          "Visualizing the pyramidal neuron",
          "The Synapse, visualizing the input-output interface",
          "Biological vs Artificial Neurons: Inputs, Outputs, Speed, etc",
          "Learning in biological and artificial neurons",
          "Planning, decision making and world models",
          "Efficiency: sparsity in biological vs artificial networks",
          "Consciousness: within the neurons",
          "The future, towards AGI / ASI",
          "Conclusion and congratulations"
        ]
      },
      "requirements": [
        "Basic knowledge of python. It's enough with the very basics, as we will code every little thing together, line by line",
        "You can code either on a free online platform like Google Colab, or using your local Laptop or Desktop",
        "Lots of enthusiasm. We are going to go really deep into both the code and the concepts around the code. You can grow and stretch your knowledge and experience a lot if you stay focused and motivated throughout the course, let's do it :)"
      ],
      "description": "Dive into the most exhilarating and hands-on LLM course you'll ever experience! This isn't just learning—it's an adventure that will transform you from an AI enthusiast into a creator at the bleeding edge of technology. AI expert Javier Ideami, the creator of one of the most successful AI related courses on Udemy, brings you a totally unique new experience around LLM technology.\nWhat Makes This Course Unmissable:\nCode Your Own AI Universe: 80% hands-on coding with Python and Pytorch. Build an LLM from scratch, line by line. Watch AI come alive through your fingertips. And then go beyond and code a compact version of an alignment process, the magic that makes ChatGPT, GPT 4, Claude and Gemini possible.\nOrigami Meets AI: Be part of a world-first! Unravel deep learning mysteries through the art of paper folding.\nVisual understanding: Combine the coding with visual exploration by going deep into the core of an LLM, seeing how its attention matrices evolve throughout training.\nDeep Dive, Gradual Learning Curve: Only basic Python needed. We'll guide you through all the complex concepts around LLMs, from attention mechanisms to cross-entropy and beyond. By the end of the course, you will have gained advanced skills and knowledge about generative AI and LLMs.\nMind-Bending Finale: Cap it off with an optional guided meditation using the \"generative AI\" in your own brain. Mind = Blown!\nFlexible requirements: Run everything on a humble 4GB GPU or anything more powerful. From Google Colab to your trusty laptop, flexibility is the mantra. On the cloud or local (Windows / Linux / Mac). All platforms and devices will work (with a minimum of 4GB of GPU memory)\nCourse Highlights:\nIntro to Generative AI: Dive into the mesmerizing world of Generative AI, where machines create and innovate beyond imagination.\nCode an LLM from Scratch: Code and nurture your very own LLM from scratch.\nUnlocking an LLM Titan: Dissect an advanced LLM architecture. Peek behind the curtain of the most powerful AI systems on the planet.\nAlignment. Code the Secret Sauce of the top LLMs: Code a cutting edge LLM alignment process. This is the crucial stage that makes ChatGPT, GPT 4, Claude and Gemini possible and we code together a cutting edge variation of it.\nOrigami AI: Fold your way to neural network nirvana. Experience a world-first fusion of ancient art and cutting-edge science to grasp deep learning like never before.\nAdvanced Visualizations of the Attention Mechanisms of LLMs: Understand visually how the core of an LLM, its attention matrices, evolves throughout the training process.\nAI Meets Zen: Cap your journey with an optional mind-bending guided meditation. Explore the ultimate generative AI - the one in your own brain - in a profound finale that bridges technology and spirituality.\nAll in One / Why You Can't Miss This:\nThe full package: You code both a small LLM as well as a cutting edge alignment technique. You also go deep into the understanding of a complex LLM architecture. In parallel you dive very deeply into all sorts of complex concepts around LLMs and deep learning, both during the coding and also during the unique Origami experience as well as the initial intro to Generative AI.\nUniqueness: Origami + AI = A learning experience you won't find anywhere else. Understand key insights about Deep Learning and Neural Networks through the magic of paper folding.\nPractical Hands-on Mastery: 80% Practical. Learn and Build, Train and Align.\nFuture-Proof Skills: Position yourself at the forefront of the AI revolution.\nAnd there's more: Added to all of that, the course connects you with free tools, articles and infographics produced by Ideami that enrich and accelerate your learning even more. Some of then, like the Loss Landscape explorer app, are unique tools in the world, created by Ideami for you.\nAccessibility: Complex concepts explained so clearly, you'll feel like an AI whisperer.\nIn summary\nThis isn't just a course—it's a ticket to the AI creator's club. By the end, you'll have coded an LLM, understood its deepest secrets, coded an alignment technique, dived deep into profound insights about deep learning and gained practical skills that will make you the AI guru in any room.\nReady to code the future, fold profound insights through origami and blow your own mind? Join us on this unparalleled journey to the heart of LLM technology. Enroll now and prepare for the most fun, deep, and transformative tech adventure of your life!",
      "target_audience": [
        "AI Enthusiasts and Developers: Individuals with a passion for artificial intelligence and a basic knowledge of Python who want to dive deep into the world of LLMs and generative AI.",
        "Tech Innovators and Creators: Aspiring AI developers who wish to be at the cutting edge of technology, learning to build and understand complex AI systems from the ground up.",
        "Students and Professionals in AI: Those studying or working in AI-related fields who seek to enhance their practical skills and theoretical understanding of LLMs and deep learning.",
        "Curious Minds with Creative Flair: Learners interested in a unique blend of technology and art, who are eager to explore deep learning concepts through innovative methods like origami.",
        "Software engineers: interested in understanding and coding LLMs as well as implementing AI alignment techniques with LLMs",
        "General: People that are curious about how LLMs work and want to understand them in great depth",
        "General: People that want to stretch their generative AI experience using Python and Pytorch to program LLMs and alignment processes",
        "General: People that want to know all the key parts of how ChatGPT, Gemini or Claude work internally, in order to get inspired about new ways of applying this technology",
        "General: Those that want a deep introduction to Generative AI, deep learning and neural networks",
        "General: Those that want to deeply understand how neural networks learn in a fun way and unique way through Origami"
      ]
    },
    {
      "title": "Data Science: Modern Deep Learning in Python",
      "url": "https://www.udemy.com/course/data-science-deep-learning-in-theano-tensorflow/",
      "bio": "Build with modern libraries like Tensorflow, Theano, Keras, PyTorch, CNTK, MXNet. Train faster with GPU on AWS.",
      "objectives": [
        "Apply momentum to backpropagation to train neural networks",
        "Apply adaptive learning rate procedures like AdaGrad, RMSprop, and Adam to backpropagation to train neural networks",
        "Understand the basic building blocks of TensorFlow",
        "Build a neural network in TensorFlow",
        "Write a neural network using Keras",
        "Write a neural network using PyTorch",
        "Understand the difference between full gradient descent, batch gradient descent, and stochastic gradient descent",
        "Understand and implement dropout regularization",
        "Understand and implement batch normalization",
        "Understand the basic building blocks of Theano",
        "Build a neural network in Theano",
        "Write a neural network using CNTK",
        "Write a neural network using MXNet",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Introduction and Outline": [
          "Introduction and Outline",
          "Where to get the Code",
          "How to Succeed in this Course"
        ],
        "Review": [
          "Review (pt 1): Neuron Predictions",
          "Review (pt 2): Neuron Learning",
          "Review (pt 3): Artificial Neural Networks",
          "Review Exercise Prompt",
          "Review Code (pt 1)",
          "Review Code (pt 2)",
          "Review Summary"
        ],
        "Stochastic Gradient Descent and Mini-Batch Gradient Descent": [
          "Stochastic Gradient Descent and Mini-Batch Gradient Descent (Theory)",
          "SGD Exercise Prompt",
          "Stochastic Gradient Descent and Mini-Batch Gradient Descent (Code pt 1)",
          "Stochastic Gradient Descent and Mini-Batch Gradient Descent (Code pt 2)"
        ],
        "Momentum and adaptive learning rates": [
          "Using Momentum to Speed Up Training",
          "Nesterov Momentum",
          "Momentum in Code",
          "Variable and adaptive learning rates",
          "Constant learning rate vs. RMSProp in Code",
          "Adam Optimization (pt 1)",
          "Adam Optimization (pt 2)",
          "Adam in Code",
          "Suggestion Box"
        ],
        "Choosing Hyperparameters": [
          "Hyperparameter Optimization: Cross-validation, Grid Search, and Random Search",
          "Sampling Logarithmically",
          "Grid Search in Code",
          "Modifying Grid Search",
          "Random Search in Code"
        ],
        "Weight Initialization": [
          "Weight Initialization Section Introduction",
          "Vanishing and Exploding Gradients",
          "Weight Initialization",
          "Local vs. Global Minima",
          "Weight Initialization Section Summary"
        ],
        "Theano": [
          "Theano Basics: Variables, Functions, Expressions, Optimization",
          "Building a neural network in Theano",
          "Is Theano Dead?"
        ],
        "TensorFlow": [
          "TensorFlow Basics: Variables, Functions, Expressions, Optimization",
          "Building a neural network in TensorFlow",
          "What is a Session? (And more)"
        ],
        "GPU Speedup, Homework, and Other Misc Topics": [
          "Setting up a GPU Instance on Amazon Web Services",
          "Installing NVIDIA GPU-Accelerated Deep Learning Libraries on your Home Computer",
          "Can Big Data be used to Speed Up Backpropagation?",
          "How to Improve your Theano and Tensorflow Skills",
          "Theano vs. TensorFlow"
        ],
        "Transition to the 2nd Half of the Course": [
          "Transition to the 2nd Half of the Course"
        ]
      },
      "requirements": [
        "Be comfortable with Python, Numpy, and Matplotlib",
        "If you do not yet know about gradient descent, backprop, and softmax, take my earlier course, Deep Learning in Python, and then return to this course."
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis course continues where my first course, Deep Learning in Python, left off. You already know how to build an artificial neural network in Python, and you have a plug-and-play script that you can use for TensorFlow. Neural networks are one of the staples of machine learning, and they are always a top contender in Kaggle contests. If you want to improve your skills with neural networks and deep learning, this is the course for you.\nYou already learned about backpropagation, but there were a lot of unanswered questions. How can you modify it to improve training speed? In this course you will learn about batch and stochastic gradient descent, two commonly used techniques that allow you to train on just a small sample of the data at each iteration, greatly speeding up training time.\nYou will also learn about momentum, which can be helpful for carrying you through local minima and prevent you from having to be too conservative with your learning rate. You will also learn about adaptive learning rate techniques like AdaGrad, RMSprop, and Adam which can also help speed up your training.\nBecause you already know about the fundamentals of neural networks, we are going to talk about more modern techniques, like dropout regularization and batch normalization, which we will implement in both TensorFlow and Theano. The course is constantly being updated and more advanced regularization techniques are coming in the near future.\n\nIn my last course, I just wanted to give you a little sneak peak at TensorFlow. In this course we are going to start from the basics so you understand exactly what's going on - what are TensorFlow variables and expressions and how can you use these building blocks to create a neural network? We are also going to look at a library that's been around much longer and is very popular for deep learning - Theano. With this library we will also examine the basic building blocks - variables, expressions, and functions - so that you can build neural networks in Theano with confidence.\nTheano was the predecessor to all modern deep learning libraries today. Today, we have almost TOO MANY options. Keras, PyTorch, CNTK (Microsoft), MXNet (Amazon / Apache), etc. In this course, we cover all of these! Pick and choose the one you love best.\nBecause one of the main advantages of TensorFlow and Theano is the ability to use the GPU to speed up training, I will show you how to set up a GPU-instance on AWS and compare the speed of CPU vs GPU for training a deep neural network.\nWith all this extra speed, we are going to look at a real dataset - the famous MNIST dataset (images of handwritten digits) and compare against various benchmarks. This is THE dataset researchers look at first when they want to ask the question, \"does this thing work?\"\nThese images are important part of deep learning history and are still used for testing today. Every deep learning expert should know them well.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nKnow about gradient descent\nProbability and statistics\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\nKnow how to write a neural network with Numpy\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "Students and professionals who want to deepen their machine learning knowledge",
        "Data scientists who want to learn more about deep learning",
        "Data scientists who already know about backpropagation and gradient descent and want to improve it with stochastic batch training, momentum, and adaptive learning rate procedures like RMSprop",
        "Those who do not yet know about backpropagation or softmax should take my earlier course, deep learning in Python, first"
      ]
    },
    {
      "title": "Neural Networks in Python: Deep Learning for Beginners",
      "url": "https://www.udemy.com/course/neural-network-understanding-and-building-an-ann-in-python/",
      "bio": "Learn Artificial Neural Networks (ANN) in Python. Build predictive deep learning models using Keras & Tensorflow| Python",
      "objectives": [
        "Get a solid understanding of Artificial Neural Networks (ANN) and Deep Learning",
        "Understand the business scenarios where Artificial Neural Networks (ANN) is applicable",
        "Building a Artificial Neural Networks (ANN) in Python",
        "Use Artificial Neural Networks (ANN) to make predictions",
        "Learn usage of Keras and Tensorflow libraries",
        "Use Pandas DataFrames to manipulate data and make statistical computations."
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course",
          "Introduction to Neural Networks and Course flow",
          "Course resources",
          "This is a Milestone!"
        ],
        "Setting up Python and Jupyter Notebook": [
          "Installing Python and Anaconda",
          "Opening Jupyter Notebook",
          "Introduction to Jupyter",
          "Arithmetic operators in Python",
          "Quick coding exercise on arithmetic operators",
          "Strings in Python: Python Basics",
          "Quick coding exercise on String operations",
          "Lists, Tuples and Directories: Python Basics",
          "Quick coding exercise on Tuples",
          "Quiz"
        ],
        "Important Python Libraries": [
          "Working with Numpy Library of Python",
          "Quick coding exercise on NumPy Library",
          "Working with Pandas Library of Python",
          "Quick coding exercise on Pandas Library",
          "Working with Seaborn Library of Python",
          "Python file for additional practice"
        ],
        "Integrating ChatGPT with Python": [
          "Integrating ChatGPT with Jupyter Notebook"
        ],
        "Single Cells - Perceptron and Sigmoid Neuron": [
          "Perceptron",
          "Activation Functions",
          "Python - Creating Perceptron model",
          "Quiz"
        ],
        "Neural Networks - Stacking cells to create network": [
          "Basic Terminologies",
          "Gradient Descent",
          "Back Propagation",
          "Quiz"
        ],
        "Important concepts: Common Interview questions": [
          "Some Important Concepts",
          "Quiz"
        ],
        "Standard Model Parameters": [
          "Hyperparameters",
          "Quiz"
        ],
        "Practice Test": [
          "Test your conceptual understanding"
        ],
        "Tensorflow and Keras": [
          "Keras and Tensorflow",
          "Installing Tensorflow and Keras",
          "Quiz"
        ]
      },
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the sameS"
      ],
      "description": "You're looking for a complete Artificial Neural Network (ANN) course that teaches you everything you need to create a Neural Network model in Python, right?\nYou've found the right Neural Networks course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using Neural network Models.\nHave a clear understanding of Advanced Neural network concepts such as Gradient Descent, forward and Backward Propagation etc.\nCreate Neural network models in Python using Keras and Tensorflow libraries and analyze their results.\nConfidently practice, discuss and understand Deep Learning concepts\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Neural networks course.\nIf you are a business Analyst or an executive, or a student who wants to learn and apply Deep learning in Real world problems of business, this course will give you a solid base for that by teaching you some of the most advanced concepts of Neural networks and their implementation in Python without getting too Mathematical.\nWhy should you choose this course?\nThis course covers all the steps that one should take to create a predictive model using Neural Networks.\nMost courses only focus on teaching how to run the analysis but we believe that having a strong theoretical understanding of the concepts enables us to create a good model . And after running the analysis, one should be able to judge how good the model is and interpret the results to actually be able to help the business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using Deep learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 250,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Practice test, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take practice test to check your understanding of concepts. There is a final practical assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Neural network based model i.e. a Deep Learning model, to solve business problems.\nBelow are the course contents of this course on ANN:\nPart 1 - Python basics\nThis part gets you started with Python.\nThis part will help you set up the python and Jupyter environment on your system and it'll teach you how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nPart 2 - Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nPart 3 - Creating Regression and Classification ANN model in Python\nIn this part you will learn how to create ANN models in Python.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. We also solve a regression problem in which we try to predict house prices in a location. We will also cover how to create complex ANN architectures using functional API. Lastly we learn how to save and restore models.\nWe also understand the importance of libraries such as Keras and TensorFlow in this part.\nPart 4 - Data Preprocessing\nIn this part you will learn what actions you need to take to prepare Data for the analysis, these steps are very important for creating a meaningful.\nIn this section, we will start with the basic theory of decision tree then we cover data pre-processing topics like  missing value imputation, variable transformation and Test-Train split.\nPart 5 - Classic ML technique - Linear Regression\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you\nunderstand where the concept is coming from and how it is important. But even if you don't understand\nit,  it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a Neural Network model in Python will soar. You'll have a thorough understanding of how to use ANN to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow are some popular FAQs of students who want to start their Deep learning journey-\n\n\nWhy use Python for Deep Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Deep Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nDeep Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Neural Network journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master ANN from Beginner level in short span of time"
      ]
    },
    {
      "title": "Complete Generative AI Course With Langchain and Huggingface",
      "url": "https://www.udemy.com/course/complete-generative-ai-course-with-langchain-and-huggingface/",
      "bio": "Complete Guide to Building, Deploying, and Optimizing Generative AI with Langchain and Huggingface",
      "objectives": [
        "Learn to create advanced generative AI applications leveraging the Langchain framework and Huggingface's state-of-the-art models.",
        "Understand the architecture and design patterns for building robust generative AI systems.",
        "Gain hands-on experience in deploying generative AI models to various environments, including cloud platforms and on-premise servers.",
        "Explore different deployment strategies, ensuring scalability and reliability of AI applications.",
        "Develop Retrieval-Augmented Generation (RAG) pipelines to enhance the performance and accuracy of generative models by integrating retrieval mechanisms.",
        "Learn to seamlessly incorporate Huggingface's pre-trained models into Langchain applications, leveraging their powerful NLP capabilities.",
        "Customize and fine-tune Huggingface models to fit specific application requirements and use cases.",
        "Work on real-world projects that illustrate the application of generative AI in various domains, such as chatbots, content generation, and data augmentation."
      ],
      "course_content": {
        "Introduction": [
          "Introduction-What We will Learn In This Course",
          "Course Materials",
          "Getting Started With VS Code",
          "Different Ways Of creating Python Environment",
          "Solve-Conda Not Recognized Issue",
          "Python Basics-Syntax And Semantics",
          "Variables In Python",
          "Basics DataTypes In Python",
          "Operators In Python"
        ],
        "Python Control Flow": [
          "Conditional Statements (if, elif, else)",
          "Loops In Python"
        ],
        "Data Structures Using Python": [
          "Lists and List Comprehension In Python",
          "Tuples In Python",
          "Dictionaries In Python",
          "Real World Use cases Of List"
        ],
        "Functions In Python": [
          "Getting Started With Functions",
          "More Coding Examples With Functions",
          "Lambda Function In Python",
          "Map Function In Python",
          "Filter Functions In Python"
        ],
        "Importing, Creating Modules And Packages": [
          "Import Modules And Packages In Python",
          "Standard Libraries Overview In Python"
        ],
        "File Handling In Python": [
          "File Operations With Python",
          "Working with File Paths"
        ],
        "Exception Handling": [
          "Exceptiion Handling With try except else and finally blocks"
        ],
        "OOPS Classes And Objects": [
          "Classes And Objects In Python",
          "Single And Multiple Inheritance",
          "Polymorphism In OOPS",
          "Encapulation In OOPS",
          "Abstraction In OOPS",
          "Magic Methods In Python",
          "Operator Overloading In Python"
        ],
        "Streamlit With Python": [
          "Getting Started With Streamlit",
          "Example Of ML APP With Streamlit"
        ],
        "Machine Learning For NLP (Prerequisites)": [
          "Roadmap To Learn NLP",
          "Practical Usecases Of NLP",
          "Tokenization and Basic Terminologies",
          "Tokenization Practicals",
          "Text Preprocessing Stemming Uing NLTK",
          "Text Preprocessing Lemmatization",
          "Text Preprocessing Stopwords",
          "Parts Of Speech Tagging Using NLTK",
          "Named Entity Recognition",
          "Whats Next",
          "One Hot Encoding",
          "Advantages and Disadvantages of OHE",
          "Bag Of Words Intuition",
          "Advantages and Disadvantages Of BOW",
          "BOW Implementation Using NLTK",
          "N Grams",
          "N gram Implementation USing NLTK",
          "TF-IDF Intuition",
          "Advantages and Disadvanatges OF TFidf",
          "TFIDF Practical Implementation",
          "Word Embeddings",
          "Word2vec Intuition",
          "Word2vec CBOW Detailed Explanation",
          "SkipGram Indepth Intuition",
          "Advantages OF Word2vec",
          "Average Word2Vec",
          "Word2vec Practical Implementation"
        ]
      },
      "requirements": [
        "Familiarity with Python programming language, including basic syntax, data structures, and functions.",
        "Basic understanding of machine learning concepts, including supervised and unsupervised learning, and fundamental AI principles.",
        "Basic knowledge of deep learning concepts and frameworks (e.g., TensorFlow or PyTorch) is advantageous but not strictly required.",
        "Understanding of APIs and how to interact with them, as the course will involve integrating various APIs for model deployment and usage.",
        "Ability to navigate and execute commands in a command line interface (CLI) or terminal."
      ],
      "description": "Unlock the full potential of Generative AI with our comprehensive course, \"Complete Generative AI Course with Langchain and Huggingface.\" This course is designed to take you from the basics to advanced concepts, providing hands-on experience in building, deploying, and optimizing AI models using Langchain and Huggingface. Perfect for AI enthusiasts, developers, and professionals, this course offers a practical approach to mastering Generative AI.\nWhat You Will Learn:\nIntroduction to Generative AI:\nUnderstand the fundamentals of Generative AI and its applications.\nExplore the differences between traditional AI models and generative models.\nGetting Started with Langchain:\nLearn the basics of Langchain and its role in AI development.\nSet up your development environment and tools.\nHuggingface Integration:\nIntegrate Huggingface's state-of-the-art models into your Langchain projects.\nCustomize and fine-tune Huggingface models for specific applications.\nBuilding Generative AI Applications:\nStep-by-step tutorials on creating advanced generative AI applications.\nReal-world projects such as chatbots, content generators, and data augmentation tools.\nDeployment Strategies:\nLearn various deployment strategies for AI models.\nDeploy your models to cloud platforms and on-premise servers for scalability and reliability.\nRAG Pipelines:\nDevelop Retrieval-Augmented Generation (RAG) pipelines to enhance AI performance.\nCombine generative models with retrieval systems for improved information access.\nOptimizing AI Models:\nTechniques for monitoring and optimizing deployed AI models.\nBest practices for maintaining and updating AI systems.\nEnd-to-End Projects:\nHands-on projects that provide real-world experience.\nBuild, deploy, and optimize AI applications from scratch.\nWho Should Take This Course:\nAI and Machine Learning Enthusiasts\nData Scientists and Machine Learning Engineers\nSoftware Developers and Engineers\nNLP Practitioners\nStudents and Academics\nTechnical Entrepreneurs and Innovators\nAI Hobbyists\nBy the end of this course, you will have the knowledge and skills to build, deploy, and optimize generative AI applications, leveraging the power of Langchain and Huggingface. Join us on this exciting journey and become a master in Generative AI!",
      "target_audience": [
        "Individuals passionate about AI and ML who want to expand their knowledge and skills in generative AI applications.",
        "Professionals looking to enhance their expertise in building and deploying generative AI models, particularly using Langchain and Huggingface.",
        "Developers interested in integrating advanced AI capabilities into their applications and learning about the deployment and optimization of AI models."
      ]
    },
    {
      "title": "R Level 1 - Data Analytics with R",
      "url": "https://www.udemy.com/course/r-level1/",
      "bio": "Use R for Data Analytics and Data Mining",
      "objectives": [
        "this course will show you how the most common types of graphs can be produced with R base",
        "you will get a good understanding of functions and loops in R which are very useful programming skills to have",
        "you will get the necessary theoretical background for R",
        "you will learn how to create and handle different types of objects",
        "you will get fluent in the R programming language to master your specific quantitative tasks"
      ],
      "course_content": {
        "Introduction - the very Basics of R": [
          "R Level 1 Intro",
          "Download R and RStudio",
          "RStudio Orientation",
          "The Structure of the R Ecosystem",
          "R Help Features",
          "R Basics Script",
          "Using the R Functions",
          "R Exercises",
          "Three Common Mistakes of R Beginners",
          "Your first Lines of R",
          "Using some basic Functions",
          "Exercise and Solutions - Basic Coding",
          "R Datasets and Dataframes",
          "Basic Graphs in R 1",
          "Basic Graphs in R 2",
          "Exercise and Solutions - Basic Graphs",
          "Thoughts on Learning R"
        ],
        "Theoretical background": [
          "Operators in R",
          "Operators script",
          "Object types",
          "Object types script",
          "Data types",
          "Data types script",
          "Random number generation",
          "Exercise random number generation",
          "Solution random number generation",
          "Random number generation script",
          "Export/Import of Excel files",
          "Data export/import script"
        ],
        "Creating objects": [
          "Matrices part 1",
          "Matrices part 2",
          "Exercise matrices",
          "Solution matrices",
          "Matrices script",
          "Lists in R",
          "Exercise and solution Lists in R",
          "Lists script",
          "Data Frames part 1",
          "Data Frames part 2",
          "Factors",
          "Exercise and solution Data Frames",
          "Data frames script",
          "Factors script"
        ],
        "Functions in R": [
          "Intro to functions in R",
          "Open ended functions",
          "Ifelse statement and summary",
          "Exercise and solution functions",
          "Functions script",
          "Functions Script txt"
        ],
        "Loops": [
          "Introduction to loops and If statement",
          "For loop",
          "While loop",
          "Repeat loop and loop summary",
          "Exercise and solution loop part 1",
          "Exercise and solution loop part 2 - hard",
          "Loop script"
        ],
        "Apply family": [
          "Introduction to the apply family",
          "Tapply and the by command",
          "Eapply, sapply, lapply",
          "Vapply, replicate, mapply",
          "Rapply and summary",
          "Apply family exercises",
          "Apply family solutions",
          "Apply family script"
        ],
        "Graphs in R": [
          "Graphs in R",
          "Graphical parameters",
          "Boxplots",
          "Exporting graphs",
          "Exercise boxplots",
          "Solution boxplots",
          "Piecharts",
          "Histograms",
          "Exercise and solution histograms",
          "Advanced scatterplots and legends",
          "Exercise and solution scatterplots",
          "Graphs script"
        ],
        "Advanced topic: working with strings": [
          "Working with Strings - Introduction",
          "Working with Strings - gsub",
          "Working with Strings - gsub advanced",
          "Regular Expressions Overview",
          "Working with Strings - Library Stringr",
          "Exercise and Solution: Strings in R",
          "Code Section: Strings"
        ],
        "Using the R Commander GUI": [
          "R Commander Intro",
          "Installation Hints",
          "Getting Data into RCmdr",
          "Modeling with RCmdr"
        ]
      },
      "requirements": [
        "interest in statistical programming",
        "R and RStudio ready on your computer",
        "basic understanding of statistics and data structure"
      ],
      "description": "Are you new to R?\nDo you want to learn more about statistical programming?\nAre you in a quantitative field?\nYou just started learning R but you struggle with all the free but unorganized material available elsewhere?\nDo you want to hack the learning curve and stay ahead of your competition?\nIf your answer is YES to some of those points - read on!\nThis Tutorial is the first step - your Level 1 - to R mastery.\nAll the important aspects of statistical programming ranging from handling different data types to loops and functions, even graphs are covered.\nWhile planing this course I used the Pareto 80/20 principle. I filtered for the most useful items in the R language which will give you a quick and efficient learning experience.\nLearning R will help you conduct your projects. On the long run it is an invaluable skill which will enhance your career.\nYour journey will start with the theoretical background of object and data types. You will then learn how to handle the most common types of objects in R. Much emphasis is put on loops in R since this is a crucial part of statistical programming. It is also shown how the apply family of functions can be used for looping.\nIn the graphics section you will learn how to create and tailor your graphs. As an example we will create boxplots, histograms and piecharts. Since the graphs interface is quite the same for all types of graphs, this will give you a solid foundation.\nWith the R Commander you will also learn about an alternative to RStudio. Especially for classic hypothesis tests the R Commander GUI can save you some time.\n\nAccording to the teaching principles of R Tutorials every section is enforced with exercises for a better learning experience.\nYou can download the code pdf of every section to try the presented code on your own.\nThis tutorial is your first step to benefit from this open source software.\nWhat R you waiting for?\nMartin",
      "target_audience": [
        "scientists",
        "data analysts",
        "entrepreneurs",
        "web developers",
        "anybody interested in statistical programming"
      ]
    },
    {
      "title": "Artificial Intelligence: Reinforcement Learning in Python",
      "url": "https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/",
      "bio": "Complete guide to Reinforcement Learning, with Stock Trading and Online Advertising Applications",
      "objectives": [
        "Apply gradient-based supervised machine learning methods to reinforcement learning",
        "Understand reinforcement learning on a technical level",
        "Understand the relationship between reinforcement learning and psychology",
        "Implement 17 different reinforcement learning algorithms",
        "Understand important foundations for OpenAI ChatGPT, GPT-4"
      ],
      "course_content": {
        "Welcome": [
          "Introduction",
          "Course Outline and Big Picture",
          "Where to get the Code",
          "How to Succeed in this Course",
          "Warmup"
        ],
        "Return of the Multi-Armed Bandit": [
          "Section Introduction: The Explore-Exploit Dilemma",
          "Applications of the Explore-Exploit Dilemma",
          "Epsilon-Greedy Theory",
          "Calculating a Sample Mean (pt 1)",
          "Epsilon-Greedy Beginner's Exercise Prompt",
          "Designing Your Bandit Program",
          "Epsilon-Greedy in Code",
          "Comparing Different Epsilons",
          "Optimistic Initial Values Theory",
          "Optimistic Initial Values Beginner's Exercise Prompt",
          "Optimistic Initial Values Code",
          "UCB1 Theory",
          "UCB1 Beginner's Exercise Prompt",
          "UCB1 Code",
          "Bayesian Bandits / Thompson Sampling Theory (pt 1)",
          "Bayesian Bandits / Thompson Sampling Theory (pt 2)",
          "Thompson Sampling Beginner's Exercise Prompt",
          "Thompson Sampling Code",
          "Thompson Sampling With Gaussian Reward Theory",
          "Thompson Sampling With Gaussian Reward Code",
          "Exercise on Gaussian Rewards",
          "Why don't we just use a library?",
          "Nonstationary Bandits",
          "Bandit Summary, Real Data, and Online Learning",
          "(Optional) Alternative Bandit Designs",
          "Suggestion Box"
        ],
        "High Level Overview of Reinforcement Learning": [
          "What is Reinforcement Learning?",
          "From Bandits to Full Reinforcement Learning"
        ],
        "Markov Decision Proccesses": [
          "MDP Section Introduction",
          "Gridworld",
          "Choosing Rewards",
          "The Markov Property",
          "Markov Decision Processes (MDPs)",
          "Future Rewards",
          "Value Functions",
          "The Bellman Equation (pt 1)",
          "The Bellman Equation (pt 2)",
          "The Bellman Equation (pt 3)",
          "Bellman Examples",
          "Optimal Policy and Optimal Value Function (pt 1)",
          "Optimal Policy and Optimal Value Function (pt 2)",
          "MDP Summary"
        ],
        "Dynamic Programming": [
          "Dynamic Programming Section Introduction",
          "Iterative Policy Evaluation",
          "Designing Your RL Program",
          "Gridworld in Code",
          "Iterative Policy Evaluation in Code",
          "Windy Gridworld in Code",
          "Iterative Policy Evaluation for Windy Gridworld in Code",
          "Policy Improvement",
          "Policy Iteration",
          "Policy Iteration in Code",
          "Policy Iteration in Windy Gridworld",
          "Value Iteration",
          "Value Iteration in Code",
          "Dynamic Programming Summary"
        ],
        "Monte Carlo": [
          "Monte Carlo Intro",
          "Monte Carlo Policy Evaluation",
          "Monte Carlo Policy Evaluation in Code",
          "Monte Carlo Control",
          "Monte Carlo Control in Code",
          "Monte Carlo Control without Exploring Starts",
          "Monte Carlo Control without Exploring Starts in Code",
          "Monte Carlo Summary"
        ],
        "Temporal Difference Learning": [
          "Temporal Difference Introduction",
          "TD(0) Prediction",
          "TD(0) Prediction in Code",
          "SARSA",
          "SARSA in Code",
          "Q Learning",
          "Q Learning in Code",
          "TD Learning Section Summary"
        ],
        "Approximation Methods": [
          "Approximation Methods Section Introduction",
          "Linear Models for Reinforcement Learning",
          "Feature Engineering",
          "Approximation Methods for Prediction",
          "Approximation Methods for Prediction Code",
          "Approximation Methods for Control",
          "Approximation Methods for Control Code",
          "CartPole",
          "CartPole Code",
          "Approximation Methods Exercise",
          "Approximation Methods Section Summary"
        ],
        "Interlude: Common Beginner Questions": [
          "This Course vs. RL Book: What's the Difference?"
        ],
        "Stock Trading Project with Reinforcement Learning": [
          "Beginners, halt! Stop here if you skipped ahead",
          "Stock Trading Project Section Introduction",
          "Data and Environment",
          "How to Model Q for Q-Learning",
          "Design of the Program",
          "Code pt 1",
          "Code pt 2",
          "Code pt 3",
          "Code pt 4",
          "Stock Trading Project Discussion"
        ]
      },
      "requirements": [
        "Calculus (derivatives)",
        "Probability / Markov Models",
        "Numpy, Matplotlib",
        "Beneficial to have experience with at least a few supervised machine learning methods",
        "Gradient descent",
        "Good object-oriented programming skills"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT and GPT-4 really work? In this course, you will learn the foundations of these groundbreaking applications.\nWhen people talk about artificial intelligence, they usually don’t mean supervised and unsupervised machine learning.\nThese tasks are pretty trivial compared to what we think of AIs doing - playing chess and Go, driving cars, and beating video games at a superhuman level.\nReinforcement learning has recently become popular for doing all of that and more.\nMuch like deep learning, a lot of the theory was discovered in the 70s and 80s but it hasn’t been until recently that we’ve been able to observe first hand the amazing results that are possible.\nIn 2016 we saw Google’s AlphaGo beat the world Champion in Go.\nWe saw AIs playing video games like Doom and Super Mario.\nSelf-driving cars have started driving on real roads with other drivers and even carrying passengers (Uber), all without human assistance.\nIf that sounds amazing, brace yourself for the future because the law of accelerating returns dictates that this progress is only going to continue to increase exponentially.\nLearning about supervised and unsupervised machine learning is no small feat. To date I have over TWENTY FIVE (25!) courses just on those topics alone.\nAnd yet reinforcement learning opens up a whole new world. As you’ll learn in this course, the reinforcement learning paradigm is very from both supervised and unsupervised learning.\nIt’s led to new and amazing insights both in behavioral psychology and neuroscience. As you’ll learn in this course, there are many analogous processes when it comes to teaching an agent and teaching an animal or even a human. It’s the closest thing we have so far to a true artificial general intelligence.  What’s covered in this course?\nThe multi-armed bandit problem and the explore-exploit dilemma\nWays to calculate means and moving averages and their relationship to stochastic gradient descent\nMarkov Decision Processes (MDPs)\nDynamic Programming\nMonte Carlo\nTemporal Difference (TD) Learning (Q-Learning and SARSA)\nApproximation Methods (i.e. how to plug in a deep neural network or other differentiable model into your RL algorithm)\nHow to use OpenAI Gym, with zero code changes\nProject: Apply Q-Learning to build a stock trading bot\nIf you’re ready to take on a brand new challenge, and learn about AI techniques that you’ve never seen before in traditional supervised machine learning, unsupervised machine learning, or even deep learning, then this course is for you.\nSee you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nCalculus\nProbability\nObject-oriented programming\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations\nLinear regression\nGradient descent\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Anyone who wants to learn about artificial intelligence, data science, machine learning, and deep learning",
        "Both students and professionals"
      ]
    },
    {
      "title": "Reinforcement Learning beginner to master - AI in Python",
      "url": "https://www.udemy.com/course/beginner-master-rl-1/",
      "bio": "Build Artificial Intelligence (AI) agents using Deep Reinforcement Learning and PyTorch: A2C, REINFORCE, DQN, etc.",
      "objectives": [
        "Understand the Reinforcement Learning paradigm and the tasks that it's best suited to solve.",
        "Understand the process of solving a cognitive task using Reinforcement Learning",
        "Understand the different approaches to solving a task using Reinforcement Learning and choose the most fitting",
        "Implement Reinforcement Learning algorithms completely from scratch",
        "Fundamentally understand the learning process for each algorithm",
        "Debug and extend the algorithms presented",
        "Understand and implement new algorithms from research papers"
      ],
      "course_content": {
        "Welcome module": [
          "[IMPORTANT] English captions available for sections 1-4",
          "Welcome",
          "Reinforcement Learning series",
          "Course structure",
          "Environment setup [Important]",
          "Connect with me on social media",
          "Complete code"
        ],
        "The Markov decision process (MDP)": [
          "Elements common to all control tasks",
          "The Markov decision process (MDP)",
          "Types of Markov decision process",
          "Trajectory vs episode",
          "Reward vs Return",
          "Discount factor",
          "Policy",
          "State values v(s) and action values q(s,a)",
          "Bellman equations",
          "Solving a Markov decision process",
          "Setup - MDP in code",
          "MDP in code - Part 1",
          "MDP in code - Part 2"
        ],
        "Dynamic Programming": [
          "Introduction to Dynamic Programming",
          "Value iteration",
          "Setup - Value iteration",
          "Coding - Value iteration 1",
          "Coding - Value iteration 2",
          "Coding - Value iteration 3",
          "Coding - Value iteration 4",
          "Coding - Value iteration 5",
          "Policy iteration",
          "Policy evaluation",
          "Setup - Policy iteration",
          "Coding - Policy iteration 1",
          "Coding - Policy iteration 2",
          "Policy Improvement",
          "Coding - Policy iteration 3",
          "Coding - Policy iteration 4",
          "Policy iteration in practice",
          "Generalized Policy Iteration (GPI)"
        ],
        "Monte Carlo methods": [
          "Monte Carlo methods",
          "Solving control tasks with Monte Carlo methods",
          "On-policy Monte Carlo control",
          "Setup - On-policy Monte Carlo control",
          "Coding - On-policy Monte Carlo control 1",
          "Coding - On-policy Monte Carlo control 2",
          "Coding - On-policy Monte Carlo control 3",
          "Setup - Constant alpha Monte Carlo",
          "Coding - Constant alpha Monte Carlo",
          "Off-policy Monte Carlo control",
          "Setup - Off-policy Monte Carlo control",
          "Coding - Off-policy Monte Carlo 1",
          "Coding - Off-policy Monte Carlo 2",
          "Coding - Off-policy Monte Carlo 3"
        ],
        "Temporal difference methods": [
          "Temporal difference methods",
          "Solving control tasks with temporal difference methods",
          "Monte Carlo vs temporal difference methods",
          "SARSA",
          "Setup - SARSA",
          "Coding - SARSA 1",
          "Coding - SARSA 2",
          "Q-Learning",
          "Setup - Q-Learning",
          "Coding - Q-Learning 1",
          "Coding - Q-Learning 2",
          "Advantages of temporal difference methods"
        ],
        "N-step bootstrapping": [
          "N-step temporal difference methods",
          "Where do n-step methods fit?",
          "Effect of changing n",
          "N-step SARSA",
          "N-step SARSA in action",
          "Setup - n-step SARSA",
          "Coding - n-step SARSA"
        ],
        "Continuous state spaces": [
          "Setup - Classic control tasks",
          "Coding - Classic control tasks",
          "Working with continuous state spaces",
          "State aggregation",
          "Setup - Continuous state spaces",
          "Coding - State aggregation 1",
          "Coding - State aggregation 2",
          "Coding - State aggregation 3",
          "Tile coding",
          "Coding - Tile coding 1",
          "Coding - Tile coding 2",
          "Coding - Tile coding 3"
        ],
        "Brief introduction to neural networks": [
          "Function approximators",
          "Artificial Neural Networks",
          "Artificial Neurons",
          "How to represent a Neural Network",
          "Stochastic Gradient Descent",
          "Neural Network optimization"
        ],
        "Deep SARSA": [
          "Deep SARSA",
          "Neural Network optimization (Deep Q-Network)",
          "Experience Replay",
          "Target Network",
          "Setup - Deep SARSA",
          "Coding - Deep SARSA 1",
          "Coding - Deep SARSA 2",
          "Coding - Deep SARSA 3",
          "Coding - Deep SARSA 4",
          "Coding - Deep SARSA 4 (Addendum)",
          "Coding - Deep SARSA 5",
          "Coding - Deep SARSA 6",
          "Coding - Deep SARSA 7",
          "Coding - Deep SARSA 8",
          "Coding - Deep SARSA 9",
          "Coding -Deep SARSA 10"
        ],
        "Deep Q-Learning": [
          "Deep Q-Learning",
          "Setup - Deep Q-Learning",
          "Coding - Deep Q-Learning 1",
          "Coding - Deep Q-Learning 2",
          "Coding - Deep Q-Learning 3"
        ]
      },
      "requirements": [
        "Be comfortable programming in Python",
        "Know basic linear algebra and calculus (matrices, vectors, determinants, derivatives, etc.)",
        "Know basic statistics and probability theory (mean, variance, normal distribution, etc.)"
      ],
      "description": "This is the most complete Reinforcement Learning course on Udemy. In it you will learn the basics of Reinforcement Learning, one of the three paradigms of modern artificial intelligence. You will implement from scratch adaptive algorithms that solve control tasks based on experience. You will also learn to combine these algorithms with Deep Learning techniques and neural networks, giving rise to the branch known as Deep Reinforcement Learning.\n\n\nThis course will give you the foundation you need to be able to understand new algorithms as they emerge. It will also prepare you for the next courses in this series, in which we will go much deeper into different branches of Reinforcement Learning and look at some of the more advanced algorithms that exist.\n\n\nThe course is focused on developing practical skills. Therefore, after learning the most important concepts of each family of methods, we will implement one or more of their algorithms in jupyter notebooks, from scratch.\n\n\nThis course is divided into three parts and covers the following topics:\n\n\nPart 1 (Tabular methods):\n\n\n- Markov decision process\n\n\n- Dynamic programming\n\n\n- Monte Carlo methods\n\n\n- Time difference methods (SARSA, Q-Learning)\n\n\n- N-step bootstrapping\n\n\nPart 2 (Continuous state spaces):\n\n\n- State aggregation\n\n\n- Tile Coding\n\n\nPart 3 (Deep Reinforcement Learning):\n\n\n- Deep SARSA\n\n\n- Deep Q-Learning\n\n\n- REINFORCE\n\n\n- Advantage Actor-Critic / A2C (Advantage Actor-Critic / A2C method)",
      "target_audience": [
        "Developers who want to get a job in Machine Learning",
        "Data scientists/analysts and ML practitioners seeking to expand their breadth of knowledge.",
        "Researchers/scholars seeking to enhance their practical coding skills."
      ]
    },
    {
      "title": "Machine Learning for Absolute Beginners - Level 1",
      "url": "https://www.udemy.com/course/machine-learning-for-absolute-beginners-level-1/",
      "bio": "Learn the Fundamental Concepts of Artificial Intelligence and Machine Learning as the Next Game-Changing Technology",
      "objectives": [
        "Artificial Intelligence, Machine Learning and Deep Learning",
        "Applied vs. Generalized AI",
        "Features, Labels, Examples",
        "The Process of Training a Model",
        "Under-fitting and Over-fitting",
        "Supervised/Unsupervised Learning",
        "Classification and Regression",
        "Clustering and Dimension Reduction",
        "Reinforcement Learning",
        "Generative AI"
      ],
      "course_content": {
        "Getting Started with Level 1!": [
          "Welcome!",
          "Before you start...."
        ],
        "The Rise of Artificial Intelligence": [
          "AI is Coming...",
          "Artificial Intelligence",
          "Classical Programming",
          "Machine Learning",
          "Deep Learning",
          "Applied vs. Generalized AI",
          "Why Now?",
          "Quick Check-Point #1"
        ],
        "Introduction to Machine Learning": [
          "Overview - ML Terminology",
          "The “Black Box” Metaphor",
          "Features and Labels",
          "Training a Model",
          "Aiming for Generalization",
          "Quick Check-Point #2"
        ],
        "Classification of ML Systems": [
          "The Degree of Supervision",
          "#1 - Supervised Learning",
          "Classification",
          "Regression",
          "Quick Check-Point #3",
          "#2 - Unsupervised Learning",
          "Clustering",
          "Dimension Reduction",
          "Quick Check-Point #4",
          "#3 - Reinforcement Learning",
          "Decision-Making Agent",
          "Quick Check-Point #5"
        ],
        "**NEW** - The Magic Behind Generative AI": [
          "Introduction",
          "Artificial Neural Networks",
          "Deep Learning Architectures",
          "Foundation Models",
          "Large Language Models (LLMs)",
          "Model Types",
          "Prompt and Tokens",
          "Total Tokens and Context Window",
          "Next Token Please!",
          "Self-Supervised Learning",
          "Improving and Adapting LLMs",
          "Summary",
          "Quiz - Gen AI"
        ],
        "**NEW** - Generative AI - Key Challenges and Limitations": [
          "Introduction",
          "Prompt Sensitivity",
          "Knowledge Cutoff",
          "It is not Deterministic",
          "Structured Data",
          "Hallucinations",
          "Lack of Common Sense",
          "Bias and Fairness",
          "Data Privacy, Security, and Misuse",
          "Summary",
          "Quiz - Challenges and Limitations"
        ],
        "**NEW** - Unleash the Power of Generative AI": [
          "Introduction",
          "Text-Image-Video-Audio Generation",
          "Web-Based vs Application-Based (APIs)",
          "Use Case - Brainstorm Assistant",
          "Use Case - Summarization",
          "Use Case – Text Enhancement",
          "Use Case - Code Generation",
          "Use Case – Content as a Framework",
          "Use Case – Images on Demand",
          "Use Case – Boosting AI-Based Apps",
          "Best Practices for Prompts",
          "Summary"
        ],
        "Course Summary": [
          "Let's Recap and Thank You!",
          "** BONUS **"
        ]
      },
      "requirements": [
        "There are no specific prerequisites for starting this training as it is designed for absolute beginners."
      ],
      "description": "***** Feedback from Students ********\nAn excellent introduction to the topic. the lessons flowed logically and the course material was well presented. A very good course and a pleasure to take. Sam D.\n\"Good course for anyone who wants to make some sense of all the proper terminology and basic methodology of AI. Idan's explanations are very clear and to the point, no fluff and no distractions!\" Grace H.\n\"The course was actually amazing, giving me much more insight into AI. \" Patrick A\n\"best ML course ever. \" Parmanand S.\n\"Good and simple enough to start learning ML.\" Cogent Systems.\n**************************************\nMachine Learning - Next Terminator is Here...?\nThe concept of Artificial Intelligence is used in sci-fiction movies to describe a virtual entity that crossed some critical threshold point and developed self-awareness. And like any good Hollywood movie, this entity will turn against humankind. OMG! It’s a great concept to fuel our basic survival fear; otherwise, no one will buy a ticket to the next Terminator movie ;-)\nAs you may guess, things, in reality, are completely different. Artificial Intelligence is one of the biggest revolutions in the software industry. It is a mind-shift on how to develop software applications. Instead of using hard-coded rules for performing something, we let the machines learn things from data, decipher the complex patterns automatically, and then use it for multiple use cases.\nAI-Powered Applications\nThere are growing amounts of AI-powered applications in a variety of practical use cases. Websites are using AI to better recommend visitors about products and services. The ability to recognize objects in real-time video streams is driven by machine learning. It is a game-changing technology, and the game just started.\nSimplifying Things\nThe concept of AI and ML can be a little bit intimidating for beginners, and specifically for people without a substantial background in complex math and programming. This training is a soft starting point to walk you through the fundamental theoretical concepts.\nWe are going to open the mysterious AI/ML black box, and take a look inside, get more familiar with the terms being used in the industry. It is going to be a super interesting story. It is important to mention that there are no specific prerequisites for starting this training, and it is designed for absolute beginners.\nWould you like to join the upcoming Machine Learning revolution?",
      "target_audience": [
        "Anyone interested to get familiar with the AI/ML concepts",
        "Developers curious about data science",
        "AI - Product Managers, Project Managers, Engineers",
        "Data Scientists"
      ]
    },
    {
      "title": "Deep Learning Prerequisites: Logistic Regression in Python",
      "url": "https://www.udemy.com/course/data-science-logistic-regression-in-python/",
      "bio": "Data science, machine learning, and artificial intelligence in Python for students and professionals",
      "objectives": [
        "program logistic regression from scratch in Python",
        "describe how logistic regression is useful in data science",
        "derive the error and update rule for logistic regression",
        "understand how logistic regression works as an analogy for the biological neuron",
        "use logistic regression to solve real-world business problems like predicting user actions from e-commerce data and facial expression recognition",
        "understand why regularization is used in machine learning",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Start Here": [
          "Introduction and Outline",
          "How to Succeed in this Course",
          "Statistics vs. Machine Learning",
          "Review of the classification problem",
          "Introduction to the E-Commerce Course Project"
        ],
        "Basics: What is linear classification? What's the relation to neural networks?": [
          "Linear Classification",
          "Biological inspiration - the neuron",
          "How do we calculate the output of a neuron / logistic classifier? - Theory",
          "How do we calculate the output of a neuron / logistic classifier? - Code",
          "Interpretation of Logistic Regression Output",
          "E-Commerce Course Project: Pre-Processing the Data",
          "E-Commerce Course Project: Making Predictions",
          "Feedforward Quiz",
          "Prediction Section Summary",
          "Suggestion Box"
        ],
        "Solving for the optimal weights": [
          "Training Section Introduction",
          "A closed-form solution to the Bayes classifier",
          "What do all these symbols mean? X, Y, N, D, L, J, P(Y=1|X), etc.",
          "The cross-entropy error function - Theory",
          "The cross-entropy error function - Code",
          "Visualizing the linear discriminant / Bayes classifier / Gaussian clouds",
          "Maximizing the likelihood",
          "Updating the weights using gradient descent - Theory",
          "Updating the weights using gradient descent - Code",
          "E-Commerce Course Project: Training the Logistic Model",
          "Training Section Summary"
        ],
        "Practical concerns": [
          "Practical Section Introduction",
          "Interpreting the Weights",
          "L2 Regularization - Theory",
          "L2 Regularization - Code",
          "L1 Regularization - Theory",
          "L1 Regularization - Code",
          "L1 vs L2 Regularization",
          "The donut problem",
          "The XOR problem",
          "Why Divide by Square Root of D?",
          "Practical Section Summary"
        ],
        "Checkpoint and applications: How to make sure you know your stuff": [
          "BONUS: Sentiment Analysis",
          "BONUS: Exercises + how to get good at this"
        ],
        "Project: Facial Expression Recognition": [
          "Facial Expression Recognition Project Introduction",
          "Facial Expression Recognition Problem Description",
          "The class imbalance problem",
          "Utilities walkthrough",
          "Facial Expression Recognition in Code",
          "Facial Expression Recognition Project Summary"
        ],
        "Background Review": [
          "Gradient Descent Tutorial"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (FAQ by Student Request)": [
          "How to Uncompress a .tar.gz file",
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Python 2 vs Python 3"
        ]
      },
      "requirements": [
        "Derivatives, matrix arithmetic, probability",
        "You should know some basic Python coding with the Numpy Stack"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis course is a lead-in to deep learning and neural networks - it covers a popular and fundamental technique used in machine learning, data science and statistics: logistic regression. We cover the theory from the ground up: derivation of the solution, and applications to real-world problems. We show you how one might code their own logistic regression module in Python.\nThis course does not require any external materials. Everything needed (Python, and some Python libraries) can be obtained for free.\nThis course provides you with many practical examples so that you can really see how deep learning can be used on anything. Throughout the course, we'll do a course project, which will show you how to predict user actions on a website given user data like whether or not that user is on a mobile device, the number of products they viewed, how long they stayed on your site, whether or not they are a returning visitor, and what time of day they visited.\nAnother project at the end of the course shows you how you can use deep learning for facial expression recognition. Imagine being able to predict someone's emotions just based on a picture!\nIf you are a programmer and you want to enhance your coding abilities by learning about data science, then this course is for you. If you have a technical or mathematical background, and you want use your skills to make data-driven decisions and optimize your business using scientific principles, then this course is for you.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus (taking derivatives)\nmatrix arithmetic\nprobability\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "Adult learners who want to get into the field of data science and big data",
        "Students who are thinking of pursuing machine learning or data science",
        "Students who are tired of boring traditional statistics and prewritten functions in R, and want to learn how things really work by implementing them in Python",
        "People who know some machine learning but want to be able to relate it to artificial intelligence",
        "People who are interested in bridging the gap between computational neuroscience and machine learning"
      ]
    },
    {
      "title": "Generative AI for Data Engineering and Data Professionals",
      "url": "https://www.udemy.com/course/generative-ai-for-data/",
      "bio": "Work Faster with Practical Gen AI for Data Engineering | For all Data Professionals (Engineers, Analysts, Scientists)",
      "objectives": [
        "Integrate Generative AI into existing data flows and data engineering lifecycle",
        "Learn how Gen AI is revolutionizing each step of the data engineering lifecycle, with hands-on practice and activities",
        "Understand the role of Generative AI in Data Engineering, especially on when to use it and when not to use it",
        "Generate and augment data using Generative AI",
        "Write data engineering code with Generative AI",
        "Explore Generative AI tools for data engineering",
        "Parse and extract insights and data from unstructured text using Generative AI",
        "Query and analyze data in data engineering with Generative AI",
        "Enrich, normalize, and standardize data using Generative AI features"
      ],
      "course_content": {
        "Introduction": [
          "About the course",
          "Course Tips",
          "How Generative AI impacts Data Engineer Tasks",
          "Course Roadmap",
          "Caveats about Using Generative AI",
          "About the Instructor",
          "Keys to Success",
          "Ways to Contact",
          "Leave a Rating",
          "Watch in 1080p"
        ],
        "Environment Setup": [
          "Environment Setup",
          "Option 1 Download Python, VSCode, and Jupyter Lab",
          "Option 2 Google Colab",
          "Set up OpenAI API",
          "Resources"
        ],
        "Data Generation and Augmentation": [
          "Introduction to using Generative AI for Data Generation and Augmentation",
          "Generating Synthetic Data with Generative AI",
          "Augmenting Existing Data with Generative AI",
          "Creating Time Series Data",
          "Generating Edge Cases in Data Engineering",
          "Handling PII Data with Generative AI",
          "Balancing Imbalanced Datasets in Data Engineering",
          "Activity: Data Augmentation App Walkthrough",
          "Activity: Creating Functions for Data Engineering",
          "Activity: Running the Backend",
          "Activity: Adding Front-End Components",
          "Activity: Running the Web App (GenAI for Data Engineering)"
        ],
        "Writing Data Engineering Code with Gen AI": [
          "Introduction to using Generative AI for Writing Data Engineering Code with Gen A",
          "Data Cleaning and Modeling with Generative AI",
          "Documenting Code for Data Projects",
          "Creating Data Schemas, Systems, and Pipelines",
          "Transferring Data with Generative AI"
        ],
        "Gen AI Data Engineering Tools": [
          "Introduction to using Generative AI for Gen AI Data Engineering Tools",
          "Use ChatGPT for Data Engineering",
          "Build a Data Engineering App with Claude",
          "Custom GPTs for Data Engineering",
          "Custom LLM or Generative AI tools for Data Engineering",
          "Copilot for Azure Data Factory and Gemini for BigQuery"
        ],
        "Data Parsing and Extraction": [
          "Introduction to using Generative AI for Data Parsing and Extraction",
          "Parsing Data (Data Engineering)",
          "Extracting Data from Web Scrapes and Images",
          "Performing Named Entity Recognition",
          "Activity: Extracting Data from Contracts"
        ],
        "Data Querying and Analysis": [
          "Introduction to using Generative AI for Data Querying and Analysis",
          "Querying Data with Generative AI",
          "Optimizing Data Queries",
          "Activity: Developing Data Engineering Query Apps",
          "Activity: Running Data Engineering Query Apps",
          "Activity: Converting to a Web App with Front-End"
        ],
        "Data Enrichment, Data Normalization and Standardization": [
          "Introduction to using Generative AI for Data Enrichment, Data Normalization and",
          "Enriching Features for Data Models",
          "Data Imputation and Normalization with Generative AI",
          "Imputation for Time Series Data Engineering",
          "Standardizing and Normalizing Textual Data with Generative AI"
        ],
        "Conclusion": [
          "Conclusion and Certificate",
          "Ways to contact"
        ],
        "Bonus": [
          "Bonus"
        ]
      },
      "requirements": [
        "Familiarity with basic data engineering concepts (data cleaning, SQL queries)",
        "Basic understanding of programming, especially Python",
        "Familiarity with coding tools like Jupyter notebooks and VS Code"
      ],
      "description": "Note: this is as practical hands-on-keyboard course on how to use Generative AI in Data Engineering (and as a Data Professional). We will be using Python, OpenAI API, and Jupyter Notebooks to write and execute code.\n\n\nGenerative AI is changing the game in data and data engineering for two reasons:\nDo tasks faster - Data professionals who use Generative AI complete tasks 16% faster. This increases to more then 45% if you code / analyze data on a day-to-day basis\nDo new tasks - Generative AI enables data engineers and analysts to do so much more. In fact, some tasks like extracting features / insights from unstructured data or augmenting textual data is now only possible with Gen AI.\n\n\nThis is why GenAI is revolutionizing each step of the data engineering lifecycle. It doesn't matter if you're a data analyst, data scientist, data engineer, data professional, or data manager - you need to learn how to embed Generative AI in your day-to-day workflows.\n\n\nThat's what this course is all about - to make you more powerful and productive as a data professional with Generative AI.\n\n\nLearn from more than 5.5 hours of relevant instructional video content, with the only course that will practically teach you the different ways that Generative AI is impacting the data engineering and data professional lifecycle, and then apply that to real-life end-to-end examples.\n\n\nWhat is this course all about?\nThis course is all about how you can practically embed Gen AI into your day-to-day workflows as a Data Engineer or Data Professional. It's a deep practical guide on how Generative AI is revolutionizing each step of the data engineering lifecycle, making you more productive and powerful. This is a technical and practical course (it's not theoretical or hand-wavy).\n\n\nWhy learn Generative AI as a Data Professional or Data Engineer?\nThere are two reasons: productivity and power. Generative AI can do certain things faster - like writing SQL queries, documentation, creating schemas, and analyzing simple data. Generative AI can do things that were not possible before, like extracting insights from unstructured text, imputing textual data, or augmenting data while maintaining context. You must know how to use Gen AI to avoid being left behind.\n\n\nHow can Generative AI impact Data Engineering?\nGen AI impacts Data Engineering in many different ways. Specifically, we'll look at 7 different archetypes:\nData Generation and Augmentation\nWriting Generative AI Code with Gen AI\nData Parsing and Extraction\nGen AI Data Engineering Tools\nData Querying and Analysis\nData Enrichment, Normalization, and Standardization\nAnomaly Detection and Compression\n\n\nWhat will you learn?\nIntegrate Generative AI - Learn how to fully embed Generative AI as a Data Professional in your workflows (including data generation, analysis, storage, visualization, pipelines, and more)\nBe more productive - Generative AI is a productivity game changer - it can help you complete data tasks up to 20% faster (McKinsey), and even more if you write or use code\nBe more powerful - Learn how to do more data tasks that weren't possible without Generative AI, like extracting insights from unstructured text or augmenting textual data\n\n\nWhy choose this course?\nComplete guide - this is the 100% start to finish, zero to hero, basic to advanced guide on using Generative AI as a Data Engineer or Data Professional. There is no other course like it that teaches you everything from start to finish. It contains over 5.5 hours of instructional content!\nStructured to succeed - this course is structured to help you succeed. We first go through the fundamentals on how Generative AI can be used for Data Engineering. Then, we go through the 7 different archetypes of how Gen AI can be embedded into your workflows. We go through each, one-by-one, in full detail.\nFully instructional - we not only go through important concepts, but also apply them. This is a practical hands-on-keyboard type course. This is not only a walkthrough of the all the features and theoretical concepts, but a course that actually uses real-life examples and integrates workflows with you.\nStep by step - we go through every single method of how Generative AI can impact Data Engineering step-by-step. We start with examples, then complete full end-to-end activities to apply what we've learned.\nTeacher response - if there's anything else you would like to learn, or if there's something you cannot figure out, I'm here for you! Look at the ways to reach out video.\n\n\nCourse overview\nIntroduction to Generative AI for Data Engineering - Get an overview of the course, learn how Generative AI impacts Data Engineering tasks, and become familiar with the course roadmap.\nEnvironment Setup - Set up your workspace with two options: download Python, VSCode, and Jupyter Lab, or use Google Colab. We'll also guide you through setting up the OpenAI API.\nData Generation and Augmentation - Generate and augment data with Generative AI. Learn to create synthetic data, handle PII, balance datasets, and more. We'll also build a data augmentation app, from backend to frontend.\nWriting Data Engineering Code with Generative AI - Discover how to use Generative AI for writing data engineering code. This section includes data cleaning, modeling, documenting code, creating data schemas, and transferring data.\nGen AI Data Engineering Tools - Explore tools like ChatGPT, Claude, custom GPTs, and other Gen AI tools for data engineering.\nData Parsing and Extraction - Parse and extract data from unstructured text using Generative AI, including data from web scrapes, images, contracts, invoices, receipts, and perform named entity recognition.\nData Querying and Analysis - Master querying and analyzing data with Generative AI. Optimize your queries, develop and run query apps, and convert them to web apps with front-end components.\nData Enrichment, Normalization, and Standardization - use Generative AI to enrich, normalize, and standardize your data, covering feature enrichment, data imputation, and standardizing textual data for better models.\nConclusion - this covers the certificate, next steps, and ways to get in touch.\n\n\nIf you want to learn how to improve your productivity and be more powerful as a data engineer (in practice, not in theory) using Generative, then this is the course for you. We're looking forward to having you in the course and hope you earn the certificate.",
      "target_audience": [
        "Data engineers who want to incorporate Generative AI into their workflows",
        "All data professionals (data engineers, data analysts, data scientists, data managers)",
        "Data professionals or engineers who want to use Generative AI for data tasks",
        "Anyone wanting to enhance their skills in data engineering and Gen AI integration",
        "Developers aiming to build data engineering applications",
        "Individuals curious about leveraging AI to streamline data workflows"
      ]
    },
    {
      "title": "The classic course on Generative AI by Martin Musiol",
      "url": "https://www.udemy.com/course/generative-ai/",
      "bio": "How the next milestone in machine learning will improve the products we build",
      "objectives": [
        "How to implement Generative AI models. We focus on proper concept implementation and relevant code (no administrative code)",
        "Get to know the broad spectrum of GAI applications and possibilities tangibly eg. 3D object generation, interactive image generation, and text generation",
        "How to identify great ideas in the GAI space and make best use of already developed models for realising your projects and ideas",
        "How to augment your dataset such that it ultimately improves your machine learning performance eg. for classifiers of rare diseases",
        "Learn about the ethical side: what are the concerns around GAI, incl. deep fakes, etc.",
        "The technical side: from the evolution of generative models, to the generator-discriminator interplay, to common implemenation issues and their remedies"
      ],
      "course_content": {
        "Introduction": [
          "Let's get started",
          "What is Generative AI?",
          "Your Instructor Martin",
          "The Course Overview",
          "Your Feedback is valuable"
        ],
        "Discriminative vs. Generative AI": [
          "Discriminative vs. Generative AI",
          "On discriminative vs. generative AI."
        ],
        "Why does Generative AI matter?": [
          "Broad Application Fields and Potential",
          "GAI in Top Strategic Tech Trends",
          "Example: Face Generation",
          "Example: Do-as-I-do-motion Transfer",
          "[Part 1] Example: GAI-generated Art AND the Interlock with Crypto & NFTs",
          "[Part 2] Example: GAI-generated Art AND the Interlock with Crypto & NFTs",
          "[Part 3] Example: GAI-generated Art AND the Interlock with Crypto & NFTs",
          "On why does Generative AI matter?"
        ],
        "Where is Generative AI located?": [
          "Where is GAI located?",
          "The Evolution of Deep Generative Models",
          "On The Evolution of Deep Generative Models"
        ],
        "The Power of Generative Adversarial Networks (GANs)": [
          "GANs are Powerful",
          "How does a GAN work?",
          "on how GANs work",
          "[Part 1] Excursion: Neural Networks",
          "[Part 2] Excursion: Neural Networks",
          "What GANs do exist? + Generative AI Ideation"
        ],
        "Why did it take them so long?": [
          "Why did it take them so long?"
        ],
        "The implementation of a simple GAN": [
          "[Part 1] GAN Demo",
          "[Part 2] GAN Demo"
        ],
        "A Deep-Dive into Various Application Fields": [
          "3D-Object Generation",
          "3D-Object Generation - Ideas",
          "on 3D-Object Generation",
          "Interactive Image Generation",
          "[Part 1] Conditional GAN (cGAN) Demo",
          "[Part 2] Conditional GAN (cGAN) Demo",
          "How does a GauGAN work - Ideas",
          "How to augment Data and Why",
          "What Data Augmentation Techniques exist & what is their Effectiveness",
          "Data Augmentation with a GAN - Demo",
          "Data Augmentation: Lessons Learnt & Outlook"
        ],
        "Concerns around Generative AI Models": [
          "Concerns around Generative AI Models"
        ],
        "Noteworthy GAN Architectures": [
          "[Part 1] Noteworthy GAN Architectures",
          "[Part 2] Noteworthy GAN Architectures"
        ]
      },
      "requirements": [
        "No hard prerequisites",
        "Nice-to-have: coding skills and pre-knowledge in machine learning"
      ],
      "description": "Recently, we have seen a shift in AI that wasn't very obvious. Generative Artificial Intelligence (GAI) - the part of AI that can generate all kinds of data - started to yield acceptable results, getting better and better. As GAI models get better, questions arise e.g. what will be possible with GAI models? Or, how to utilize data generation for your own projects?\nIn this course, we answer these and more questions as best as possible.\nThere are 3 angles that we take:\nApplication angle: we get to know many GAI application fields, where we then ideate what further projects could emerge from that. Ultimately, we point to good starting points and how to get GAI models implemented effectively.\nThe application list is down below.\nTech angle: we see what GAI models exist. We will focus on only relevant parts of the code and not on administrative code that won't be accurate a year from now (it's one google away). Further, there will be an excursion: from computation graphs, to neural networks, to deep neural networks, to convolutional neural networks (the basis for image and video generation).\nThe architecture list is down below.\nEthical angle/ Ethical AI: we discuss the concerns of GAI models and what companies and governments do to prevent further harm.\nEnjoy your GAI journey!\n\n\nList of discussed application fields:\nCybersecurity 2.0 (Adversarial Attack vs. Defense)\n3D Object Generation\nText-to-Image Translation\nVideo-to-Video Translation\nSuperresolution\nInteractive Image Generation\nFace Generation\nGenerative Art\nData Compression with GANs\nDomain-Transfer (i.e. Style-Transfer, Sketch-to-Image, Segmentation-to-Image)\nCrypto, Blockchain, NFTs\nIdea Generator\nAutomatic Video Generation and Video Prediction\nText Generation, NLP Models (incl. Coding Suggestions like Co-Pilot)\nGAI Outlook\netc.\nGenerative AI Architectures/ Models that we cover in the course (at least conceptually):\n(Vanilla) GAN\nAutoEncoder\nVariational AutoEncoder\nStyle-GAN\nconditional GAN\n3D-GAN\nGauGAN\nDC-GAN\nCycleGAN\nGPT-3\nProgressive GAN\nBiGAN\nGameGAN\nBigGAN\nPix2Vox\nWGAN\nStackGAN\netc.",
      "target_audience": [
        "Potential entrepreneurs, as we will provoke various project ideas",
        "Tech-enthusiasts that want to learn/ stay up-to-date with the newest advancements in AI",
        "Visionaries that want to help shaping the future with (G)AI",
        "Everyone who would enjoy a smooth journey through the world of Generative AI"
      ]
    },
    {
      "title": "Introduction to AI, Machine Learning and Data Science",
      "url": "https://www.udemy.com/course/introduction-to-ai-machine-learning-and-data-science/",
      "bio": "Introductory Course for Beginners to AI, Machine Learning, Data Science, Deep Learning, Supervised and Unsupervised ML",
      "objectives": [
        "Introduction to buzz words like AI, Machine Learning, Data Science and Deep Learning etc.",
        "Real time examples where Machine Learning can be used to solve real world business problems",
        "Introduction to Supervised Learning and Unsupervised Learning",
        "Introduction to Natural Language Processing",
        "Why python is popular for Machine Learning"
      ],
      "course_content": {},
      "requirements": [
        "You just need computer or mobile phone with internet connection to access course material.",
        "No prerequisites !"
      ],
      "description": "Lets learn basics to transform your career.\nI promise not to exhaust you with huge number of videos.\n\n\nArtificial Intelligence, Machine Learning, Data Science are the most hot skills in the markets which has potential to help you earn highest salary. These skills has potential to turn your financial to better level which can provide you growth and prosperity.\n\n\nWelcome to the most comprehensive Introduction to AI, Machine Learning and Data Science course!\n\n\nAn excellent choice for beginners and professionals looking to expand their knowledge on Artificial Intelligence, Machine Learning, Data Science, Deep Learning, Supervised and Unsupervised Learning.\n\n\nThis is an introductory course for beginners to boost your knowledge. This course gives introduction to to AI, Machine Learning, Data Science, Deep Learning, Supervised and Unsupervised learning with real time examples where machine learning can be applied to solve or simplify real world business problems.\n\n\nWhat you'll learn\nIntroduction to buzz words like AI, Machine Learning, Data Science and Deep Learning etc.\nReal time examples where Machine Learning can be used to solve real world business problems\nIntroduction to Supervised Learning and Unsupervised Learning\nIntroduction to Natural Language Processing\nWhy python is popular for Machine Learning\nPrerequisite:\nYou just need computer or mobile phone with internet connection to access course material.\nNo prerequisites !\nHappy Learning!",
      "target_audience": [
        "Beginners to AI, Machine Learning and Data Science",
        "Students or Professionals",
        "Testers, Database Developers",
        "Managers",
        "Java Developers",
        "Full Stack Developers etc."
      ]
    },
    {
      "title": "Full stack generative and Agentic AI with python",
      "url": "https://www.udemy.com/course/full-stack-ai-with-python/",
      "bio": "Hands-on guide to modern AI: Tokenization, Agents, RAG, Vector DBs, and deploying scalable AI apps. Complete AI course",
      "objectives": [
        "Write Python programs from scratch, using Git for version control and Docker for deployment.",
        "Use Pydantic to handle structured data and validation in Python applications.",
        "Understand how Large Language Models (LLMs) work: tokenization, embeddings, attention, and transformers.",
        "Call and integrate APIs from OpenAI and Gemini with Python.",
        "Design effective prompts: zero-shot, one-shot, few-shot, chain-of-thought, persona-based, and structured prompting.",
        "Run and deploy models locally using Ollama, Hugging Face, and Docker.",
        "Implement Retrieval-Augmented Generation (RAG) pipelines with LangChain and vector databases.",
        "Use LangGraph to design stateful AI systems with nodes, edges, and checkpointing.",
        "Understand Model Context Protocol (MCP) and build MCP servers with Python."
      ],
      "course_content": {
        "Introduction": [
          "Installation of Tools (VSCode and Python)",
          "VS Code Setup (Extensions and Themes)",
          "Get your code files here"
        ],
        "Introduction to Coding world with python": [
          "Meet your Instructor - Hitesh",
          "What is Programming..?",
          "Convert that into Python Code",
          "A Real World Python Code Intro",
          "Why to use Python",
          "Writing first Python code on MAC",
          "Writing first Python code on WINDOWS",
          "Get everything in Virtual Environment",
          "Organize Python Code like a Pro",
          "PEP8 and Zen of python"
        ],
        "Data Types in Python": [
          "Objects - Mutable and Immutable in Python",
          "Numbers, Booleans and Operators in Depth in Python",
          "String - Index, Slice and Encoding",
          "Tuples and Membership Testing",
          "Basics of List in Python",
          "Operator overloading and bytearray in python",
          "Set and frozenset in python",
          "Dictionary in Python",
          "Touch on Advance Data types like Collections"
        ],
        "Conditionals in python": [
          "Kettle Boiling Story Project",
          "Building a Snack System",
          "Building a Chai Price Calculator",
          "Building Smart Thermostat System",
          "Delivery Fees Waiver System",
          "Build a train seat information system"
        ],
        "Loops in python": [
          "Introduction to Loops",
          "Tea Token Dispenser",
          "Batch Chai Preparation",
          "Looping through list - Orders Name",
          "Why to use Enumerate",
          "Zip Can Combine Lists",
          "Introducing While Loop in Python",
          "Break, Continue and Loop Fallback",
          "Walrus Operator is Interesting in Python",
          "Dictionary in place of Match Case"
        ],
        "Functions in python": [
          "Functions - Reducing Duplication and Splitting Complex Tasks",
          "Functions - 3 More Features",
          "Scope and Named Space in Functions",
          "Non local vs Global scopes",
          "Handling Arguments in Function in Python",
          "Handle Multiple Return in Python",
          "Lambdas, Pure vs Impure functions",
          "Documenting your Functions and Built-in Functions",
          "Python Imports, Modules and Init File"
        ],
        "Comprehensions in python": [
          "What are Comprehensions in Python?",
          "List Comprehensions in Python",
          "Set Comprehensions in Python",
          "Dictionary Comprehensions in Python",
          "Generator Comprehensions for Memory Optimization"
        ],
        "Generators and Decorators in python": [
          "Generators with Yield and Next Methods",
          "Infinite Generators in Python",
          "Send Value to Generators",
          "Yield From and Close the Generators",
          "Decorators in Python",
          "Build a Logger with Decorator",
          "Build an Authorization Decorator"
        ],
        "Object oriented programming in python": [
          "Building your 1st Class and Object in Python",
          "Class and Object Namespace",
          "Attribute Shadowing in Python",
          "Self argument in python",
          "Constructors and Init in Python Classes",
          "Inheritance and Composition in Python Classes",
          "3 Ways to Access Base Class",
          "Method Resolution Order - MRO",
          "Static Methods in Python",
          "Classmethod vs Staticmethod",
          "Property decorator - Getter and Setter"
        ],
        "File and exception handling in python": [
          "What is Error handling",
          "Try except else and finally",
          "Catching multiple exceptions",
          "Raise your own errors",
          "Creating custom exceptions",
          "Mini project with exception learning",
          "File handling with try except and with"
        ]
      },
      "requirements": [
        "No prior AI knowledge is required — we start from the basics.",
        "A computer (Windows, macOS, or Linux) with internet access.",
        "Basic programming knowledge is helpful but not mandatory (the course covers Python from scratch)."
      ],
      "description": "Welcome to the Complete AI & LLM Engineering Bootcamp – your one-stop course to learn Python, Git, Docker, Pydantic, LLMs, Agents, RAG, LangChain, LangGraph, and Multi-Modal AI from the ground up.\nThis is not just another theory course. By the end, you will be able to code, deploy, and scale real-world AI applications that use the same techniques powering ChatGPT, Gemini, and Claude.\nWhat You’ll Learn\nFoundations\nPython programming from scratch — syntax, data types, OOP, and advanced features.\nGit & GitHub essentials — branching, merging, collaboration, and professional workflows.\nDocker — containerization, images, volumes, and deploying applications like a pro.\nPydantic — type-safe, structured data handling for modern Python apps.\nAI Fundamentals\nWhat are LLMs and how GPT works under the hood.\nTokenization, embeddings, attention, and transformers explained simply.\nUnderstanding multi-head attention, positional encodings, and the \"Attention is All You Need\" paper.\nPrompt Engineering\nMaster prompting strategies: zero-shot, one-shot, few-shot, chain-of-thought, persona-based prompts.\nUsing Alpaca, ChatML, and LLaMA-2 formats.\nDesigning prompts for structured outputs with Pydantic.\nRunning & Using LLMs\nSetting up OpenAI & Gemini APIs with Python.\nRunning models locally with Ollama + Docker.\nUsing Hugging Face models and INSTRUCT-tuned models.\nConnecting LLMs to FastAPI endpoints.\nAgents & RAG Systems\nBuild your first AI Agent from scratch.\nCLI-based coding agents with Claude.\nThe complete RAG pipeline — indexing, retrieval, and answering.\nLangChain: document loaders, splitters, retrievers, and vector stores.\nAdvanced RAG with Redis/Valkey Queues for async processing.\nScaling RAG with workers and FastAPI.\nLangGraph & Memory\nIntroduction to LangGraph — state, nodes, edges, and graph-based AI.\nAdding checkpointing with MongoDB.\nMemory systems: short-term, long-term, episodic, semantic memory.\nImplementing memory layers with Mem0 and Vector DB.\nGraph memory with Neo4j and Cypher queries.\nConversational & Multi-Modal AI\nBuild voice-based conversational agents.\nIntegrate speech-to-text (STT) and text-to-speech (TTS).\nCode your own AI voice assistant for coding (Cursor IDE clone).\nMulti-modal LLMs: process images and text together.\nModel Context Protocol (MCP)\nWhat is MCP and why it matters for AI apps.\nMCP transports: STDIO and SSE.\nCoding an MCP server with Python.\nReal-World Projects You’ll Build\nTokenizer from scratch.\nLocal Ollama + FastAPI AI app.\nPython CLI-based coding assistant.\nDocument RAG pipeline with LangChain & Vector DB.\nQueue-based scalable RAG system with Redis & FastAPI.\nAI conversational voice agent (STT + GPT + TTS).\nGraph memory agent with Neo4j.\nMCP-powered AI server.\nWho Is This Course For?\nBeginners who want a complete start-to-finish course on Python + AI.\nDevelopers who want to build real-world AI apps using LLMs, RAG, and LangChain.\nData Engineers/Backend Developers looking to integrate AI into existing stacks.\nStudents & Professionals aiming to upskill in modern AI engineering.\nWhy Take This Course?\nThis course combines theory, coding, and deployment in one place. You’ll start from the basics of Python and Git, and by the end, you’ll be coding cutting-edge AI applications with LangChain, LangGraph, Ollama, Hugging Face, and more.\nUnlike other courses, this one doesn’t stop at “calling APIs.” You will go deeper into system design, queues, scaling, memory, and graph-powered AI agents — everything you need to stand out as an AI Engineer.\nBy the end of this course, you won’t just understand AI—you’ll be able to build it.",
      "target_audience": [
        "Beginners who want a step-by-step path into AI, Python, and modern development tools.",
        "Developers who want to learn how to integrate LLMs, RAG, and agents into real-world applications.",
        "Data engineers and backend developers looking to upgrade their skills with AI-powered systems.",
        "Students and professionals who want to stand out in the job market with cutting-edge AI engineering knowledge."
      ]
    },
    {
      "title": "Advanced SQL for Data Engineering",
      "url": "https://www.udemy.com/course/advanced-sql-for-data-engineering/",
      "bio": "Mastering SQL Database Management and SQL Query Optimization for High-Performance Data Engineering",
      "objectives": [
        "Execute Database Manipulation",
        "Manage DateTime in SQL",
        "Handle Complex Data Types",
        "Master Advanced Query Techniques",
        "Optimize Databases"
      ],
      "course_content": {},
      "requirements": [
        "Fundamental SQL Knowledge",
        "Basic Database Understanding",
        "Familiarity with Relational Databases"
      ],
      "description": "Do you want to become a data engineer?\nAre you ready to take your SQL skills to the next level and stand out in today’s competitive, data-driven industry?\nThe Advanced SQL for Data Engineering course is designed to give you the edge you need. Whether you’re just starting out or looking to refine your expertise, this course will help you master the SQL techniques essential for solving real-world data challenges.\nAcross eight detailed sections, you’ll build a solid foundation and progressively dive into advanced topics. You'll begin by setting up your environment and gaining a deep understanding of relational databases. Then, you'll get hands-on with core SQL operations—Data Definition Language (DDL), Data Manipulation Language (DML), Data Query Language (DQL), and Data Control Language (DCL).\nBut that’s just the start! As the course progresses, you’ll learn how to master advanced SQL techniques like database manipulation, DateTime management, and working with complex data types such as ENUMs, Ranges, and nested data. You’ll perfect key SQL statements like CREATE, ALTER, INSERT, UPDATE, DELETE, and more, preparing you to handle sophisticated data tasks.\nWhat truly sets this course apart are the advanced query techniques you'll explore—OVER, different types of JOINS, CASE, CONCAT, and Recursive Common Table Expressions (CTEs). These are the techniques that top data engineers rely on to extract and manipulate complex datasets, making you a true SQL pro.\nYou’ll also deep dive into data optimization and design with lessons on data normalization, the STAR schema, and Snowflake model. Plus, you’ll master performance-enhancing techniques using stored procedures, User-Defined Functions (UDFs), materialized views, and transactions—key skills to ensure your databases run at peak efficiency.\nAnd it doesn't stop there! By the end of the course, you’ll test your skills through real-world exercises that prepare you for the challenges of a real data engineering role.\nLearn from a true industry expert. Shashank Kalanithi an experienced Senior Software Engineer with an extensive expertise in Data. He has worked as a Data Analyst, Data Scientist, and Data Engineer in leading Big Tech firms.\nSo, are you ready to become a SQL expert and elevate your career?\nEnroll today in the Advanced SQL for Data Engineering course, and take the first step toward becoming a high-demand data professional. Your journey to mastering SQL starts here!",
      "target_audience": [
        "Aspiring Data Engineers",
        "Data Professionals",
        "Database Administrators"
      ]
    },
    {
      "title": "Financial Engineering and Artificial Intelligence in Python",
      "url": "https://www.udemy.com/course/ai-finance/",
      "bio": "Financial Analysis, Time Series Analysis, Portfolio Optimization, CAPM, Algorithmic Trading, Q-Learning, and MORE!",
      "objectives": [
        "Forecasting stock prices and stock returns",
        "Time series analysis",
        "Holt-Winters exponential smoothing model",
        "ARIMA",
        "Efficient Market Hypothesis",
        "Random Walk Hypothesis",
        "Exploratory data analysis",
        "Alpha and Beta",
        "Distributions and correlations of stock returns",
        "Modern portfolio theory",
        "Mean-Variance Optimization",
        "Efficient frontier, Sharpe ratio, Tangency portfolio",
        "CAPM (Capital Asset Pricing Model)",
        "Q-Learning for Algorithmic Trading"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "Scope of the course",
          "How to Practice",
          "Warmup (Optional)"
        ],
        "Getting Set Up": [
          "Where to get the code, notebooks, and data",
          "How to Succeed in This Course",
          "Temporary 403 Errors"
        ],
        "Financial Basics": [
          "Financial Basics Section Introduction",
          "Getting Financial Data",
          "Getting Financial Data (Code)",
          "Understanding Financial Data",
          "Understanding Financial Data (Code)",
          "Dealing with Missing Data",
          "Dealing with Missing Data (Code)",
          "Returns",
          "Adjusted Close, Stock Splits, and Dividends",
          "Adjusted Close (Code)",
          "Back to Returns (Code)",
          "QQ-Plots",
          "QQ-Plots (Code)",
          "The t-Distribution",
          "The t-Distribution (Code)",
          "Skewness and Kurtosis",
          "Confidence Intervals",
          "Confidence Intervals (Code)",
          "Statistical Testing",
          "Statistical Testing (Code)",
          "Covariance and Correlation",
          "Covariance and Correlation (Code)",
          "Alpha and Beta",
          "Alpha and Beta (Code)",
          "Mixture of Gaussians",
          "Mixture of Gaussians (Code)",
          "Volatility Clustering",
          "Price Simulation",
          "Price Simulation (Code)",
          "Financial Basics Section Summary",
          "Suggestion Box"
        ],
        "Time Series Analysis": [
          "Time Series Analysis Section Introduction",
          "Efficient Market Hypothesis",
          "Random Walk Hypothesis",
          "The Naive Forecast",
          "Simple Moving Average (Theory)",
          "Simple Moving Average (Code)",
          "Exponentially-Weighted Moving Average (Theory)",
          "Exponentially-Weighted Moving Average (Code)",
          "Simple Exponential Smoothing for Forecasting (Theory)",
          "Simple Exponential Smoothing for Forecasting (Code)",
          "Holt's Linear Trend Model (Theory)",
          "Holt's Linear Trend Model (Code)",
          "Holt-Winters (Theory)",
          "Holt-Winters (Code)",
          "Autoregressive Models - AR(p)",
          "Moving Average Models - MA(q)",
          "ARIMA",
          "ARIMA in Code (pt 1)",
          "Stationarity",
          "Stationarity Code",
          "ACF (Autocorrelation Function)",
          "PACF (Partial Autocorrelation Function)",
          "ACF and PACF in Code (pt 1)",
          "ACF and PACF in Code (pt 2)",
          "Auto ARIMA and SARIMAX",
          "Model Selection, AIC and BIC",
          "ARIMA in Code (pt 2)",
          "ARIMA in Code (pt 3)",
          "ACF and PACF for Stock Returns",
          "Forecasting",
          "Time Series Analysis Section Conclusion"
        ],
        "Portfolio Optimization and CAPM": [
          "Portfolio Optimization Section Introduction",
          "The S&P500",
          "What is Risk?",
          "Why Diversify?",
          "Describing a Portfolio (pt 1)",
          "Describing a Portfolio (pt 2)",
          "Visualizing Random Portfolios and Monte Carlo Simulation (pt 1)",
          "Visualizing Random Portfolios and Monte Carlo Simulation (pt 2)",
          "Maximum and Minimum Portfolio Return",
          "Maximum and Minimum Portfolio Return in Code",
          "Mean-Variance Optimization",
          "The Efficient Frontier",
          "Mean-Variance Optimization And The Efficient Frontier in Code",
          "Global Minimum Variance (GMV) Portfolio",
          "Global Minimum Variance (GMV) Portfolio in Code",
          "Sharpe Ratio",
          "Maximum Sharpe Ratio in Code",
          "Portfolio with a Risk-Free Asset and Tangency Portfolio",
          "Risk-Free Asset and Tangency Portfolio in Code",
          "Capital Asset Pricing Model (CAPM)",
          "Problems with Markowitz Portfolio Theory and Robust Estimation",
          "Portfolio Optimization Section Conclusion"
        ],
        "VIP: Algorithmic Trading": [
          "Algorithmic Trading Section Introduction",
          "Trend-Following Strategy",
          "Trend-Following Strategy in Code (pt 1)",
          "Trend-Following Strategy in Code (pt 2)",
          "Machine Learning-Based Trading Strategy",
          "Machine Learning-Based Trading Strategy in Code",
          "Classification-Based Trading Strategy in Code",
          "Using a Random Forest Classifier for Machine Learning-Based Trading",
          "Algorithmic Trading Section Summary"
        ],
        "VIP: The Basics of Reinforcement Learning": [
          "Reinforcement Learning Section Introduction",
          "Elements of a Reinforcement Learning Problem",
          "States, Actions, Rewards, Policies",
          "Markov Decision Processes (MDPs)",
          "The Return",
          "Value Functions and the Bellman Equation",
          "What does it mean to “learn”?",
          "Solving the Bellman Equation with Reinforcement Learning (pt 1)",
          "Solving the Bellman Equation with Reinforcement Learning (pt 2)",
          "Epsilon-Greedy",
          "Q-Learning",
          "How to Learn Reinforcement Learning"
        ],
        "VIP: Reinforcement Learning for Algorithmic Trading": [
          "Trend-Following Strategy with Reinforcement Learning API",
          "Trend-Following Strategy Revisited (Code)",
          "Q-Learning in an Algorithmic Trading Context",
          "Representing States",
          "Q-Learning for Algorithmic Trading in Code"
        ],
        "VIP: Statistical Factor Models and Unsupervised Machine Learning": [
          "Statistical Factor Models (Beginner)",
          "Statistical Factor Models (Intermediate)",
          "Statistical Factor Models (Advanced)",
          "Statistical Factor Models (Code)"
        ],
        "VIP: Regime Detection and Sequence Modeling with Hidden Markov Models": [
          "Why Sequence Models? (pt 1)",
          "Why Sequence Models? (pt 2)",
          "HMM Parameters",
          "HMM Tasks and the Viterbi Algorithm",
          "HMM for Modeling Volatility Clustering in Code"
        ]
      },
      "requirements": [
        "Decent Python coding skills",
        "Numpy, Matplotlib, Pandas, and Scipy (I teach this for free! My gift to the community)",
        "Matrix arithmetic",
        "Probability"
      ],
      "description": "Have you ever thought about what would happen if you combined the power of machine learning and artificial intelligence with financial engineering?\nToday, you can stop imagining, and start doing.\nThis course will teach you the core fundamentals of financial engineering, with a machine learning twist.\nWe will cover must-know topics in financial engineering, such as:\nExploratory data analysis, significance testing, correlations, alpha and beta\nTime series analysis, simple moving average, exponentially-weighted moving average\nHolt-Winters exponential smoothing model\nARIMA and SARIMA\nEfficient Market Hypothesis\nRandom Walk Hypothesis\nTime series forecasting (\"stock price prediction\")\nModern portfolio theory\nEfficient frontier / Markowitz bullet\nMean-variance optimization\nMaximizing the Sharpe ratio\nConvex optimization with Linear Programming and Quadratic Programming\nCapital Asset Pricing Model (CAPM)\nAlgorithmic trading (VIP only)\nStatistical Factor Models (VIP only)\nRegime Detection with Hidden Markov Models (VIP only)\nIn addition, we will look at various non-traditional techniques which stem purely from the field of machine learning and artificial intelligence, such as:\nRegression models\nClassification models\nUnsupervised learning\nReinforcement learning and Q-learning\n***VIP-only sections (get it while it lasts!) ***\nAlgorithmic trading (trend-following, machine learning, and Q-learning-based strategies)\nStatistical factor models\nRegime detection and modeling volatility clustering with HMMs\nWe will learn about the greatest flub made in the past decade by marketers posing as \"machine learning experts\" who promise to teach unsuspecting students how to \"predict stock prices with LSTMs\". You will learn exactly why their methodology is fundamentally flawed and why their results are complete nonsense. It is a lesson in how not to apply AI in finance.\nAs the author of ~30 courses in machine learning, deep learning, data science, and artificial intelligence, I couldn't help but wander into the vast and complex world of financial engineering.\nThis course is for anyone who loves finance or artificial intelligence, and especially if you love both!\nWhether you are a student, a professional, or someone who wants to advance their career - this course is for you.\nThanks for reading, I will see you in class!\n\n\nSuggested Prerequisites:\nMatrix arithmetic\nProbability\nDecent Python coding skills\nNumpy, Matplotlib, Scipy, and Pandas (I teach this for free, no excuses!)\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Anyone who loves or wants to learn about financial engineering",
        "Students and professionals who want to advance their career in finance or artificial intelligence and machine learning"
      ]
    },
    {
      "title": "Master LangChain & Gen AI -Build #16 AI Apps HuggingFace LLM",
      "url": "https://www.udemy.com/course/learn-langchain-go-from-zero-to-hero-build-ai-apps/",
      "bio": "Learn End to End LLM Generative AI (Gen AI) projects - Langchain - OpenAI, HuggingFace, LLAMA 2 Gemin models Langchain",
      "objectives": [
        "Develop Langchain based AI apps",
        "Implement LLM powered apps",
        "Learn Langchain from end to end",
        "Complete project based approach"
      ],
      "course_content": {
        "Gen AI and LangChain Introduction": [
          "What You'll Get In This Course",
          "Generative AI Introduction",
          "What is LangChain?",
          "⏱️ Course RoadMap",
          "Join our online classroom",
          "A request to you... :)❤️",
          "Let's understand the LangChain and Generative AI benefits",
          "Guidelines For Source Code ✅ - Gen AI and Langchain"
        ],
        "Huggingface Introduction": [
          "HuggingFace Intro & Access Token Generation"
        ],
        "OpenAI Introduction": [
          "What is OpenAI?",
          "OpenAI API Key Generation"
        ],
        "Demo & Environment Setup": [
          "A LangChain Example - Implementation Demo",
          "Anaconda Installation"
        ],
        "Langchain - Models Module Concept": [
          "LangChain's Modules Overview"
        ],
        "✅***Beginner level***": [
          "Let's work on basic-level projects"
        ],
        "Project #1 - Simple Question & Answer App || Hugging Face Spaces || Gen AI": [
          "LLMs Walkthrough & HuggingFace Token Generation",
          "LLM Practical Implementation using Python",
          "LLM Practical Implementation using Python - Source Code",
          "Project Environment Setup",
          "Lets' Build Simple Question Answering Application",
          "Lets' Build Simple Question Answering Application - Source Code"
        ],
        "Project #2 - Simple Conversational App || Hugging Face || Gen AI || langchain": [
          "Chat Model Walkthrough",
          "Chat Model Practical Implementation using Python",
          "Chat Model Practical Implementation using Python - Source Code",
          "Let's Build Simple Conversational Application",
          "Let's Build Simple Conversational Application - Source Code"
        ],
        "Project #3 - Find Similar Things App For Kids | Streamlit | Gen AI | Langchain": [
          "Text Embedding Walkthrough",
          "Text Embeddings Practical Implementation using Python",
          "Embeddings Practical Implementation using Python - Source Code",
          "Embeddings Example using Python",
          "Embeddings Example using Python - Source Code",
          "Let's build Similar Words Finder Application",
          "Let's build Similar Words Finder Application - Source Code"
        ],
        "Langchain - Prompt Module Concept & Implementation Using Python || Langchain": [
          "Prompts Module Introduction",
          "Prompt Template Walkthrough",
          "Prompt Template Walkthrough - Source Code",
          "Example Selectors Walkthrough",
          "Example Selectors Walkthrough - Source Code",
          "Adding More Examples To Input Prompt",
          "Output Parsers Walkthrough",
          "Output Parsers Walkthrough - Source Code"
        ]
      },
      "requirements": [
        "AI Enthusiast",
        "Basic Python Programming"
      ],
      "description": "Are you interested in harnessing the power of AI to create groundbreaking language-based applications?\nLook no further than LangChain and Gen AI - a comprehensive course that will take you from a novice to an expert in no time.\n\nImplement Generative AI (GenAI) apps with langchain framework using different LLMs.\n\n\nBy implementing AI applications powered with state-of-the-art LLM models like OpenAI and Hugging Face using Python, you will embark on an exciting project-based learning journey.\n\n\nWith LangChain, you will gain the skills and knowledge necessary to develop innovative LLM solutions for a wide range of problems.\n\n\nHere are some of the projects we will work on:\nProject 1: Construct a dynamic question-answering application with the unparalleled capabilities of LangChain, OpenAI, and Hugging Face Spaces.\nProject 2: Develop an engaging conversational bot using LangChain and OpenAI to deliver an interactive user experience.\nProject 3: Create an AI-powered app tailored for children, facilitating the discovery of related classes of objects and fostering educational growth.\nProject 4: Build a captivating marketing campaign app that utilizes the persuasive potential of well-crafted sales copy, boosting sales and brand reach.\nProject 5: Develop a ChatGPT clone with an added summarization feature, delivering a versatile and invaluable chatbot experience.\nProject 6: MCQ Quiz Creator App - Seamlessly create multiple-choice quizzes for your students using LangChain and Pinecone.\nProject 7: CSV Data Analysis Toll - Helps you analyze your CSV file by answering your queries about its data.\nProject 8: Youtube Script Writing Tool -  Effortlessly create compelling YouTube scripts with this user-friendly and efficient script-writing tool.\nProject 9 - Support Chat Bot For Your Website - Helps your visitors/customers to find the relevant data or blog links that can be useful to them.\nProject 10 - Automatic Ticket Classification Tool - The Automatic Ticket Classification Tool categorizes support tickets based on content to streamline ticket management and response processes.\nProject 11 - HR - Resume Screening  Assistance - HR project using AI to assist in screening resumes, optimizing the hiring process with smart analysis and recommendations\nProject 12 - Email Generator using LLAMA 2- The Email Generator is a tool that automatically creates customized emails, saving time and effort in crafting personalized messages.\nProject 13 - Invoice Extraction Bot using LLAMA 2- Invoice Extraction Bot: AI-powered tool that extracts key details from invoices accurately and efficiently. Simplify your data entry process.\nProject 14 - Text to SQL Query Helper Tool: Convert natural language text into structured SQL queries effortlessly using the Text to SQL Query Tool for streamlined database interaction and data retrieval.\nProject 15 - Customer Care Call Summary Alert - Concise notification highlighting key points and outcomes from recent customer service calls, aiding quick understanding and response\n\n\nThis course is designed to provide you with a complete understanding of LangChain, starting from the basics and progressing toward creating practical LLM-powered applications.\n\n\nLangChain empowers programmers to fully utilize large language models, such as ChatGPT, pinecone, LLAMA 2, and Huggingface, and seamlessly integrate them with external data sources. This integration enhances the models' ability to comprehend and respond to human language.\n\n\nBuilt with Python, LangChain offers a user-friendly interface tailored specifically for beginners, making it accessible to aspiring developers.\n\n\nCourse Overview:\nAspiring to build sophisticated language-based applications\nLangChain is the perfect library for you.\nMove beyond basic techniques like keyword matching or rule-based systems and maximize your reach by langchain.\nLeverage the power of LLMs, and applications using LangChain and combine them with cognitive or information sources & pinecone.\nUnlock tremendous potential and explore new possibilities with applications using LangChain and Pinecone.\n\n\nCourse Contents:\nLangChain\nLLMs\nChat Models\nPrompts\nIndexes\nChains\nAgents\nMemory\nGoogle Gemini Pro\n\n\n\n\nBut this isn't just a theory-based course; it's a hands-on experience. You will engage in practical activities and real-world projects, reinforcing your understanding of these concepts and techniques.\n\n\nBy the end of the course, you will be equipped with the skills to apply Langchain effectively, building robust, pinecone, powerful, and scalable LLM applications for various purposes.\n\n\nDon't miss this opportunity to become a language model expert.\nEnroll in the LangChain course and embark on a transformative journey that will elevate your AI app development skills. LangChain , OpenAI , ChatGPT , LLM, langchain pinecone , LLAMA 2 ,  Huggingface Google Gemini Pro Python - these are the tools that will empower you to create cutting-edge AI applications that push the boundaries of what's possible.\n\n\nGet ready to unlock your full potential and become a hero in the world of language-based AI development through langchain.\n\n\nYou will do practical activities and real-world projects throughout the applications using LangChain pinecone Google Gemini Pro course to strengthen your understanding of the concepts and techniques.\n\n\nYou will be competent in applying Langchain pinecone to build strong, effective, and scalable LLM applications for a variety of uses by the end of the course.",
      "target_audience": [
        "Someone who ready to explore the AI world"
      ]
    },
    {
      "title": "Bayesian Computational Analyses with R",
      "url": "https://www.udemy.com/course/bayesian-computational-analyses-with-r/",
      "bio": "Learn the concepts and practical side of using the Bayesian approach to estimate likely event outcomes.",
      "objectives": [
        "Understand Bayesian concepts, and gain a great deal of practical \"hands-on\" experience creating and estimating Bayesian models using R software.",
        "Effectively use the Bayesian approach to estimate likely event outcomes, or probabilities, using their own data.",
        "Be able to apply a range of Bayesian functions using R software in order to model and estimate single parameter, multi-parameter, conjugate mixture, multinomial, and rejection and importance sampling Bayesian models.",
        "Understand and use both predictive priors and predictive posteriors in Bayesian applications.",
        "Be able to compare and evaluate alternative, competing Bayesian models."
      ],
      "course_content": {
        "Introduction to Bayesian Course and to R Software": [
          "Introduction to Bayesian Computational Analyses with R",
          "Introduction to Course Materials",
          "Introduction to R Software (slides, part 1)",
          "Introduction to R Software (slides, part 2)",
          "Introduction to R Software (slides, part 3)",
          "Introduction to R Software with Scripts (part 1)",
          "Introduction to R Software with Scripts (part 2)",
          "Introduction to R Software with Scripts (part 3)",
          "Introduction to R Software with Scripts (part 4)",
          "Introduction to R Software with Scripts (part 5)",
          "Programming a Monte Carlo Simulation",
          "Section 1 R Scripting Exercises"
        ],
        "Introduction to Bayesian Thinking": [
          "More on the Course and Materials",
          "Session 1 R Scripting Exercise Solutions",
          "Background on Probability Density Functions (PDFs)",
          "Normal dnorm() Functions (part 1)",
          "Normal dnorm() Functions (part 2)",
          "Normal pnorm() Function",
          "Bayes' Rule and More",
          "Likelihood Function",
          "Using Discrete Priors (part 1)",
          "Using Discrete Priors (part 2)",
          "Using a Beta Prior (part 1)",
          "Using a Beta Prior (part 2)",
          "Using a Beta Prior (part 3)",
          "Simulating Beta Posteriors",
          "Brute Force Posterior Simulation using Histogram Prior",
          "Predictive Priors (slides)",
          "Predictive Priors (scripts, part 1)",
          "Predictive Priors (scripts, part 2)",
          "Section 2 Exercises"
        ],
        "Single Parameter Bayesian Models": [
          "Section 2 Exercise Solution",
          "Prelude to Single Parameter Models",
          "Single Parameter Models",
          "Heart Transplant Mortality Rate (part 1)",
          "Heart Transplant Mortality Rate (part 2)",
          "Test of Bayesian Robustness (part 1)",
          "Test of Bayesian Robustness (part 2)",
          "Exercise: How Many Taxis?"
        ],
        "Conjugate Mixtures": [
          "Exercise Solution: How Many Taxis?",
          "Conjugate Mixtures (part 1)",
          "Conjugate Mixtures (part 2)",
          "A Bayesian Test of the Fairness of a Coin (part 1)",
          "A Bayesian Test of the Fairness of a Coin (part 2)",
          "More on the Fairness of a Coin (part 3)",
          "Introduction to Probability Density Functions (part 1)",
          "Intro to PDFs (part 2)",
          "Intro to PDFs (part 3)"
        ],
        "Multi-Parameter Bayesian Models": [
          "Mortality Rate Exercise Solution (part 1)",
          "Mortality Rate Exercise Solution (part 2)",
          "Normal Multiparameter Models (part 1)",
          "Normal Multiparameter Models (part 2)",
          "Normal Multiparameter Models (part 3)",
          "Multinomial Multiparameter Models (part 1)",
          "Multinomial Multiparameter Models (part 2)",
          "Bioassay Experiment (part 1)",
          "Bioassay Experiment (part 2)",
          "Exercise: Comparing Two Proportions"
        ],
        "Bayesian Computation": [
          "Exercise Solution: Comparing Two Proportions (part 1)",
          "Exercise Solution: Comparing Two Proportions (part 2)",
          "Introduction to Bayesian Computation Section",
          "Computing Integrals to Estimate a Probability (part 1)",
          "Computing Integrals to Estimate a Probability (part 2)",
          "A Beta-Binomial Model of Overdispersion (part 1)",
          "A Beta-Binomial Model of Overdispersion (part 2)",
          "Exercise: Inference About a Normal Population"
        ],
        "Rejection and Importance Sampling": [
          "Exercise Solution: Inference about a Normal Population",
          "Rejection Sampling (part 1)",
          "Rejection Sampling (part 2)",
          "Rejection Sampling (part 3)",
          "Rejection Sampling (part 4)",
          "Rejection Sampling (part 5)",
          "Rejection Sampling (part 6)",
          "Importance Sampling"
        ],
        "Comparing Bayesian Models": [
          "One-Sided Test of a Normal Mean (part 1)",
          "One-Sided Test of a Normal Mean (part 2)",
          "One-Sided Test of a Normal Mean (part 3)",
          "Two-Sided Test of a Normal Mean",
          "Streaky Behavior (part 1)",
          "Streaky Behavior (part 2)",
          "Streaky Behavior (part 3)",
          "Streaky Behavior (part 4)"
        ]
      },
      "requirements": [
        "Students will need to install R and RStudio software, but ample instruction for doing so is provided in the course materials."
      ],
      "description": "Bayesian Computational Analyses with R is an introductory course on the use and implementation of Bayesian modeling using R software. The Bayesian approach is an alternative to the \"frequentist\" approach where one simply takes a sample of data and makes inferences about the likely parameters of the population. In contrast, the Bayesian approach uses both likelihood functions and a sample of observed data (the 'prior') to estimate the most likely values and distributions for the estimated population parameters (the 'posterior'). The course is useful to anyone who wishes to learn about Bayesian concepts and is suited to both novice and intermediate Bayesian students and Bayesian practitioners. It is both a practical, \"hands-on\" course with many examples using R scripts and software, and is conceptual, as the course explains the Bayesian concepts. All materials, software, R scripts, slides, exercises and solutions are included with the course materials. It is helpful to have some grounding in basic inferential statistics and probability theory. No experience with R is necessary, although it is also helpful.\nThe course begins with an introductory section (12 video lessons) on using R and R 'scripting.' The introductory section is intended to introduce RStudio and R commands so that even a novice R user will be comfortable using R. Section 2 introduces the Bayesian Rule, with examples of both discrete and beta priors, predictive priors, and beta posteriors in Bayesian estimation. Section 3 explains and demonstrates the use of Bayesian estimation for single parameter models, for example, when one wishes to estimate the most likely value of a mean OR of a standard deviation (but not both). Section 4 explains and demonstrates the use of \"conjugate mixtures.\" These are single-parameter models where the functional form of the prior and post are similar (for example, both normally distributed). But 'mixtures' imply there may be more than one component for the prior or posterior density functions. Mixtures enable the simultaneous test of competing, alternative theories as to which is more likely. Section 5 deals with multi-parameter Bayesian models where one is estimating the likelihood of more than one posterior variable value, for example, both mean AND standard deviation. Section 6 extends the Bayesian discussion by examining the estimation of integrals to estimate a probability. Section 7 covers the application the Bayesian approach to rejection and importance sampling and Section 8 looks at examples of comparing and validating Bayesian models.",
      "target_audience": [
        "The course is ideal for anyone interested in learning both the conceptual and practical side of using Bayes' Rule to model likely event outcomes.",
        "The course is best suited for both students and professionals who currently make use of quantitative or probabilistic modeling.",
        "It is useful to have a working knowledge of either basic inferential statistics or probability theory.",
        "It is NOT necessary to have prior experience using R software to successfully complete and to benefit from this course."
      ]
    },
    {
      "title": "A deep understanding of deep learning (with Python intro)",
      "url": "https://www.udemy.com/course/deeplearning_x/",
      "bio": "Master deep learning in PyTorch using an experimental scientific approach, with lots of examples and practice problems.",
      "objectives": [
        "The theory and math underlying deep learning",
        "How to build artificial neural networks",
        "Architectures of feedforward and convolutional networks",
        "Building models in PyTorch",
        "The calculus and code of gradient descent",
        "Fine-tuning deep network models",
        "Learn Python from scratch (no prior coding experience necessary)",
        "How and why autoencoders work",
        "How to use transfer learning",
        "Improving model performance using regularization",
        "Optimizing weight initializations",
        "Understand image convolution using predefined and learned kernels",
        "Whether deep learning models are understandable or mysterious black-boxes!",
        "Using GPUs for deep learning (much faster than CPUs!)"
      ],
      "course_content": {
        "Introduction": [
          "How to learn from this course",
          "Using Udemy like a pro"
        ],
        "Download all course materials": [
          "Downloading and using the code",
          "My policy on code-sharing"
        ],
        "Concepts in deep learning": [
          "What is an artificial neural network?",
          "How models \"learn\"",
          "The role of DL in science and knowledge",
          "Running experiments to understand DL",
          "Are artificial \"neurons\" like biological neurons?"
        ],
        "About the Python tutorial": [
          "Should you watch the Python tutorial?"
        ],
        "Math, numpy, PyTorch": [
          "PyTorch or TensorFlow?",
          "Introduction to this section",
          "Spectral theories in mathematics",
          "Terms and datatypes in math and computers",
          "Converting reality to numbers",
          "Vector and matrix transpose",
          "OMG it's the dot product!",
          "Matrix multiplication",
          "Softmax",
          "Logarithms",
          "Entropy and cross-entropy",
          "Min/max and argmin/argmax",
          "Mean and variance",
          "Random sampling and sampling variability",
          "Reproducible randomness via seeding",
          "The t-test",
          "Derivatives: intuition and polynomials",
          "Derivatives find minima",
          "Derivatives: product and chain rules"
        ],
        "Gradient descent": [
          "Overview of gradient descent",
          "What about local minima?",
          "Gradient descent in 1D",
          "CodeChallenge: unfortunate starting value",
          "Gradient descent in 2D",
          "CodeChallenge: 2D gradient ascent",
          "Parametric experiments on g.d.",
          "CodeChallenge: fixed vs. dynamic learning rate",
          "Vanishing and exploding gradients",
          "Tangent: Notebook revision history"
        ],
        "ANNs (Artificial Neural Networks)": [
          "The perceptron and ANN architecture",
          "A geometric view of ANNs",
          "ANN math part 1 (forward prop)",
          "ANN math part 2 (errors, loss, cost)",
          "ANN math part 3 (backprop)",
          "ANN for regression",
          "CodeChallenge: manipulate regression slopes",
          "ANN for classifying qwerties",
          "Learning rates comparison",
          "Multilayer ANN",
          "Linear solutions to linear problems",
          "Why multilayer linear models don't exist",
          "Multi-output ANN (iris dataset)",
          "CodeChallenge: more qwerties!",
          "Comparing the number of hidden units",
          "Depth vs. breadth: number of parameters",
          "Defining models using sequential vs. class",
          "Model depth vs. breadth",
          "CodeChallenge: convert sequential to class",
          "Diversity of ANN visual representations",
          "Reflection: Are DL models understandable yet?"
        ],
        "Overfitting and cross-validation": [
          "What is overfitting and is it as bad as they say?",
          "Cross-validation",
          "Generalization",
          "Cross-validation -- manual separation",
          "Cross-validation -- scikitlearn",
          "Cross-validation -- DataLoader",
          "Splitting data into train, devset, test",
          "Cross-validation on regression"
        ],
        "Regularization": [
          "Regularization: Concept and methods",
          "train() and eval() modes",
          "Dropout regularization",
          "Dropout regularization in practice",
          "Dropout example 2",
          "Weight regularization (L1/L2): math",
          "L2 regularization in practice",
          "L1 regularization in practice",
          "Training in mini-batches",
          "Batch training in action",
          "The importance of equal batch sizes",
          "CodeChallenge: Effects of mini-batch size"
        ],
        "Metaparameters (activations, optimizers)": [
          "What are \"metaparameters\"?",
          "The \"wine quality\" dataset",
          "CodeChallenge: Minibatch size in the wine dataset",
          "Data normalization",
          "The importance of data normalization",
          "Batch normalization",
          "Batch normalization in practice",
          "CodeChallenge: Batch-normalize the qwerties",
          "Activation functions",
          "Activation functions in PyTorch",
          "Activation functions comparison",
          "CodeChallenge: Compare relu variants",
          "CodeChallenge: Predict sugar",
          "Loss functions",
          "Loss functions in PyTorch",
          "More practice with multioutput ANNs",
          "Optimizers (minibatch, momentum)",
          "SGD with momentum",
          "Optimizers (RMSprop, Adam)",
          "Optimizers comparison",
          "CodeChallenge: Optimizers and... something",
          "CodeChallenge: Adam with L2 regularization",
          "Learning rate decay",
          "How to pick the right metaparameters"
        ]
      },
      "requirements": [
        "Interest in learning about deep learning!",
        "Python/Pytorch skills are taught in the course",
        "A Google account (google-colab is used as the Python IDE)"
      ],
      "description": "Deep learning is increasingly dominating technology and has major implications for society.\nFrom self-driving cars to medical diagnoses, from face recognition to deep fakes, and from language translation to music generation, deep learning is spreading like wildfire throughout all areas of modern technology.\nBut deep learning is not only about super-fancy, cutting-edge, highly sophisticated applications. Deep learning is increasingly becoming a standard tool in machine-learning, data science, and statistics. Deep learning is used by small startups for data mining and dimension reduction, by governments for detecting tax evasion, and by scientists for detecting patterns in their research data.\nDeep learning is now used in most areas of technology, business, and entertainment. And it's becoming more important every year.\n\n\nHow does deep learning work?\nDeep learning is built on a really simple principle: Take a super-simple algorithm (weighted sum and nonlinearity), and repeat it many many times until the result is an incredibly complex and sophisticated learned representation of the data.\nIs it really that simple? mmm OK, it's actually a tiny bit more complicated than that ;)   but that's the core idea, and everything else -- literally everything else in deep learning -- is just clever ways of putting together these fundamental building blocks. That doesn't mean the deep neural networks are trivial to understand: there are important architectural differences between feedforward networks, convolutional networks, and recurrent networks.\nGiven the diversity of deep learning model designs, parameters, and applications, you can only learn deep learning -- I mean, really learn deep learning, not just have superficial knowledge from a youtube video -- by having an experienced teacher guide you through the math, implementations, and reasoning. And of course, you need to have lots of hands-on examples and practice problems to work through. Deep learning is basically just applied math, and, as everyone knows, math is not a spectator sport!\n\n\nWhat is this course all about?\nSimply put: The purpose of this course is to provide a deep-dive into deep learning. You will gain flexible, fundamental, and lasting expertise on deep learning. You will have a deep understanding of the fundamental concepts in deep learning, so that you will be able to learn new topics and trends that emerge in the future.\nPlease note: This is not a course for someone who wants a quick overview of deep learning with a few solved examples. Instead, this course is designed for people who really want to understand how and why deep learning works; when and how to select metaparameters like optimizers, normalizations, and learning rates; how to evaluate the performance of deep neural network models; and how to modify and adapt existing models to solve new problems.\n\n\nYou can learn everything about deep learning in this course.\nIn this course, you will learn\nTheory: Why are deep learning models built the way they are?\nMath: What are the formulas and mechanisms of deep learning?\nImplementation: How are deep learning models actually constructed in Python (using the PyTorch library)?\nIntuition: Why is this or that metaparameter the right choice? How to interpret the effects of regularization? etc.\nPython: If you're completely new to Python, go through the 8+ hour coding tutorial appendix. If you're already a knowledgeable coder, then you'll still learn some new tricks and code optimizations.\nGoogle-colab: Colab is an amazing online tool for running Python code, simulations, and heavy computations using Google's cloud services. No need to install anything on your computer.\n\n\nUnique aspects of this course\nClear and comprehensible explanations of concepts in deep learning, including transfer learning, generative modeling, convolutional neural networks, feedforward networks, generative adversarial networks (GAN), and more.\nSeveral distinct explanations of the same ideas, which is a proven technique for learning.\nVisualizations using graphs, numbers, and spaces that provide intuition of artificial neural networks.\nLOTS of exercises, projects, code-challenges, suggestions for exploring the code. You learn best by doing it yourself!\nActive Q&A forum where you can ask questions, get feedback, and contribute to the community.\n8+ hour Python tutorial. That means you don't need to master Python before enrolling in this course.\n\n\nSo what are you waiting for??\nWatch the course introductory video and free sample videos to learn more about the contents of this course and about my teaching style. If you are unsure if this course is right for you and want to learn more, feel free to contact with me questions before you sign up.\nI hope to see you soon in the course!\nMike",
      "target_audience": [
        "Students in a deep learning course",
        "Machine-learning enthusiasts",
        "Anyone interested in mechanisms of AI (artificial intelligence)",
        "Data scientists who want to expand their library of skills",
        "Aspiring data scientists",
        "Scientists and researchers interested in deep learning"
      ]
    },
    {
      "title": "Machine Learning with Python : COMPLETE COURSE FOR BEGINNERS",
      "url": "https://www.udemy.com/course/machine-learning-a-z-with-python-with-project-beginner/",
      "bio": "Complete Machine Learning Course with Python for beginners",
      "objectives": [
        "Master Machine Learning on Python",
        "Make powerful analysis",
        "Make accurate predictions",
        "Make robust Machine Learning models",
        "Use Machine Learning for personal purpose",
        "Build an army of powerful Machine Learning models and know how to combine them to solve any problem",
        "Classify data using K-Means clustering, Support Vector Machines (SVM), KNN, Decision Trees, Naive Bayes, and PCA",
        "Clean your input data to remove outliers"
      ],
      "course_content": {
        "Foundation": [
          "Introduction",
          "Udemy Reviews Update"
        ],
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning"
        ],
        "Applied Statistics": [
          "Statistics 101",
          "Descriptive Statistics",
          "Descriptive Statistics (Part-2)",
          "Measures of Spread",
          "Probability",
          "Conditional Probability",
          "Probability Distribution",
          "Hypothesis Testing"
        ],
        "ntroduction to Python": [
          "Python Installation",
          "Python IDE",
          "Python_Basics",
          "Python Basics II",
          "Data Structures",
          "Numpy",
          "Pandas",
          "Data Visualisation",
          "Data Transformation"
        ],
        "Let's dig Machine Learning": [
          "Machine Learning Intro"
        ],
        "Regression": [
          "Linear Regression"
        ],
        "Classification": [
          "Logistic Regression",
          "KNN",
          "Naïve Bayes",
          "SVM",
          "Decision Tree"
        ],
        "Clustering": [
          "K-means",
          "Hierarchical Clustering",
          "DBScan"
        ],
        "Ensemble ML": [
          "Bagging",
          "Boosting"
        ],
        "Our Project (Recomendation System)": [
          "PCA",
          "Recommendations System"
        ]
      },
      "requirements": [
        "No prior experience needed, you will learn what is needed. (A basic python knowledge will definetly increase your chances of learning fast))"
      ],
      "description": "Machine Learning and artificial intelligence (AI) is everywhere; if you want to know how companies like Google, Amazon, and even Udemy extract meaning and insights from massive data sets, this data science course will give you the fundamentals you need. Data Scientists enjoy one of the top-paying jobs, with an average salary of $120,000 according to Glassdoor and Indeed. That's just the average! And it's not just about money - it's interesting work too!\n\n\nMachine Learning (Complete course Overview)\nFoundations\nIntroduction to Machine Learning\nIntro\nApplication of machine learning in different fields.\nAdvantage of using Python libraries. (Python for machine learning).\nPython for AI & ML\nPython Basics\nPython functions, packages, and routines.\nWorking with Data structure, arrays, vectors & data frames. (Intro Based with some examples)\nJupyter notebook- installation & function\nPandas, NumPy, Matplotib, Seaborn\nApplied Stastistics\nDescriptive statistics\nProbability & Conditional Probability\nHypothesis Testing\nInferential Statistics\nProbability distributions – Types of distribution – Binomial, Poisson & Normal distribution\nMachine Learning\nSupervised Learning\nMultiple variable Linear regression\nRegression\nIntroduction to Regression\nSimple linear regression\nModel Evaluation in Regression Models\nEvaluation Metrics in Regression Models\nMultiple Linear Regression\nNon-Linear Regression\nNaïve bayes classifiers\nMultiple regression\nK-NN classification\nSupport vector machines\nUnsupervised Learning\nIntro to Clustering\nK-means clustering\nHigh-dimensional clustering\nHierarchical clustering\nDimension Reduction-PCA\nClassification\nIntroduction to Classification\nK-Nearest Neighbours\nEvaluation Metrics in Classification\nIntroduction to decision tress\nBuilding Decision Tress\nInto Logistic regression\nLogistic regression vs Linear Regression\nLogistic Regression training\nSupport vector machine\nEnsemble Techniques\nDecision Trees\nBagging\nRandom Forests\nBoosting\nFeaturization, Model selection & Tuning\nFeature engineering\nModel performance\nML pipeline\nGrid search CV\nK fold cross-validation\nModel selection and tuning\nRegularising Linear models\nBootstrap sampling\nRandomized search CV\nRecommendation Systems\nIntroduction to recommendation systems\nPopularity based model\nHybrid models\nContent based recommendation system\nCollaborative filtering\nAdditional Modules\nEDA\nPandas-profiling library\nTime series forecasting\nARIMA Approach\nModel Deployment\nKubernetes\nCapstone Project\n\n\nIf you've got some programming or scripting experience, this course will teach you the techniques used by real data scientists and machine learning practitioners in the tech industry - and prepare you for a move into this hot career path.\nEach concept is introduced in plain English, avoiding confusing mathematical notation and jargon. It’s then demonstrated using Python code you can experiment with and build upon, along with notes you can keep for future reference. You won't find academic, deeply mathematical coverage of these algorithms in this course - the focus is on practical understanding and application of them. At the end, you'll be given a final project to apply what you've learned!\nOur Learner's Review: Excellent course. Precise and well-organized presentation. The complete course is filled with a lot of learning not only theoretical but also practical examples. Mr. Risabh is kind enough to share his practical experiences and actual problems faced by data scientists/ML engineers. The topic of \"The ethics of deep learning\" is really a gold nugget that everyone must follow. Thank you, 1stMentor  and SelfCode Academy for this wonderful course.",
      "target_audience": [
        "Beginner Python Developers enthusiastic about Learning Machine Learning and Data Science",
        "Anyone interested in Machine Learning.",
        "Students who have at least high school knowledge in math and who want to start learning Machine Learning.",
        "Any intermediate level people who know the basics of machine learning, including the classical algorithms like linear regression or logistic regression, but who want to learn more about it and explore all the different fields of Machine Learning.",
        "Any people who are not that comfortable with coding but who are interested in Machine Learning and want to apply it easily on datasets.",
        "Any students in college who want to start a career in Data Science.",
        "Any data analysts who want to level up in Machine Learning.",
        "Any people who want to create added value to their business by using powerful Machine Learning tools."
      ]
    },
    {
      "title": "OpenAI & ChatGPT API's: Expert Fine-tuning for Developers",
      "url": "https://www.udemy.com/course/mastering-chatgpt-models-from-fine-tuning-to-deployment-openai/",
      "bio": "API, ChatGPT, Prompt Engineering, Finetuning, OpenAI, Integrate ChatGPT, Business AI, Generative AI, Chat GPT 4",
      "objectives": [
        "Understand key concepts of fine-tuning pre-trained models.",
        "Learn to determine if fine-tuning is appropriate for a specific problem.",
        "Identify types of pre-trained models and their strengths and weaknesses.",
        "Prepare data for fine-tuning with tokenization and encoding.",
        "Fine-tune a pre-trained model for a specific NLP application"
      ],
      "course_content": {
        "Introduction to Fine-Tuning": [
          "Understanding GPT Models and Fine-Tuning for Specific Domains",
          "Mastering Fine-tuning for NLP Applications",
          "Introduction to OpenAI Playground",
          "GPT Models and Fine-tuning"
        ],
        "Formatting data for Fine-Tuning": [
          "Maximizing Model Accuracy with Data Preparation and Formatting",
          "Fine-tuning a GPT Model: Understanding Data Formats",
          "Data Cleaning with Python: Removing Missing Values and Duplicates",
          "Generating Questions and Answers from Text for Fine-tuning AI Models",
          "Generate Questions and Answers from a raw text",
          "Using ChatGPT to generate Python code for data manipulation",
          "Running and Executing Python Code on Google Colaboratory",
          "Creating prompts and completions from CSV data.",
          "Building a Well-Structured Project Directory"
        ],
        "Fine-Tuning a Model": [
          "How to Choose a Pre-Trained Model for Fine-Tuning",
          "Preview to Fine Tuning Process",
          "Fine-tuning a Pre-trained Model: A Three-Stage Process",
          "Testing Your Fine-Tuned Model on OpenAI Playground"
        ],
        "Develop your own chatbot based on GPT 3.5 Model": [
          "Getting Started with Postman and OpenAI API: Configuring the Environment",
          "Creating a Chatbot Using OpenAI APIs: Incorporating Model Integration and Rules",
          "Creating a Basic Chatbot Web Page with Integration of OpenAI API"
        ],
        "Data Preparation, Fine-Tuning, and Testing of GPT-3.5 Turbo Models": [
          "Fine-Tuning GPT-3.5 Turbo Models: An Introduction",
          "Data Preparation for Fine-Tuning GPT-3.5 Turbo Models",
          "Executing Fine-Tuning on GPT-3.5 Turbo with Python",
          "Verifying and Testing Your Fine-Tuned GPT-3.5 Turbo Model",
          "Fine-Tuning of GPT-3.5 Turbo via OpenAI Playground"
        ]
      },
      "requirements": [
        "Basic knowledge of programming concepts and Python.",
        "Basic understanding of natural language processing (NLP)."
      ],
      "description": "In this course, you will discover the power of GPT-3 in creating conversational AI solutions.\n\nWe will start with an introduction to chatbots and their use cases, and then dive deep into GPT-3 and its capabilities. You will learn how to fine-tune the model for specific tasks, such as customer service, lead generation, or entertainment. We will cover techniques for improving the accuracy and fluency of the chatbot's responses, as well as strategies for handling user input and managing conversation flow.\nNext, we will explore different ways to integrate GPT-3 chatbots with various platforms and channels, such as messaging apps, voice assistants, and social media. You will learn how to use APIs and SDKs to connect your chatbot to these platforms and leverage their features, such as natural language processing, voice recognition, or rich media support. We will also cover best practices for designing chatbot user interfaces and testing and deploying your chatbot in production.\nBy the end of this course, you will have a solid understanding of how GPT-3 works and how to use it to build powerful and engaging chatbots for your business or personal projects. You will have hands-on experience with fine-tuning GPT-3 models and integrating them with various platforms and channels, and you will be ready to apply these skills in real-world scenarios.",
      "target_audience": [
        "Tech-savvy individuals",
        "Data scientists and machine learning engineers.",
        "Software developers interested in NLP.",
        "Business analysts or consultants.",
        "Researchers or academics."
      ]
    },
    {
      "title": "Time Series Analysis and Forecasting with Python",
      "url": "https://www.udemy.com/course/time-series-analysis-and-forecasting-with-python/",
      "bio": "Learn Python for Pandas, Statsmodels, ARIMA, SARIMAX, Deep Learning, LSTM and Forecasting into Future",
      "objectives": [
        "Basic Packages, NumPy, Pandas & Matplotlib",
        "Time Series with Pandas (Creating Date Time index, Resampling, ...)",
        "Analyzing Time Series Data Using Statsmodels Package",
        "The Concept of ARIMA and SARIMAX method and How to Forecast into the Future Using Them",
        "The Concept of Deep Learning from A-Z",
        "Forecast into the Future Using LSTM Model for Single Variant",
        "Forecast into the Future Using LSTM Model for Multi Variant"
      ],
      "course_content": {
        "Introduction": [
          "Course Content",
          "Python IDE Installation 1",
          "Python IDE Installation 2",
          "Python IDE Installation 3",
          "Installing Required Libraries"
        ],
        "Useful Packages": [
          "Source Codes",
          "Notice!!",
          "NumPy 1",
          "NumPy 2",
          "NumPy 3",
          "NumPy 4",
          "NumPy 5",
          "NumPy 6",
          "Pandas 1",
          "Pandas 2",
          "Pandas 3",
          "Pandas 4",
          "Matplotlib 1",
          "Matplotlib 2",
          "Matplotlib 3",
          "Matplotlib 4",
          "Matplotlib 5"
        ],
        "Pandas for Time Series Analysis": [
          "Create a Datetime with Pandas",
          "Set Datetime as Index",
          "Resampling Method",
          "Pandas Date Frequencies"
        ],
        "Statistical Models for Time Series Forecasting": [
          "Introduction to ARIMA",
          "ARIMA Model Development 1",
          "ARIMA Model Development 2",
          "ARIMA Model Development 3",
          "Introduction to SARIMAX",
          "SARIMAX Model Development 1",
          "SARIMAX Model Development 2",
          "SARIMAX Model Development 3"
        ],
        "Deep Learning for Time Series Forecasting": [
          "Introduction to Deep Learning - Basic Concepts",
          "Introduction to Deep Learning - Activation Function",
          "Introduction to Deep Learning - How Neural Network Learn?",
          "Introduction to Deep Learning - Optimization",
          "Introduction to Deep Learning - Recurrent Neural Network",
          "Introduction to Deep Learning - LSTM Method",
          "Development of Univariate LSTM Model 1",
          "Development of Univariate LSTM Model 2",
          "Development of Univariate LSTM Model 3",
          "Development of Univariate LSTM Model 4",
          "Development of Univariate LSTM Model 5",
          "Development of Univariate LSTM Model 6",
          "Development of Multivariate LSTM Model 1",
          "Development of Multivariate LSTM Model 2",
          "Development of Multivariate LSTM Model 3",
          "Development of Multivariate LSTM Model 4",
          "Development of Multivariate LSTM Model 5"
        ],
        "Bonus": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "General and Basic Python Skills"
      ],
      "description": "\"Time Series Analysis and Forecasting with Python\" Course is an ultimate source for learning the concepts of Time Series and forecast into the future.\nIn this course, the most famous methods such as statistical methods (ARIMA and SARIMAX) and Deep Learning Method (LSTM) are explained in detail. Furthermore, several Real World projects are developed in a Python environment and have been explained line by line!\nIf you are a researcher, a student, a programmer, or a data science enthusiast that is seeking a course that shows you all about time series and prediction from A-Z, you are in a right place. Just check out what you will learn in this course below:\nBasic libraries (NumPy, Pandas, Matplotlib)\nHow to use Pandas library to create DateTime index and how to set that as your Dataset index\nWhat are statistical models?\nHow to forecast into future using the ARIMA model?\nHow to capture the seasonality using the SARIMAX model?\nHow to use endogenous variables and predict into future?\nWhat is Deep Learning (Very Basic Concepts)\nAll about Artificial and Recurrent Neural Network!\nHow the LSTM method Works!\nHow to develop an LSTM model with a single variate?\nHow to develop an LSTM model using multiple variables (Multivariate)\nAs I mentioned above, in this course we tried to explain how you can develop an LSTM model when you have several predictors (variables) for the first time and you can use that for several applications and use the source code for your project as well!\nThis course is for Everyone! yes everyone! that wants t to learn time-series and forecasting into the future using statistics and artificial intelligence with any kind of background! Even if you are not a programmer, I show you how to code and develop your model line by line!\nIf you want to master the basics of Machine Learning in Python as well, you can check my other courses!",
      "target_audience": [
        "Data Science Enthusiast",
        "Beginner Programmers",
        "Python Developers",
        "Recheachers who like to forecast into future",
        "Data Analysts",
        "Anyone who is interested in Time Series and Future Forecasting"
      ]
    },
    {
      "title": "Learn By Example: Statistics and Data Science in R",
      "url": "https://www.udemy.com/course/statistics-and-data-science-in-r/",
      "bio": "A gentle yet thorough introduction to Data Science, Statistics and R using real life examples",
      "objectives": [
        "Harness R and R packages to read, process and visualize data",
        "Understand linear regression and use it confidently to build models",
        "Understand the intricacies of all the different data structures in R",
        "Use Linear regression in R to overcome the difficulties of LINEST() in Excel",
        "Draw inferences from data and support them using tests of significance",
        "Use descriptive statistics to perform a quick study of some data and present results"
      ],
      "course_content": {
        "Introduction": [
          "You, This course and Us",
          "Top Down vs Bottoms Up : The Google vs McKinsey way of looking at data",
          "R and RStudio installed"
        ],
        "The 10 second answer : Descriptive Statistics": [
          "Descriptive Statistics : Mean, Median, Mode",
          "Our first foray into R : Frequency Distributions",
          "Draw your first plot : A Histogram",
          "Computing Mean, Median, Mode in R",
          "What is IQR (Inter-quartile Range)?",
          "Box and Whisker Plots",
          "The Standard Deviation",
          "Computing IQR and Standard Deviation in R"
        ],
        "Inferential Statistics": [
          "Drawing inferences from data",
          "Random Variables are ubiquitous",
          "The Normal Probability Distribution",
          "Sampling is like fishing",
          "Sample Statistics and Sampling Distributions"
        ],
        "Case studies in Inferential Statistics": [
          "Case Study 1 : Football Players (Estimating Population Mean from a Sample)",
          "Case Study 2 : Election Polling (Estimating Population Proportion from a Sample)",
          "Case Study 3 : A Medical Study (Hypothesis Test for the Population Mean)",
          "Case Study 4 : Employee Behavior (Hypothesis Test for the Population Proportion)",
          "Case Study 5: A/B Testing (Comparing the means of two populations)",
          "Case Study 6: Customer Analysis (Comparing the proportions of 2 populations)"
        ],
        "Diving into R": [
          "Harnessing the power of R",
          "Assigning Variables",
          "Printing an output",
          "Numbers are of type numeric",
          "Characters and Dates",
          "Logicals"
        ],
        "Vectors": [
          "Data Structures are the building blocks of R",
          "Creating a Vector",
          "The Mode of a Vector",
          "Vectors are Atomic",
          "Doing something with each element of a Vector",
          "Aggregating Vectors",
          "Operations between vectors of the same length",
          "Operations between vectors of different length",
          "Generating Sequences",
          "Using conditions with Vectors",
          "Find the lengths of multiple strings using Vectors",
          "Generate a complex sequence (using recycling)",
          "Vector Indexing (using numbers)",
          "Vector Indexing (using conditions)",
          "Vector Indexing (using names)"
        ],
        "Arrays": [
          "Creating an Array",
          "Indexing an Array",
          "Operations between 2 Arrays",
          "Operations between an Array and a Vector",
          "Outer Products"
        ],
        "Matrices": [
          "A Matrix is a 2-Dimensional Array",
          "Creating a Matrix",
          "Matrix Multiplication",
          "Merging Matrices",
          "Solving a set of linear equations"
        ],
        "Factors": [
          "What is a factor?",
          "Find the distinct values in a dataset (using factors)",
          "Replace the levels of a factor",
          "Aggregate factors with table()",
          "Aggregate factors with tapply()"
        ],
        "Lists and Data Frames": [
          "Introducing Lists",
          "Introducing Data Frames",
          "Reading Data from files",
          "Indexing a Data Frame",
          "Aggregating and Sorting a Data Frame",
          "Merging Data Frames"
        ]
      },
      "requirements": [
        "No prerequisites : We start from basics and cover everything you need to know. We will be installing R and RStudio as part of the course and using it for most of the examples. Excel is used for one of the examples and basic knowledge of excel is assumed."
      ],
      "description": "Taught by a Stanford-educated, ex-Googler and an IIT, IIM - educated ex-Flipkart lead analyst. This team has decades of practical experience in quant trading, analytics and e-commerce.\n\nThis course is a gentle yet thorough introduction to Data Science, Statistics and R using real life examples.\nLet’s parse that.\n\nGentle, yet thorough: This course does not require a prior quantitative or mathematics background. It starts by introducing basic concepts such as the mean, median etc and eventually covers all aspects of an analytics (or) data science career from analysing and preparing raw data to visualising your findings.\n\nData Science, Statistics and R: This course is an introduction to Data Science and Statistics using the R programming language. It covers both the theoretical aspects of Statistical concepts and the practical implementation using R.\n\nReal life examples: Every concept is explained with the help of examples, case studies and source code in R wherever necessary. The examples cover a wide array of topics and range from A/B testing in an Internet company context to the Capital Asset Pricing Model in a quant finance context.\n\nWhat's Covered:\nData Analysis with R: Datatypes and Data structures in R, Vectors, Arrays, Matrices, Lists, Data Frames, Reading data from files, Aggregating, Sorting & Merging Data Frames\n\nLinear Regression: Regression, Simple Linear Regression in Excel, Simple Linear Regression in R, Multiple Linear Regression in R, Categorical variables in regression, Robust regression, Parsing regression diagnostic plots\nData Visualization in R: Line plot, Scatter plot, Bar plot, Histogram, Scatterplot matrix, Heat map, Packages for Data Visualisation : Rcolorbrewer, ggplot2\n\nDescriptive Statistics: Mean, Median, Mode, IQR, Standard Deviation, Frequency Distributions, Histograms, Boxplots\nInferential Statistics: Random Variables, Probability Distributions, Uniform Distribution, Normal Distribution, Sampling, Sampling Distribution, Hypothesis testing, Test statistic, Test of significance",
      "target_audience": [
        "Yep! MBA graduates or business professionals who are looking to move to a heavily quantitative role",
        "Yep! Engineers who want to understand basic statistics and lay a foundation for a career in Data Science",
        "Yep! Analytics professionals who have mostly worked in Descriptive analytics and want to make the shift to being modelers or data scientists",
        "Yep! Folks who've worked mostly with tools like Excel and want to learn how to use R for statistical analysis"
      ]
    },
    {
      "title": "A Beginner's Guide To Machine Learning with Unity",
      "url": "https://www.udemy.com/course/machine-learning-with-unity/",
      "bio": "Advanced games AI with genetic algorithms, neural networks & Q-learning in C# and Tensorflow for Unity",
      "objectives": [
        "Build a genetic algorithm from scratch in C#.",
        "Build a neural network from scratch in C#.",
        "Setup and explore the Unity ML-Agents plugin.",
        "Setup and use Tensorflow to train game characters.",
        "Apply newfound knowledge of machine learning to integrate contemporary research ideas in the field into their own projects.",
        "Distill the mathematics and statistic behind machine learning to working program code.",
        "Use a Proximal Policy Optimisation to train a neural network."
      ],
      "course_content": {},
      "requirements": [
        "You should be familiar with the Unity Game Engine.",
        "You should have a working knowledge of C#.",
        "You should have a healthy appreciation for mathematics and statistics."
      ],
      "description": "What if you could build a character that could learn while it played?  Think about the types of gameplay you could develop where the enemies started to outsmart the player. This is what machine learning in games is all about. In this course, we will discover the fascinating world of artificial intelligence beyond the simple stuff and examine the increasingly popular domain of machines that learn to think for themselves.\nIn this course, Penny introduces the popular machine learning techniques of genetic algorithms and neural networks using her internationally acclaimed teaching style and knowledge from a Ph.D in game character AI and over 25 years experience working with games and computer graphics.  In addition she's written two award winning books on games AI and two others best sellers on Unity game development. Throughout the course you will follow along with hands-on workshops designed to teach you about the fundamental machine learning techniques, distilling the mathematics in a way that the topic becomes accessible to the most noob of novices.\nLearn how to program and work with:\ngenetic algorithms\nneural networks\nhuman player captured training sets\nreinforcement learning\nUnity's ML-Agent plugin\nTensorflow\nContents and Overview\nThe course starts with a thorough examination of genetic algorithms that will ease you into one of the simplest machine learning techniques that is capable of extraordinary learning. You'll develop an agent that learns to camouflage, a Flappy Bird inspired application in which the birds learn to make it through a maze and environment-sensing bots that learn to stay on a platform.\nFollowing this, you'll dive right into creating your very own neural network in C# from scratch.  With this basic neural network, you will find out how to train behaviour, capture and use human player data to train an agent and teach a bot to drive.  In the same section you'll have the Q-learning algorithm explained, before integrating it into your own applications.\nBy this stage, you'll feel confident with the terminology and techniques used throughout the deep learning community and be ready to tackle Unity's experimental ML-Agents. Together with Tensorflow, you'll be throwing agents in the deep-end and reinforcing their knowledge to stay alive in a variety of game environment scenarios.\nBy the end of the course, you'll have a well-equipped toolset of basic and solid machine learning algorithms and applications, that will see you able to decipher the latest research publications and integrate the latest developments into your work, while keeping abreast of Unity's ML-Agents as they evolve from experimental to production release.\nWhat students are saying about this course:\nAbsolutely the best beginner to Advanced course for Neural Networks/ Machine Learning if you are a game developer that uses C# and Unity. BAR NONE x Infinity.\nA perfect course with great math examples and demonstration of the TensorFlow power inside Unity. After this course, you will get the strong basic background in the Machine Learning.\nThe instructor is very engaging and knowledgeable. I started learning from the first lesson and it never stopped. If you are interested in Machine Learning , take this course.",
      "target_audience": [
        "Anyone wanting to learn about the potential of machine learning in games.",
        "Anyone wanting a deeper understanding of the algorithms and theories underlying Unity's ML-Agents.",
        "Anyone wanting to know how to setup and work with ML-Agents."
      ]
    },
    {
      "title": "Fundamentals of Responsible Artificial Intelligence/ML",
      "url": "https://www.udemy.com/course/responsible-ai-ml/",
      "bio": "Designing and mantaining AI/ML models that help data subjects, are explainable, are not biased, and are compliant.",
      "objectives": [
        "Most problems with AI/ML models or their data, as well as how to address them",
        "How to identify and mitigate ethical risks from AI/ML models, as well as comply with regulation",
        "What is XAI (explainable AI), as well as the most common explanation elements and popular frameworks",
        "Relevant regulation that impacts AI models, and how"
      ],
      "course_content": {},
      "requirements": [
        "Have a basic knowledge of AI and ML"
      ],
      "description": "ARTIFICIAL INTELLIGENCE? NATURAL KNOWLEDGE\nThere's no doubt that AI is everywhere.\nIn our cell phones, our computers, our cars, our apps, and many other aspects of life.\nKnowing how to design and train effective AI and ML models is not an easy task.\nBut even when you master it, these may not be responsible.\nDue to bias, error, malicious management, or other factors, AI/ML models may hurt data subjects.\nThere are, naturally, several courses on fragmented topics of the AI industry.\nHow to train models, how to debias datasets, and other specific areas.\nFrequently, you can find information on aspects of AI, or on aspects of responsible AI. But not both.\nAnd on top that, many courses use different definitions, so you may become confused.\nIn short, most courses on AI don't present a single, united source of training on responsible AI.\nAnd this has consequences not just for your career, but yourself personally as well.\nWhat happens when you don't have enough information (or in the adequate format)?\nYou'll become confused by what are responsible - and irresponsible - algorithms. Do we need debiasing? Do we need explainability? Others?\nYou won't be able to properly diagnose - and address - model problems that may be hurting data subjects;\nYou'll become frustrated and irritated due to not knowing what is wrong with a specific model;\nYou won't be able to make choices in terms of the models - from specific to sensitive classifiers, accurate versus explainable models, or many other specific model inferences - that, each, carry ethical consequences;\nYou won't be able to tell an employer - or the end user - that you can design \"responsible\" AI, with confidence;\nSo, if you want to know everything about what makes AI/ML models responsible (or not), as well as how to address issues, where should you head?\nThis new course, of course!\n\n\n\n\nA RESPONSIBLE COURSE FOR RESPONSIBLE AI\nUnlike other responsible AI model courses you'll find out there, this course is comprehensive and updated.\nIn other words, not only did I make sure that you'll find more topics (and more in-depth) than in any other course you may find, but I also made sure to keep the information relevant to the types of models and use cases you will find nowadays.\nDesigning responsible AI models may seem complex (and it is, to a point), but it relies on a few key, simple principles.\nIn this course, you'll learn about the essentials of how models are designed without bias, how they can become explainable, and how to mitigate the ethical risks posed by them.\nNot only that, we'll dive deep into the activities, stakeholders, projects and resources involved in responsible AI model design.\nIn this 8.5-hour+ masterclass, you'll find the following modules:\nYou'll learn about the Fundamentals. We start by clarifying what is model bias, XAI (explainable AI), the usual ethical risks posed by AI, and an introduction to the different disciplines;\nYou'll learn about Responsible Data and Models. All types of problems that may occur with a model or its data, from data drift, to overloaded/correlated features, wrong inferences, overfitting, and many other issues with either the model or the data - and how to address them;\nYou'll learn about Transparency and Explainability. We will cover the discipline of XAI, or explainable AI, the basics of justifications, recipients, what makes good justifications, as well as some popular frameworks for AI explainability, such as LIME, SHAP and TCAV;\nYou'll learn about Ethics and Ethical Risks in AI. What are the specific ethical risks associated with a given AI/ML model, with the product that contains it, with the management of the company itself, as well as what regulation may affect your model decisions, and how your AI model may impact society, as a whole, with time and scale;\nBy the end of this course, you will know exactly what makes AI/ML models irresponsible, and how to design responsible models, which help people, not hurt them, while still being useful and accurate.\nThe best of this course? Inside you'll find all of these 4 modules.\n\n\n\n\nTHE PERFECT COURSE... FOR WHOM?\nThis course is targeted at different types of people.\nNaturally, if you're a current or future AI/ML practitioner, you will find this course useful, as well as if you are any other professional or executive involved in the design of AI/ML models for any purpose.\nBut even if you're any other type of professional that aims to know more about how AI/ML works, and how it may become responsible, you'll find the course useful.\nMore specifically, you're the ideal student for this course if:\nYou're someone who wants to know more about AI/ML themselves (how they transform inputs into outputs, different types of models, their characteristics, and what may go wrong with each);\nYou're someone who is interested in scrutinising AI/ML models (what makes models be sensitive, yield wrong outputs, and/or develop biases for specific parts of the population);\nYou're some who is interested in ethics in tech (how AI/ML models may worsen societal problems, discriminate minorities, or otherwise make automated decisions, at scale, that may damage data subjects);\n\n\n\n\nLET ME TELL YOU... EVERYTHING\nSome people - including me - love to know what they're getting in a package.\nAnd by this, I mean, EVERYTHING that is in the package.\nSo, here is a list of everything that this masterclass covers:\nFundamentals\nYou'll learn about the basics of irresponsible models. Models that have data or model issues, that are not explainable, or whose ethical risks are not hedged against (or not acknowledged) by the company;\nYou'll learn about the ethical consequences of classifiers and regressors, placing inputs in wrong categories, or estimating wrong values for these, as well as what this may cause. Also, the differences between discriminative and generative models, in terms of what may happen;\nYou'll learn about the specific ethical consequences of sensitive versus specific classifiers, and what they cause with different thresholds, as well as the analogous dilemma in regressors - being flexible versus efficient;\nYou'll learn about the dilemma of accuracy versus explainability, where different models sit on the accuracy/explainability spectrum, and how your use case usually guides model architecture;\nResponsible Data and Models:\nYou'll learn about model feature issues, which are issues caused by the selection of wrong (or biased) features in the model, including reduced features, having a historical focus, using proxies, defaulting, having overloaded/correlated features, \"overefficiency\" and feedback loops;\nYou'll learn about the use of reduced features, when a model tries to distill reality into one (or few) features, and its consequences;\nYou'll learn about having a historical focus, which traps people (or other inputs) into their \"past versions\", many times perpetuating feedback loops and disadvantages;\nYou'll learn about proxies, which are features that approximate other, unavailable features, but bring biases of their own, and of what types;\nYou'll learn about defaulting - forcing people (or inputs) into specific categories, and what happens when inputs default to the wrong categories (or to no category);\nYou'll learn about overloaded or correlated features, which are features that seem independent at first sight, but actually have additional meaning, many times encoding financial, racial and other information;\nYou'll learn about \"overefficiency\", the name that I give to the goal of maximising the value of one single feature, at all costs, regardless of the consequences that has to people and other elements;\nYou'll learn about feedback loops - what happens when model outputs are later used as inputs, and how that can perpetuate biases and discrimination;\nYou'll learn about model issues, which are problems with the model itself, or its training;\nYou'll learn about adversarial sensitivity - what happens when a model is not trained for noise, and, therefore, with small changes in inputs produces wild fluctuations in outputs;\nYou'll learn about overfitting - what happens when a model is trained just for one use case (or type of data), and the specific ethical consequences of it;\nYou'll learn about data issues, which are problems either with the training data or production data themselves:\nYou'll learn about biased data, how they occur, the biases that you may have, yourself, as a practitioner, and how to address them;\nYou'll learn about data drift - when production data starts to have different characteristics from training data, as well as the specific consequences of this process;\nYou'll learn about data-centric approaches - techniques to improve data quality;\nYou'll learn about some data quantity dilemmas - what to do when we have too much data for a feature, as well as when we don't have data at all, and the consequences of different choices;\nYou'll learn about dataset hygiene, including responsibilities for sourcing and maintaining data, handling metadata, and other basic elements to assure high-quality training data;\nYou'll learn about diversity and debiasing - how datasets may become biased (in terms of location, ethnicity, financial status, or any other feature), and how to address your own biases as a practitioner, such as anchoring bias, survivorship bias, confirmation bias, and many others;\nYou'll learn about data profiling - how to increase the quality of data by detecting formats, patterns, business rules, statistical measures and more insights about data;\nYou'll learn about ethical data dimensions - besides the \"usual\" data dimensions in profiling, such as accuracy or completeness, using dimensions that specifically measure how ethical data are, such as fairness, privacy, transparency and others, and that indicate fair AI/ML model decisions (or the lack of them!);\nYou'll learn about data usage purpose/authority, and how the same data may be processed, in a company, for one purpose, and not for another one, safeguarding data subjects if the company does not have a purpose for their data;\nYou'll learn about model-centric approaches - decisions about the model, itself, to make its inferences more ethical:\nYou'll learn about the dilemmas of human overrides, and the ethical consequences of allowing users to go against AI outputs - or of not allowing them to;\nYou'll learn about different inference decisions, from the thresholds selected for classifiers, tuning recommendations based on personal information, and other dilemmas, as well as their consequences;\nYou'll learn about specialist validation - why it's crucial to validate model design choices and goals with business experts, and not make the AI/ML practitioner responsible for these - and why;\nYou'll learn about subpopulation considerations - what happens when your model treats a specific subpopulation in a different way, and the dilemma of \"breaking off\" a model copy for a different subpopulation versus optimising the model, itself;\nTransparency and Explainability:\nYou'll learn about the basics of explainability. What is the discipline of XAI, or explainable AI, what is transparency, what is interpretability, other terms, and what are justifications or explanations;\nYou'll learn about the key elements of explanations, from fidelity, to the level of abstraction, contrasting information, and other elements of justifications of high-quality, explainable AI;\nYou'll learn about the different explanation recipients - internal users, external users, observes, regulators, and more, and what each may demand, in terms of explanations;\nYou'll learn about the downsides and challenges of transparency, including users gaming the system, copying AI models, attacking the model itself, and more;\nYou'll learn about an overview of XAI methods, including data-centric and data profiling methods, visualising different results, calculating the influence of different features, and counterfactual explanations;\nYou'll learn about common elements in XAI, including what are \"concepts\", what is \"activation\", and what are \"surrogate models\", used by many popular frameworks;\nYou'll learn about LIME, or Local, Interpretable, Model-Agnostic Explanations, an explainability framework using a linear surrogate model explainer;\nYou'll learn about SHAP, or Shapley Additive Explanations, an explainability framework using Shapley values to calculate feature contributions in a more subtle and advanced manner, and with specific implementations for all major model architectures;\nYou'll learn about TCAV, or Training with Concept Activation Vectors, an explainability framework that uses the activation of \"concepts\" instead of pixel regions for output determination, more user-friendly than other methods such as LRP (Layer-wise Relevance Propagation);\nEthics and Ethical Risks\nYou'll learn about some product considerations - risk and insights related to the products containing AI models;\nYou'll learn about the key components of good consent, such as choice, freedom, understanding, and others, and how to obtain \"true consent\" using these;\nYou'll learn about some dilemmas regarding recordkeeping - what types of logs to keep, and for how long, and the consequences this has for data subjects;\nYou'll learn about assessing the ethical risks posed by an AI model, at different stages. The risks posed by its creation, by its usage, by its possible deterioration with time, and more;\nYou'll learn about some management considerations - ethical risks posed by the behavior of people in the company;\nYou'll learn about ethical alignment - defining ethical values the company lives by, and how these are impacted by your AI/ML models, and implementing them in practice;\nYou'll learn about ethical governance - defining structures and responsible people that govern whether ethical values are obeyed or not - and the consequences;\nYou'll learn about the \"compliance approach\" - considering data ethics a type of compliance, which makes it quantifiable and measurable, and easier to adhere to;\nYou'll learn about enforcement and accountability, including how employee incentives may be perverse and contribute to malevolous model usage, as well as how to hold accountable employees that make unethical decisions with AI/ML models;\nYou'll learn about the three levels of oversight framework, considering AI/ML models to be scrutinised at three distinct levels of depth - isolated actions, the contributions to a system, and the contributions to the bigger society, in general;\nYou'll learn about relevant regulatory frameworks, and how they impact AI decisions;\nYou'll learn about the GDPR and its guidelines for data, but specifically, how it affects AI decisions, forbidding automated decisions with significant/legal impact on individuals and forcing basic model architecture disclosure, for example;\nYou'll learn about the CCPA, and its similarities and differences with the GDPR - and specifically, those that affect AI decisions;\nYou'll learn about fairness in finance regulation in the US, and how the CFPB can scrutinise any AI/ML model, not just in terms of outputs but also processes and documentation, for any activity that may cause harm to consumers in any financial services market;\nYou'll learn about societal considerations - what your AI model can cause, with time and at scale, to society, including possible acceleration of bias, provider dependency, contributing to a focus on surveillance, the reduction (or elimination) of objective experiences, and more;\n\n\n\n\nMY INVITATION TO YOU\nRemember that you always have a 30-day money-back guarantee, so there is no risk for you.\nAlso, I suggest you make use of the free preview videos to make sure the course really is a fit. I don't want you to waste your money.\nIf you think this course is a fit and can take your responsible AI/ML model knowledge to the next level... it would be a pleasure to have you as a student.\nSee you on the other side!",
      "target_audience": [
        "AI and ML practitioners that care about how impact data subjects",
        "Ethics practitioners that will deal with AI/ML models or automated decisions",
        "Any professional that will deal with AI/ML models"
      ]
    },
    {
      "title": "The Data Science Course: Complete Data Science Bootcamp 2025",
      "url": "https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/",
      "bio": "Complete Data Science Training: Math, Statistics, Python, Advanced Statistics in Python, Machine and Deep Learning",
      "objectives": [
        "The course provides the entire toolbox you need to become a data scientist",
        "Fill up your resume with in demand data science skills: Statistical analysis, Python programming with NumPy, pandas, matplotlib, and Seaborn, Advanced statistical analysis, Tableau, Machine Learning with stats models and scikit-learn, Deep learning with TensorFlow",
        "Impress interviewers by showing an understanding of the data science field",
        "Learn how to pre-process data",
        "Understand the mathematics behind Machine Learning (an absolute must which other courses don’t teach!)",
        "Start coding in Python and learn how to use it for statistical analysis",
        "Perform linear and logistic regressions in Python",
        "Carry out cluster and factor analysis",
        "Be able to create Machine Learning algorithms in Python, using NumPy, statsmodels and scikit-learn",
        "Apply your skills to real-life business cases",
        "Use state-of-the-art Deep Learning frameworks such as Google’s TensorFlowDevelop a business intuition while coding and solving tasks with big data",
        "Unfold the power of deep neural networks",
        "Improve Machine Learning algorithms by studying underfitting, overfitting, training, validation, n-fold cross validation, testing, and how hyperparameters could improve performance",
        "Warm up your fingers as you will be eager to apply everything you have learned here to more and more real-life situations"
      ],
      "course_content": {},
      "requirements": [
        "No prior experience is required. We will start from the very basics",
        "You’ll need to install Anaconda. We will show you how to do that step by step",
        "Microsoft Excel 2003, 2010, 2013, 2016, or 365"
      ],
      "description": "*Update 2025: Intro to Data Science module updated for recent AI developments*\nThe Problem\nData scientist is one of the best suited professions to thrive this century. It is digital, programming-oriented, and analytical. Therefore, it comes as no surprise that the demand for data scientists has been surging in the job marketplace.\nHowever, supply has been very limited. It is difficult to acquire the skills necessary to be hired as a data scientist.\nAnd how can you do that?\nUniversities have been slow at creating specialized data science programs. (not to mention that the ones that exist are very expensive and time consuming)\nMost online courses focus on a specific topic and it is difficult to understand how the skill they teach fit in the complete picture\nThe Solution\nData science is a multidisciplinary field. It encompasses a wide range of topics.\nUnderstanding of the data science field and the type of analysis carried out\nMathematics\nStatistics\nPython\nApplying advanced statistical techniques in Python\nData Visualization\nMachine Learning\nDeep Learning\nEach of these topics builds on the previous ones. And you risk getting lost along the way if you don’t acquire these skills in the right order. For example, one would struggle in the application of Machine Learning techniques before understanding the underlying Mathematics. Or, it can be overwhelming to study regression analysis in Python before knowing what a regression is.\nSo, in an effort to create the most effective, time-efficient, and structured data science training available online, we created The Data Science Course 2024.\nWe believe this is the first training program that solves the biggest challenge to entering the data science field – having all the necessary resources in one place.\nMoreover, our focus is to teach topics that flow smoothly and complement each other. The course teaches you everything you need to know to become a data scientist at a fraction of the cost of traditional programs (not to mention the amount of time you will save).\nThe Skills\n1. Intro to Data and Data Science\nBig data, business intelligence, business analytics, machine learning and artificial intelligence. We know these buzzwords belong to the field of data science but what do they all mean?\nWhy learn it? As a candidate data scientist, you must understand the ins and outs of each of these areas and recognise the appropriate approach to solving a problem. This ‘Intro to data and data science’ will give you a comprehensive look at all these buzzwords and where they fit in the realm of data science.\n2. Mathematics\nLearning the tools is the first step to doing data science. You must first see the big picture to then examine the parts in detail.\nWe take a detailed look specifically at calculus and linear algebra as they are the subfields data science relies on.\nWhy learn it?\nCalculus and linear algebra are essential for programming in data science. If you want to understand advanced machine learning algorithms, then you need these skills in your arsenal.\n3. Statistics\nYou need to think like a scientist before you can become a scientist. Statistics trains your mind to frame problems as hypotheses and gives you techniques to test these hypotheses, just like a scientist.\nWhy learn it?\nThis course doesn’t just give you the tools you need but teaches you how to use them. Statistics trains you to think like a scientist.\n4. Python\nPython is a relatively new programming language and, unlike R, it is a general-purpose programming language. You can do anything with it! Web applications, computer games and data science are among many of its capabilities. That’s why, in a short space of time, it has managed to disrupt many disciplines. Extremely powerful libraries have been developed to enable data manipulation, transformation, and visualisation. Where Python really shines however, is when it deals with machine and deep learning.\nWhy learn it?\nWhen it comes to developing, implementing, and deploying machine learning models through powerful frameworks such as scikit-learn, TensorFlow, etc, Python is a must have programming language.\n5. Tableau\nData scientists don’t just need to deal with data and solve data driven problems. They also need to convince company executives of the right decisions to make. These executives may not be well versed in data science, so the data scientist must but be able to present and visualise the data’s story in a way they will understand. That’s where Tableau comes in – and we will help you become an expert story teller using the leading visualisation software in business intelligence and data science.\nWhy learn it?\nA data scientist relies on business intelligence tools like Tableau to communicate complex results to non-technical decision makers.\n6. Advanced Statistics\nRegressions, clustering, and factor analysis are all disciplines that were invented before machine learning. However, now these statistical methods are all performed through machine learning to provide predictions with unparalleled accuracy. This section will look at these techniques in detail.\nWhy learn it?\nData science is all about predictive modelling and you can become an expert in these methods through this ‘advance statistics’ section.\n7. Machine Learning\nThe final part of the program and what every section has been leading up to is deep learning. Being able to employ machine and deep learning in their work is what often separates a data scientist from a data analyst. This section covers all common machine learning techniques and deep learning methods with TensorFlow.\nWhy learn it?\nMachine learning is everywhere. Companies like Facebook, Google, and Amazon have been using machines that can learn on their own for years. Now is the time for you to control the machines.\n**What you get**\nA $1250 data science training program\nActive Q&A support\nAll the knowledge to get hired as a data scientist\nA community of data science learners\nA certificate of completion\nAccess to future updates\nSolve real-life business cases that will get you the job\nYou will become a data scientist from scratch   We are happy to offer an unconditional 30-day money back in full guarantee. No risk for you. The content of the course is excellent, and this is a no-brainer for us, as we are certain you will love it.\nWhy wait? Every day is a missed opportunity.\nClick the “Buy Now” button and become a part of our data scientist program today.",
      "target_audience": [
        "You should take this course if you want to become a Data Scientist or if you want to learn about the field",
        "This course is for you if you want a great career",
        "The course is also ideal for beginners, as it starts from the fundamentals and gradually builds up your skills"
      ]
    },
    {
      "title": "Regression, Data Mining, Text Mining, Forecasting using R",
      "url": "https://www.udemy.com/course/data-science-using-r/",
      "bio": "Learn Regression Techniques, Data Mining, Forecasting, Text Mining using R",
      "objectives": [
        "Learn about the basic statistics, including measures of central tendency, dispersion, skewness, kurtosis, graphical representation, probability, probability distribution, etc.",
        "Learn about scatter diagram, correlation coefficient, confidence interval, Z distribution & t distribution, which are all required for Linear Regression understanding",
        "Learn about the usage of R for building Linear Regression",
        "Learn about the K-Means clustering algorithm & how to use R to accomplish this",
        "Learn about the science behind text mining, word cloud & sentiment analysis & accomplish the same using R"
      ],
      "course_content": {
        "Introduction To Data Science": [
          "Introduction",
          "Data Generation And Information Age",
          "Big Data And Getting Drenched In Data",
          "Why Data Science.....?"
        ],
        "Basic Statistics": [
          "Data Types And Preliminaries",
          "Random Variable",
          "What Is Probability...?",
          "Probability Distribution",
          "Recap Of Concepts And Probability Applications",
          "Sampling Funnel",
          "Measures Of Central Tendency",
          "Measures Of Dispersion",
          "Measures Of Dispersion Part-2",
          "Excpected Value And Variance For Discrete Data",
          "Preliminaries Of R and RStudio",
          "Various Components And Basics Of RStudio",
          "Data Visualization Using R-Barplot,Histogram,Skewness",
          "3rd And 4th Moment Business Decision",
          "Recap And Box Plot",
          "Normal Distribution-Part 1",
          "Normal Distribution-Part 2 & Standarad Normal Distribution",
          "Standard Normal Distribution -Part 2",
          "Calculating Probabilities From Z-Distribution",
          "Sampling Variation, Sample size & Central Limit Theorem",
          "Normal Quantile Plot (Q-QPlot)",
          "Recap Confidence Interval",
          "Confidence Interval Z-Distribution Part-1",
          "Confidence Interval Z-Distribution Part-2",
          "Confidence Interval Interpretation",
          "Confidence Interval T-Distribution",
          "Recap Basic Statistics",
          "Confidence Interval In Data Science",
          "EXPLORATORY DATA ANALYSIS IN DATA SCIENCE",
          "Quiz-2"
        ],
        "Hypothesis Testing Introduction": [
          "Hypothesis Testing Introduction",
          "Hypothesis Testing Formulation"
        ],
        "Hypothesis Testing- Parametric": [
          "2 Sample T-Test Part-1",
          "2 Sample T-Test Part-2",
          "2 Sample T-Test Part-3",
          "1 Sample Z Test Part-1",
          "1 Sample Z Test Part-2",
          "1 Sample T Test",
          "One Way ANOVA Part-1",
          "One Way ANOVA Part-2",
          "One Way ANOVA Part-3",
          "ANOVA-Intuition Part 1",
          "ANOVA-Intuition Part 2",
          "ANOVA-1,2 Multiple Way",
          "Tukey Pairwise Comparisons Part-1",
          "Tukey Pairwise Comparisons Part-2",
          "2 Proportion Test",
          "Chi Square Test",
          "Quiz 3"
        ],
        "Hypothesis Testing-Non Parametric": [
          "1 Sample Sign Test",
          "Mann Whitney Test",
          "Paired T Test Assumption",
          "Paired T Test",
          "Moods Median Test"
        ],
        "BASICS OF R-PROGRAMMING": [
          "Basic Programing Using R Part 1",
          "Basic Programing Using R Part 2",
          "Basic Programing Using R Part 3",
          "Basic Programing Using R Part 4",
          "R Programming Case Study Part 1",
          "R Programming Case Study Part 2",
          "R Programming Case Study Part 3",
          "R Programming Case Study Part 4",
          "Stastical Packages In R",
          "R Programming Case Study Using Inbuilt Datasets",
          "Case Study On Data Visualization Using R Part 1",
          "Case Study On Data Visualization Using R Part 2",
          "Case Studay On Data Visualization Using R Part 3",
          "Case Study On Data Visualization Using R Part 4",
          "Recap Exploratory Data Analysis",
          "Must Know Packages For a Successful Data Scientist",
          "Quiz 4"
        ],
        "Predictive Analytics": [
          "Scatter Diagram",
          "Correlation Coefficient",
          "Simple Linear Regression Introduction",
          "Simple Linear Regression Using R -Part 1",
          "Simple Linear Regression Using R - Part 2",
          "Simple Linear Regression Using R - Part 3",
          "Simple Linear Regression Using R - Part 4",
          "Multiple Linear Regression Introduction",
          "Multiple Linear Regression Using R - Part 1",
          "Multiple Linear Regression Using R - Part 2",
          "Multiple Linear Regression Using R - Part 3",
          "Recap Linear Regression",
          "Understanding Logistic Regression Concepts",
          "Logistic Regression Part 1",
          "Logistic Regression Part 2",
          "Logistic Regression Part 3",
          "Logistic Regression Part 4",
          "Logistic Regression-Logistic Function Representation",
          "Logistic Regression Part 5",
          "Logistic Regression- Confusion Matrix",
          "Logistic Regression-ROC",
          "Logistic Regression-ROC Case studies part 1",
          "Logistic Regression-ROC Case studies part 2",
          "Binary Logistic Regression Interpretation",
          "Quiz 5",
          "Regression Analysis Knowledge check"
        ],
        "Data Mining/Clustering Using R": [
          "Introduction To Clustering",
          "Types Of Data Mining Techniques",
          "Hierarchical Clustering Introduction",
          "Hierarchical Clustering Case Study",
          "Calculating Distance For Categorical Data",
          "Calculating Distance For Mixed Data With Case Study",
          "Calculating Distance For Mixed Data Case Study Part 2",
          "Calculating Distance Between Clusters With Case Study",
          "Hierarchical Clustering Synopsis",
          "Hierarchical Clustering Using R Part 1",
          "Hierarchical Clustering Using R Part 2",
          "K Means Clustering Introduction",
          "K Means Clustering Using R - Part 1",
          "K Means Clustering Using R - Part 2",
          "K Means Clustering Using R - Part 3",
          "K Means Clustering Using R - Part 4",
          "Summary Of K Means Clustering",
          "Difference Between K Means And Hierarchical",
          "K Means Clustering Case Study",
          "Recap Data Mining Clustering",
          "Quiz 6"
        ],
        "Clustering on Mixed Data": [
          "Mixed data-Dummy variable creation for Categorical variables",
          "Mixed data-Normalizing entire data to a scale of 0 to 1",
          "Mixed data - Distance Matrix, Hierarchy and Dendrogram",
          "Mixed data-Hierarchical clustering and interpretation",
          "Mixed data-Scree plot / Elbow curve part1",
          "Mixed data-Scree plot / Elbow curve part2",
          "Mixed data-K Means clustering and Insights"
        ],
        "High Dimension Data Analysis - Dimension Reduction": [
          "Dimension Reduction Introduction",
          "Dimension Reduction Applications",
          "PCA Key Benefits",
          "PCA Intuition",
          "PCA Preliminaries And Weights",
          "Standardize Variables And PCA Calculation",
          "PCA First Goal",
          "PCA Second Goal",
          "PCA Third Goal And Additional Benefits",
          "Quiz-7"
        ]
      },
      "requirements": [
        "Download R & RStudio before starting this tutorial",
        "Download datasets folder in zipfile which is uploaded in starting of all sections"
      ],
      "description": "Data Science using R is designed to cover majority of the capabilities of R from Analytics & Data Science perspective, which includes the following:\nLearn about the basic statistics, including measures of central tendency, dispersion, skewness, kurtosis, graphical representation, probability, probability distribution, etc.\nLearn about scatter diagram, correlation coefficient, confidence interval, Z distribution & t distribution, which are all required for Linear Regression understanding\nLearn about the usage of R for building Regression models\nLearn about the K-Means clustering algorithm & how to use R to accomplish the same\nLearn about the science behind text mining, word cloud, sentiment analysis & accomplish the same using R\nLearn about Forecasting models including AR, MA, ES, ARMA, ARIMA, etc., and how to accomplish the same using R\nLearn about Logistic Regression & how to accomplish the same using R",
      "target_audience": [
        "All the IT professionals, whose experience ranges from '0' onwards are eligible to take this session. Especially professionals from data analysis, data warehouse, data mining, business intelligence, reporting, data science, etc, will naturally fit in well to take this course."
      ]
    },
    {
      "title": "Complete Computer Vision Bootcamp With PyTorch & Tensorflow",
      "url": "https://www.udemy.com/course/complete-computer-vision-bootcamp-with-pytoch-tensorflow/",
      "bio": "Learn Computer Vision with CNN, TensorFlow, and PyTorch — Master Object Detection from Basics to Advanced",
      "objectives": [
        "Master CNN concepts from basics to advanced with TensorFlow & PyTorch.",
        "Learn object detection models like YOLO and Faster R-CNN.",
        "Implement real-world computer vision projects step-by-step.",
        "Gain hands-on experience with data preprocessing and augmentation.",
        "Build custom CNN models for various computer vision tasks.",
        "Master transfer learning with pre-trained models like ResNet and VGG",
        "Gain practical skills with TensorFlow and PyTorch libraries"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Course",
          "Important Note"
        ],
        "Python Prerequisites": [
          "Complete Python Materials",
          "Anaconda Installation",
          "Getting Started With VS Code",
          "Python Basics- Syntax and Semantics",
          "Variables In Python",
          "Basic Datatypes In Python",
          "Operators In Python",
          "Conditional Statements(if,elif,else)",
          "Loops In Python",
          "List and List Comprehension In Python",
          "Preactical Exmaples Of List",
          "Sets In Python",
          "Dictionaries In Python",
          "Tuples In Python",
          "Getting Started With Functions",
          "More Coding Examples With Functions",
          "Python Lambda Functions",
          "Map Functions In Python",
          "Filter Function In Python",
          "Import Modules And Package In Python",
          "Standard Library Overview",
          "File Operation In Python",
          "Working With File Paths",
          "Exception Handling",
          "Classes And Objects In Python",
          "Inheritance In OOPS",
          "Polymorphism In OOPS",
          "Encapsulation In OOPS",
          "Abstraction In OOPS",
          "Magic Methods In Python",
          "Operator Overloading In Python",
          "Custom Exception Handling",
          "Iterators In Python",
          "Generators In Python",
          "Function Copy,Closures And Decorators",
          "Numpy In Python",
          "Pandas-DataFrame And Series",
          "Data Manipulation With Pandas And Numpy",
          "Reading Data From Various Data Source Using Pandas",
          "Logging Practical Implementation In Python",
          "Logging With Multiple Loggers",
          "Logging With a Real World Examples",
          "Python CodeQuiz"
        ],
        "Introduction To Deep Learning": [
          "Introduction",
          "Why Deep Learning is Becoming Popular"
        ],
        "Deep Learning-ANN, Optimizers, Loss Functions, Activation Functions,CNN Theory": [
          "Perceptron Intuition",
          "Adv and Diadvantaes of Perceptron",
          "ANN intuition and Working.mov",
          "Back Propogation and Weight Updation",
          "Chain Rule Of Derivatives",
          "Vanishing Gradient Problem and Sigmoid",
          "Sigmoid Activation Function",
          "Sigmoid Activation Function part -2",
          "Tanh Activation Function.",
          "Relu Activation Function",
          "Leaky Relu and Parametric Relu",
          "ELU Activation Function.",
          "Softmax for Multiclass Classification",
          "Which Activation Function To Apply When",
          "Loss Function Vs Cost Function.",
          "Regression Cost Function.",
          "Loss Function Classification Problem",
          "Which Loss Function To Use When",
          "Gradient Descent Optimizers.",
          "SGD",
          "Mini Batch With SGD",
          "SGD with Momentum",
          "Adagard",
          "RMSPROP",
          "Adam Optimiser",
          "Exploding Gradient Problem",
          "Weight Initialisation Techniques.",
          "Dropout Layers",
          "CNN Introduction",
          "Human Brain V CNN",
          "All you need to know about Images",
          "Convolution Operatuin In CNN",
          "Padding In CNN",
          "Operation Of CNN Vs ANN",
          "Max, Min and Average Pooling.",
          "Flattening and Fully Connected Layers.",
          "CNN Example with RGB"
        ],
        "computer vision (Open CV With Python)": [
          "Introduction to OpenCV",
          "Reading and Writing Images",
          "Working with the video Files",
          "Exploring Color Space",
          "Color Thresholding",
          "image Resizing, Scaling and interpolation",
          "Flip, Rotate and Crop Images",
          "Understanding Coordinate system in openCV",
          "Drawing lines and shapes using opencv",
          "Adding Text to Image",
          "Affine",
          "Image FIlters",
          "Applying Blur filters Average, Gaussian, Median",
          "Edge Detection Using Sobel, Canny & Laplacian",
          "Calculating and Plotting Histogram",
          "Histogram Equalization",
          "CLAHE",
          "Contours",
          "Image Segmentation Using openCV",
          "Haar Cascade for face detection"
        ],
        "PyTorch": [
          "Introduction PyTorch",
          "Introduction to Tensors",
          "indexing Tensors",
          "Using Random Numbers to create noise image",
          "Tensors of Zero's and One's",
          "Tensor data types",
          "Tensor Manuplation",
          "Matrix Aggregation",
          "View and Reshape Operation",
          "Stack Operation",
          "Understanding Pytorch neural network components",
          "Create Linear Regression model with Pytorch components",
          "Multi Class classification with pytorch using custom neural networks",
          "Understanding components of custom data loader in pytorch",
          "Defining custom Image Dataset loader and usage",
          "CNN Training Using a Custom Dataset.",
          "Understanding Components of an Application",
          "What is Deployment ?",
          "Tools to create interactive demos",
          "Hosting platform",
          "Setting up gradio app in local space",
          "Implementing gradio app inference backend",
          "Setting hugging face space",
          "Deploying gradio app on hugging face space"
        ],
        "Deep Dive Visualizing CNNs": [
          "Image Understanding with CNNs vs ANNs",
          "CNN Explainer",
          "Visualization with Tensorspace",
          "CNN Filters",
          "Building Your Own Filters",
          "Feature Map Size Calculation",
          "CNN Parameter Calculations",
          "Receptive Fields"
        ],
        "Image Classification": [
          "What is Image Classification?",
          "LeNet Architecture",
          "LeNet with Keras",
          "LeNet with Pytorch",
          "AlexNet Architecture",
          "AlexNet with Keras",
          "AlexNet with Pytorch",
          "VGG Architecture",
          "Transfer Learning vs Pretrained",
          "VGG Pretrained Keras",
          "VGG Pretrained Pytorch",
          "VGG Transfer Learning",
          "Inception Architecture",
          "Inception Pretrained Keras",
          "Inception Pretrained Pytorch",
          "Inception Transfer Learning",
          "ResNet Architecture",
          "Resnet Pretrained Keras",
          "Resnet Pretrained Pytorch",
          "Resnet Transfer Learning"
        ],
        "Data Augmentation": [
          "What is Data Augmentation?",
          "Data Augmentation with Albumentations",
          "Data Augmentation with Imgaug"
        ],
        "Basics of Object Detection": [
          "What is Object Detection?",
          "Object Detection Metrics",
          "What are Bounding Boxes?",
          "Getting started with YOLO",
          "Getting started with Detectron2",
          "Object Detection Architectures",
          "RCNN",
          "FAST RCNN",
          "FASTER RCNN",
          "FASTER RCNN with Pytorch Implementation",
          "Custom Object Detection with YOLOv11",
          "Custom Object Detection with Detectron2"
        ]
      },
      "requirements": [
        "Basic understanding of Python programming.",
        "Familiarity with fundamental machine learning concepts.",
        "Knowledge of basic linear algebra and calculus.",
        "Understanding of image data and its structure.",
        "Enthusiasm to learn computer vision with hands-on projects."
      ],
      "description": "In this comprehensive course, you will master the fundamentals and advanced concepts of computer vision, focusing on Convolutional Neural Networks (CNN) and object detection models using TensorFlow and PyTorch. This course is designed to equip you with the skills required to build robust computer vision applications from scratch.\nWhat You Will Learn\nThroughout this course, you will gain expertise in:\nIntroduction to Computer Vision\nUnderstanding image data and its structure.\nExploring pixel values, channels, and color spaces.\nLearning about OpenCV for image manipulation and preprocessing.\nDeep Learning Fundamentals for Computer Vision\nIntroduction to Neural Networks and Deep Learning concepts.\nUnderstanding backpropagation and gradient descent.\nKey concepts like activation functions, loss functions, and optimization techniques.\nConvolutional Neural Networks (CNN)\nIntroduction to CNN architecture and its components.\nUnderstanding convolution layers, pooling layers, and fully connected layers.\nImplementing CNN models using TensorFlow and PyTorch.\nData Augmentation and Preprocessing\nTechniques for improving model performance through data augmentation.\nUsing libraries like imgaug, Albumentations, and TensorFlow Data Pipeline.\nTransfer Learning for Computer Vision\nUtilizing pre-trained models such as ResNet, VGG, and EfficientNet.\nFine-tuning and optimizing transfer learning models.\nObject Detection Models\nExploring object detection algorithms like:\nYOLO (You Only Look Once)\nFaster R-CNN\nImplementing these models with TensorFlow and PyTorch.\nImage Segmentation Techniques\nUnderstanding semantic and instance segmentation.\nImplementing U-Net and Mask R-CNN models.\nReal-World Projects and Applications\nBuilding practical computer vision projects such as:\nFace detection and recognition system.\nReal-time object detection with webcam integration.\nImage classification pipelines with deployment.\n\n\nWho Should Enroll?\nThis course is ideal for:\nBeginners looking to start their computer vision journey.\nData scientists and ML engineers wanting to expand their skill set.\nAI practitioners aiming to master object detection models.\nResearchers exploring computer vision techniques for academic projects.\nProfessionals seeking practical experience in deploying CV models.\nPrerequisites\nBefore enrolling, ensure you have:\nBasic knowledge of Python programming.\nFamiliarity with fundamental machine learning concepts.\nBasic understanding of linear algebra and calculus.\nHands-on Learning with Real Projects\nThis course emphasizes practical learning through hands-on projects. Each module includes coding exercises, project implementations, and real-world examples to ensure you gain valuable skills.\nBy the end of this course, you will confidently build, train, and deploy computer vision models using TensorFlow and PyTorch. Whether you are a beginner or an experienced practitioner, this course will empower you with the expertise needed to excel in the field of computer vision.\nEnroll now and take your computer vision skills to the next level!",
      "target_audience": [
        "Beginners eager to learn computer vision from scratch.",
        "Data scientists looking to expand their skill set with CNN and object detection.",
        "AI and ML engineers aiming to build computer vision models.",
        "Researchers and students exploring deep learning for visual tasks.",
        "Professionals interested in deploying real-world CV applications"
      ]
    },
    {
      "title": "Statistics & Mathematics for Data Science & Data Analytics",
      "url": "https://www.udemy.com/course/statistics-for-data-science-data-analytics/",
      "bio": "Learn the statistics & probability for data science and business analysis",
      "objectives": [
        "Master the fundamentals of statistics for data science & data analytics",
        "Master descriptive statistics & probability theory",
        "Machine learning methods like Decision Trees and Decision Forests",
        "Probability distributions such as Normal distribution, Poisson Distribution and more",
        "Hypothesis testing, p-value, type I & type II error",
        "Logistic Regressions, Multiple Linear Regression, Regression Trees",
        "Correlation, R-Square, RMSE, MAE, coefficient of determination and more"
      ],
      "course_content": {
        "Let's get started": [
          "Welcome!",
          "What will you learn in this course?",
          "How can you get the most out of it?",
          "Download: Formula cheat sheet"
        ],
        "Descriptive statistics": [
          "Intro",
          "Mean",
          "Quiz: Mean",
          "Median",
          "Quiz: Median",
          "Mode",
          "Quiz: Mode",
          "Mean or Median?",
          "Skewness",
          "Practice: Skewness",
          "Solution: Skewness",
          "Range & IQR",
          "Sample vs. Population",
          "Variance & Standard deviation",
          "Quiz: Variance",
          "Impact of Scaling & Shifting",
          "Statistical moments"
        ],
        "Distributions": [
          "What is a distribution?",
          "Normal distribution",
          "Z-Scores",
          "Practise: Normal distribution",
          "Solution: Normal distribution",
          "Normal distribution",
          "More distributions"
        ],
        "Probability theory": [
          "Intro",
          "Probability Basics",
          "Calculating Simple Probabilities",
          "Practice: Simple Probabilities",
          "Quick solution: Simple Probabilites",
          "Detailed solution: Simple Probabilities",
          "Rule of addition",
          "Practice: Rule of addition",
          "Quick solution: Rule of addition",
          "Detailed solution: Rule of addition",
          "Rule of multiplication",
          "Practice: Rule of multiplication",
          "Solution: Rule of multiplication",
          "Bayes Theorem",
          "Bayes Theorem - Practical example",
          "Expected value",
          "Practice: Expected value",
          "Solution: Expected value",
          "Law of Large Numbers",
          "Central Limit Theorem - Theory",
          "Central Limit Theorem - Intuition",
          "Central Limit Theorem - Challenge",
          "Central Limit Theorem - Exercise",
          "Central Limit Theorem - Solution",
          "Quiz: Bayes Theorem",
          "Binomial distribution",
          "Poisson distribtuion",
          "Real life problems"
        ],
        "Hypothesis testing": [
          "Intro",
          "What is an hypothesis?",
          "Significance level and p-value",
          "Type I and Type II errors",
          "Confidence intervals and margin of error",
          "Excursion: Calculating sample size & power",
          "Performing the hypothesis test",
          "Practice: Hypothesis test",
          "Solution: Hypothesis test",
          "t-test and t-distribution",
          "Proportion testing",
          "Important p-z pairs",
          "Quiz: Hypothesis Testing"
        ],
        "Regressions": [
          "Intro",
          "Linear Regression",
          "Correlation coefficient",
          "Practice: Correlation",
          "Solution: Correlation",
          "Practice: Linear Regression",
          "Solution: Linear Regression",
          "Residual, MSE & MAE",
          "Practice: MSE & MAE",
          "Solution: MSE & MAE",
          "Coefficient of determination",
          "Root Mean Square Error",
          "Practice: RMSE",
          "Solution: RMSE",
          "Quiz: Regression"
        ],
        "Advanced regression & machine learning algorithms": [
          "Multiple Linear Regression",
          "Overfitting",
          "Polynomial Regression",
          "Logistic Regression",
          "Decision Trees",
          "Regression Trees",
          "Random Forests",
          "Dealing with missing data"
        ],
        "ANOVA (Analysis of Variance)": [
          "ANOVA - Basics & Assumptions",
          "One-way ANOVA",
          "F-Distribution",
          "Two-way ANOVA – Sum of Squares",
          "Two-way ANOVA – F-ratio & conclusions",
          "Quiz: ANOVA",
          "Wrap up"
        ],
        "Wrap up": [
          "Bonus lecture"
        ]
      },
      "requirements": [
        "Absolutely no previous experience required. We will learn everything right from the basics and then work our way up step by step",
        "Eagerness and motivation to learn"
      ],
      "description": "Are you aiming for a career in Data Science or Data Analytics?\nGood news, you don't need a Maths degree - this course is equipping you with the practical knowledge needed to master the necessary statistics.\nIt is very important if you want to become a Data Scientist or a Data Analyst to have a good knowledge in statistics & probability theory.\nSure, there is more to Data Science than only statistics. But still it plays an essential role to know these fundamentals ins statistics.\nI know it is very hard to gain a strong foothold in these concepts just by yourself. Therefore I have created this course.\nWhy should you take this course?\nThis course is the one course you take in statistic that is equipping you with the actual knowledge you need in statistics if you work with data\nThis course is taught by an actual mathematician that is in the same time also working as a data scientist.\nThis course is balancing both: theory & practical real-life example.\nAfter completing this course you ll have everything you need to master the fundamentals in statistics & probability need in data science or data analysis.\nWhat is in this course?\nThis course is giving you the chance to systematically master the core concepts in statistics & probability, descriptive statistics, hypothesis testing, regression analysis, analysis of variance and some advance regression / machine learning methods such as logistics regressions, polynomial regressions , decision trees and more.\nIn real-life examples you will learn the stats knowledge needed in a data scientist's or data analyst's career very quickly.\nIf you feel like this sounds good to you, then take this chance to improve your skills und advance career by enrolling in this course.",
      "target_audience": [
        "Anybody that wants to master statistics & probabilities for data science & data analysis",
        "Anybody who wants to pursue a career in Data Science",
        "Professionals and students who want to understand the necessary statistics for data analysis"
      ]
    },
    {
      "title": "Species Distribution Models with GIS & Machine Learning in R",
      "url": "https://www.udemy.com/course/species-distribution-models-with-gis-machine-learning-in-r/",
      "bio": "Mapping Habitat Suitability for Conservation Using Machine Learning and GIS in R",
      "objectives": [
        "You will have a greater clarity of basic spatial data concepts and data types",
        "Carry out practical spatial data analysis tasks in freely available software in R",
        "Analyze spatial data using R"
      ],
      "course_content": {},
      "requirements": [
        "Ability to Install Packages in R and RStudio",
        "Interest in learning the implementation of GIS techniques in R",
        "Interest in applying machine learning to spatial data"
      ],
      "description": "Are You an Ecologist or Conservationist Interested in Learning GIS and Machine Learning in R?\nAre you an ecologist/conservationist looking to carry out habitat suitability mapping?\nAre you an ecologist/conservationist looking to get started with R for accessing ecological data and GIS analysis?\nDo you want to implement practical machine learning models in R?\nThen this course is for you! I will take you on an adventure into the amazing of field Machine Learning and GIS for ecological modelling. You will learn how to implement species distribution modelling/map suitable habitats for species in R.\nMy name is MINERVA SINGH and i am an Oxford University MPhil (Geography and Environment) graduate. I finished a PhD at Cambridge University (Tropical Ecology and Conservation). I have several years of experience in analyzing real life spatial data from different sources and producing publications for international peer reviewed journals.\nIn this course, actual spatial data from Peninsular Malaysia will be used to give a practical hands-on experience of working with real life spatial data for mapping habitat suitability in conjunction with classical SDM models like MaxEnt and machine learning alternatives such as Random Forests. The underlying motivation for the course is to ensure you can put spatial data and machine learning analysis into practice today. Start ecological data for your own projects, whatever your skill level and IMPRESS your potential employers with an actual examples of your GIS and Machine Learning skills in R.\n\nSo Many R based Machine Learning and GIS Courses Out There, Why This One?\nThis is a valid question and the answer is simple. This is the ONLY course on Udemy which will get you implementing some of the most common machine learning algorithms on real ecological data in R. Plus, you will gain exposure to working your way through a common ecological modelling technique- species distribution modelling (SDM) using real life data. Students will also gain exposure to implementing some of the most common Geographic Information Systems (GIS) and spatial data analysis techniques in R. Additionally,  students will learn how to access ecological data via R.\nYou will learn to harness the power of both GIS and Machine Learning in R for ecological modelling.\nI have designed this course for anyone who wants to learn the state of the art in Machine learning in a simple and fun way without learning complex math or boring explanations. Yes, even non-ecologists can get started with practical machine learning techniques in R while working their way through real data.\nWhat you will Learn in this Course\nThis is how the course is structured:\nIntroduction – Introduction to SDMs and mapping habitat suitability\nThe Basics of GIS for Species Distribution Models (SDMs) – You will learn some of the most common GIS and data analysis tasks related to SDMs including accessing species presence data via R\nPre-Processing Raster and Spatial Data for SDMs - Your R based GIS training and will continue and you will earn to perform some of the most common GIS techniques on raster and other spatial data\nClassical SDM Techniques - Introduction to the classical models and their implementation in R (MaxENT and Bioclim)\nMachine Learning Models for Habitat Suitability - Implement and interpret common ML techniques to build habitat suitability maps for the birds of Peninsular Malaysia.\n\n\nIt is a practical, hands-on course, i.e. we will spend some time dealing with some of the theoretical concepts . However, majority of the course will focus on implementing different  techniques on real data and interpret the results. After each video you will learn a new concept or technique which you may apply to your own projects.\nTAKE ACTION TODAY! I will personally support you and ensure your experience with this course is a success. And for any reason you are unhappy with this course, Udemy has a 30 day Money Back Refund Policy, So no questions asked, no quibble and no Risk to you. You got nothing to lose. Click that enroll button and we'll see you in side the course.",
      "target_audience": [
        "Academics",
        "Researchers",
        "Conservation managers",
        "Anybody who works/will work with spatial data"
      ]
    },
    {
      "title": "Regression Analysis / Data Analytics in Regression",
      "url": "https://www.udemy.com/course/regression-statistics/",
      "bio": "Gain Important and Highly Marketable Skills in Regression Analysis - Tame the Regression Beast Today!",
      "objectives": [
        "Understand when to use simple, multiple, and hierarchical regression",
        "Understand the meaning of R-Square and the role it plays in regression",
        "Assess a regression model for statistical significance, including both the overall model and the individual predictors",
        "Effectively utilize regression models in your own work and be able to critically evaluate the work of others",
        "Understand predicted values and their role in the overall quality of a regression model",
        "Understand hierarchical regression, including its purpose and when it should be used",
        "Use regression to assess the relative value of competing predictors",
        "Make business decisions about the best models to maximize profits while minimizing risk",
        "Critically evaluate regression models used by others",
        "Learn how to conduct correlation and regression using both IBM SPSS and Microsoft Excel"
      ],
      "course_content": {},
      "requirements": [
        "Many of the videos use SPSS in running regression models and some use the Microsfot Excel Data Analysis ToolPak. While SPSS is not required to understand the material or follow the videos, if you want to reproduce the analyses on your own, SPSS will be needed. However, other software (such as R, SAS, or Minitab) can be used to reach the same statistical decisions about the regressions models (as are illustrated here)."
      ],
      "description": "November, 2019.\nGet marketable and highly sought after skills in this course while substantially increasing your knowledge of data analytics in regression. All course videos created and narrated by an award winning instructor and textbook author of quantitative methods.\nThis course covers running and evaluating linear regression models (simple regression, multiple regression, and hierarchical regression), including assessing the overall quality of models and interpreting individual predictors for significance. R-Square is explored in depth, including how to interpret R-Square for significance. Together with coverage of simple, multiple and hierarchical regression, we'll also explore correlation, an important statistical procedure that is closely related to regression.\nBy the end of this course you will be skilled in running and interpreting your own linear regression analyses, as well as critically evaluating the work of others. Examples of running regression in both SPSS and Excel programs provided. Lectures provided in high quality, HD video with course quizzes available to help cement the concepts. Taught by a PhD award-winning university instructor with over 15 years of teaching experience. At Quantitative Specialists, our highest priority is in creating crystal-clear, accurate, easy-to-follow videos.\nTame the regression beast once and for all – enroll today!",
      "target_audience": [
        "Anyone interested in learning more about regression analysis.",
        "This course is not for those looking for a general introduction to statistics course. For this we recommend taking a look at our descriptive statistics or inferential statistics courses. (This course specializes in regression analysis.)",
        "Those looking to increase their knowledge of regression."
      ]
    },
    {
      "title": "Master in Data Science to become a Data Scientist",
      "url": "https://www.udemy.com/course/a-complete-roadmap-to-become-a-successful-data-scientist/",
      "bio": "Machine Learning, Artificial Intelligence, AI ML using Python, R, statistics. Big Data Analysis and Analytics.",
      "objectives": [
        "What are the responsibilities of a Data Scientist?",
        "What qualifications are required to become a Data Scientist?",
        "How can you become a successful Data Scientist?",
        "How to deliver the key responsibilities of a Successful Data Scientist?",
        "Your Roadmap to become a Successful Data Scientist",
        "Data Analysis and Exploration",
        "Model Development",
        "Feature Engineering",
        "Data Visualization",
        "Experimentation"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Overview": [
          "Overview I",
          "Overview I",
          "Overview II",
          "Overview II",
          "Overview III",
          "Overview III"
        ],
        "Data Analysis and Exploration": [
          "Data Analysis and Exploration I",
          "Data Analysis and Exploration I",
          "Data Analysis and Exploration II",
          "Data Analysis and Exploration II"
        ],
        "Model Development": [
          "Model Development I",
          "Model Development I",
          "Model Development II",
          "Model Development II",
          "Model Development III",
          "Model Development III",
          "Model Development IV",
          "Model Development IV",
          "Model Development V",
          "Model Development V",
          "Model Development VI",
          "Model Development VI"
        ],
        "Feature Engineering": [
          "Feature Engineering I",
          "Feature Engineering I",
          "Feature Engineering II",
          "Feature Engineering II",
          "Feature Engineering III",
          "Feature Engineering III"
        ],
        "Data Visualization": [
          "Data Visualization I",
          "Data Visualization I",
          "Data Visualization II",
          "Data Visualization II",
          "Data Visualization III",
          "Data Visualization III"
        ],
        "Collaboration": [
          "Collaboration I",
          "Collaboration I",
          "Collaboration II",
          "Collaboration II",
          "Collaboration III",
          "Collaboration III"
        ],
        "Experimentation": [
          "Experimentation I",
          "Experimentation I",
          "Experimentation II",
          "Experimentation II",
          "Experimentation III",
          "Experimentation III",
          "Experimentation IV",
          "Experimentation IV",
          "Experimentation V",
          "Experimentation V",
          "Experimentation VI",
          "Experimentation VI",
          "Experimentation VII",
          "Experimentation VII"
        ],
        "Continuous Learning": [
          "Continuous Learning I",
          "Continuous Learning I",
          "Continuous Learning II",
          "Continuous Learning II"
        ],
        "Documentation": [
          "Documentation I",
          "Documentation I",
          "Documentation II",
          "Documentation II",
          "Documentation III",
          "Documentation III",
          "Documentation IV",
          "Documentation IV"
        ]
      },
      "requirements": [
        "Willing to devote 5 hours"
      ],
      "description": "Want to become an Successful Data Scientist but don’t know what to do and how?\nTake a look at this course where you will\nNot only learn responsibilities, qualifications, all the knowledge and tools required in detail to become a Successful Data Scientist including Data Analysis and Exploration, Model Development, Feature Engineering, Data Visualization, Collaboration, Experimentation, Continuous Learning and\nDocumentation but also\nThe roadmap for becoming a Successful Data Scientist\nPreview many lectures for free to see the content for yourself\nGet Udemy’s 30 days Money Back Guarantee\nGet 4 bonus lectures of my new courses every month through educational announcements  in your email\nEnroll for free in my any other course using the coupon link in my YouTube channel videos from time to time subject to availability\nMy exposure to Data Analysis started 40 years back in 1979-81 at Indian Institute of Management Bangalore when I learnt about various methodologies and tools like Multivariate and Conjoint Analysis to make better management decisions. Data Science was not yet born as it has developed and matured now with developments in Machine Learning and other Algorithms that have made it possible to use those methodologies and tools\nWhile I continued to use the Data Analysis during next about 30 years while working in Unilever, Johnson and Johnson and Danone, I came to know about the fully developed Data Science in 2016 when I started working at IIM Udaipur teaching and coaching MBA students\nDuring past 8 years, I have researched and learnt a lot more about the capabilities of Data Science to help all of us make better decisions\nI bring in this course my learnings from this journey and share with you how can you also become an Successful Data Scientist and join this attractive and growing field\nLook at what other students saying about this course\n\"It's been super amazing so far, I know it will get better and better\"\n\"The course is amazing and worth the money\"\n\"One of the finest courses by a professor on Udemy\"\n\"Great stuff, am learning a lot through this\"\n\"Great explanation, simple and precise\"\n\"Great\"\n\"As a sophomore in college who aspires to major in data science, this course is a really good overview of what I will need to do and to know. I am also impressed at how fast Mr. Singhal responded to my questions. This course goes in depth with everything and does a good job of limiting the assumptions of what we already know.\"\n\"Amazing\"\n\"Awesome: detailed, not too fast, proper use of visualization that enhanced concentration and understanding of the class.\"\n\"Really a great lecture. Hope I might get my apprenticeship in Google. Thank you\"\n\"Very well taught!\"\n\"Very important information concerning data scientist career\"\n\"very good and simple explanations\"\n\"it is a good lecture and refreshing time of adding value to my data analysis package with more insightful points\"\n\"I am impressed. The video was easy to watch. No stress at all\"\n\"nice and informative lectures\"\n\"i can able study practical level of knowledge of all\"\nPreview for yourself many lectures free. If you like the content, enroll for the course, enjoy and skill yourself to Become a Successful Data Scientist. If don't like the content, please message about how can we modify it to meet your expectations.",
      "target_audience": [
        "Aspiring Data Scientists"
      ]
    },
    {
      "title": "Machine Learning and Business Intelligence Masterclass",
      "url": "https://www.udemy.com/course/machine-learning-masterclass/",
      "bio": "Machine Learning with Python and TensorFlow, BI techniques using Siebel and BIP, Gain hands-on in diverse projects",
      "objectives": [
        "Python and PySpark Fundamentals: Master the basics of Python and PySpark, including programming with RDD, MySQL connectivity, and PySpark joins.",
        "Intermediate PySpark Techniques: Explore advanced PySpark concepts like linear regression, generalized linear regression, forest regression, etc",
        "Advanced PySpark Applications: Dive into advanced PySpark applications such as RFM analysis, K-Means clustering, image to text, PDF to text, and Monte Carlo",
        "Machine Learning with TensorFlow: Gain expertise in TensorFlow for machine learning, covering topics from installation and libraries to data manipulation",
        "Practical Data Science Projects: Apply your knowledge to real-world projects, including shipping and time estimation, supply chain-demand trends analysis",
        "Deep Learning and NLP: Understand the fundamentals of deep learning, neural networks, and natural language processing (NLP), with hands-on in keras.",
        "Bayesian Machine Learning: Learn the principles of Bayesian machine learning, A/B testing, and hierarchical models for multiple variant testing.",
        "Machine Learning with R: Explore machine learning using R, covering regression, classification, decision trees, support vector machines, dimension reduction",
        "AWS Machine Learning: Gain insights into Amazon Machine Learning (AML), connecting to data sources, creating ML models, batch predictions, and advanced setting",
        "Business Intelligence (BI) and Data Warehousing: Understand BI concepts, multidimensional databases, metadata, ETL processes, and various tools in BI",
        "Deep Dive into Specific BI Topics: Explore specific BI topics such as break-even analysis, multivariate analysis, graphs, cluster analysis, outlier discovery",
        "Practical Application of Clustering and Regression: Apply clustering algorithms like K-Means and DBSCAN, and delve into regression analysis for market basket",
        "Comprehensive Data Science Techniques: Cover a wide range of data science techniques, including sequential data analysis, regression models, market basket",
        "Machine Learning in Business: Understand the strategic imperative of BI, BI algorithms, benefits of BI, information governance, and BI applications in business",
        "Latest Developments in Machine Learning: Stay updated on new developments in machine learning, the role of data scientists, types of detection in ML",
        "Business Intelligence Publisher (BIP) using Siebel: Learn to use BIP with Siebel, covering user types, running modes, BIP add-ins, report development",
        "Business Intelligence (BI): Explore BI frameworks, strategic imperatives, data warehousing, ETL processes, and the role of BI in organizations.",
        "Advanced BI Concepts: Delve into advanced BI concepts such as semantic technologies, BI algorithms, benefits of BI, and real-world applications",
        "Meta Data and Project Management: Understand the importance of meta data, essentials for IT, business meta data, project planning, deployment processes",
        "Statistical and Machine Learning Models: Learn and implement various statistical and machine learning models, including linear regression, decision trees",
        "Time Series Analysis: Dive into time series analysis, covering topics like moving average models, auto-correlation functions, forecasting using stock prices",
        "Hands-on Programming and Tools: Gain practical programming experience with tools like TensorFlow, PySpark, R, and BI tools, ensuring hands-on application",
        "Practical Skills for Data Scientists: Develop practical skills in data science, data analysis, machine learning, deep learning, NLP, and BI",
        "Real-world Projects and Applications: Work on diverse projects—from predictive modeling and regression analysis to fraud detection and supply chain analysis",
        "Cloud-based Machine Learning with AWS: Acquire skills in cloud-based machine learning with AWS, covering AML lifecycle, data source connections, ML models",
        "In-depth Understanding of Neural Networks: Explore the structure of neural networks, activation functions, optimization techniques, and implementation",
        "Natural Language Processing (NLP) Techniques: Learn text preprocessing, feature extraction, and NLP algorithms, applying them to tasks like sentiment analysis",
        "Bayesian Machine Learning for A/B Testing: Understand Bayesian machine learning principles for A/B testing, hierarchical models, and practical applications",
        "Data Warehousing and ETL Processes: Explore data warehousing concepts, ETL design, meta data, and deployment processes, gaining a comprehensive understanding",
        "Machine Learning in Business and Industry: Gain insights into the strategic imperatives of BI in business, BI algorithms, benefits of BI, and the practicals"
      ],
      "course_content": {},
      "requirements": [
        "No prior knowledge of machine learning required",
        "Basic knowledge of R tool is an added advantage",
        "Basic Python and Mathematics (Linear Algebra Basics) is an added advantage",
        "Computer Access"
      ],
      "description": "Course Introduction:\nWelcome to the Machine Learning Mastery course, a comprehensive journey through the key aspects of machine learning. In this course, we'll delve into the essentials of statistics, explore PySpark for big data processing, advance to intermediate and advanced PySpark topics, and cover various machine learning techniques using Python and TensorFlow. The course will culminate in hands-on projects across different domains, giving you practical experience in applying machine learning to real-world scenarios.\n\n\nSection 1: Machine Learning - Statistics Essentials\nThis foundational section introduces you to the world of machine learning, starting with the basics of statistics. You'll understand the core concepts of machine learning, its applications, and the role of analytics. The section progresses into big data machine learning and explores emerging trends in the field. The statistics essentials cover a wide range of topics such as data types, probability distributions, hypothesis testing, and various statistical tests. By the end of this section, you'll have a solid understanding of statistical concepts crucial for machine learning.\n\n\nSection 2: Machine Learning with TensorFlow for Beginners\nThis section is designed for beginners in TensorFlow and machine learning with Python. It begins with an introduction to machine learning using TensorFlow, guiding you through setting up your workstation, understanding program languages, and using Jupyter notebooks. The section covers essential libraries like NumPy and Pandas, focusing on data manipulation and visualization. Practical examples and hands-on exercises will enhance your proficiency in working with TensorFlow and preparing you for more advanced topics.\n\n\nSection 3: Machine Learning Advanced\nAdvancing from the basics, this section explores advanced topics in machine learning. It covers PySpark in-depth, delving into RFM analysis, K-Means clustering, and image to text conversion. The section introduces Monte Carlo simulation and applies machine learning models to solve complex problems. The hands-on approach ensures that you gain practical experience and develop a deeper understanding of advanced machine learning concepts.\n\n\nSection 4-7: Machine Learning Projects\nThese sections are dedicated to hands-on projects, providing you with the opportunity to apply your machine learning skills in real-world scenarios. The projects cover shipping and time estimation, supply chain-demand trends analysis, predicting prices using regression, and fraud detection in credit payments. Each project is designed to reinforce your understanding of machine learning concepts and build a portfolio of practical applications.\n\n\nSection 8: AWS Machine Learning\nIn this section, you'll step into the world of cloud-based machine learning with Amazon Machine Learning (AML). You'll learn how to connect to data sources, create data schemes, and build machine learning models using AWS services. The section provides hands-on examples, ensuring you gain proficiency in leveraging cloud platforms for machine learning applications.\n\n\nSection 9: Deep Learning Tutorials\nDelving into deep learning, this section covers the structure of neural networks, activation functions, and the practical implementation of deep learning models using TensorFlow and Keras. It includes insights into image classification using neural networks, preparing you for more advanced applications in the field.\n\n\nSection 10: Natural Language Processing (NLP) Tutorials\nFocused on natural language processing (NLP), this section equips you with the skills to work with textual data. You'll learn text preprocessing techniques, feature extraction, and essential NLP algorithms. Practical examples and demonstrations ensure you can apply NLP concepts to analyze and process text data effectively.\n\n\nSection 11: Bayesian Machine Learning - A/B Testing\nThis section introduces Bayesian machine learning and its application in A/B testing. You'll understand the principles of Bayesian modeling and hierarchical models, gaining insights into how these methods can be used to make informed decisions based on experimental data.\n\n\nSection 12: Machine Learning with R\nDesigned for those interested in using R for machine learning, this section covers a wide range of topics. From data manipulation to regression, classification, clustering, and various algorithms, you'll gain practical experience using R for machine learning applications. Hands-on examples and real-world scenarios enhance your proficiency in leveraging R for data analysis and machine learning.\n\n\nSection 13: BIP - Business Intelligence Publisher using Siebel\nThis section focuses on Business Intelligence Publisher (BIP) in the context of Siebel applications. You'll learn about different user types, running modes, and BIP add-ins. Practical examples and demonstrations guide you through developing reports within the Siebel environment, providing valuable insights into the integration of BI tools in enterprise solutions.\n\n\nSection 14: BI - Business Intelligence\nThe final section explores the broader landscape of Business Intelligence (BI). Covering multidimensional databases, metadata, ETL processes, and strategic imperatives of BI, you'll gain a comprehensive understanding of the BI ecosystem. The section also touches upon BI algorithms, benefits, and real-world applications, preparing you for a holistic view of business intelligence.\nEach section in the course builds upon the previous one, ensuring a structured and comprehensive learning journey from fundamentals to advanced applications in machine learning and business intelligence. The hands-on projects and practical examples provide you with valuable experience to excel in the field.",
      "target_audience": [
        "Aspiring Data Scientists: Individuals aiming to build a career in data science, machine learning, and analytics.",
        "Data Analysts: Professionals seeking to enhance their skills in handling and analyzing data for actionable insights.",
        "Software Engineers: Those interested in transitioning or upskilling to work on data-driven projects using Python, PySpark, TensorFlow, and R.",
        "Business Intelligence Professionals: Individuals looking to integrate machine learning and advanced analytics into business intelligence practices.",
        "Students and Graduates: Those pursuing studies in computer science, data science, or related fields with an interest in machine learning.",
        "Professionals in IT and Database Management: Seeking to broaden their expertise by understanding the practical applications of machine learning.",
        "Anyone Interested in Data-Driven Decision Making: Individuals from diverse backgrounds keen on leveraging data for informed decision-making processes.",
        "The course accommodates a range of backgrounds and provides foundational to advanced knowledge, making it suitable for both beginners and those with some experience in data-related fields."
      ]
    },
    {
      "title": "Probability for Statistics and Data Science",
      "url": "https://www.udemy.com/course/probability-for-statistics-and-data-science/",
      "bio": "Probability for improved business decisions: Introduction, Combinatorics, Bayesian Inference, Distributions",
      "objectives": [
        "Understand probability theory",
        "Discover Combinatorics",
        "Learn how to use and interpret Bayesian Notation",
        "Different types of distributions variables can follow"
      ],
      "course_content": {},
      "requirements": [
        "Absolutely no experience is required. We will start from the basics and gradually build up your knowledge.",
        "A willingness to learn and practice"
      ],
      "description": "Probability is probably the most fundamental skill you need to acquire if you want to be successful in the world of business. What most people don’t realize is that having a probabilistic mindset is much more important than knowing “absolute truths”.\nYou are already here, so actually you know that.\nAnd it doesn’t matter if it is pure probability, statistics, business intelligence, finance or data science where you want to apply your probability knowledge…\nProbability for Statistics and Data Science has your back!\nThis is the place where you’ll take your career to the next level – that of probability, conditional probability, Bayesian probability, and probability distributions.\nYou may be wondering: “Hey, but what makes this course better than all the rest?”\nProbability for Statistics and Data Science has been carefully crafted to reflect the most in-demand skills that will enable you to understand and compute complicated probabilistic concepts. This course is:\nEasy to understand\nComprehensive\nPractical\nTo the point\nBeautifully animated (with amazing video quality)\nPacked with plenty of exercises and resources\n\n\nThat’s all great, but what will you actually learn? Probability. And nothing less.\n\n\nTo be more specific, we focus on the business implementation of probability concepts. This translates into a comprehensive course consisting of:\nAn introductory part that will acquaint you with the most basic concepts in the field of probability: event, sample space, complement, expected value, variance, probability distribution function\nWe gradually build on your knowledge with the first widely applicable formulas:\nCombinatorics or the realm of permutations, variations, and combinations. That’s the place where you’ll learn the laws that govern “everyday probability”\nOnce you’ve got a solid background, you’ll be ready for some deeper probability theory – Bayesian probability.\nHave you seen this expression: P(A|B) = P(B|A)P(A)/P(B) ? That’s the Bayes’ theorem – the most fundamental building block of Bayesian inference. It seems complicated but it will take you less than 1 hour to understand not only how to read it, but also how to use it and prove it\nTo get there you’ll learn about unions, intersections, mutually exclusive sets, overlapping sets, conditional probability, the addition rule, and the multiplication rule\n\n\nMost of these topics can be found online in one form or another. But we are not bothered by that because we are certain of the outstanding quality of teaching that we provide.\nWhat we are really proud of, though, is what comes next in the course. Distributions.\nDistributions are something like the “heart” of probability applied in data science. You may have heard of many of them, but this is the only place where you’ll find detailed information about many of the most common distributions.\nDiscrete: Uniform distribution, Bernoulli distribution, Binomial distribution (that’s where you’ll see a lot of the combinatorics from the previous parts), Poisson\nContinuous: Normal distribution, Standard normal distribution, Student’s T, Chi-Squared, Exponential, Logistic\nNot only do we have a dedicated video for each one of them, how to determine them, where they are applied, but also how to apply their formulas.\nFinally, we’ll have a short discussion on 3 of the most common places where you can stumble upon probability:\nFinance\nStatistics\nData Science\n\n\nIf that’s not enough, keep in mind that we’ve got real-life cases after each of our sections. We know that nobody wants to learn dry theory without seeing it applied to real business situations so that’s in store, too!\nWe think that this will be enough to convince you curriculum-wise. But we also know that you really care about WHO is teaching you, too.\nTeaching is our passion\nWe worked hard for over four months to create the best possible Probability course that would deliver the most value to you. We want you to succeed, which is why the course aims to be as engaging as possible. High-quality animations, superb course materials, quiz questions, handouts and course notes, are just some of the perks you will get. What else?\nExceptional Q&A support. Yes. That’s our favorite part – interacting with you on the various topics you learn about (and you are going to love it, too!)\nWhat makes this course different from the rest of the Probability courses out there?\nHigh-quality production – HD video and animations (This isn’t a collection of boring lectures!)\nKnowledgeable instructor (an adept mathematician who has competed at an international level) who will bring you not only his probability knowledge but the complicated interconnections between his areas of expertise – finance and data science\nComprehensive – we will cover all major probability topics and skills you need to level up your career\nExtensive case studies - helping you reinforce everything you’ve learned\nExceptional support – we said that, but let’s say it again - if you don’t understand a concept or you simply want to drop us a line, you’ll receive an answer within 1 business day\nSuccinct – the biggest investment you’ll make is your own time. And we will not waste it. All our teaching is straight to the point\nStill not convinced?\nHere’s why you need these skills?\nSalary/Income – most businesses are starting to realize      the advantages of implementing data-driven decisions. And those are all stepping on probability. A probabilistic mindset is definitely one of the non-automatable skills that managers of the next decade will be  expected to have\nPromotions and secure future – If you understand probability well, you will be able to back up your business and positions in much more convincing way, draining from quantitative evidence; needless to say, that’s the path to career growth\nNew horizons – probability is a pathway to many positions in any industry. While it is rarely a full-time position, it is crucial for most business jobs nowadays. And it’s not a boring aspect!\nPlease bear in mind that the course comes with Udemy’s 30-day money-back guarantee. And why not give such a guarantee? We are certain this course will provide a ton of value for you.\nLet's start learning together now!",
      "target_audience": [
        "People who want a career in Data Science",
        "People interested in a Business Intelligence career",
        "Business analysts",
        "Business executives",
        "Individuals who are passionate about numbers and quant analysis",
        "Anyone who wants to learn the subtleties of Probability and how it is used in the business world",
        "People who want to start learning probability",
        "People who want to learn the fundamentals of probability",
        "People who wish to extract insights from summarized statistics to understand academic papers"
      ]
    },
    {
      "title": "Machine Learning A-Z: AI, Python & R + ChatGPT Prize [2025]",
      "url": "https://www.udemy.com/course/machinelearning/",
      "bio": "Learn to create Machine Learning Algorithms in Python and R from two Data Science experts. Code templates included.",
      "objectives": [
        "Master Machine Learning on Python & R",
        "Have a great intuition of many Machine Learning models",
        "Make accurate predictions",
        "Make powerful analysis",
        "Make robust Machine Learning models",
        "Create strong added value to your business",
        "Use Machine Learning for personal purpose",
        "Handle specific topics like Reinforcement Learning, NLP and Deep Learning",
        "Handle advanced techniques like Dimensionality Reduction",
        "Know which Machine Learning model to choose for each type of problem",
        "Build an army of powerful Machine Learning models and know how to combine them to solve any problem"
      ],
      "course_content": {},
      "requirements": [
        "Just some high school mathematics level."
      ],
      "description": "Interested in the field of Machine Learning? Then this course is for you!\nThis course has been designed by a Data Scientist and a Machine Learning expert so that we can share our knowledge and help you learn complex theory, algorithms, and coding libraries in a simple way.\nOver 1 Million students world-wide trust this course.\nWe will walk you step-by-step into the World of Machine Learning. With every tutorial, you will develop new skills and improve your understanding of this challenging yet lucrative sub-field of Data Science.\nThis course can be completed by either doing either the Python tutorials, or R tutorials, or both - Python & R. Pick the programming language that you need for your career.\nThis course is fun and exciting, and at the same time, we dive deep into Machine Learning. It is structured the following way:\nPart 1 - Data Preprocessing\nPart 2 - Regression: Simple Linear Regression, Multiple Linear Regression, Polynomial Regression, SVR, Decision Tree Regression, Random Forest Regression\nPart 3 - Classification: Logistic Regression, K-NN, SVM, Kernel SVM, Naive Bayes, Decision Tree Classification, Random Forest Classification\nPart 4 - Clustering: K-Means, Hierarchical Clustering\nPart 5 - Association Rule Learning: Apriori, Eclat\nPart 6 - Reinforcement Learning: Upper Confidence Bound, Thompson Sampling\nPart 7 - Natural Language Processing: Bag-of-words model and algorithms for NLP\nPart 8 - Deep Learning: Artificial Neural Networks, Convolutional Neural Networks\nPart 9 - Dimensionality Reduction: PCA, LDA, Kernel PCA\nPart 10 - Model Selection & Boosting: k-fold Cross Validation, Parameter Tuning, Grid Search, XGBoost\nEach section inside each part is independent. So you can either take the whole course from start to finish or you can jump right into any specific section and learn what you need for your career right now.\nMoreover, the course is packed with practical exercises that are based on real-life case studies. So not only will you learn the theory, but you will also get lots of hands-on practice building your own models.\nAnd last but not least, this course includes both Python and R code templates which you can download and use on your own projects.",
      "target_audience": [
        "Anyone interested in Machine Learning.",
        "Students who have at least high school knowledge in math and who want to start learning Machine Learning.",
        "Any intermediate level people who know the basics of machine learning, including the classical algorithms like linear regression or logistic regression, but who want to learn more about it and explore all the different fields of Machine Learning.",
        "Any people who are not that comfortable with coding but who are interested in Machine Learning and want to apply it easily on datasets.",
        "Any students in college who want to start a career in Data Science.",
        "Any data analysts who want to level up in Machine Learning.",
        "Any people who are not satisfied with their job and who want to become a Data Scientist.",
        "Any people who want to create added value to their business by using powerful Machine Learning tools."
      ]
    },
    {
      "title": "Azure Databricks and Spark SQL (Python)",
      "url": "https://www.udemy.com/course/azure-databricks-and-spark-sql-python/",
      "bio": "Your Hands-On Guide to Databricks Data Engineering with PySpark and Spark SQL, including a 4-Part Course Project",
      "objectives": [
        "How to use Databricks to build and run data engineering workflows",
        "The principles of the Lakehouse architecture with Delta Lake",
        "How to process data with Spark SQL and PySpark",
        "Best practices for Databricks compute, jobs, and orchestration",
        "How to apply governance with Unity Catalog and manage secure access",
        "Working with streaming pipelines using Structured Streaming and Lakeflow",
        "Applying concepts to real-world projects with modular code and version control",
        "Real World Scenarios"
      ],
      "course_content": {},
      "requirements": [
        "Basic to intermediate SQL",
        "Basic to intermediate Python"
      ],
      "description": "[ This course has been completely refreshed with 17 hours of brand-new content (Sept 2025)]\n\n\nI’m Malvik Vaghadia, a Data Engineer and Architect with nearly 15 years of professional experience. I’ve worked on multiple large-scale lakehouse implementations and consulted for enterprise clients. As an instructor, I’ve taught 200,000+ students worldwide and hold a 4.6+ instructor rating. Since launching this course, it has become one of Udemy’s best-sellers in the Databricks category, and this new version (Sept 2025) has been completely rebuilt with 17 hours of brand-new content.\n\n\nWhy Learn Databricks\nDatabricks is recognised as a Leader in the Gartner Magic Quadrant for Data & AI platforms. It has become the go-to lakehouse platform for modern data engineering, enabling organisations to build, orchestrate, and optimise pipelines at scale. By mastering Databricks, you’ll be learning one of the most in-demand skills in today’s data landscape.\n\n\nCourse Delivery Style\nThis course is designed with the right balance of theory, hands-on coding, and practical projects. Every concept is explained clearly, then demonstrated live in Databricks, and reinforced with a multi-phase, end-to-end project that you’ll build step by step. You’ll also get all course notebooks as downloadable materials, containing the full code, step-by-step documentation, and extra resources so you can follow along easily.\n\n\nCurriculum Highlights:\nFour Part Course Project: End-to-end NYC Taxi project and further pipeline builds across multiple parts as you develop your knowledge.\nFoundations: What data engineering is, why Databricks, the Spark architecture, PySpark, and the Lakehouse.\nAzure setup: Account creation, resources, role-based access control, naming conventions, and cost management.\nDatabricks setup: Creating and configuring a workspace, navigating the UI, and handling personal email restrictions.\nDatabricks notebooks and workspace: Markdown, comments, organising objects, mixing languages, and notebook tips.\nDatabricks compute: Clusters, DBU pricing, runtimes, serverless vs all-purpose compute, instance pools, and SQL warehouses.\nSpark SQL (Python): Writing Spark SQL code using both SQL syntax and DataFrame APIs, reading/writing different file formats, defining schemas, and managing tables and views.\nPySpark Transformations: Column operations, functions, filtering, sorting, joining, aggregations, pivots, and conditional logic.\nMedallion architecture: Bronze, Silver, and Gold layers explained and implemented.\nDelta Lake: Transaction log, schema enforcement and evolution, time travel, and DML operations (MERGE, UPDATE, DELETE).\nWorkflows and jobs: Passing parameters, handling failures, concurrency, conditional tasks, and monitoring.\nGit & local development: VS Code setup, linking with GitHub, repos, and workflow best practices.\nFunctions and modularization: Creating and importing Python modules, UDFs, and project structuring.\nUnity Catalog & governance: Metastores, securable objects, workspace roles, external locations, and permissions.\nStreaming & Lakeflow pipelines: Structured Streaming concepts, Auto Loader, watermarking, triggers, and the new Lakeflow (DLT) pipeline model.\nPerformance: Lazy evaluation, explain plans, caching, shuffles, broadcast joins, partitioning, Z-ORDER, and Liquid Clustering.\nAutomation & CI/CD: Programmatic interaction with Databricks, CLI demo, and high-level CI/CD overview.\nBy the end of the course, you’ll have both the knowledge and confidence to design, build, and optimise production-grade data pipelines on Databricks.",
      "target_audience": [
        "Anyone interested in working with Big Data and Spark",
        "Anyone interested in working with Databricks",
        "Anyone interested in working with cloud platforms",
        "Aspiring Data Engineers"
      ]
    },
    {
      "title": "Complete A.I. & Machine Learning, Data Science Bootcamp",
      "url": "https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/",
      "bio": "Learn Data Science, Data Analysis, Machine Learning (Artificial Intelligence) and Python with Tensorflow, Pandas & more!",
      "objectives": [
        "Become a Data Scientist and get hired",
        "Master Machine Learning and use it on the job",
        "Deep Learning, Transfer Learning and Neural Networks using the latest Tensorflow 2.0",
        "Use modern tools that big tech companies like Google, Apple, Amazon and Meta use",
        "Present Data Science projects to management and stakeholders",
        "Learn which Machine Learning model to choose for each type of problem",
        "Real life case studies and projects to understand how things are done in the real world",
        "Learn best practices when it comes to Data Science Workflow",
        "Implement Machine Learning algorithms",
        "Learn how to program in Python using the latest Python 3",
        "How to improve your Machine Learning Models",
        "Learn to pre process data, clean data, and analyze large data.",
        "Build a portfolio of work to have on your resume",
        "Developer Environment setup for Data Science and Machine Learning",
        "Supervised and Unsupervised Learning",
        "Machine Learning on Time Series data",
        "Explore large datasets using data visualization tools like Matplotlib and Seaborn",
        "Explore large datasets and wrangle data using Pandas",
        "Learn NumPy and how it is used in Machine Learning",
        "A portfolio of Data Science and Machine Learning projects to apply for jobs in the industry with all code and notebooks provided",
        "Learn to use the popular library Scikit-learn in your projects",
        "Learn about Data Engineering and how tools like Hadoop, Spark and Kafka are used in the industry",
        "Learn to perform Classification and Regression modelling",
        "Learn how to apply Transfer Learning"
      ],
      "course_content": {
        "Introduction": [
          "Course Outline",
          "Join Our Online Classroom!",
          "Exercise: Meet Your Classmates & Instructor",
          "Asking Questions + Getting Help",
          "Your First Day"
        ],
        "Machine Learning 101": [
          "What Is Machine Learning?",
          "AI/Machine Learning/Data Science",
          "ZTM Resources",
          "Exercise: Machine Learning Playground",
          "How Did We Get Here?",
          "Exercise: YouTube Recommendation Engine",
          "Types of Machine Learning",
          "Are You Getting It Yet?",
          "What Is Machine Learning? Round 2",
          "Section Review",
          "Monthly Coding Challenges, Free Resources and Guides"
        ],
        "Machine Learning and Data Science Framework": [
          "Section Overview",
          "Introducing Our Framework",
          "6 Step Machine Learning Framework",
          "Types of Machine Learning Problems",
          "Types of Data",
          "Types of Evaluation",
          "Features In Data",
          "Modelling - Splitting Data",
          "Modelling - Picking the Model",
          "Modelling - Tuning",
          "Modelling - Comparison",
          "Overfitting and Underfitting Definitions",
          "Experimentation",
          "Tools We Will Use",
          "Optional: Elements of AI"
        ],
        "The 2 Paths": [
          "The 2 Paths",
          "Python + Machine Learning Monthly",
          "Endorsements On LinkedIN"
        ],
        "Data Science Environment Setup": [
          "Section Overview",
          "Introducing Our Tools",
          "What is Conda?",
          "Conda Environments",
          "Mac Environment Setup",
          "Mac Environment Setup 2",
          "Windows Environment Setup",
          "Windows Environment Setup 2",
          "Linux Environment Setup",
          "Sharing your Conda Environment",
          "Jupyter Notebook Walkthrough",
          "Jupyter Notebook Walkthrough 2",
          "Jupyter Notebook Walkthrough 3"
        ],
        "Pandas: Data Analysis": [
          "Section Overview",
          "Downloading Workbooks and Assignments",
          "Pandas Introduction",
          "Series, Data Frames and CSVs",
          "Data from URLs",
          "Quick Note: Upcoming Videos",
          "Describing Data with Pandas",
          "Selecting and Viewing Data with Pandas",
          "Quick Note: Upcoming Videos",
          "Selecting and Viewing Data with Pandas Part 2",
          "Manipulating Data",
          "Manipulating Data 2",
          "Manipulating Data 3",
          "Assignment: Pandas Practice",
          "How To Download The Course Assignments"
        ],
        "NumPy": [
          "Section Overview",
          "NumPy Introduction",
          "Quick Note: Correction In Next Video",
          "NumPy DataTypes and Attributes",
          "Creating NumPy Arrays",
          "NumPy Random Seed",
          "Viewing Arrays and Matrices",
          "Manipulating Arrays",
          "Manipulating Arrays 2",
          "Standard Deviation and Variance",
          "Reshape and Transpose",
          "Dot Product vs Element Wise",
          "Exercise: Nut Butter Store Sales",
          "Comparison Operators",
          "Sorting Arrays",
          "Turn Images Into NumPy Arrays",
          "Exercise: Imposter Syndrome",
          "Assignment: NumPy Practice",
          "Optional: Extra NumPy resources"
        ],
        "Matplotlib: Plotting and Data Visualization": [
          "Section Overview",
          "Matplotlib Introduction",
          "Importing And Using Matplotlib",
          "Anatomy Of A Matplotlib Figure",
          "Scatter Plot And Bar Plot",
          "Histograms And Subplots",
          "Subplots Option 2",
          "Quick Tip: Data Visualizations",
          "Plotting From Pandas DataFrames",
          "Quick Note: Regular Expressions",
          "Plotting From Pandas DataFrames 2",
          "Plotting from Pandas DataFrames 3",
          "Plotting from Pandas DataFrames 4",
          "Plotting from Pandas DataFrames 5",
          "Plotting from Pandas DataFrames 6",
          "Plotting from Pandas DataFrames 7",
          "Customizing Your Plots",
          "Customizing Your Plots 2",
          "Saving And Sharing Your Plots",
          "Assignment: Matplotlib Practice"
        ],
        "Scikit-learn: Creating Machine Learning Models": [
          "Section Overview",
          "Scikit-learn Introduction",
          "Quick Note: Upcoming Video",
          "Refresher: What Is Machine Learning?",
          "Quick Note: Upcoming Videos",
          "Scikit-learn Cheatsheet",
          "Typical scikit-learn Workflow",
          "Optional: Debugging Warnings In Jupyter",
          "Getting Your Data Ready: Splitting Your Data",
          "Quick Tip: Clean, Transform, Reduce",
          "Getting Your Data Ready: Convert Data To Numbers",
          "Note: Update to next video (OneHotEncoder can handle NaN/None values)",
          "Getting Your Data Ready: Handling Missing Values With Pandas",
          "Extension: Feature Scaling",
          "Note: Correction in the upcoming video (splitting data)",
          "Getting Your Data Ready: Handling Missing Values With Scikit-learn",
          "NEW: Choosing The Right Model For Your Data",
          "NEW: Choosing The Right Model For Your Data 2 (Regression)",
          "Quick Note: Decision Trees",
          "Quick Tip: How ML Algorithms Work",
          "Choosing The Right Model For Your Data 3 (Classification)",
          "Fitting A Model To The Data",
          "Making Predictions With Our Model",
          "predict() vs predict_proba()",
          "NEW: Making Predictions With Our Model (Regression)",
          "NEW: Evaluating A Machine Learning Model (Score) Part 1",
          "NEW: Evaluating A Machine Learning Model (Score) Part 2",
          "Evaluating A Machine Learning Model 2 (Cross Validation)",
          "Evaluating A Classification Model 1 (Accuracy)",
          "Evaluating A Classification Model 2 (ROC Curve)",
          "Evaluating A Classification Model 3 (ROC Curve)",
          "Reading Extension: ROC Curve + AUC",
          "Evaluating A Classification Model 4 (Confusion Matrix)",
          "NEW: Evaluating A Classification Model 5 (Confusion Matrix)",
          "Evaluating A Classification Model 6 (Classification Report)",
          "NEW: Evaluating A Regression Model 1 (R2 Score)",
          "NEW: Evaluating A Regression Model 2 (MAE)",
          "NEW: Evaluating A Regression Model 3 (MSE)",
          "Machine Learning Model Evaluation",
          "NEW: Evaluating A Model With Cross Validation and Scoring Parameter",
          "NEW: Evaluating A Model With Scikit-learn Functions",
          "Improving A Machine Learning Model",
          "Tuning Hyperparameters",
          "Tuning Hyperparameters 2",
          "Tuning Hyperparameters 3",
          "Note: Metric Comparison Improvement",
          "Quick Tip: Correlation Analysis",
          "Saving And Loading A Model",
          "Saving And Loading A Model 2",
          "Putting It All Together",
          "Putting It All Together 2",
          "Scikit-Learn Practice"
        ],
        "Supervised Learning: Classification + Regression": [
          "Milestone Projects!"
        ]
      },
      "requirements": [
        "No prior experience is needed (not even Math and Statistics). We start from the very basics.",
        "A computer (Linux/Windows/Mac) with internet connection.",
        "Two paths for those that know programming and those that don't.",
        "All tools used in this course are free for you to use."
      ],
      "description": "Become a complete A.I., Data Scientist and Machine Learning engineer! Join a live online community of 900,000+ engineers and a course taught by industry experts that have actually worked for large companies in places like Silicon Valley and Toronto. Graduates of Andrei’s courses are now working at Google, Tesla, Amazon, Apple, IBM, JP Morgan, Meta, + other top tech companies. You will go from zero to mastery!\n\n\nLearn Data Science and Machine Learning from scratch, get hired, and have fun along the way with the most modern, up-to-date Data Science course on Udemy (we use the latest version of Python, Tensorflow 2.0 and other libraries). This course is focused on efficiency: never spend time on confusing, out of date, incomplete Machine Learning tutorials anymore. We are pretty confident that this is the most comprehensive and modern course you will find on the subject anywhere (bold statement, we know).\nThis comprehensive and project based course will introduce you to all of the modern skills of a Data Scientist and along the way, we will build many real world projects to add to your portfolio. You will get access to all the code, workbooks and templates (Jupyter Notebooks) on Github, so that you can put them on your portfolio right away! We believe this course solves the biggest challenge to entering the Data Science and Machine Learning field: having all the necessary resources in one place and learning the latest trends and on the job skills that employers want.\n\nThe curriculum is going to be very hands on as we walk you from start to finish of becoming a professional Machine Learning and Data Science engineer. The course covers 2 tracks. If you already know programming, you can dive right in and skip the section where we teach you Python from scratch. If you are completely new, we take you from the very beginning and actually teach you Python and how to use it in the real world for our projects. Don't worry, once we go through the basics like Machine Learning 101 and Python, we then get going into advanced topics like Neural Networks, Deep Learning and Transfer Learning so you can get real life practice and be ready for the real world (We show you fully fledged Data Science and Machine Learning projects and give you programming Resources and Cheatsheets)!\n\nThe topics covered in this course are:\n\n\n- Data Exploration and Visualizations\n- Neural Networks and Deep Learning\n- Model Evaluation and Analysis\n- Python 3\n- Tensorflow 2.0\n- Numpy\n- Scikit-Learn\n- Data Science and Machine Learning Projects and Workflows\n- Data Visualization in Python with MatPlotLib and Seaborn\n- Transfer Learning\n- Image recognition and classification\n- Train/Test and cross validation\n- Supervised Learning: Classification, Regression and Time Series\n- Decision Trees and Random Forests\n- Ensemble Learning\n- Hyperparameter Tuning\n- Using Pandas Data Frames to solve complex tasks\n- Use Pandas to handle CSV Files\n- Deep Learning / Neural Networks with TensorFlow 2.0 and Keras\n- Using Kaggle and entering Machine Learning competitions\n- How to present your findings and impress your boss\n- How to clean and prepare your data for analysis\n- K Nearest Neighbours\n- Support Vector Machines\n- Regression analysis (Linear Regression/Polynomial Regression)\n- How Hadoop, Apache Spark, Kafka, and Apache Flink are used\n- Setting up your environment with Conda, MiniConda, and Jupyter Notebooks\n- Using GPUs with Google Colab\n\n\nBy the end of this course, you will be a complete Data Scientist that can get hired at large companies. We are going to use everything we learn in the course to build professional real world projects like Heart Disease Detection, Bulldozer Price Predictor, Dog Breed Image Classifier, and many more. By the end, you will have a stack of projects you have built that you can show off to others.\n\n\nHere’s the truth: Most courses teach you Data Science and do just that. They show you how to get started. But the thing is, you don’t know where to go from there or how to build your own projects. Or they show you a lot of code and complex math on the screen, but they don't really explain things well enough for you to go off on your own and solve real life machine learning problems.\n\n\nWhether you are new to programming, or want to level up your Data Science skills, or are coming from a different industry, this course is for you. This course is not about making you just code along without understanding the principles so that when you are done with the course you don’t know what to do other than watch another tutorial. No! This course will push you and challenge you to go from an absolute beginner with no Data Science experience, to someone that can go off, forget about Daniel and Andrei, and build their own Data Science and Machine learning workflows.\n\nMachine Learning has applications in Business Marketing and Finance, Healthcare, Cybersecurity, Retail, Transportation and Logistics, Agriculture, Internet of Things, Gaming and Entertainment, Patient Diagnosis, Fraud Detection, Anomaly Detection in Manufacturing, Government, Academia/Research, Recommendation Systems and so much more. The skills learned in this course are going to give you a lot of options for your career.\nYou hear statements like Artificial Neural Network, or Artificial Intelligence (AI), and by the end of this course, you will finally understand what these mean!\n\n\nClick “Enroll Now” and join others in our community to get a leg up in the industry, and learn Data Scientist and Machine Learning. We guarantee this is better than any bootcamp or online course out there on the topic. See you inside the course!\n\n\nTaught By:\n\nDaniel Bourke:\nA self-taught Machine Learning Engineer who lives on the internet with an uncurable desire to take long walks and fill up blank pages.\nMy experience in machine learning comes from working at one of Australia's fastest-growing artificial intelligence agencies, Max Kelsen.\nI've worked on machine learning and data problems across a wide range of industries including healthcare, eCommerce, finance, retail and more.\nTwo of my favourite projects include building a machine learning model to extract information from doctors notes for one of Australia's leading medical research facilities, as well as building a natural language model to assess insurance claims for one of Australia's largest insurance groups.\nDue to the performance of the natural language model (a model which reads insurance claims and decides which party is at fault), the insurance company were able to reduce their daily assessment load by up to 2,500 claims.\nMy long-term goal is to combine my knowledge of machine learning and my background in nutrition to work towards answering the question \"what should I eat?\".\nAside from building machine learning models on my own, I love writing about and making videos on the process. My articles and videos on machine learning on Medium, personal blog and YouTube have collectively received over 5-million views.\nI love nothing more than a complicated topic explained in an entertaining and educative matter. I know what it's like to try and learn a new topic, online and on your own. So I pour my soul into making sure my creations are accessible as possible.\nMy modus operandi (a fancy term for my way of doing things) is learning to create and creating to learn. If you know the Japanese word for this concept, please let me know.\nQuestions are always welcome.\n\n\nAndrei Neagoie:\nAndrei is the instructor of the highest rated Development courses on Udemy as well as one of the fastest growing. His graduates have moved on to work for some of the biggest tech companies around the world like Apple, Google, Amazon, JP Morgan, IBM, UNIQLO etc... He has been working as a senior software developer in Silicon Valley and Toronto for many years, and is now taking all that he has learned, to teach programming skills and to help you discover the amazing career opportunities that being a developer allows in life.\nHaving been a self taught programmer, he understands that there is an overwhelming number of online courses, tutorials and books that are overly verbose and inadequate at teaching proper skills. Most people feel paralyzed and don't know where to start when learning a complex subject matter, or even worse, most people don't have $20,000 to spend on a coding bootcamp. Programming skills should be affordable and open to all. An education material should teach real life skills that are current and they should not waste a student's valuable time.   Having learned important lessons from working for Fortune 500 companies, tech startups, to even founding his own business, he is now dedicating 100% of his time to teaching others valuable software development skills in order to take control of their life and work in an exciting industry with infinite possibilities.\nAndrei promises you that there are no other courses out there as comprehensive and as well explained. He believes that in order to learn anything of value, you need to start with the foundation and develop the roots of the tree. Only from there will you be able to learn concepts and specific skills(leaves) that connect to the foundation. Learning becomes exponential when structured in this way.\nTaking his experience in educational psychology and coding, Andrei's courses will take you on an understanding of complex subjects that you never thought would be possible.\nSee you inside the course!",
      "target_audience": [
        "Anyone with zero experience (or beginner/junior) who wants to learn Machine Learning, Data Science and Python",
        "You are a programmer that wants to extend their skills into Data Science and Machine Learning to make yourself more valuable",
        "Anyone who wants to learn these topics from industry experts that don’t only teach, but have actually worked in the field",
        "You’re looking for one single course to teach you about Machine learning and Data Science and get you caught up to speed with the industry",
        "You want to learn the fundamentals and be able to truly understand the topics instead of just watching somebody code on your screen for hours without really “getting it”",
        "You want to learn to use Deep learning and Neural Networks with your projects",
        "You want to add value to your own business or company you work for, by using powerful Machine Learning tools."
      ]
    },
    {
      "title": "Neural Networks In Python From Scratch. Build step by step!",
      "url": "https://www.udemy.com/course/build-neural-networks-from-scratch-with-python-step-by-step/",
      "bio": "Understand machine learning and deep learning by building linear regression and gradient descent from the ground up.",
      "objectives": [
        "The basic functions for any neural network, by coding linear regression, cost functions and back propagation",
        "Understand the properties of neural networks by adjusting learning rates and biases",
        "Train a network by implementing a gradient descent algorithm",
        "Normalizing inputs for multi-input networks",
        "Create classification networks by implementing multiple output neurons and activation",
        "Improve network accuracy by implementing hidden layers for non-linear data"
      ],
      "course_content": {},
      "requirements": [
        "You have an interest in neural networks.",
        "You have some programming experience in Python or another language."
      ],
      "description": "You will learn how to build Neural Networks with Python. Without the need for any library, you will see how a simple neural network from 4 lines of code, evolves into a artificial intelligence network that is able to recognize handwritten digits.\n\nDuring this process, you will learn concepts like: Feed forward, Cost functions, Back propagation, Hidden layers, Linear regression, Gradient descent and Matrix multiplication. And all this with plain Python.\nTarget audience\nDevelopers who especially benefit from this course, are:\nDeveloper who want to learn the mechanics of neural networks\nDevelopers who want to avoid using neural network libraries and frameworks\nOr developers who use frameworks but want to learn the meaning of the individual network parameters\nChallenges\n\nMany tutorials claim to start from scratch, but import external libraries or rapidly type in code and before executing even once, you are looking at 50 lines of code. When finally the code is run, you are totally lost and still stuck trying to understand line 3.\nThis causes many students to give up learning Neural Networks.\nThis course is different! It starts with the absolute beginning and each topic is a continuation of a previous example. This way, you will learn neural networks from the ground up, step by step.\n\n\nWhat can you do after this course?\nYou understand neural network concepts and ideas, like back propagation and gradient descent.\nYou are able to build a neural network in any programming language of choice, without the help of frameworks and libraries.\nYou understand how to better configure the network by plugging in different cost functions and adding hidden layers.\n\nTopics\nLinear regression\nCost functions\nBias\nMultiple inputs\nNormalisation\nGradient descent\nClassification\nActivation\nMulti-class classification\nNon-linear data\nHidden layers\nDuration\n3 hour video time. This course has no exercises.\nThe teacher\nThis course is taught by Loek van den Ouweland, a senior software engineer with 25 years of professional experience. Loek is the creator of Wunderlist for windows, Microsoft To-do and Mahjong for Windows and loves to teach software engineering.\nStudents of this course tell me:\n* * * * * “Great, simple explanations. Perfect for beginners that have little pre knowledge of the topic.”\n* * * * * “Straight to the point starting with the foundations.”\n* * * * * “Clearly explained step by step how Neural Networks work and can be developed in a pure development language of choice without the usage of any external package..”",
      "target_audience": [
        "Developer who want to learn the mechanics of neural networks",
        "Developers who want to avoid using neural network libraries and frameworks",
        "Developers who use frameworks but want to learn the meaning of the individual network parameters"
      ]
    },
    {
      "title": "2025 Master class on Data Science using Python A-Z for ML",
      "url": "https://www.udemy.com/course/master-class-on-datascience/",
      "bio": "Python NumPy, Pandas, Matplotlib and Seaborn for Data Analysis, Data Science and ML. Pre-machine learning Analysis.",
      "objectives": [
        "Students will learn how to create and manipulate arrays, perform mathematical operations on arrays, and use functions such as sorting, searching, and statistics",
        "Students will learn how to create and manipulate Series and Data Frames.",
        "Students will learn how to create plots and charts, customize the appearance of visualizations, and add annotations and labels.",
        "NumPy, Pandas, and Matplotlib will typically teach students how to use these tools to analyze and visualize data."
      ],
      "course_content": {},
      "requirements": [
        "Little knowledge in Python will be an added advantage.",
        "If you are new to python then don't worry you can still learn python basics from the BONUS section of this course."
      ],
      "description": "Welcome to 2025 Master class on Data Science using Python.\nNumPy is a leading scientific computing library in Python while Pandas is for data manipulation and analysis. Also, learn to use Matplotlib for data visualization. Whether you are trying to go into Data Science, dive into machine learning, or deep learning, NumPy and Pandas are the top Modules in Python you should understand to make the journey smooth for you. In this course, we are going to start from the basics of Python NumPy and Pandas to the advanced NumPy and Pandas. This course will give you a solid understanding of NumPy, Pandas, and their functions.\nAt the end of the course, you should be able to write complex arrays for real-life projects, manipulate and analyze real-world data using Pandas.\n\n\nWHO IS THIS COURSE FOR?\n√ This course is for you if you want to master the in-and-out of NumPy, Pandas, and data visualization.\n√ This course is for you if you want to build real-world applications using NumPy or Panda and visualize them with Matplotlib and Seaborn.\n√ This course is for you if you want to learn NumPy, Pandas, Matplotlib and Seaborn for the first time or get a deeper knowledge of NumPy and Pandas to increase your productivity with deep and Machine learning.\n√ This course is for you if you are coming from other programming languages and want to learn Python NumPy and Pandas fast and know it really well.\n√ This course is for you if you are tired of NumPy, Pandas, Matplotlib and Seaborn courses that are too brief, too simple, or too complicated.\n√ This course is for you if you have to get the prerequisite knowledge to understanding Data Science and Machine Learning using NumPy and Pandas.\n√ This course is for you if you want to learn NumPy and Pandas by doing exciting real-life challenges that will distinguish you from the crowd.\n√ This course is for you if plan to pass an interview soon.",
      "target_audience": [
        "Students who want to learn data science using Python.",
        "Anyone with an interest in data science and machine learning"
      ]
    },
    {
      "title": "R Programming Hands-on Specialization for Data Science (Lv1)",
      "url": "https://www.udemy.com/course/r-programming-data-science-hands-on-course/",
      "bio": "An in-depth course on R language with real-world Data Science examples to supercharge your R data analysis skills",
      "objectives": [
        "Setup and Use Development Environment for R",
        "Install and Use Packages in R",
        "Learn and use Atomic Data Types in R",
        "Learn and apply advanced explicit/Implicit Coercioning in R",
        "Learn multiple approaches to create vectors in R",
        "Understand nuances and implications in Vector Coercions",
        "Understand Vector indexing principles in R",
        "Understand and leverage Vectors' flatness property",
        "Understand Vector Labels and Attributes and their practical use-cases",
        "Learn Matrices and multiple approaches for creation",
        "Learn how Matrices Dimension Property works",
        "Learn advanced techniques for Matrices Indexing",
        "Learn Matrices Operations and Important Functions",
        "Learn the amazing use-cases of Lists",
        "Learn to leverage Lists' Recursive Nature",
        "Learn multiple ways to create Lists (including from other data structures)",
        "Learn critical nuances in Lists Indexing, Labels and Lists Properties",
        "Learn multiple approaches to create Data Frames (including from other data structures)",
        "Learn Data Frames sub-setting (beginner to advanced)",
        "Learn how to impute missing values in Data Frames for efficient Data Analysis",
        "Learn R Control Structures (Conditional statements and loops)",
        "Learn to create and use R Functions",
        "Understand Web Scraping Process",
        "Learn R's Apply family of functions for advanced data manipulation",
        "Learn Multiple ways to perform Web Scraping in R",
        "Learn how to perform Data Munging, Cleansing and Transformation in R",
        "Learn HTML and Document Object Model in the context of Web Scraping",
        "Learn XPath Query Language for Web Scraping",
        "Learn RSelenium setup and usage for advanced Web Scraping",
        "Learn Regular Expression Functions in R for advanced analysis",
        "Learn advanced Data Frames techniques for efficient data analysis",
        "Learn how to perform statistical analysis and visualisation to derive insights in R"
      ],
      "course_content": {
        "Introduction": [
          "Warm Welcome!",
          "Why you should learn R?",
          "What you will learn in this course?"
        ],
        "R Fundamentals": [
          "Installing R (console) and RStudio (IDE)",
          "Getting to know R - Setting Context",
          "R Basics - Working Directory, Environment Variables and more!",
          "R Basics - Loading and Executing R scripts from local file system",
          "Handling Working Directory"
        ],
        "R Data Types": [
          "R Atomic Data Types Intro - What you must know about Numeric and Integers in R?",
          "Complex and Character Data Types (Atomic)",
          "Character Data Type (Atomic) + Important Data Transformation Functions (1)",
          "Character Data Type (Atomic) + Important Data Transformation Functions (2)",
          "Character Data Type (Atomic) + Important Data Transformation Functions (3)",
          "Logical Data Type (Atomic) and Its known Implications",
          "Atomic Data Types and Nuances in Coercioning (Explicit/Implicit)",
          "Data Types Coercions"
        ],
        "R Data Structure - Vectors": [
          "Vectors - Creation, Homogeneity, Coercion Implications and Important Functions!",
          "Vectors - Comparing different ways to create vectors in R!",
          "Vectors - Understanding Indexing like never before!",
          "Vectors - Indexing (Out of Bound scenarios) and How Pros use it!",
          "Vectors - Flatness property and its critical implications in Indexing!",
          "Vectors - Labels and their Advanced Usage in Indexing",
          "Vectors - Assigning Attributes and its use-case as Metadata",
          "Indexing Vectors"
        ],
        "R Data Structure - Matrices": [
          "Matrices - Getting Acquainted, Creation and its operational functions!",
          "Matrices - Creation and Implications related to its Dimensions",
          "Matrices - Creation from Vectors + Naming Dimensions (Explicit, Implicit)",
          "Matrices - Dimensions (Advanced) and Intro to Indexing",
          "Matrices - Indexing Continued",
          "Matrices - Advanced Indexing using DimensionNames",
          "Matrices - Even more Advanced Indexing!",
          "Matrices - Operations!"
        ],
        "R Data Structure - Lists": [
          "Lists - Getting Introduced to one of the most powerful data structures in R",
          "Lists - Comparing with Vectors w.r.t Heterogeneity and Introducing Indexing",
          "Lists - Comprehending their Recursive Nature in comparison with Vectors",
          "Lists - Converting to and from Vectors and implications (coercion, flatness)",
          "Lists - Nuances in Determining Length in the context of Recursiveness",
          "Lists - Nuances in Determining Length and Class of Elements",
          "List - Advanced Indexing also using Labels",
          "List - Comparison of Indexing ways and Implications"
        ],
        "R Data Structure - Data Frames": [
          "Data Frames - Introducing The holy grail of processing Structured Data",
          "Data Frames - Creation and important functions for Basic Exploratory Analysis",
          "Data Frames - More Important Functions for Basic Exploratory Analysis",
          "Data Frames - Creation from Lists",
          "Data Frames - Creation from Lists, Matrices and Vectors",
          "Data Frames - Everything you need to know about Subsetting",
          "Data Frames - Handling Missing Values like Pros!",
          "Data Frames - Imputing Missing Values like Pros!",
          "Data Frames - Advanced Subsetting Techniques for robust analytics"
        ],
        "R Control Structures": [
          "While Loops in R",
          "For Loops in R - Intro and Practical Use-Cases",
          "If Else Structures in R",
          "If Else Structures in R (2)",
          "If Else Structures in R (3)"
        ],
        "Data Science Application in R - Automated Web Scraping Bot": [
          "Web Scraping - Setting Context + Highlighting Use-Cases",
          "Web Scraping - One Simple yet Powerful Way to do so!",
          "Web Scraping - Use Case: Custom Churn Analysis",
          "Use Case: Custom Churn - Performing Data Munging and Transformations",
          "Use Case: Custom Churn - Performing Data Munging and Transformations",
          "Use Case: Custom Churn - Performing Data Cleansing",
          "Web Scraping - Contextual understanding of HTML",
          "Web Scraping - Contextual Understanding of HTML Tags",
          "Web Scraping - How to exploit the Structure of Web Page for Efficient Scraping",
          "Web Scraping - Contextual Understanding of HTML Document Object Model (DOM)",
          "Web Scraping on Steroids - XPath in R!",
          "Web Scraping on Steroids - XPath in R (2)",
          "Web Scraping using XPath - Programmatic Extraction of Data from HTML Tags",
          "Web Scraping using XPath - Programmatic Extraction of Data from HTML Tags (2)",
          "Automating Web Scraping - RSelenium!",
          "Automated Web Scraping - Contextual Understanding of Selenium Components",
          "Automated Web Scraping - installing RSelenium in R",
          "Automated Web Scraping - Initialising RSelenium Server",
          "Automated Web Scraping - Connecting to RSelenium Server using Reference Class",
          "Automated Web Scraping - Navigating and Sending Key Strokes in Web Pages",
          "Web Scraping Use Case Context Setting",
          "Web Scraping Pipeline - Deep dive of workflow pattern",
          "Systematic analysis of website for efficient Scraping",
          "Installing and Loading RSelenium",
          "Starting Selenium Server - The right way!",
          "Handling RSelenium's Driver Issues",
          "Launching Selenium Server jar with correct driver settings (part 2)",
          "Web Scraper Program Initialisation and Remote Driver Object Instantiation",
          "Navigating web pages using RSelenium and Using Xpath for data extraction",
          "Using R's Apply Family of Functions for Data Extraction from RSelenium Objects",
          "Advanced Data Munging using R Regex and String Processing Functions",
          "Advanced Data Munging using R Regex and String processing functions (II)",
          "Advanced Data Munging - Discretizing Continuous Values",
          "Advanced Data Frames Manipulation",
          "Orchestrating Automation of Web Scraping Routine",
          "Advanced Statistical Analysis and Visualisation for Informed Decision Making"
        ]
      },
      "requirements": [
        "There is only one pre-requisite: Passion and commitment to learn!",
        "No prior Programming or Data Science experience needed",
        "All the software/tools are open-source and available for Free!",
        "A computer (Windows or Linux) with internet connection needed for hands-on exercises"
      ],
      "description": "R is considered as lingua franca of Data Science. Candidates with expertise in R programming language are in exceedingly high demand and paid lucratively in Data Science. IEEE has repeatedly ranked R as one of the top and most popular Programming Languages. Almost every Data Science and Machine Learning job posted globally mentions the requirement for R language proficiency. All the top ranked universities like MIT have included R in their respective Data Science courses curriculum.\nWith its growing community of users in Open Source space, R allows you to productively work on complex Data Analysis and Data Science projects to acquire, transform/cleanse, analyse, model and visualise data to support informed decision making. But there's one catch: R has quite a steep learning curve!\nHow's this course different from so many other courses?\nMany of the available training courses on R programming don't cover it its entirety. To be proficient in R for Data Science requires thorough understanding of R programming constructs, data structures and none of the available courses cover them with the comprehensiveness and depth that each topic deserves. Many courses dive straight into Machine Learning algorithms and advanced stuff without thoroughly comprehending the R programming constructs. Such approaches to teach R have a lot of drawbacks and that's where many Data Scientists struggle with in their professional careers.\nAlso, the real value in learning R lies in learning from professionals who are experienced, proficient and are still working in Industry on huge projects; a trait which is missing in 90% of the training courses available on Udemy and other platforms.\nThis is what makes this course stand-out from the rest. This course has been designed to address these and many other fallacies and uniquely teaches R in a way that you won't find anywhere else. Taught by me, an experienced Data Scientist currently working in Deloitte (World's largest consultancy firm) in Australia and has worked on a number of Data Science projects in multiple niches like Retail, Web, Telecommunication and web-sector. I have over 5 years of diverse experience of working in my own start-ups (related to Data Science and Networking), BPO and digital media consultancy firms, and in academia's Data Science Research Labs. Its a rare combination of exposure that you will hardly find in any other instructor. You will be leveraging my valuable experience to learn and specialize R.\nWhat you're going to learn in this course?\nThe course will start from the very basics of introducing Data Science, importance of R and then will gradually build your concepts. In the first segment, we'll start from setting up R development environment, R Data types, Data Structures (the building blocks of R scripts), Control Structures and Functions.\nThe second segment comprises of applying your learned skills on developing industry-grade Data Science Application. You will be introduced to the mind-set and thought-process of working on Data Science Projects and Application development. The project will then focus on developing automated and robust Web Scraping bot in R. You will get the amazing opportunities to discover what multiple approaches are available and how to assess alternatives while making design decisions (something that Data Scientists do everyday). You will also be exposed to web technologies like HTML, Document Object Model, XPath, RSelenium in the context of web scraping, that will take your data analysis skills to the next level. The course will walk you through the step by step process of scraping real-life and live data from a classifieds website to analyse real-estate trends in Australia. This will involve acquiring, cleansing, munging and analyzing data using R statistical and visualisation capabilities.\nEach and every topic will be thoroughly explained with real-life hands-on examples, exercises along with disseminating implications, nuances, challenges and best-practices based on my years of experience.\n\nWhat you will gain from this course will be incomparable to what's currently available out there as you will be leveraging my growing experience and exposure in Data Science. This course will position you to not only apply for Data Science jobs but will also enable you to use R for more challenging industry-grade projects/problems and ultimately it will super-charge your career.\nSo take the decision and enrol in this course and lets work together to make you specialize in R Programming like never before!",
      "target_audience": [
        "Anyone who wants to get started or advance further in Data Science",
        "Anyone who wants to develop expertise in R programming based on best-practices",
        "Anyone who wants to learn how to use R for real-life challenging Data Science projects and applications"
      ]
    },
    {
      "title": "LangChain: Build 26 LLM Apps with OpenAI, Llama & DeepSeek",
      "url": "https://www.udemy.com/course/learn-langchain-build-12-llm-apps-using-openai-llama-2/",
      "bio": "Build Real World LLM powered applications with LangChain, OpenAI, Llama, DeepSeek. Create Web Apps with Streamlit.",
      "objectives": [
        "Master the basics of LangChain and the fundamentals of Large Language Models (LLMs)",
        "How to Use LangChain, OpenAI, Llama 2, Hugging Face to Build LLM-Powered Applications.",
        "Learn about LangChain components, including LLM wrappers, prompt templates, chains, agents, memory and document loaders",
        "Learn to apply LLM techniques to personal documents and projects",
        "Learn how to use embeddings and vector data stores.",
        "Learn about FAISS and Similarity Search.",
        "Learn about Pinecone and ChromaDB",
        "Project: Create a Simple Chatbot with Llama 2 and LangChain",
        "Project: Quiz MCQ Creator Application",
        "Project: YouTube Script Writing Application",
        "Project: PDF Chat App (GUI) | ChatGPT for Your PDF File",
        "Project: Chat with Multiple PDF Documents | Streamlit Application",
        "Project: Summarize PDF Using LangChain, OpenAI & Gradio",
        "Project: YouTube Video Summarizer",
        "Project: PrivateGPT- Chat with your Files Offline and Free",
        "Project: Support Chat Bot For Your Website",
        "Project: Question a Book with (LangChain + Llama 2 + Pinecone)",
        "Project: Create a chatbot to chat with multiple documents including pdf, .docs, .txt using Llama 2 , LangChain/ OpenAI and ChromaDB",
        "Project: Create a Custom Chatbot for any Website with LangChain and Llama 2/ OpenAI",
        "Project: Creating a Flask API for Automatic Content Summarization using LangChain and Llama 2/ Open AI",
        "Fine-Tune Llama 2 Model with LangChain on Custom Dataset",
        "Introducing 'GPT-LLM-Trainer' — the world's simplest way to train a task-specific model. Just input your idea, and let the AI do the rest.",
        "Project: Create a Medical Chatbot with Llama2, Pinecone and LangChain",
        "Project: ChatCSV App - Chat with CSV files using LangChain and Llama 2",
        "Project: Chat with Multiple PDFs using Llama 2, Pinecone and LangChain",
        "Project: Source Code Analysis with LangChain, OpenAI and ChromaDB",
        "Project: Run Code Llama on CPU and Create a Web App with Gradio",
        "Run PaLM 2 in Google Colab | How to use Free Google PaLM API",
        "Project: Chat with Multiple PDFs using PaLM 2, Pinecone and LangChain",
        "Project: Streamlit App | Chat with Multiple PDFs using PaLM 2, FAISS and LangChain",
        "Project: Chat with Your Documents using Llama-Index and Google PaLM 2",
        "Project: Create a Streamlit AI Chatbot with DeepSeek R1 LLM (via Ollama)",
        "Project: Build a RAG-Powered Streamlit App with DeepSeek R1 via Ollama",
        "Project: Build MCP Servers from Scratch with LangChain in Python"
      ],
      "course_content": {
        "Generative AI and Large Language Models": [
          "Welcome to this course",
          "What is Generative AI",
          "What are Large Language Models",
          "How ChatGPT is trained",
          "Introduction to LangChain"
        ],
        "OpenAI and Hugging Face Introduction": [
          "OpenAI API Key Generation",
          "Hugging Face API Key Generation"
        ],
        "Deep Dive into LangChain": [
          "Setting Up the Environment: LangChain and LLM Models (Wrappers)",
          "Prompt Templates",
          "LangChain Agents and Tools",
          "Chains",
          "Memory",
          "Document Loaders"
        ],
        "Llama 2": [
          "Introduction to Llama 2",
          "Run Llama 2 on Google Colab",
          "Introduction to Llama 2 with LangChain",
          "Prompt Templates and Chains",
          "Memory",
          "Document Loaders",
          "Tokens | Prompts | Create a Simple Chatbot"
        ],
        "Multi-dataframe Agents": [
          "Multi-dataframe Agents | CSV/Pandas"
        ],
        "LangChain: Run Hugging Face Models": [
          "Run Hugging Face Models"
        ],
        "Project # 1: YouTube Script Writing App": [
          "Introduction",
          "Creating a title for the Youtube Video using Basic Prompt",
          "Creating a title for the Youtube Video using Prompt Template",
          "Creating a title and Script for the Youtube Video using Prompt Template",
          "Title and Script Generation using Simple Sequential Chain",
          "Memory in LangChain",
          "YouTube Script Creation using LangChain Agents"
        ],
        "Project # 2: Chat with your PDF files using LangChain and OpenAI.": [
          "Chat with your PDF files using LangChain and OpenAI."
        ],
        "Project # 3: PDF Chat App | ChatGPT for Your PDF File - Streamlit Application": [
          "Streamlit Application to chat with your PDF file using LangChain and OpenAI."
        ],
        "Project # 4: YouTube Video Summarizer": [
          "YouTube Video Summarizer, powered by the dynamic duo of LangChain and OpenAI"
        ]
      },
      "requirements": [
        "Basic programming experience with Python!",
        "Curiosity to learn AI field"
      ],
      "description": "Master LangChain, OpenAI, Llama, DeepSeek and Hugging Face. Learn to Create hands-on generative LLM-powered applications with LangChain.\nCreate powerful web-based front-ends for your LLM Application using Streamlit.\nBy the end of this course, you will have a solid understanding of the fundamentals of LangChain OpenAI, Llama, DeepSeek and HuggingFace. You'll also be able to create modern front-ends using Streamlit in Python.\n\n\nDive into hands-on projects that will shape your expertise, including:\nProject 1: Create a Simple Chatbot with Llama 2 and LangChain\nProject 2: PDF Chat App (GUI) | ChatGPT for Your PDF File - Streamlit Application to chat with your PDF file using LangChain and OpenAI.\nProject 3: YouTube Script Writing App - Effortlessly create title and script for the YouTube video using LangChain and OpenAI\nProject 4:  MCQ Quiz Creator App - Seamlessly create multiple-choice quizzes for your students using LangChain and OpenAI/ Hugging Face\nProject 5: Chat with Multiple PDF Documents | Streamlit Application- Chat with your PDF files using LangChain and OpenAI.\nProject 6: Support Chat Bot For Your Website - Helps your visitors/customers to find the relevant data or blog links that can be useful to them.\nProject 7: YouTube Video Summarizer - YouTube Video Summarizer, powered by the dynamic duo of LangChain and OpenAI! In this groundbreaking tool, we have harnessed the cutting-edge capabilities of language processing technology to transform the way you consume YouTube content.\nProject 8: Summarize PDF Using LangChain,  OpenAI  and Gradio:  Summarize PDF files using Lang Chain and OpenAI  and create a sharable web interface using Gradio\nProject 9: PrivateGPT- Chat with your Files Offline and Free\nProject 10: Question a Book with (LangChain + Llama 2 + Pinecone):  Create a chatbot to chat with Books or with  PDF files. using  LangChain, Llama 2 Model and Pinecone as vector store.\nProject 11: Chat with Multiple Documents with Llama 2/ OpenAI and ChromaDB: Create a chatbot to chat with multiple documents including pdf, .docs, .txt using LangChain, Llama 2/ OpenAI  and ChromaDB as our vector database.\nProject 12: Create a Custom Chatbot for any Website with LangChain and Llama 2/ OpenAI: Create a chatbot for your own or for any website using LangChain, Llama 2/ OpenAI and FAISS as the vector store / vector database\nProject 13: Creating a Flask API for Automatic Content Summarization using LangChain and Llama 2/ Open AI\nProject 14: Introducing 'GPT-LLM-Trainer' — the world's simplest way to train a task-specific model.  Just input your idea, and let the AI do the rest.\nProject 15: Create a Medical Chatbot with Llama2, Pinecone and LangChain\nProject 16: Fine-Tune Llama 2 Model with LangChain on Custom Dataset\nProject 17: ChatCSV App - Chat with CSV files using LangChain and Llama 2\nProject 18: Chat with Multiple PDFs using Llama 2, Pinecone and LangChain\nProject 19: Run Code Llama on CPU and Create a Web App with Gradio\nProject 20: Source Code Analysis with LangChain, OpenAI and ChromaDB\nProject 21: Chat with Multiple PDFs using PaLM 2, Pinecone and LangChain\nProject 22: Streamlit App | Chat with Multiple PDFs using PaLM 2, FAISS and LangChain\nProject 23: Chat with Your  Documents using Llama-Index and Google PaLM 2\nProject 24: Create a Streamlit AI Chatbot with DeepSeek R1 LLM (via Ollama)\nProject 25: Build a RAG-Powered Streamlit App with DeepSeek R1 via Ollama\nProject 26: Build MCP Servers from Scratch with LangChain in Python\nCourse Content:\nIn this course, we will explore the capabilities of LangChain, to build scalable and performant AI applications.\nYou will gain in-depth knowledge of LangChain components, including LLM wrappers, Prompt Template, Chains, Agents, Memory and Document Loaders. Additionally, we will delve into embeddings and vector databases",
      "target_audience": [
        "Anyone who is excited to build AI powered LLM apps using Langchain",
        "AI Enthusiast"
      ]
    },
    {
      "title": "Face Recognition with Machine Learning + Deploy Flask App",
      "url": "https://www.udemy.com/course/build-face-recognition-app-using-machine-learning-in-flask/",
      "bio": "Create an Face Recognition project from scratch with Python, OpenCV , Machine Learning Algorithms, Flask, Heroku Deploy",
      "objectives": [
        "Automatic Face Recognition in images and videos",
        "Automatically detect faces from images and videos",
        "Evaluate and Tune Machine Learning",
        "Building Machine Learning Model for Classification",
        "Make Pipeline Model for deploying your application",
        "Image Processing with OpenCV",
        "Data Preprocessing for Images",
        "Create REST APIs in Flask",
        "Template Inheritance in Flask",
        "Integrating Machine Learning Model in Flask App",
        "Deploy Flask App in Heroku Cloud"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Face Recognition Project Components",
          "Download all Resourses",
          "Install Python",
          "Clone Face Recognition Template",
          "Create and Install Virtual Environment & Packages",
          "Next step"
        ],
        "Image Processing with OpenCV": [
          "OpenCV & Image",
          "What is Processing an Image (Information Extraction)",
          "Download Resources",
          "OpenCV: Values & Pixels",
          "OpenCV: Values & Pixels (Another Example)",
          "OpenCV: Read Image",
          "OpenCV: Pixels in Image",
          "OpenCV: Display Image",
          "OpenCV: Color Space",
          "OpenCV: Grayscale",
          "Image Resizing",
          "Face Detection",
          "Working on Videos"
        ],
        "Develop Face Recognition Model with Machine Learning from Scratch": [
          "Face Recognition Model Introduction",
          "Download Resources",
          "About Data",
          "Face Recognition Training Flow",
          "Data Preprocess : Idea of Detect & Crop Face",
          "Data Preprocessing: Get Data",
          "Data Preprocessing: Import Required Libraries",
          "Data Preprocess: Get List of path of all Images",
          "Data Preprocess: Detect Face with Haar Cascade Classifier",
          "Data Preprocess: Crop Detected Face",
          "Data Preprocess: Crop All Faces",
          "Data Preprocess: Idea of Structuring Images",
          "Data Preprocessing: Structuring Data Part - 1",
          "Data Preprocessing: Exploratory Data Analysis",
          "Data Preprocessing: Filter Low Resolution Images and Resize images",
          "Data Preprocessing: Structure all Images",
          "Eigen Face: Flow",
          "Eigen: Mean Face and PCA",
          "Eigen Face: Get Optimal components for PCA",
          "Eigen Face: Save PCA ML model",
          "Eigen Face: Visualize Eigen Face",
          "Train Face Recognition Model Part - 1",
          "Train Face Recognition Model - Part 2",
          "Train Face Recognition Model Part - 3",
          "Best Estimator",
          "Model Evaluation",
          "Save Face Recognition Model",
          "Face Recognition Pipeline part 1",
          "Face Recognition Pipeline part 2",
          "Face Recognition Pipeline part 3",
          "Face Recognition Pipeline part 4",
          "Face Recognition Pipeline part 5",
          "Predictions part -1",
          "Predictions part -2",
          "Predictions part -3"
        ],
        "Face Recognition Project (Integrating HTML Model to Flask App)": [
          "Download Resources",
          "Face Recognition Web App",
          "Install Visual Studio Code",
          "Folder Structure",
          "main.py (connect to virtual environment)",
          "main.py - basic app",
          "views.py and add url rule",
          "base.html part-1",
          "base.html part-2",
          "Home page",
          "App Page",
          "Gender App Page",
          "Gender App Page part 2",
          "Gender App Page part 3",
          "Gender App Page part 4",
          "Final App"
        ],
        "Deploy Web App in Heroku Cloud": [
          "Getting ready for Deployment",
          "Install Git",
          "Setup Code for Deployment",
          "Push code to GitHub",
          "Deploy Flask App",
          "Heroku is not free tier anymore.",
          "You App Deployed in Heroku"
        ],
        "Deploying in another open-course cloud.": [
          "Deploy Face Recognition Flask app in Railway.app (Free open source)",
          "Your Final App",
          "Download Final App"
        ],
        "Appendix - Python Crash Course": [
          "Download the Resources",
          "Walk through on Jupyter Notebook",
          "Print Statements",
          "Escape and Insert keys",
          "Variables & Assignments",
          "Data Types",
          "Data Type Casting",
          "List",
          "List Methods",
          "Tuple",
          "Sets",
          "Dictionaries",
          "in operator",
          "plus operator",
          "User Defined Functions",
          "Control Statements (if else)",
          "Range & Zip",
          "For Loop",
          "Python Reference"
        ],
        "[Optional]: Flask Crash Course": [
          "Introduction",
          "Installing Flask and Visual Studio Code",
          "Your First Flask App",
          "Flask Routing",
          "URL Building",
          "Flask Jinja Templates - Part 1",
          "Flask Jinja Templates - Part 2",
          "Flask Jinja Templates - Part 3",
          "Template Inheritance",
          "Static Files ( CSS, JS)",
          "Http Methods in Flask",
          "File Upload in Flask"
        ],
        "Extra Tips": [
          "Connect Python Environment to Virtual Environment in Visual Studio Code",
          "Reference Books and PDFs"
        ],
        "Bonus Lecture": [
          "Bonus Lecture: Next Steps"
        ]
      },
      "requirements": [
        "Should be at-least beginner level in Python",
        "Be able to understand HTML and CSS",
        "Basic Understanding of Machine Learning Concepts"
      ],
      "description": "MLOPs: AI based Face Recognition Web App in Flask & Deploy\nFace recognition is one of the most widely used in my application. If at all you want to develop and deploy the application on the web only knowledge of machine learning or deep learning is not enough. You also need to know the creation of pipeline architecture and call it from the client-side, HTTP request, and many more. While doing so you might face many challenges while developing the app. This course is structured in such a way that you can able to develop the face recognition based web app from scratch.\nWhat you will learn?\nPython\nImage Processing with OpenCV\nImage Data Preprocessing\nImage Data Analysis\nEigenfaces with PCA\nFace Recognition Classification Model with Support Vector Machines\nPipeline Model\nFlask (Jinja Template, HTML, CSS, HTTP Methods)\nDevelop Face Recognition Web\nDeploy Flask App in Cloud (Heroku)\n\n\nYou will learn image processing techniques in OpenCV and the concepts behind the images. We will also do the necessary image analysis and required preprocessing steps for images.\nFor the preprocess images, we will extract features from the images, ie. computing Eigen images using principal component analysis. With Eigen images, we will train the Machine learning model and also learn to test our model before deploying, to get the best results from the model we will tune with the Grid search method for the best hyperparameters.\nOnce our machine learning model is ready, will we learn and develop a web server gateway interphase in flask by rendering HTML CSS and bootstrap in the frontend and in the backend written in Python.  Finally, we will create the project on the Face Recognition project by integrating the machine learning model to Flask App.",
      "target_audience": [
        "Any one who want to learn image processing and build data science applications",
        "Beginners on Python who want to data science project",
        "Who want to start their career in artificial intelligence and data science",
        "Data science beginner who want to build end to end data science project"
      ]
    },
    {
      "title": "The Complete Guide to Stata",
      "url": "https://www.udemy.com/course/the-complete-guide-to-stata/",
      "bio": "Master Data Analysis and Statistical Modeling Like a Professional",
      "objectives": [
        "An essential introduction to Stata",
        "Data manipulation in Stata",
        "Data analysis in Stata",
        "Regression modelling",
        "Stata code",
        "Advanced Stata code",
        "Fast, and to the point, useful tips to use in Stata",
        "Data management",
        "Programming",
        "Graphics",
        "Statistics",
        "Basic plot types",
        "Intermediate plot types",
        "Advanced plot types"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Essential Stata - Getting Started": [
          "The Stata Interface",
          "Using Help in Stata",
          "Command Syntax",
          ".do and .ado Files",
          "Log Files",
          "Importing Data"
        ],
        "Essential Stata - Exploring Data": [
          "Viewing Raw Data",
          "Describing and Summarizing",
          "Tabulating and Tables",
          "Missing Values",
          "Numerical Distributional Analysis",
          "Using Weights",
          "The New Table Command (Stata 17)"
        ],
        "Essential Stata - Manipulating Data": [
          "Recoding an Existing Variable",
          "Creating New Variables, Replacing Old Variables",
          "Naming and Labelling Variables",
          "Extensions to Generate",
          "Indicator Variables",
          "Keep and Drop Data/Variables",
          "Saving Data",
          "Converting String Data",
          "Combining Data",
          "Using Macro's and Loop's Effectively",
          "Accessing Stored Information",
          "Multiple Loops",
          "Date Variables",
          "Subscripting over Groups"
        ],
        "Essential Stata - Visualizing Data": [
          "Graphing in Stata",
          "Bar Graphs and Dot Charts",
          "Graphing Distributions",
          "Pie Charts",
          "Scatterplots and Lines of Best Fit",
          "Graphing Custom Functions",
          "Contour Plots (and Interaction Effects)",
          "Jitter Data in Scatterplots",
          "Sunflower Plots",
          "Combining Graphs",
          "Changing Graph Sizes",
          "Graphing by Groups",
          "Changing Graph Colours",
          "Adding Text to Graphs",
          "Scatterplots with Categories"
        ],
        "Essential Stata - Testing Means, Correlations and ANOVA": [
          "Association Between Two Categorical Variables",
          "Testing Means",
          "Bivariate Correlation",
          "Analysis of Variance (ANOVA)"
        ],
        "Essential Stata - Linear Regression": [
          "Ordinary Least Squares (OLS) Regression",
          "Factor Variables in OLS Regression",
          "Diagnostic Statistics for OLS Regression",
          "Log Dependent Variables and Interaction Effects in OLS Regression",
          "Hypothesis Testing in OLS Regression",
          "Presenting Estimates from OLS Regression",
          "Standardizing Regression Estimates",
          "Graphing Regression Estimates",
          "Oaxaca Decomposition Analysis",
          "Mixed Models: Random Intercepts and Random Coefficients",
          "Constrained Linear Regression"
        ],
        "Essential Stata - Categorical Choice Models": [
          "Binary Choice Models (Logit/Probit Regression)",
          "Diagnostics and Interpretation of Logit and Probit Regression",
          "Ordered and Multinomial Choice Models",
          "Fractional Logit, Beta Regression and Zero-inflated Beta Regression"
        ],
        "Essential Stata - Random Numbers and Simulation": [
          "Random Numbers",
          "Data Generating Process",
          "Simulating a Violation of Statistical Assumptions",
          "Monte Carlo Simulation"
        ],
        "Essential Stata - Count Data Models": [
          "Features of Count Data",
          "Poisson Regression",
          "Negative Binomial Regression",
          "Truncated and Censored Count Regression",
          "Hurdle Count Regression"
        ]
      },
      "requirements": [
        "There are no requirements"
      ],
      "description": "[Updated with new Audio in 2025]\nThe Complete Guide to Stata\nAre you ready to harness the full power of Stata for data analysis? Whether you’re a complete beginner or an experienced analyst looking to sharpen your skills, this course provides a thorough, hands-on introduction to Stata’s most useful features. You’ll learn to manipulate, explore, visualize, and model complex datasets—all while developing “good practice” habits that will help you code efficiently, interpret results correctly, and present your findings with confidence.\nThis course is split into three main sub-courses to guide your learning:\n\nStata Fundamentals – A step-by-step introduction to the essentials of Stata, from data loading and cleaning to basic descriptive statistics and graphing.\nStata Tips and Tricks – Explore 125 short, standalone tips to help you solve common challenges, speed up your workflow, and uncover hidden capabilities within Stata.\nAdvanced Data Visualization – Master a wide variety of visualization techniques, learning how (and when) to use each one effectively.\nI'll consistently focus on practical application - rather than diving into lengthy statistical theory - and show you how to implement and interpret commonly used statistical methods with real-world data.\nWhat You’ll Learn\n\nGetting Started: Install and navigate Stata with ease.\nData Exploration and Management: Load, view, clean, and manipulate datasets for proper analysis.\nBasic and Advanced Visualizations: Create histograms, box plots, scatter plots, violin plots, spike plots, line charts, and more—while understanding the pros and cons of each.\nStatistical Analysis:\nCorrelation and ANOVA\nRegression (including diagnostics and model building)\nHypothesis Testing\nBinary Outcome Models (Logit/Probit)\nFractional Response Models\nCategorical Choice Models (Ordered Logit/Multinomial Logit)\nSimulation Techniques\nCount Data Models (Poisson/Negative Binomial)\nSurvival Data Analysis (Parametric, Cox-Proportional Hazard, Parametric Survival Regression)\nPanel Data Analysis (including Lags, Leads, Fixed/Random Effects, Hausman Tests)\nDifference-in-Differences Analysis\nInstrumental Variable Regression (Endogenous Variables, Sample Selection, Non-Linear Endogenous Models)\nEpidemiological Tables: Cohort studies, case-control studies, and matched case-control studies.\nPower Analysis: Determine sample size, power size, and effect size.\nMatrix Operations: Operators, functions, and subscripting.\nYou’ll also get 125 “Tips and Tricks” to help you become a Stata power user, covering topics like data management, graphing, statistics, and programming. Each standalone tip takes just a couple of minutes to learn and immediately apply.\nFinally, the Advanced Data Visualization portion of the course will guide you through a diverse range of graphing techniques—from histograms and rootograms to bubble plots and mosaic plots—giving you the skills to present data in clear, compelling ways.\nPrerequisites and Target Audience\nPrerequisites: No prior experience with Stata is required. Familiarity with basic quantitative concepts is helpful but not mandatory.\nWho Should Enroll: Anyone who wants to enhance their data analytics skill set with professional Stata techniques, including students, researchers, data scientists, and analysts in business, academia, and the public sector.\nSuggested Learning Paths\nDepending on your goals, you may wish to focus on certain sections:\n\nBasic Stata Fundamentals: Sections 2, 3, 4, 5, 6, 7, 8\nAdvanced Stata Concepts: Sections 8, 9, 10, 11, 12, 13, 14, 15, 16, 17\nQuick Tips and Tricks: Sections 18, 19, 20, 21\nData Visualization: Sections 5, 21, 22, 23, 24, 25, 26\nData Management: Sections 3, 4, 18\nTake this course to experience Stata at its finest; learn the coding, analytic, and visualization skills you need to excel in modern data analysis, and gain the confidence to tackle real-life projects with competence and clarity.",
      "target_audience": [
        "Anyone wanting to work with Stata",
        "Data analysts",
        "Data scientists",
        "Quantitative degree students",
        "Quantitative business users",
        "Economists, Social Scientists, Political Scientists, Biostatisticians, and other disciplines",
        "Those wanting to skill-up in Stata"
      ]
    },
    {
      "title": "R Programming A-Z™: R For Data Science With Real Exercises!",
      "url": "https://www.udemy.com/course/r-programming/",
      "bio": "Learn Programming In R And R Studio. Data Analytics, Data Science, Statistical Analysis, Packages, Functions, GGPlot2",
      "objectives": [
        "Learn to program in R at a good level",
        "Learn how to use R Studio",
        "Learn the core principles of programming",
        "Learn how to create vectors in R",
        "Learn how to create variables",
        "Learn about integer, double, logical, character and other types in R",
        "Learn how to create a while() loop and a for() loop in R",
        "Learn how to build and use matrices in R",
        "Learn the matrix() function, learn rbind() and cbind()",
        "Learn how to install packages in R",
        "Learn how to customize R studio to suit your preferences",
        "Understand the Law of Large Numbers",
        "Understand the Normal distribution",
        "Practice working with statistical data in R",
        "Practice working with financial data in R",
        "Practice working with sports data in R"
      ],
      "course_content": {
        "Hit The Ground Running": [
          "Welcome to the R Programming Course!",
          "Installing R and R Studio (MAC & Windows)",
          "Exercise - Get Excited!",
          "Get the Datasets here",
          "Prizes $$ for Learning"
        ],
        "Core Programming Principles": [
          "Welcome to this section. This is what you will learn!",
          "Types of variables",
          "Using Variables",
          "Logical Variables and Operators",
          "The \"While\" Loop",
          "Using the console",
          "The \"For\" Loop",
          "The \"If\" statement",
          "Section Recap",
          "HOMEWORK: Law of Large Numbers",
          "Core Programming Principles"
        ],
        "Fundamentals Of R": [
          "Welcome to this section. This is what you will learn!",
          "What is a Vector?",
          "Let's create some vectors",
          "Using the [] brackets",
          "Vectorized operations",
          "The power of vectorized operations",
          "Functions in R",
          "Packages in R",
          "Section Recap",
          "HOMEWORK: Financial Statement Analysis",
          "Fundamentals of R"
        ],
        "Matrices": [
          "Welcome to this section. This is what you will learn!",
          "Project Brief: Basketball Trends",
          "Matrices",
          "Building Your First Matrix",
          "Naming Dimensions",
          "Colnames() and Rownames()",
          "Matrix Operations",
          "Visualizing With Matplot()",
          "Subsetting",
          "Visualizing Subsets",
          "Creating Your First Function",
          "Basketball Insights",
          "Section Recap",
          "HOMEWORK: Basketball Free Throws",
          "Matrices"
        ],
        "Data Frames": [
          "Welcome to this section. This is what you will learn!",
          "Project Brief: Demographic Analysis",
          "Importing data into R",
          "Exploring your dataset",
          "Using the $ sign",
          "Basic operations with a Data Frame",
          "Filtering a Data Frame",
          "Introduction to qplot",
          "Visualizing With Qplot: Part I",
          "Building Dataframes",
          "Merging Data Frames",
          "Visualizing With Qplot: Part II",
          "Section Recap",
          "HOMEWORK: World Trends",
          "Data Frames"
        ],
        "Advanced Visualization With GGPlot2": [
          "Welcome to this section. This is what you will learn!",
          "Project Brief: Movie Ratings",
          "Grammar Of Graphics - GGPlot2",
          "What is a Factor?",
          "Aesthetics",
          "Plotting With Layers",
          "Overriding Aesthetics",
          "Mapping vs Setting",
          "Histograms and Density Charts",
          "Starting Layer Tips",
          "Statistical Transformations",
          "Using Facets",
          "Coordinates",
          "Perfecting By Adding Themes",
          "Section Recap",
          "HOMEWORK: Movie Domestic % Gross",
          "Advanced Visualization With GGPlot2"
        ],
        "Homework Solutions": [
          "Homework Solution Section 2: Law Of Large Numbers",
          "Homework Solution Section 3: Financial Statement Analysis",
          "Homework Solution Section 4: Basketball Free Throws",
          "Homework Solution Section 5: World Trends",
          "Homework Solution Section 6: Movie Domestic % Gross (Part 1)",
          "Homework Solution Section 6: Movie Domestic % Gross (Part 2)",
          "THANK YOU Video"
        ],
        "Special Offer": [
          "BoxPlots"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Find Your Career Path!",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "No prior knowledge or experience needed. Only a passion to be successful!"
      ],
      "description": "Learn R Programming by doing!\nThere are lots of R courses and lectures out there. However, R has a very steep learning curve and students often get overwhelmed. This course is different!\nThis course is truly step-by-step. In every new tutorial we build on what had already learned and move one extra step forward.\nAfter every video, you learn a new valuable concept that you can apply right away. And the best part is that you learn through live examples.\nThis training is packed with real-life analytical challenges which you will learn to solve. Some of these we will solve together, some you will have as homework exercises.\nIn summary, this course has been designed for all skill levels and even if you have no programming or statistical background you will be successful in this course!\nI can't wait to see you in class,\nWhat you will learn:\nLearn how to use R Studio\nLearn the core principles of programming\nLearn how to create vectors in R\nLearn how to create variables\nLearn about integer, double, logical, character, and other types in R\nLearn how to create a while() loop and a for() loop in R\nLearn how to build and use matrices in R\nLearn the matrix() function, learn rbind() and cbind()\nLearn how to install packages in R\nSincerely,\nKirill Eremenko",
      "target_audience": [
        "This course is for you if you want to learn how to program in R",
        "This course is for you if you are tired of R courses that are too complicated",
        "This course is for you if you want to learn R by doing",
        "This course is for you if you like exciting challenges",
        "You WILL have homework in this course so you have to be prepared to work on it"
      ]
    },
    {
      "title": "LangChain MasterClass- OpenAI LLAMA 2 LLM AI Apps|| Gen AI",
      "url": "https://www.udemy.com/course/langchain-masterclass-develop-llm-apps-using-python/",
      "bio": "Discover real-world uses of LangChain, Pinecone, OpenAI, LLAMA 2 ,LLM Build AI Apps Generative AI - Hugging Face",
      "objectives": [
        "Helps you to create LLM powered applications usimg Langchain that will help you save time and increase efficiency",
        "By the end of the course , you will be capable of working on langchain LLM real time projects",
        "Explore useful concepts that can be used in your Langchain projects",
        "You will gain a solid understanding of LangChain components like LLM wrappers, prompt templates, and Memory.",
        "Additionally, we will delve into the concept of embeddings and vector data stores, learning how to utilize them effectively to enhance the performance of LangCh",
        "Project 1: Construct a question-answering application powered by LLM using LangChain, OpenAI, and Hugging Face Spaces.",
        "Project 2: Develop a conversational bot using LangChain,LLM and OpenAI.",
        "Project 3: Build an AI-powered app for kids that helps them find similar classes of things.",
        "Project 4: Create a marketing campaign app focused on increasing sales through well-crafted sales copy.",
        "Project 5: Develop a ChatGPT clone with a summarization option, offering a useful chatbot experience.",
        "Project 6 - MCQ Quiz Creator App - Helps you create MCQs for your students",
        "Project 7: CSV Data Analysis Toll - Helps you analyze your CSV file by answering your queries about its data.",
        "Project 8: Youtube Script Writing Tool - Effortlessly create compelling YouTube scripts with this user-friendly and efficient script-writing tool.",
        "Project 9 - Support Chat Bot For Your Website - Helps your visitors/customers to find the relevant data or blog links that can be useful to them.",
        "Project 10 - Automatic Ticket Classification Tool - The Automatic Ticket Classification Tool categorizes support tickets based on content to streamline ticket m",
        "Project #11 - HR - Resume Screening Assistance - HR project using AI to assist in screening resumes, optimizing the hiring process with smart analysis and reco"
      ],
      "course_content": {
        "LangChain Introduction": [
          "What You'll Get In This Course",
          "Generative AI Introduction",
          "What is LangChain?",
          "⏱️ Course RoadMap",
          "A request to you... :)❤️",
          "Let's understand the LangChain benefits",
          "Guidelines For Source Code ✅"
        ],
        "HuggingFace Introduction": [
          "HuggingFace Intro & Access Token Generation"
        ],
        "OpenAI Introduction": [
          "What is OpenAI?",
          "OpenAI API Key Generation"
        ],
        "Demo & Environment Setup": [
          "A LangChain Example - Implementation Demo",
          "Anaconda Installation"
        ],
        "Langchain - Models Module Concept": [
          "LangChain's Modules Overview"
        ],
        "✅***Beginner level***": [
          "Let's work on basic-level projects"
        ],
        "Project #1 - Simple Question & Answer App": [
          "LLMs Walkthrough & HuggingFace Token Generation",
          "LLM Practical Implementation using Python",
          "Project Environment Setup",
          "Lets' Build Simple Question Answering Application"
        ],
        "Project #2 - Simple Conversational App": [
          "Chat Model Walkthrough",
          "Chat Model Practical Implementation using Python",
          "Let's Build Simple Conversational Application"
        ],
        "Project #3 - Find Similar Things App For Kids": [
          "Text Embedding Walkthrough",
          "Text Embeddings Practical Implementation using Python",
          "Let's build Similar Words Finder Application"
        ],
        "Langchain - Prompt Module Concept & Implementation Using Python": [
          "Prompts Module Introduction",
          "Prompt Template Walkthrough",
          "Example Selectors Walkthrough",
          "Adding More Examples To Input Prompt",
          "Output Parsers Walkthrough"
        ]
      },
      "requirements": [
        "A little programming / coding knowledge is required",
        "Curiosity to learn AI field"
      ],
      "description": "Unlock the Power of AI with LangChain: Learn to Create Revolutionary Language-Based Applications\n\n\nLooking to harness the full potential of AI and revolutionize the world of language-based applications? Look no further than LangChain, the comprehensive course designed to transform you from a novice to an expert in record time.\nGen AI apps and LLM projects!\n\n\nDive into hands-on projects that will shape your expertise, including:\n\n\nProject 1: Construct a dynamic question-answering application with the unparalleled capabilities of LangChain, OpenAI, and Hugging Face Spaces,  Google Gemini Pro .\nProject 2: Develop an engaging conversational bot using LangChain and OpenAI to deliver an interactive user experience.\nProject 3: Create an AI-powered app tailored for children, facilitating the discovery of related classes of objects and fostering educational growth.\nProject 4: Build a captivating marketing campaign app that utilizes the persuasive potential of well-crafted sales copy, boosting sales and brand reach.\nProject 5: Develop a ChatGPT clone with an added summarization feature, delivering a versatile and invaluable chatbot experience.\nProject 6: MCQ Quiz Creator App - Seamlessly create multiple-choice quizzes for your students using LangChain and Pinecone.\nProject 7: CSV Data Analysis Toll - Helps you analyze your CSV file by answering your queries about its data.\nProject 8: Youtube Script Writing Tool -  Effortlessly create compelling YouTube scripts with this user-friendly and efficient script-writing tool.\nProject 9 - Support Chat Bot For Your Website - Helps your visitors/customers to find the relevant data or blog links that can be useful to them.\nProject 10 - Automatic Ticket Classification Tool - The Automatic Ticket Classification Tool categorizes support tickets based on content to streamline ticket management and response processes.\nProject 11 - HR - Resume Screening  Assistance - HR project using AI to assist in screening resumes, optimizing the hiring process with smart analysis and recommendations\nProject 12 - Email Generator using LLAMA 2- The Email Generator is a tool that automatically creates customized emails, saving time and effort in crafting personalized messages.\nProject 13 - Invoice Extraction Bot using LLAMA 2- Invoice Extraction Bot: AI-powered tool that extracts key details from invoices accurately and efficiently. Simplify your data entry process.\n\nCourse Content:\nIn this course, we will explore the capabilities of LangChain, an open-source framework that combines LLMs like GPT-4 with external computation and data sources to build scalable and performant AI applications.\nYou will gain in-depth knowledge of LangChain components, including LLM wrappers, Chains, and Agents. Additionally, we will delve into embeddings and vector databases, with a focus on Pinecone.\nThrough a learning-by-doing approach, we will collaboratively build real-world LLM applications using Python, LangChain, and OpenAI, complete with modern web app front-ends developed with Streamlit.\n\n\nCourse Highlights:\nUnlock the true potential of LangChain, LLMs, Google Gemini Pro , Chat Models, Prompts, Indexes, Data Connections, Chains, Agents, and Memory.\nHarness the power of LLMs and LangChain to develop robust applications, integrating cognitive and information sources with Pinecone.\nExplore new horizons and unleash unprecedented possibilities with LangChain and Google Gemini Pro ,  Pinecone-powered applications.",
      "target_audience": [
        "Anyone who is excited to buil;d AI powered LLM apps using Langchain",
        "Someone who is planning to start his a career as an LangChiain Professional and would like to explore the tool.",
        "AI Enthusiast"
      ]
    },
    {
      "title": "Mastering Generative AI: LLM Apps, LangChain, RAG & Chatbots",
      "url": "https://www.udemy.com/course/mastering-generative-ai/",
      "bio": "Master Generative AI with hands-on LLMs, ChatGPT, LangChain, RAG, and real-world projects.",
      "objectives": [
        "Build a solid foundation in Python programming to effectively implement AI concepts and applications.",
        "Understand the complete pipeline of Natural Language Processing, from data preprocessing to model deployment.",
        "Learn how transformer models revolutionize NLP tasks, and how to leverage them for various applications.",
        "Explore the essentials of Large Language Models (LLMs) and their applications in generative tasks.",
        "Gain hands-on experience with Retrieval-Augmented Generation (RAG) and Langchain for building advanced AI applications.",
        "Develop skills in crafting effective prompts to optimize model performance and achieve desired outputs.",
        "Learn how to utilize vector databases for efficient storage and retrieval of embeddings in AI projects."
      ],
      "course_content": {
        "Introduction to the Course": [
          "Introduction",
          "Course Resources"
        ],
        "Python begins!!": [
          "Welcome to the module on Python",
          "Introduction",
          "Datatypes & Operators",
          "Variables & Keywords"
        ],
        "Python Data Structures": [
          "Lists",
          "Tuples",
          "Sets",
          "Dictionary"
        ],
        "Python File Handling, Loops & Functions": [
          "Loops & Iterations",
          "Functions in Python",
          "Map, Reduce & Filter Functions",
          "File Handling"
        ],
        "Control Structures & OOPs": [
          "Control Structures",
          "Object Oriented Programming Concepts"
        ],
        "Python for Data Science & Analysis": [
          "Pandas",
          "Numpy"
        ],
        "Python for Data Visualization": [
          "Data Visualization",
          "Matplotlib",
          "Seaborn"
        ],
        "Introduction to NLP (Pre-Requisite)": [
          "Welcome to the module on NLP",
          "Introduction",
          "Introduction (Part II)",
          "NLP Key Challenges",
          "Linguistics"
        ],
        "NLP Basics (Pre-Requisite)": [
          "Case Folding",
          "Special Character Removal",
          "Handling Contractions",
          "Tokenization",
          "Stop Words Removal",
          "nGrams",
          "Vectorization",
          "Word Embeddings",
          "Bag of Words (Theory)",
          "Bag of Words (Practicals)",
          "TF-IDF (Theory)",
          "TF-IDF (Practicals)",
          "Part of Speech Tagging & NER",
          "Named Entity Recognition (Practicals)"
        ],
        "Word Embeddings (Pre-Requisite)": [
          "Word2Vec Intro",
          "Word2Vec Part 2",
          "Pretrained Word2Vec",
          "Word2Vec INTUITION",
          "Word2Vec 50Features",
          "Word2Vec CBOW",
          "Word2Vec SkipGrams",
          "GloVe",
          "fastText",
          "Cosine Similarity"
        ]
      },
      "requirements": [
        "Basic knowledge on Machine Learning is beneficial but not mandatory"
      ],
      "description": "Unlock the potential of Generative AI with our comprehensive course, \"Gen AI Masters 2025 - From Python To LLMs and Deployment\" This course is designed for both beginners and seasoned developers looking to deepen their understanding of the rapidly evolving field of artificial intelligence.\nLearn how to build Generative AI applications using Python and LLMs. Understand prompt engineering, explore vector databases like FAISS, and deploy real-world AI chatbots using RAG architecture.\nIn this course, you will explore a wide range of essential topics, including:\nPython Programming: Learn the fundamentals of Python, the go-to language for AI development, and become proficient in data manipulation using libraries like Pandas and NumPy.\nNatural Language Processing (NLP): Dive into the world of NLP, mastering techniques for text processing, feature extraction, and leveraging powerful libraries like NLTK and SpaCy.\nDeep Learning and Transformers: Understand the architecture of Transformer models, which are at the heart of many state-of-the-art AI applications. Discover the principles of deep learning and how to implement neural networks using TensorFlow and PyTorch.\nLarge Language Models (LLMs): Gain insights into LLMs, their training, fine-tuning processes (including PEFT, LoRA, and QLoRA), and learn how to effectively use these models in various applications, from chatbots to content generation.\nRetrieval-Augmented Generation (RAGs): Explore the innovative concept of RAG, which combines retrieval techniques with generative models to enhance AI performance. You'll also learn about RAG evaluation methods, including the RAGAS framework, BLEU, ROUGE, BARScore, and BERTScore.\nPrompt Engineering: Master the art of crafting effective prompts to improve interactions with LLMs and optimize outputs for specific tasks.\nVector Databases: Discover how to implement and utilize vector databases for storing and retrieving high-dimensional data, a crucial skill in managing AI-generated content.\nThe course culminates in a Capstone Project, where you will apply everything you've learned to solve a real-world problem using Generative AI techniques.\nProjects List:\nAI Career Coach: A personalized chatbot that guides users in career development and job search strategies using real-time data and insights.\nAI Powered Automated Claims Processing: An intelligent system that streamlines insurance claims by automating data extraction and decision-making processes.\nChat Scholar Chatbot + Essay Grading System: An interactive chatbot that assists students with writing and provides AI-driven grading and feedback on essays.\nResearch RAG Chatbot: A research assistant chatbot that retrieves relevant academic information and generates summaries based on user queries.\nSustainability Chatbot (GROK AI): An eco-focused chatbot that educates users on sustainable practices and provides actionable tips for reducing their carbon footprint.\nIf you have a specific project idea in mind, feel free to share it, and we will do our best to bring your vision to life.\nBy the end of this course, you will have a solid foundation in Generative AI and the skills to implement complex AI solutions. Whether you're looking to enhance your career, transition into AI development, or simply explore this fascinating field, this course is your gateway to mastering Generative AI.\nEnroll now and take the first step toward becoming an expert in Generative AI!",
      "target_audience": [
        "Individuals passionate about AI and ML who want to expand their knowledge and skills in generative AI applications.",
        "Professionals looking to enhance their expertise in building and deploying generative AI models",
        "Developers interested in integrating advanced AI capabilities into their applications and learning about the deployment and optimization of AI models."
      ]
    },
    {
      "title": "Artificial Intelligence for Business + ChatGPT Prize [2025]",
      "url": "https://www.udemy.com/course/ai-for-business/",
      "bio": "Solve Real World Business Problems with AI Solutions implemented in Python. Code templates included.",
      "objectives": [
        "OPTIMIZE BUSINESS PROCESSES",
        "Master the General AI Framework",
        "Implement Q-Learning",
        "Save and Load a model",
        "Build an Optimization Model",
        "Implement Early Stopping",
        "Maximize Efficiency",
        "MAXIMIZE REVENUES",
        "MINIMIZE COSTS",
        "Implement Thompson Sampling",
        "Implement Deep Q-Learning",
        "Leverage AI to make the best decision",
        "Build an AI Environment from scratch",
        "Implement Online Learning",
        "Build an Artificial Brain",
        "Implement Regret Analysis"
      ],
      "course_content": {},
      "requirements": [
        "High School Maths",
        "Basic Python Knowledge"
      ],
      "description": "Structure of the course:\n\nPart 1 - Optimizing Business Processes\nCase Study: Optimizing the Flows in an E-Commerce Warehouse\nAI Solution: Q-Learning\n\nPart 2 - Minimizing Costs\nCase Study: Minimizing the Costs in Energy Consumption of a Data Center\nAI Solution: Deep Q-Learning\n\nPart 3 - Maximizing Revenues\nCase Study: Maximizing Revenue of an Online Retail Business\nAI Solution: Thompson Sampling\n\n\nReal World Business Applications:\n\n\nWith Artificial Intelligence, you can do three main things for any business:\nOptimize Business Processes\nMinimize Costs\nMaximize Revenues\n\nWe will show you exactly how to succeed these applications, through Real World Business case studies. And for each of these applications we will build a separate AI to solve the challenge.\n\nIn Part 1 - Optimizing Processes, we will build an AI that will optimize the flows in an E-Commerce warehouse!\n\nIn Part 2 - Minimizing Costs, we will build a more advanced AI that will minimize the costs in energy consumption of a data center by more than 50%! Just as Google did last year thanks to DeepMind!\n\nIn Part 3 - Maximizing Revenues, we will build a different AI that will maximize revenue of an Online Retail Business, making it earn more than 1 Billion dollars in revenue!\n\n\nBut that's not all, this time, and for the first time, we’ve prepared a huge innovation for you. With this course, you will get an incredible extra product, highly valuable for your career:\n\"a 100-pages book covering everything about Artificial Intelligence for Business!\".\n\n\nThe Book:\n\n\nThis book includes:\n100 pages of crystal clear explanations, written in beautiful and clean latex\nAll the AI intuition and theory, including the math explained in detail\nThe three Case Studies of the course, and their solutions\nThree different AI models, including Q-Learning, Deep Q-Learning, and Thompson Sampling\nCode Templates\nHomework and their solutions for you to practice\nPlus, lots of extra techniques and tips like saving and loading models, early stopping, and much much more.\n\n\nConclusion:\n\n\nIf you want to land a top-paying job or create your very own successful business in AI, then this is the course you need.\nTake your AI career to new heights today with Artificial Intelligence for Business -- the ultimate AI course to propel your career further.",
      "target_audience": [
        "Business Driven people, who are eager to learn how to leverage AI to optimize their Business, maximize profitability and efficiency",
        "AI practitioners, who want to know what projects they can offer to their Employees",
        "Aspiring Data Scientists, looking for Business Cases to add to their Portfolio",
        "Technology Enthusiasts interested in leveraging Machine Learning and Artificial Intelligence to solve Business Problems",
        "Consultants, who want to transition companies into AI Driven Businesses"
      ]
    },
    {
      "title": "Master AI Image Generation using Stable Diffusion",
      "url": "https://www.udemy.com/course/master-ai-image-generation-using-stable-diffusion/",
      "bio": "Create stunning images using Generative Artificial Intelligence! Step by step with Stable Diffusion and Python!",
      "objectives": [
        "Understand the basic of Stable Diffusion to create new images",
        "Learn how to use Stable Diffusion parameters to get different results",
        "Create images using other models provided by the Open Source community",
        "Learn about Prompt Engineering to choose the best keywords to generate the best images",
        "How to use negative prompts to indicate what should not appear in the images",
        "Use fine-tuning to create your custom model to generate your own images",
        "Send initial images to condition image generation",
        "Use inpainting to edit images, remove unwanted elements or swap objects"
      ],
      "course_content": {},
      "requirements": [
        "Programming logic and Python basics are desirable but not required",
        "It is possible to follow the course without having technological skills"
      ],
      "description": "The generation of images using Artificial Intelligence is an area that is gaining a lot of attention, both from technology professionals and people from other areas who want to create their own custom images. The tools used for this purpose are based on advanced and modern techniques from machine learning and computer vision, which can contribute to the creation of new compositions with high graphic quality. It is possible to create new images just by sending a textual description: you ask the AI (artificial intelligence) to create an image exactly as you want! For example, you can send the text \"a cat reading a book in space\" and the AI will create an image according to that description! This technique has been gaining a lot of attention in recent years and it tends to growth in the next few years.\nThere are several available tools for this purpose and one of the most used is Stable Diffusion developed by StabilityAI. It is Open Source, has great usability, speed, and is capable of generating high quality images. As it is open source, developers have created many extensions that are capable of generating an infinite variety of images in the most different styles.\nIn this course you will learn everything you need to know to create new images using Stable Diffusion and Python programming language. See below what you will learn in this course that is divided into six parts:\n\n\nPart 1: Stable Diffusion basics: Intuition on how the technology works and how to create the first images. You will also learn about the main parameters to get different results, as well as how to create images with different styles\nPart 2: Prompt Engineering: You will learn how to send the proper texts so the AI understands exactly what you want to generate\nPart 3: Training a custom model: How about putting your own photos in the most different environments? In this section you will learn how to use your own images and generate your avatars\nPart 4: Image to image: In addition to creating images by sending texts, it is also possible to send images as a starting point for the AI to generate the images\nPart 5: Inpainting - exchaning classes: You will learn how to edit images to remove objects or swap them. For example: remove the dog and replace it with a cat\nPart 6: ControlNet: In this section you will implement digital image processing techniques (edge and pose detection) to improve the results\nAll implementations will be done step by step in Google Colab online with GPU, so you don't need a powerful computer to get amazing results in a matter of seconds! More than 50 lessons and more than 6 hours of videos!",
      "target_audience": [
        "People who want to learn how to create images using Artificial Intelligence",
        "People who want to create their own avatars",
        "Beginners in Computer Vision",
        "Undergraduate and graduate students who are taking courses on Computer Vision, Artificial Intelligence, Digital Image Processing or Computer Graphics"
      ]
    },
    {
      "title": "AI Automation: Build LLM Apps & AI-Agents with n8n & APIs",
      "url": "https://www.udemy.com/course/ai-automation-build-llm-apps-ai-agents-with-n8n-apis/",
      "bio": "Automate Everything: n8n, LLMs, OpenAI API, Claude, Ollama, MCP & RAG! Business & Private Agents; More than just ChatGPT",
      "objectives": [
        "Fundamentals of Automation, AI Agents & LLMs (ChatGPT, Claude, Gemini, Deepseek, Llama, Mistral & more)",
        "Introduction to automation & key tools (n8n, Make, Zapier, LangChain, LangGraph, Flowise)",
        "Understanding and utilizing APIs for automation",
        "OpenAI API: Pricing structure, compliant usage & project setup",
        "Function calling with LLMs: Using calendars, emails, web search, webhooks, Airtable, Google Sheets & more",
        "Everything about vector databases, embedding models & Retrieval-Augmented Generation (RAG)",
        "n8n Basics & Automation Applications",
        "Basics of n8n: Installation, importing, exporting & selling workflows",
        "Automations with Airtable, Google Sheets & Google Cloud",
        "Using simple JavaScript variables in automation",
        "Expanding AI automation with LLMs: Email automation, sentiment analysis, databases",
        "Integrating open-source LLMs (Deepseek R1, Llama, Mistral) into automation",
        "Using external LLM APIs in n8n (Deepseek API, Groq API & more)",
        "AI Agents & RAG Chatbots in Workflows",
        "Integrating AI agents & RAG chatbots into workflows",
        "Automated vector database updates with Google Drive",
        "RAG chatbot with AI agent node, embeddings & retrieval techniques",
        "AI-powered email agents for automated summaries & responses",
        "Prompt engineering: Principles, best practices & avoiding errors",
        "Hosting, Social Media & Advanced Automations",
        "n8n self-hosting with Render & other options",
        "Using AI agents in WhatsApp, Telegram & social media",
        "Web scraping & automation with sub-workflows & webhooks",
        "Debugging strategies for error-free n8n automation",
        "Connecting Flowise AI agents with webhooks & Google Sheets",
        "Extending n8n with Flowise & JavaScript custom tools",
        "Business & Market Aspects of AI Automation",
        "AI automation as a business: Selling automation & AI agents",
        "Creating market-ready RAG bots for lead generation & website integration",
        "Marketing strategies for successfully selling AI solutions",
        "Optimizing RAG chatbots: Chunk size, overlap & data quality",
        "LlamaIndex & LlamaParse for data preprocessing in Google Colab",
        "Using Firecrawl for web data extraction in Markdown format",
        "Security, privacy & ethical concerns: Jailbreaks, prompt injections & data poisoning",
        "Copyright & data protection for AI-generated data",
        "Legal frameworks: EU AI Act & more"
      ],
      "course_content": {
        "Introduction": [
          "Welcome!",
          "Course Overview",
          "Important Tips for the Course",
          "Explanation of the Links that you need in the Course",
          "Important Links",
          "Instructor Introduction: Arnold Oberleiter (Arnie)"
        ],
        "Basics – Automation, LLMs, Function Calling, Vector Databases & RAG Explained": [
          "What to Expect in This Section",
          "What Are Automations, AI Automation, and AI Agents?",
          "What Is an API (Client and Server)",
          "Tools for Automation & AI Agents: n8n, Make, Zapier, LangChain, Flowise & More",
          "What are LLMs like ChatGPT, Claude, Gemini, Llama, Deepseek, Grok etc.",
          "OpenAI API Explained: Pricing, Project Setup, Management & Compliance",
          "Test Time Compute (TTS) Explained: Thinking Models like Deepseek R1 & OpenAI o3",
          "What is Function Calling in LLMs for AI Agents and AI Automations",
          "Vector Databases, Embedding Models & Retrieval-Augmented Generation (RAG)",
          "Key Takeaways"
        ],
        "n8n Basics – Installation, Interface & First Simple Workflows": [
          "What This Section Covers",
          "Local Installation of n8n with Node.js & Interface Overview",
          "For Mac",
          "Managing Node Versions (Fixing Errors in n8n Installation)",
          "Updating n8n Locally via Node.js",
          "Testing n8n for Free Without Local Installation",
          "First Automation: Automatically Save Bookings from On Form Submit in Airtable",
          "Importing, Exporting, and Selling Workflows as JSON",
          "Automatically Backing Up Airtable Data Locally",
          "Connecting Google Sheets with n8n (Google Cloud Platform Console)",
          "Recap"
        ],
        "Expanding Automations with LLMs & AI": [
          "Overview of This Section",
          "Email Automation for Customer Bookings with OpenAI (ChatGPT) Gmail & Airtable",
          "Sentiment Analysis with LLMs & Storing Data in Airtable: OpenAI API",
          "Using Open-Source LLMs with Ollama: Deepseek R1, Llama, Mistral & More",
          "Integrating Any LLM into n8n via APIs: Deepseek API, Groq, Gemini, Claude & More",
          "Recap of Automations with LLMs in n8n"
        ],
        "AI Agents & RAG Chatbots in Your Automations & Email Automation": [
          "What to Expect in This Section",
          "RAG Agent (Part 1): Automatic Vector Database Updates with Google Drive",
          "Problems with Pinecone Embeddings",
          "RAG Chatbot (Part 2): AI Agent Node, Vector Database, Embeddings & More",
          "Build n8n Workflows with this n8n Agent!",
          "Email Agent with Sub-Workflows, Vector Database, Google Sheets & More",
          "The Fastest Way to Build an Email Agent!",
          "Automatically Summarizing All New Emails of the Day with LLMs at 7AM",
          "AI-Powered Email Automation: Filtering Messages & Auto-Replying",
          "Recap & Practical Task"
        ],
        "Prompt Engineering for AI Agents & AI Automations": [
          "Prompt Engineering for AI Agents & AI Automations (Systemprompts)",
          "Key Principles of Prompt Engineering for AI Agents & Automations (Atricle)"
        ],
        "Hosting & Tool Integration: Telegram, WhatsApp, Calendar, Scraping & More": [
          "Hosting n8n: Self-Hosting with Render & Other Options",
          "Integrating AI Agents & Automations into WhatsApp",
          "Code Snippet (Dynamic Expression) for WhatsApp as Download",
          "Using AI Agents & Sub-Workflows with Telegram Trigger Node",
          "Telegram Agent: Automating Emails, Calendars & More via Voice & Text",
          "Push the Boundaries: Big AI-Agent that can talk and automate everything",
          "IMPORTANT: Security Warning for Telegram in n8n",
          "More Practical Examples. Social Media Automation, Scraping, Crawling & More",
          "My BEST Tip for Building and Prompting AI-Agents",
          "Recap"
        ],
        "Debugging Workflows & Integrating Other Apps/APIs with HTTP Requests & Webhooks": [
          "What to Expect: Debugging & Controlling n8n from Other Apps with Webhooks",
          "Finding Errors in n8n Workflows with This Automation (Debugging n8n)",
          "Flowise AI Agent & n8n Webhook: Integrate Sheets with JavaScript & HTTP request",
          "JavaScript Code for the Flowise Custom Tool as Download (fetch for HTTP-Request)",
          "One more example for Webhooks and HTTPS request",
          "Recap of Webhooks, HTTP Requests and Error Trigger Nodes"
        ],
        "MCP QuickStart": [
          "MCP (Model Context Protocol) Explained: MCP Server & Client",
          "Clade Desktop as an AI Agent Host with MCP Server in n8n",
          "MCP Server & MCP Host in n8n"
        ],
        "Integrate Apps in Websites and Build a Business with AI Automation & AI Agents": [
          "Overview of This Section: AI Automation as a Business",
          "What AI Automations & Agents Can Be Sold?",
          "Market-Ready RAG Bot for Lead Generation (n8n, Pinecone & Google Sheets)",
          "RAG Lead Bot as a Standalone App with a Published URL",
          "Integrating RAG Bots into Websites: HTML, WordPress & Custom CSS Branding",
          "Selling Automations & AI Agents: Marketing, Offers, Price, Sales & More",
          "Web Scraping with Software – Quickly Find Many Leads",
          "Summary & Additional Tips"
        ]
      },
      "requirements": [
        "No prior knowledge required, everything is shown step by step."
      ],
      "description": "AI Automation is the Future!\nBut how does it really work? And how can AI optimize business processes—on a whole new level, far beyond ChatGPT? The answer: AI Agents.\nThis course guides you through both essential and advanced concepts in automation using AI automation, AI agents, LLMs, vector databases, Retrieval-Augmented Generation (RAG), and n8n. You'll learn how to create powerful automations, build intelligent AI agents, and seamlessly integrate them into your workflows to enhance both business and personal projects.\nAdditionally, you'll receive 29 downloadable JSON workflows to accelerate your learning and implementation.\nWhat You’ll Learn in This Course:\nFundamentals of Automation, AI Agents & LLMs\nDive into the world of AI automation:\nIntroduction to automation, AI agents & essential tools (n8n, Make, Zapier, LangChain, LangGraph, Flowise).\nUnderstanding APIs and their role in automation.\nLLMs explained: ChatGPT, Claude, Gemini, Deepseek, Llama, Mistral & more.\nOpenAI API: Pricing structure, GDPR-compliant usage & project setup.\nFunction calling with LLMs: How AI agents use tools like calendars, emails, web search, webhooks, Airtable, Google Sheets, and more.\nRAG (Retrieval-Augmented Generation): Vector databases & embeddings explained.\nn8n Basics: Installation & First Workflows\nMaster the fundamentals of n8n, the key to intelligent automation:\nLocal installation with Node.js & using the web version without installation.\nImporting, exporting, and selling workflows.\nSetting up automations with Airtable, Google Sheets & Google Cloud.\nUsing simple JavaScript variables in automation.\nExpanding AI Automation with LLMs\nBuild advanced AI-powered automations:\nEmail automation with OpenAI API, Gmail, and Airtable.\nReal-time sentiment analysis & database storage.\nIntegrating open-source LLMs (Deepseek R1, Llama, Mistral) into automation.\nUsing any LLM API in n8n (Deepseek API, Groq API & more).\nIntegrating AI Agents & RAG Chatbots into Workflows\nAutomate customer communication & data processing:\nRAG Agent: Automatically updating vector databases with Google Drive.\nRAG Chatbot using AI agent nodes, embeddings & retrieval techniques.\nAI-powered email agents for automated summaries & responses.\nPrompt Engineering for AI Agents\nOptimize your prompts for better AI responses:\nPrinciples & best practices for effective prompt engineering.\nAvoiding errors & precisely controlling AI outputs.\nHosting, Social Media & Advanced Automations\nExpand your automations with self-hosting & real-time integrations:\nn8n self-hosting with Render & other options.\nUsing AI agents in WhatsApp & Telegram.\nSocial media automation with sub-workflows, webhooks & web scraping.\nDebugging & Optimizing API Integrations\nEnhance performance & error handling in n8n workflows:\nDebugging strategies for error-free n8n automations.\nConnecting Flowise AI agents with webhooks & Google Sheets.\nExtending n8n with Flowise & JavaScript custom tools.\nMCP Quickstart: Build Smarter AI Agents with Model Context Protocol\nLearn to integrate MCP into your n8n workflows:\nWhat MCP is and how Server & Client interact\nHosting AI agents with Clade Desktop and MCP\nUsing MCP Server & Host directly inside n8n\nBuilding a Business with AI Automation & AI Agents\nLeverage your skills to create a profitable AI automation business:\nSelling automations & AI agents as services.\nDeveloping market-ready RAG bots for lead generation & website integration.\nMarketing strategies for successfully selling AI solutions.\nOptimizing RAG Chatbots: Data Quality & Chunking\nImprove AI responses with optimized data strategies:\nChunk size, overlap & data quality for better chatbot performance.\nUsing Firecrawl for web data extraction in Markdown format.\nLlamaIndex & LlamaParse for data preprocessing in Google Colab.\nSecurity, Privacy & Ethical Considerations\nProtect your AI agents & ensure GDPR compliance:\nUnderstanding & preventing jailbreaks, prompt injections & data poisoning.\nEnsuring copyright & data protection for AI-generated content.\nKey legal frameworks: EU AI Act & more\nAdditionally, you'll gain access to 29 ready-to-use JSON workflows, available for download to streamline your learning experience and accelerate implementation.\nBecome an Expert in AI Agents & Automation!\nAfter this course, you will have a deep understanding of AI automation, n8n, LLMs & RAG and be able to develop, optimize, and deploy powerful AI agents for business applications.\nSign up now and step into the future of AI automation.",
      "target_audience": [
        "For entrepreneurs who want to become more efficient, save money, or build an AI business.",
        "For anyone eager to learn something new and gain deep insights into AI automation.",
        "For individuals interested in AI and automation who want to build their own agents.",
        "For developers and data scientists who want to stay on top of GenAI, automation, AI agents, and frameworks.",
        "For anyone looking to automate tasks."
      ]
    },
    {
      "title": "An Introduction to Machine Learning for Data Engineers",
      "url": "https://www.udemy.com/course/an-introduction-to-machine-learning-for-data-engineers/",
      "bio": "A Prerequisite for Tensorflow on Google's Cloud Platform for Data Engineers",
      "objectives": [
        "You'll be familiar with many of the basic algorithms used in machine learning.",
        "You'll have solid understanding of how real world models are built using Python.",
        "You'll know exactly what machine learning is and what it isn't.",
        "You'll be prepared for the machine learning questions on the Google Certified Data Engineering Exam."
      ],
      "course_content": {
        "An Introduction": [
          "Introduction",
          "Section Contents",
          "Is this Course for You?",
          "Machine Learning Defined",
          "Machine Learning Types",
          "The Modeling Process",
          "Terminology",
          "Summary",
          "Quiz"
        ],
        "Model Building in Python": [
          "Why Applied Machine Learning is Mostly Python",
          "Creating Datalab Notebooks on Google's Cloud Platform",
          "Cloud Datalab Notebook Navigation",
          "Lab: Creating Our Datalab Virtual Machine",
          "Summary",
          "Quiz"
        ],
        "Data Wrangling": [
          "Data Massaging Introduction",
          "Lesson Speed Warning",
          "Using Pandas to Massage Data - Data Structures",
          "Using Pandas to Massage Data - Data Frame",
          "Lab: Working with Dataframes",
          "Summary",
          "Quiz"
        ],
        "Machine Learning algorithms": [
          "Linear Regression",
          "Naive Bayes",
          "Decision Trees",
          "Logistic Regression",
          "Neural Network",
          "Support Vector Machines",
          "K-Means Clustering",
          "Google Sample Questions",
          "Summary",
          "Quiz"
        ],
        "Building a Single Perceptron Model": [
          "Section Approach",
          "The Perceptron",
          "Model Building with 1 Perceptron",
          "The Perceptron Code",
          "Linear Function Code",
          "The Entire Perceptron Model",
          "Summary",
          "Quiz"
        ],
        "Neural Networks in Under Ten Minutes": [
          "Backpropagation",
          "Layers",
          "Batching",
          "Lab: A Simple Neural Network in TensorFlow",
          "Summary",
          "Quiz"
        ],
        "Testing": [
          "Gradient Descent",
          "Overfitting and How to Correct it",
          "Feature Engineering",
          "Lab: Pick the Features that Matter",
          "Feature Engineering Lab Review",
          "Summary",
          "Quiz",
          "Bonus Lecture: Free Machine Learning Content"
        ]
      },
      "requirements": [
        "You should be familiar with any programming language.",
        "A basic understanding of the concepts of machine learning will be helpful but isn't required."
      ],
      "description": "THE REVIEWS ARE IN:\nAnother Excellent course from a brilliant Instructor. Really well explained, and precisely the right amount of information. Mike provides clear and concise explanations and has a deep subject knowledge of Google's Cloud.  -- Julie Johnson\nAwesome!  -- Satendra\nGreat learning experience!! -- Lakshminarayana\nWonderful learning... -- Rajesh\nExcellent -- Dipthi\nClear and to the point. Fit's a lot of knowledge into short, easy to understand concepts/thoughts/scenarios. -- Sam\nCourse was fantastic. -- Narsh\nGreat overview of ML -- Eli\nVery helpful for beginners, All concept explained well. Overall insightful training session. Thank you ! --Vikas\nVery good training. Concepts were well explained. -- Jose\nI like the real world touch given to course material . This is extremely important. -- Soham\nLearned some new terms and stuffs in Machine Learning. Ideal for learners who needs to get some overview of ML. -- Akilan\nThis session is very good and giving more knowledge about machine learning -- Neethu\nGot to know many things on machine learning with data as a beginner. Thanks Mike. --Velumani\nReally well explained and very informative. -- Vinoth\nCOURSE INTRODUCTION:\nWelcome to An Introduction to Machine Learning for Data Engineers. This course is part of my series for data engineering. The course is a prerequisite for my course titled Tensorflow on the Google Cloud Platform for Data Engineers.\nThis course will show you the basics of machine learning for data engineers. The course is geared towards answering questions for the Google Certified Data Engineering exam.\nThis is NOT a general course or introduction to machine learning. This is a very focused course for learning the concepts you'll need to know to pass the Google Certified Data Engineering Exam.\nAt this juncture, the Google Certified Data Engineer is the only real world certification for data and machine learning engineers.\nMachine learning is a type of artificial intelligence (AI) that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The key part of that definition is “without being explicitly programmed.”\nThe vast majority of applied machine learning is supervised machine learning. The word applied means you build models in the real world. Supervised machine learning is a type of machine learning that involves building models from data that exists.\nA good way to think about supervised machine learning is:  If you can get your data into a tabular format, like that of an excel spreadsheet, then most machine learning models can model it.\nIn the course, we’ll learn the different types of algorithms used. We will also cover the nomenclature specific to machine learning. Every discipline has their own vernacular and data science is not different.\nYou’ll also learn why the Python programming language has emerged as the gold standard for building real world machine learning models.\nAdditionally, we will write a simple neural network and walk through the process and the code step by step. Understanding the code won't be as important as understanding the importance and effectiveness of one simple artificial neuron.\n*Five Reasons to take this Course.*\n1) You Want to be a Data Engineer\nIt's the number one job in the world. (not just within the computer space) The growth potential career wise is second to none. You want the freedom to move anywhere you'd like. You want to be compensated for your efforts. You want to be able to work remotely. The list of benefits goes on.\n2) The Google Certified Data Engineer\nGoogle is always ahead of the game. If you were to look back at a timeline of their accomplishments in the data space you might believe they have a crystal ball. They've been a decade ahead of everyone.  Now, they are the first and the only cloud vendor to have a data engineering certification. With their track record I'll go with Google.\n3) The Growth of Data is Insane\nNinety percent of all the world's data has been created in the last two years. Business around the world generate approximately 450 billion transactions a day. The amount of data collected by all organizations is approximately 2.5 Exabytes a day. That number doubles every month.\n4) Machine Learning in Plain English\nMachine learning is one of the hottest careers on the planet and understanding the basics is required to attaining a job as a data engineer.  Google expects data engineers to be able to build machine learning models. In this course, we will cover all the basics of machine learning at a very high level.\n5) You want to be ahead of the Curve\nThe data engineer role is fairly new.  While you’re learning, building your skills and becoming certified you are also the first to be part of this burgeoning field.  You know that the first to be certified means the first to be hired and first to receive the top compensation package.\nThanks for your interest in  An Introduction to Machine Learning for Data Engineers.",
      "target_audience": [
        "Data engineering students that need to learn the basics of machine learning for the Google Certified Data Engineering exam.",
        "Anyone interested in learning what machine learning is and why Python is the gold standard for building models."
      ]
    },
    {
      "title": "Machine Learning Deep Learning Model Deployment",
      "url": "https://www.udemy.com/course/machine-learning-deep-learning-model-deployment/",
      "bio": "Serving TensorFlow Keras PyTorch Python model Flask Serverless REST API MLOps MLflow NLP Generative AI OpenAI GPT",
      "objectives": [
        "Machine Learning Deep Learning Model Deployment techniques",
        "Simple Model building with Scikit-Learn , TensorFlow and PyTorch",
        "Deploying Machine Learning Models on cloud instances",
        "TensorFlow Serving and extracting weights from PyTorch Models",
        "Creating Serverless REST API for Machine Learning models",
        "Deploying tf-idf and text classifier models for Twitter sentiment analysis",
        "Deploying models using TensorFlow js and JavaScript",
        "Machine Learning experiment and deployment using MLflow"
      ],
      "course_content": {},
      "requirements": [
        "Prior Machine Learning and Deep Learning background required but not a must have as we are covering Model building process also"
      ],
      "description": "In this course you will learn how to deploy Machine Learning Deep Learning Models using various techniques.  This course takes you beyond model development and explains how the model can be consumed by different applications with hands-on examples\n\n\nCourse Structure:\nCreating a Classification Model using Scikit-learn\nSaving the Model and the standard Scaler\nExporting the Model to another environment - Local and Google Colab\nCreating a REST API using Python Flask and using it locally\nCreating a Machine Learning REST API on a Cloud virtual server\nCreating a Serverless Machine Learning REST API using Cloud Functions\nBuilding and Deploying TensorFlow and Keras models using TensorFlow Serving\nBuilding and Deploying  PyTorch Models\nConverting a PyTorch model to TensorFlow format using ONNX\nCreating REST API for Pytorch and TensorFlow Models\nDeploying tf-idf and text classifier models for Twitter sentiment analysis\nDeploying models using TensorFlow.js and JavaScript\nTracking Model training experiments and deployment with MLFLow\nRunning MLFlow on Colab and Databricks\nAppendix - Generative AI - Miscellaneous Topics.\nOpenAI and the history of GPT models\nCreating an OpenAI account and invoking a text-to-speech model from Python code\nInvoking OpenAI Chat Completion, Text Generation, Image Generation models from Python code\nCreating a Chatbot with OpenAI API and ChatGPT Model using Python on Google Colab\nChatGPT, Large Language Models (LLM) and prompt engineering\nPython basics and Machine Learning model building with Scikit-learn will be covered in this course.  This course is designed for beginners with no prior experience in Machine Learning and Deep Learning\n\n\nYou will also learn how to build and deploy a Neural Network using TensorFlow Keras and PyTorch. Google Cloud (GCP) free trial account is required to try out some of the labs designed for cloud environment.",
      "target_audience": [
        "Machine Learning beginners"
      ]
    },
    {
      "title": "Complete Data Science Training with Python for Data Analysis",
      "url": "https://www.udemy.com/course/complete-data-science-training-with-python-for-data-analysis/",
      "bio": "Beginners python data analytics : Data science introduction : Learn data science : Python data analysis methods tutorial",
      "objectives": [
        "Python data analytics - Install Anaconda & Work Within The iPytjhon/Jupyter Environment, A Powerful Framework For Data Science Analysis",
        "Python Data Science - Become Proficient In Using The Most Common Python Data Science Packages Including Numpy, Pandas, Scikit & Matplotlib",
        "Data analysis techniques - Be Able To Read In Data From Different Sources (Including Webpage Data) & Clean The Data",
        "Data analytics - Carry Out Data Exploratory & Pre-processing Tasks Such As Tabulation, Pivoting & Data Summarizing In Python",
        "Become Proficient In Working With Real Life Data Collected From Different Sources",
        "Carry Out Data Visualization & Understand Which Techniques To Apply When",
        "Carry Out The Most Common Statistical Data Analysis Techniques In Python Including T-Tests & Linear Regression",
        "Understand The Difference Between Machine Learning & Statistical Data Analysis",
        "Implement Different Unsupervised Learning Techniques On Real Life Data",
        "Implement Supervised Learning (Both In The Form Of Classification & Regression) Techniques On Real Data",
        "Evaluate The Accuracy & Generality Of Machine Learning Models",
        "Build Basic Neural Networks & Deep Learning Algorithms",
        "Use The Powerful H2o Framework For Implementing Deep Neural Networks"
      ],
      "course_content": {
        "Introduction to the Data Science in Python Bootcamp": [
          "What is Data Science?",
          "Introduction to the Course & Instructor",
          "Data For the Course",
          "Introduction to the Python Data Science Tool",
          "For Mac Users",
          "Introduction to the Python Data Science Environment",
          "Some Miscellaneous IPython Usage Facts",
          "Online iPython Interpreter",
          "Conclusion to Section 1"
        ],
        "Introduction to Python Pre-Requisites for Data Science": [
          "Rationale Behind This Section",
          "Different Types of Data Used in Statistical & ML Analysis",
          "Different Types of Data Used Programatically",
          "Python Data Science Packages To Be Used",
          "Conclusions to Section 2"
        ],
        "Introduction to Numpy": [
          "Numpy: Introduction",
          "Create Numpy Arrays",
          "Numpy Operations",
          "Matrix Arithmetic and Linear Systems",
          "Numpy for Basic Vector Arithmetric",
          "Numpy for Basic Matrix Arithmetic",
          "Broadcasting with Numpy",
          "Solve Equations with Numpy",
          "Numpy for Statistical Operation",
          "Conclusion to Section 3",
          "Section 3 Quiz"
        ],
        "Introduction to Pandas": [
          "Data Structures in Python",
          "Read in Data",
          "Read in CSV Data Using Pandas",
          "Read in Excel Data Using Pandas",
          "Reading in JSON Data",
          "Read in HTML Data",
          "Conclusion to Section 4"
        ],
        "Data Pre-Processing/Wrangling": [
          "Rationale behind this section",
          "Removing NAs/No Values From Our Data",
          "Basic Data Handling: Starting with Conditional Data Selection",
          "Drop Column/Row",
          "Subset and Index Data",
          "Basic Data Grouping Based on Qualitative Attributes",
          "Crosstabulation",
          "Reshaping",
          "Pivoting",
          "Rank and Sort Data",
          "Concatenate",
          "Merging and Joining Data Frames",
          "Conclusion to Section 5"
        ],
        "Introduction to Data Visualizations": [
          "What is Data Visualization?",
          "Some Theoretical Principles Behind Data Visualization",
          "Histograms-Visualize the Distribution of Continuous Numerical Variables",
          "Boxplots-Visualize the Distribution of Continuous Numerical Variables",
          "Scatter Plot-Visualize the Relationship Between 2 Continuous Variables",
          "Barplot",
          "Pie Chart",
          "Line Chart",
          "Conclusions to Section 6"
        ],
        "Statistical Data Analysis-Basic": [
          "What is Statistical Data Analysis?",
          "Some Pointers on Collecting Data for Statistical Studies",
          "Some Pointers on Exploring Quantitative Data",
          "Explore the Quantitative Data: Descriptive Statistics",
          "Grouping & Summarizing Data by Categories",
          "Visualize Descriptive Statistics-Boxplots",
          "Common Terms Relating to Descriptive Statistics",
          "Data Distribution- Normal Distribution",
          "Check for Normal Distribution",
          "Standard Normal Distribution and Z-scores",
          "Confidence Interval-Theory",
          "Confidence Interval-Calculation",
          "Conclusions to Section 7"
        ],
        "Statistical Inference & Relationship Between Variables": [
          "What is Hypothesis Testing?",
          "Test the Difference Between Two Groups",
          "Test the Difference Between More Than Two Groups",
          "Explore the Relationship Between Two Quantitative Variables",
          "Correlation Analysis",
          "Linear Regression-Theory",
          "Linear Regression-Implementation in Python",
          "Conditions of Linear Regression",
          "Conditions of Linear Regression-Check in Python",
          "Polynomial Regression",
          "GLM: Generalized Linear Model",
          "Logistic Regression",
          "Conclusions to Section 8",
          "Section 8 Quiz"
        ],
        "Machine Learning for Data Science": [
          "How is Machine Learning Different from Statistical Data Analysis?",
          "What is Machine Learning (ML) About? Some Theoretical Pointers"
        ],
        "Unsupervised Learning in Python": [
          "Unsupervised Classification- Some Basic Ideas",
          "KMeans-theory",
          "KMeans-implementation on the iris data",
          "Quantifying KMeans Clustering Performance",
          "KMeans Clustering with Real Data",
          "How Do We Select the Number of Clusters?",
          "Hierarchical Clustering-theory",
          "Hierarchical Clustering-practical",
          "Principal Component Analysis (PCA)-Theory",
          "Principal Component Analysis (PCA)-Practical Implementation",
          "Conclusions to Section 10"
        ]
      },
      "requirements": [
        "Be Able To Use PC At A Beginner Level, Including Being Able To Install Programs",
        "A Desire To Learn Data Science",
        "Prior Knowledge Of Python Will Be Useful But NOT Necessary"
      ],
      "description": "Complete Guide to Practical Data Science with Python: Learn Statistics, Visualization, Machine Learning & More\nTHIS IS A COMPLETE DATA SCIENCE TRAINING WITH PYTHON FOR DATA ANALYSIS:\nIt's A Full 12-Hour Python Data Science BootCamp To Help You Learn Statistical Modelling, Data Visualization, Machine Learning & Basic Deep Learning In Python!\nHERE IS WHY YOU SHOULD TAKE THIS COURSE:\nFirst of all, this course a complete guide to practical data science using Python...\nThat means, this course covers ALL the aspects of practical data science and if you take this course alone, you can do away with taking other courses or buying books on Python-based data science.\nIn this age of big data, companies across the globe use Python to sift through the avalanche of information at their disposal. By storing, filtering, managing, and manipulating data in Python, you can give your company a competitive edge & boost your career to the next level!\nTHIS IS MY PROMISE TO YOU:\nCOMPLETE THIS ONE COURSE & BECOME A PRO IN PRACTICAL PYTHON BASED DATA SCIENCE!\nBut, first things first, My name is MINERVA SINGH and I am an Oxford University MPhil (Geography and Environment), graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation).\nI have several years of experience in analyzing real-life data from different sources using data science-related techniques and producing publications for international peer-reviewed journals.\nOver the course of my research, I realized almost all the Python data science courses and books out there do not account for the multidimensional nature of the topic and use data science interchangeably with machine learning...\nThis gives the student an incomplete knowledge of the subject. This course will give you a robust grounding in all aspects of data science, from statistical modelling to visualization to machine learning.\nUnlike other Python instructors, I dig deep into the statistical modelling features of Python and gives you a one-of-a-kind grounding in Python Data Science!\nYou will go all the way from carrying out simple visualizations and data explorations to statistical analysis to machine learning to finally implementing simple deep learning-based models using Python\nDISCOVER 12 COMPLETE SECTIONS ADDRESSING EVERY ASPECT OF PYTHON DATA SCIENCE (INCLUDING):\n• A full introduction to Python Data Science and powerful Python driven framework for data science, Anaconda\n• Getting started with Jupyter notebooks for implementing data science techniques in Python\n• A comprehensive presentation about basic analytical tools- Numpy Arrays, Operations, Arithmetic, Equation-solving, Matrices, Vectors, Broadcasting, etc.\n• Data Structures and Reading in Pandas, including CSV, Excel, JSON, HTML data\n• How to Pre-Process and “Wrangle” your Python data by removing NAs/No data, handling conditional data, grouping by attributes, etc.\n• Creating data visualizations like histograms, boxplots, scatterplots, bar plots, pie/line charts, and more!\n• Statistical analysis, statistical inference, and the relationships between variables\n• Machine Learning, Supervised Learning, Unsupervised Learning in Python\n• You’ll even discover how to create artificial neural networks and deep learning structures...& MUCH MORE!\nWith this course, you’ll have the keys to the entire Python Data Science kingdom!\nNO PRIOR PYTHON OR STATISTICS/MACHINE LEARNING KNOWLEDGE IS REQUIRED:\nYou’ll start by absorbing the most valuable Python Data Science basics and techniques...\nI use easy-to-understand, hands-on methods to simplify and address even the most difficult concepts in Python.\nMy course will help you implement the methods using real data obtained from different sources. Many courses use made-up data that does not empower students to implement Python-based data science in real life.\nAfter taking this course, you’ll easily use packages like Numpy, Pandas, and Matplotlib to work with real data in Python.\nYou’ll even understand deep concepts like statistical modelling in Python’s Statsmodels package and the difference between statistics and machine learning (including hands-on techniques).\nI will even introduce you to deep learning and neural networks using the powerful H2o framework!\nWith this Powerful All-In-One Python Data Science course, you’ll know it all: visualization, stats, machine learning, data mining, and deep learning!\nThe underlying motivation for the course is to ensure you can apply Python-based data science on real data and put into practice today. Start analyzing data for your own projects, whatever your skill level and IMPRESS your potential employers with actual examples of your data science abilities.\nHERE IS WHAT THIS COURSE WILL DO FOR YOU:\nThis course is your one shot way of acquiring the knowledge of statistical data analysis skills that I acquired from the rigorous training received at two of the best universities in the world, a perusal of numerous books and publishing statistically rich papers in renowned international journal like PLOS One.\nThis course will:\n(a) Take students without a prior Python and/or statistics background from a basic level to performing some of the most common advanced data science techniques using the powerful Python-based Jupyter notebooks.\n(b) Equip students to use Python for performing different statistical data analysis and visualization tasks for data modelling.\n(c) Introduce some of the most important statistical and machine learning concepts to students in a practical manner such that students can apply these concepts for practical data analysis and interpretation.\n(d) Students will get a strong background in some of the most important data science techniques.\n(e) Students will be able to decide which data science techniques are best suited to answer their research questions and applicable to their data and interpret the results.\nIt is a practical, hands-on course, i.e. we will spend some time dealing with some of the theoretical concepts related to data science. However, the majority of the course will focus on implementing different techniques on real data and interpret the results. After each video, you will learn a new concept or technique which you may apply to your own projects.\nJOIN THE COURSE NOW!\n\n\n#data #analysis #python #anaconda #analytics",
      "target_audience": [
        "Anyone Who Wishes To Learn Practical Data Science Using Python",
        "Anyone Interested In Learning How To Implement Machine Learning Algorithms Using Python",
        "People Looking To Get Started In Deep Learning Using Python",
        "People Looking To Work With Real Life Data In Python",
        "Anyone With A Prior Knowledge Of Python Looking To Branch Out Into Data Analysis",
        "Anyone Looking To Become Proficient In Exploratory Data Analysis, Statistical Modelling & Visualizations Using iPython"
      ]
    },
    {
      "title": "The Complete Healthcare Artificial Intelligence Course 2024",
      "url": "https://www.udemy.com/course/the-complete-healthcare-artificial-intelligence-course-2021/",
      "bio": "Creating powerful AI model for Real-World Healthcare applications with Data Science, Machine Learning and Deep Learning",
      "objectives": [
        "Pandas.",
        "Matplotlib.",
        "Sigmoid activation function.",
        "Tanh activation function.",
        "ReLU activation function.",
        "Leaky Relu activation function.",
        "Exponential Linear Unit activation function.",
        "Swish activation function.",
        "Markov models.",
        "Support Vector Machines",
        "Other common classifiers",
        "Import data from the UCI repository.",
        "Convert text input to numerical data.",
        "Build and train classification algorithms.",
        "Compare and contrast classification machine learning.",
        "Building the AI.",
        "Machine learning and deep learning model based on the given data with high accuracy.",
        "RF with Response Coding.",
        "Maximum voting Classifier.",
        "Stacking model.",
        "Random Forest Classifier.",
        "One-hot Encoding.",
        "NLP (Natural Language Processing)",
        "NLTK (Natural Language Toolkit)",
        "Logistic Regression.",
        "Naive Bayes",
        "Response Encoding",
        "Linear Support Vector Machines",
        "Geolocation Features.",
        "Handling Missing Data And Anomalies in Python.",
        "Data standardization.",
        "Temporal Features.",
        "Seaborn",
        "Deep Learning.",
        "Keras.",
        "Google Colab .",
        "Anaconda.",
        "Jupiter Notebook."
      ],
      "course_content": {
        "Introduction (UPDATED 2024)": [
          "Course structure",
          "How to make the most out of this course",
          "Introduction to AI in healthcare",
          "Basic concept of machine learning (Updated on 2025)",
          "Basic of supervised learning, unsupervised and reinforcement learning",
          "What is sklearn? (Updated on 2025)",
          "What is pandas (Updated on 2025)",
          "What is matplotlib? (Updated on 2025)",
          "What is standardization? (Updated on 2025)",
          "Introduction to numpy"
        ],
        "Implementation of breast cancer detection (UPDATED 2024)": [
          "What is breast cancer?",
          "What is classification report?",
          "What is logistic regression (Updated 2025)",
          "What is confusion matrix",
          "what is mplot3d",
          "what is mpatches",
          "what is train test split?",
          "Implementation of breast cancer detection Part 1",
          "Implementation of breast cancer detection Final Part"
        ],
        "Implementation of diabetes detection (UPDATED 2024)": [
          "What is diabetes",
          "What is artificial neural network?",
          "What is perceptron?",
          "what does pre-processing mean?",
          "How to handle missing values",
          "What is seaborn?",
          "Implementation of diabetes detection Part 1",
          "Implementation of diabetes detection Part 2",
          "Implementation of diabetes detection Part 3",
          "Implementation of diabetes detection Part 4",
          "Implementation of diabetes detection Part 5",
          "Implementation of diabetes detection Part 6",
          "Implementation of diabetes detection final Part"
        ],
        "Implementation of DNA classification and Heart disease detection (UPDATED 2024)": [
          "Introduction to DNA sequences",
          "Introduction to DNA classification",
          "What is Decision Tree?",
          "What is support vector machine?",
          "What is RandomForestClassifier",
          "What is Radial Basis Function",
          "What is K-nearest Neighbors",
          "What is AdaBoostClassifier",
          "What is MLPClassifier?",
          "Implementation of DNA classification Part 1",
          "Implementation of DNA classification Part 2",
          "Implementation of DNA classification Final Part",
          "What is Heart disease?",
          "Implementation of Heart disease detection Part 1",
          "Implementation of Heart Disease Final Part"
        ],
        "Discharge statuses detection for emergency department patients (UPDATED 2024)": [
          "What is ED (Emergency department ) patients?",
          "what is discharge statuses for ED patients?",
          "What is one-hot-encoding",
          "Discharge statuses detection for ED patients implementation Part 1",
          "Discharge statuses detection for ED patients implementation Part 2",
          "Discharge statuses detection for ED patients implementation Part 3",
          "Discharge statuses detection for ED patients implementation Part 4",
          "Discharge statuses detection for ED patients implementation Part 5",
          "Discharge statuses detection for ED patients implementation Part 6",
          "Discharge statuses detection for ED patients implementation Part 7"
        ],
        "Introduction (OLD Content)": [
          "Course Structure",
          "How To Make The Most Out Of This Course",
          "AI in Healthcare",
          "What is Neuron",
          "What is Deep Learning",
          "What is ANN",
          "What is keras",
          "Introduction to Pandas Part 1",
          "Introduction to Pandas Part 2",
          "Data Visualization with Pandas",
          "Data Preprocessing by Pandas",
          "How to install Anaconda",
          "Important terms in Neural Network"
        ],
        "Activation function (OLD Content)": [
          "What is activation function",
          "What is sigmoid function",
          "What is tanh function",
          "What is Rectified Linear Unit function",
          "What is Leaky ReLU function",
          "What is The Exponential Linear Unit Function",
          "What is The Swish function",
          "What is The softmax function",
          "Time to code all the activation functions"
        ],
        "DNA Classification Project (OLD Content)": [
          "Introduction to DNA Classifier",
          "Importing library and data",
          "Showing data",
          "Generating a DNA sequence",
          "Splitting the dataset into training test and test set",
          "Scoring method and results",
          "Summary of the project"
        ],
        "Heart Disease Classification Project (OLD Content)": [
          "Introduction to the project",
          "Important Parameters",
          "Objective of this project",
          "Importing library and data",
          "Exploratory analysis",
          "Handling missing data in Python",
          "Data scaling",
          "Data visualization",
          "Splitting training set into test set and Evaluating the model",
          "Summary of the project"
        ],
        "Diagnosing Coronary Artery Disease Project (OLD Content)": [
          "Introduction",
          "Importing data and Analysing data",
          "Fixing missing data",
          "Splitting the dataset into training test and test set",
          "Training Neural Network",
          "A comparison of categorical and binary problem",
          "Summary of the project"
        ]
      },
      "requirements": [
        "There will be no Prerequisites.",
        "Basic knowledge of Python will be good.",
        "But everything will be taught from the round up."
      ],
      "description": "Interested in the field of Machine Learning, Deep Learning and Artificial Intelligence? Then this course is for you!\nThis course has been designed by a software engineer. I hope with my experience and knowledge I did gain throughout years, I can share my knowledge and help you learn complex theory, algorithms, and coding libraries in a simple way.\nI will walk you step-by-step into the Machine Learning, Artificial Intelligence and Deep Learning. With every tutorial, you will develop new skills and improve your understanding of this challenging yet lucrative sub-field of Data Science.\nThis course is fun and exciting, but at the same time, we dive deep into Machine Learning, Deep Learning and Artificial Intelligence . Throughout the brand new version of the course we cover tons of tools and technologies including:\nDeep Learning.\nGoogle Colab\nAnaconda\nJupiter Notebook\nArtificial Intelligent In Healthcare.\nArtificial Neural Network.\nNeuron.\nActivation Function.\nKeras.\nPandas.\nSeaborn.\nFeature scaling.\nMatplotlib.\nGenerating a DNA Sequence.\nData Pre-processing.\nSigmoid Function.\nTanh Function.\nReLU Function.\nLeaky Relu Function.\nExponential Linear Unit Function.\nSwish function.\nMarkov Models.\nK-Nearest Neighbors Algorithms (KNN).\nSupport Vector Machines (SVM).\nImporting library and data.\nDeep feedforward networks.\nAnalysing Data.\nExploratory Analysis.\nHandling Missing Data And Anomalies in Python.\nData standardization.\nTemporal Features.\nGeolocation Features.\nData Scaling.\nData Visualization.\nVisualizing Geolocation Data.\nUnderstanding Machine Learning Algorithm.\nSplitting Data into Training Set and Test Set.\nTraining Neural Network.\nModel building.\nAnalysing Results.\nModel compilation.\nA Comparison Of Categorical And Binary Problem.\nMake a Prediction.\nTesting Accuracy.\nConfusion Matrix.\nROC Curve.\nOne-hot Encoding.\nNLP (Natural Language Processing).\nNLTK (Natural Language Toolkit).\nLogistic Regression.\nNaive Bayes.\nResponse Encoding.\nLinear Support Vector Machines.\nRF with Response Coding.\nRandom Forest Classifier.\nStacking model.\nMaximum voting Classifier.\nMoreover, the course is packed with practical exercises that are based on real-life examples. So not only will you learn the theory, but you will also get some hands-on practice building your own models. There are five big projects on healthcare problems and one small project to practice. These projects are listed below:\nPredicting Taxi Fares in New York City\nDNA Classification Project.\nHeart Disease Classification Project.\nDiagnosing Coronary Artery Disease Project.\nBreast Cancer Detection Project.\nPredicting Diabetes with Multilayer Perceptrons Project.\nIris Flower.\nMedical Treatment Project.",
      "target_audience": [
        "Anyone interested in Machine Learning.",
        "Students who have at least high school knowledge in math and who want to start learning Machine Learning, Deep Learning, and Artificial Intelligence",
        "Any intermediate level people who know the basics of machine learning, including the classical algorithms like linear regression or logistic regression, but who want to learn more about it and explore all the different fields of Machine Learning, Deep Learning, Artificial Intelligence.",
        "Any people who are not that comfortable with coding but who are interested in Machine Learning, Deep Learning, Artificial Intelligence and want to apply it easily on datasets.",
        "Any students in college who want to start a career in Data Science",
        "Any data analysts who want to level up in Machine Learning, Deep Learning and Artificial Intelligence.",
        "Any people who are not satisfied with their job and who want to become a Data Scientist.",
        "Any people who want to create added value to their business by using powerful Machine Learning, Artificial Intelligence and Deep Learning tools. Any people who want to work in a Car company as a Data Scientist, Machine Learning, Deep Learning and Artificial Intelligence engineer.",
        "Any people who want to create added value to the local hospital by using powerful Machine Learning, Artificial Intelligence and Deep Learning tools.",
        "Any people who want to work in healthcare field as a Data Scientist, Machine Learning, Deep Learning and Artificial Intelligence engineer.",
        "Any people who want to work in a Taxi Company as a Data Scientist, Machine Learning, Deep Learning and Artificial Intelligence engineer."
      ]
    },
    {
      "title": "Artificial Intelligence II - Hands-On Neural Networks (Java)",
      "url": "https://www.udemy.com/course/neural-networks-from-scratch-in-java/",
      "bio": "Neural networks, gradient descent and backpropagation algorithms explained step by step",
      "objectives": [
        "Basics of neural networks",
        "Hopfield networks",
        "Concrete implementation of neural networks",
        "Backpropagation",
        "Optical character recognition"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Artificial Intelligence Basics": [
          "Why to learn artificial intelligence and machine learning?",
          "Types of artificial intelligence learning methods"
        ],
        "Neural Networks With Backpropagation Theory": [
          "What are feed-forward neural networks?",
          "Artificial neural networks - the model",
          "Why to use activation functions?",
          "Neural networks - the big picture",
          "Using bias nodes in the neural network",
          "How to measure the error of the network?",
          "Optimization with gradient descent",
          "Gradient descent with backpropagation",
          "Backpropagation explained",
          "Mathematical formulation of feed-forward neural networks",
          "Neural Networks Quiz"
        ],
        "Single Perceptron Model": [
          "Perceptron model training",
          "Perceptron model implementation I",
          "Perceptron model implementation II",
          "Perceptron model implementation III",
          "Trying to solve XOR problem",
          "Conclusion: linearity and hidden layers"
        ],
        "Backpropagation Implementation": [
          "Structure of the feedforward network",
          "Backpropagation implementation I - activation function",
          "Backpropagation implementation II - NeuralNetwork",
          "Backpropagation implementation III - Layer",
          "Backpropagation implementation IV - run",
          "Backpropagation implementation V - train"
        ],
        "Logical Operators": [
          "Logical operators introduction",
          "Running the neural network: AND",
          "Running the neural network: OR",
          "Running the neural network: XOR"
        ],
        "Clustering": [
          "Clustering with neural networks I",
          "Clustering with neural networks II"
        ],
        "Classification - Iris Dataset": [
          "About the Iris dataset",
          "Constructing the neural network",
          "Testing the neural network",
          "Calculating the accuracy of the model"
        ],
        "Optical Character Recognition (OCR)": [
          "Optical character recognition theory",
          "Installing paint.net",
          "Transform an image into numerical data",
          "Creating the datasets",
          "OCR with neural network"
        ],
        "Course Materials (DOWNLOADS)": [
          "Course materials"
        ]
      },
      "requirements": [
        "Basic Java"
      ],
      "description": "This course is about artificial neural networks. Artificial intelligence and machine learning are getting more and more popular nowadays. In the beginning, other techniques such as Support Vector Machines outperformed neural networks, but in the 21th century neural networks again gain popularity. In spite of the slow training procedure, neural networks can be very powerful. Applications ranges from regression problems to optical character recognition and face detection.\nSection 1:\nwhat are neural networks\nmodeling the human brain\nthe big picture\nSection 2:\nwhat is back-propagation\nfeedforward neural networks\noptimizing the cost function\nerror calculation\nbackpropagation and gradient descent\nSection 3:\nthe single perceptron model\nsolving linear classification problems\nlogical operators (AND and XOR operation)\nSection 4:\napplications of neural networks\nclustering\nclassification (Iris-dataset)\noptical character recognition (OCR)\nsmile-detector application from scratch\nIn the first part of the course you will learn about the theoretical background of neural networks, later you will learn how to implement them.\nIf you are keen on learning methods, let's get started!\nIn the first part of the course you will learn about the theoretical background of neural networks, later you will learn how to implement them.\nIf you are keen on learning methods, let's get started!",
      "target_audience": [
        "This course is recommended for students who are interested in artificial intelligence focusing on neural networks"
      ]
    },
    {
      "title": "Applied Time Series Analysis in Python",
      "url": "https://www.udemy.com/course/applied-time-series-analysis-in-python/",
      "bio": "Use Python and Tensorflow to apply the latest statistical and deep learning techniques for time series analysis",
      "objectives": [
        "Descriptive vs inferential statistics",
        "Random walk model",
        "Moving average model",
        "Autoregression",
        "ACF and PACF",
        "Stationarity",
        "ARIMA, SARIMA, SARIMAX",
        "VAR, VARMA, VARMAX",
        "Apply deep learning for time series analysis with Tensorflow",
        "Linear models, DNN, LSTM, CNN, ResNet",
        "Automate time series analysis with Prophet"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "What are Time Series?"
        ],
        "Statistical Learning Approach: The Building Blocks": [
          "Basic Statistics",
          "Setup for coding exercises",
          "Coding Exercise: Descriptive and Inferential Statistics",
          "Autocorrelation and White Noise",
          "Stationarity and Differencing"
        ],
        "Statistical Learning Approach: Basic Models": [
          "Random Walk",
          "Coding Excercise: Random Walk",
          "Moving Average Model",
          "Coding Exercise: Moving Average Model",
          "Autoregressive Model",
          "Mini Project: Autoregressive Model",
          "ARMA: Autoregressive Moving Average Model'",
          "Coding Exercise: ARMA"
        ],
        "Statistical Learning Approach: Advanced Models": [
          "ARIMA: Autoregressive Integrated Moving Average Model",
          "Project 1: ARIMA",
          "SARIMA",
          "Project 2: SARIMA",
          "AIC: Akaike Information Criterion",
          "SARIMAX",
          "Project 3: SARIMAX",
          "General Modelling Procedure",
          "VAR: Vector Autoregressions",
          "Project 4 - Part 1: VAR",
          "Project 4 - Part 2: VARMA",
          "Project 4 - Part 3: VARMAX"
        ],
        "Deep Learning Approach: Theory": [
          "Introduction",
          "Deep Neural Networks (DNN)",
          "Recurrent Neural Network and Long Short-Term Memory (RNN and LSTM)",
          "Convolutional Neural Networks (CNN)"
        ],
        "Deep Learning Approach: End-to-end Project": [
          "Project 5 - Part 1: Initial setup",
          "Project 5 - Part 2: Exploratory Data Analysis (EDA)",
          "Project 5 - Part 3: Feature Engineering",
          "Project 5 - Part 4: Data Windowing and Training Function",
          "Project 5 - Part 5: Single Step Models",
          "Project 5 - Part 6: Multi Output Models",
          "Project 5 - Part 7: Multi Step Models"
        ],
        "Conclusion and References": [
          "Congratulations and Thank You!",
          "References"
        ],
        "Bonus: Automated Time Series Analysis with Prophet": [
          "Introduction to Prophet",
          "Working with Prophet",
          "Project: Predict Bus Ridership with Prophet"
        ]
      },
      "requirements": [
        "Basic knowledge of Python",
        "Basic knowledge of deep learning",
        "Jupyter notebook installed (or access to Google Colab)"
      ],
      "description": "This is the only course that combines the latest statistical and deep learning techniques for time series analysis. First, the course covers the basic concepts of time series:\nstationarity and augmented Dicker-Fuller test\nseasonality\nwhite noise\nrandom walk\nautoregression\nmoving average\nACF and PACF,\nModel selection with AIC (Akaike's Information Criterion)\nThen, we move on and apply more complex statistical models for time series forecasting:\nARIMA (Autoregressive Integrated Moving Average model)\nSARIMA (Seasonal Autoregressive Integrated Moving Average model)\nSARIMAX (Seasonal Autoregressive Integrated Moving Average model with exogenous variables)\nWe also cover multiple time series forecasting with:\nVAR (Vector Autoregression)\nVARMA (Vector Autoregressive Moving Average model)\nVARMAX (Vector Autoregressive Moving Average model with exogenous variable)\nThen, we move on to the deep learning section, where we will use Tensorflow to apply different deep learning techniques for times series analysis:\nSimple linear model (1 layer neural network)\nDNN (Deep Neural Network)\nCNN (Convolutional Neural Network)\nLSTM (Long Short-Term Memory)\nCNN + LSTM models\nResNet (Residual Networks)\nAutoregressive LSTM\nThroughout the course, you will complete more than 5 end-to-end projects in Python, with all source code available to you.",
      "target_audience": [
        "Beginner data scientists looking to gain experience with time series",
        "Deep learning beginners curious about times series",
        "Professional data scientists who need to analyze time series",
        "Data scientists looking to transition from R to Python"
      ]
    },
    {
      "title": "Modern Computer Vision GPT, PyTorch, Keras, OpenCV4 in 2024!",
      "url": "https://www.udemy.com/course/modern-computer-vision/",
      "bio": "Next-Gen Computer Vision: YOLOv8, DINO-GPT4V, OpenCV4, Face Recognition, GenerativeAI, Diffusion Models & Transformers",
      "objectives": [
        "All major Computer Vision theory and concepts (updated in late 2023!)",
        "Learn to use PyTorch, TensorFlow 2.0 and Keras for Computer Vision Deep Learning tasks",
        "YOLOv8: Cutting-edge Object Recognition",
        "DINO-GPT4V: Next-Gen Vision Models",
        "Learn all major Object Detection Frameworks from YOLOv8, R-CNNs, Detectron2, SSDs, EfficientDetect and more!",
        "Deep Segmentation with Segment Anything, U-Net, SegNet and DeepLabV3",
        "Understand what CNNs 'see' by Visualizing Different Activations and applying GradCAM",
        "Generative Adverserial Networks (GANs) & Autoencoders - Generate Digits, Anime Characters, Transform Styles and implement Super Resolution",
        "Training, fine tuning and analyzing your very own Classifiers",
        "Facial Recognition along with Gender, Age, Emotion and Ethnicity Detection",
        "Neural Style Transfer and Google Deep Dream",
        "Transfer Learning, Fine Tuning and Advanced CNN Techniques",
        "Important Modern CNNs designs like ResNets, InceptionV3, DenseNet, MobileNet, EffiicentNet and much more!",
        "Tracking with DeepSORT",
        "Siamese Networks, Facial Recognition and Analysis (Age, Gender, Emotion and Ethnicity)",
        "Image Captioning, Depth Estimination and Vision Transformers",
        "Point Cloud (3D data) Classification and Segmentation",
        "Making a Computer Vision API and Web App using Flask",
        "OpenCV4 in detail, covering all major concepts with lots of example code",
        "All Course Code works in accompanying Google Colab Python Notebooks",
        "Meta CLIP for Enhanced Image Analysis"
      ],
      "course_content": {
        "Introduction": [
          "Course Introduction",
          "Course Overview",
          "What Makes Computer Vision Hard",
          "What are Images?"
        ],
        "Download Code and Setup Colab": [
          "Download Course Resources",
          "Setup - Download Code and Configure Colab"
        ],
        "OpenCV - Image Operations": [
          "Getting Started with OpenCV4",
          "Grayscaling Images",
          "Colour Spaces - RGB and HSV",
          "Drawing on Images",
          "Transformations - Translations and Rotations",
          "Scaling, Re-sizing, Interpolations and Cropping",
          "Arithmetic and Bitwise Operations",
          "Convolutions, Blurring and Sharpening Images",
          "Thresholding, Binarization & Adaptive Thresholding",
          "Dilation, Erosion and Edge Detection"
        ],
        "OpenCV - Image Segmentation": [
          "Contours - Drawing, Hierarchy and Modes",
          "Moments, Sorting, Approximating and Matching Contours",
          "Line, Circle and Blob Detection",
          "Counting Circles, Ellipses and Finding Waldo with Template Matching",
          "Finding Corners"
        ],
        "OpenCV - Haar Cascade Classifiers": [
          "Face and Eye Detection with Haar Cascade Classifiers",
          "Vehicle and Pedestrian Detection"
        ],
        "OpenCV - Image Analysis and Transformation": [
          "Perspective Transforms",
          "Histograms and K-Means Clustering for Dominant Colors",
          "Comparing Images MSE and Structual Similarity",
          "Filtering on Colour",
          "Watershed Algorithm Marker-Dased Image Segmentation",
          "Background and Foreground Subtraction"
        ],
        "OpenCV - Motion and Object Tracking": [
          "Motion Tracking with Mean Shift and CAMSHIFT",
          "Object Tracking with Optical Flow",
          "Simple Object Tracking by Color"
        ],
        "OpenCV - Facial Landmark Detection & Face Swaps": [
          "Facial Landmark Detection with Dlib",
          "Face Swapping with Dlib"
        ],
        "OpenCV Projects": [
          "Tilt Shift Effects",
          "GrabCut Algorithm for Background Removal",
          "OCR with PyTesseract and EasyOCR (Text Detection)",
          "Barcode, QR Generation and Reading",
          "YOLOv3 in OpenCV",
          "Neural Style Transfer with OpenCV",
          "SSDs in OpenCV",
          "Colorize Black and White Photos using a Caffe Model in OpenCV",
          "Inpainting to Restore Damaged Photos",
          "Add and Remove Noise and Fix Contrast with Histogram Equalization",
          "Detect Blur in Images",
          "Facial Recognition"
        ],
        "OpenCV - Working With Video": [
          "Using Your Webcam and Creating a Live Sketch of Yourself",
          "Opening Video Files in OpenCV",
          "Saving or Recording Videos in OpenCV",
          "Video Streams and CCTV - RTSP and IP",
          "Auto Reconnect to Video Streams",
          "Capturing Video using Screenshots",
          "Importing YouTube Videos into OpenCV"
        ]
      },
      "requirements": [
        "No programming experience (some Python would be beneficial)",
        "Basic highschool mathematics",
        "A broadband internet connection"
      ],
      "description": "Welcome to Modern Computer Vision Tensorflow, Keras & PyTorch!\nAI and Deep Learning are transforming industries and one of the most intriguing parts of this AI revolution is in Computer Vision!\n\n\nUpdate for 2024: Modern Computer Vision Course\nWe're excited to bring you the latest updates for our 2024 modern computer vision course. Dive into an enriched curriculum covering the most advanced and relevant topics in the field:\nYOLOv8: Cutting-edge Object Recognition\nDINO-GPT4V: Next-Gen Vision Models\nMeta CLIP for Enhanced Image Analysis\nDetectron2 for Object Detection\nSegment Anything\nFace Recognition Technologies\nGenerative AI Networks for Creative Imaging\nTransformers in Computer Vision\nDeploying & Productionizing Vision Models\nDiffusion Models for Image Processing\nImage Generation and Its Applications\nAnnotation Strategy for Efficient Learning\nRetrieval Augmented Generation (RAG)\nZero-Shot Classifiers for Versatile Applications\nUsing Roboflow: Streamlining Vision Workflows\n\n\nWhat is Computer Vision?\nBut what exactly is Computer Vision and why is it so exciting? Well, what if Computers could understand what they’re seeing through cameras or in images? The applications for such technology are endless from medical imaging, military, self-driving cars, security monitoring, analysis, safety, farming, industry, and manufacturing! The list is endless.\nJob demand for Computer Vision workers are skyrocketing and it’s common that experts in the field are making USD $200,000 and more salaries. However, getting started in this field isn’t easy. There’s an overload of information, many of which is outdated, and a plethora of tutorials that neglect to teach the foundations. Beginners thus have no idea where to start.\n\n\n\n\nThis course aims to solve all of that!\n\n\nTaught using Google Colab Notebooks (no messy installs, all code works straight away)\n27+ Hours of up-to-date and relevant Computer Vision theory with example code\nTaught using both PyTorch and Tensorflow Keras!\nIn this course, you will learn the essential very foundations of Computer Vision, Classical Computer Vision (using OpenCV) I then move on to Deep Learning where we build our foundational knowledge of CNNs and learn all about the following topics:\nComputer vision applications involving Deep Learning are booming!\n\n\nHaving Machines that can see will change our world and revolutionize almost every industry out there. Machines or robots that can see will be able to:\n\n\nPerform surgery and accurately analyze and diagnose you from medical scans.\nEnable self-driving cars\nRadically change robots allowing us to build robots that can cook, clean, and assist us with almost any task\nUnderstand what's being seen in CCTV surveillance videos thus performing security, traffic management, and a host of other services\nCreate Art with amazing Neural Style Transfers and other innovative types of image generation\nSimulate many tasks such as Aging faces, modifying live video feeds, and realistically replacing actors in films\n\n\nDetailed OpenCV Guide covering:\nImage Operations and Manipulations\nContours and Segmentation\nSimple Object Detection and Tracking\nFacial Landmarks, Recognition and Face Swaps\nOpenCV implementations of Neural Style Transfer, YOLOv3, SSDs and a black and white image colorizer\nWorking with Video and Video Streams\nOur Comprehensive Deep Learning Syllabus includes:\nClassification with CNNs\nDetailed overview of CNN Analysis, Visualizing performance, Advanced CNNs techniques\nTransfer Learning and Fine Tuning\nGenerative Adversarial Networks - CycleGAN, ArcaneGAN, SuperResolution, StyleGAN\nAutoencoders\nNeural Style Transfer and Google DeepDream\nModern CNN Architectures including Vision Transformers (ResNets, DenseNets, MobileNET, VGG19, InceptionV3, EfficientNET and ViTs)\nSiamese Networks for image similarity\nFacial Recognition (Age, Gender, Emotion, Ethnicity)\nPyTorch Lightning\nObject Detection with YOLOv5 and v4, EfficientDetect, SSDs, Faster R-CNNs,\nDeep Segmentation - MaskCNN, U-NET, SegNET, and DeepLabV3\nTracking with DeepSORT\nDeep Fake Generation\nVideo Classification\nOptical Character Recognition (OCR)\nImage Captioning\n3D Computer Vision using Point Cloud Data\nMedical Imaging - X-Ray analysis and CT-Scans\nDepth Estimation\nMaking a Computer Vision API with Flask\nAnd so much more\nThis is a comprehensive course, is broken up into two (2) main sections. This first is a detailed OpenCV (Classical Computer Vision tutorial) and the second is a detailed Deep Learning\nThis course is filled with fun and cool projects including these Classical Computer Vision Projects:\nSorting contours by size, location, using them for shape matching\nFinding Waldo\nPerspective Transforms (CamScanner)\nImage Similarity\nK-Means clustering for image colors\nMotion tracking with MeanShift and CAMShift\nOptical Flow\nFacial Landmark Detection with Dlib\nFace Swaps\nQR Code and Barcode Reaching\nBackground removal\nText Detection\nOCR with PyTesseract and EasyOCR\nColourize Black and White Photos\nComputational Photography with inpainting and Noise Removal\nCreate a Sketch of yourself using Edge Detection\nRTSP and IP Streams\nCapturing Screenshots as video\nImport Youtube videos directly\n\n\nDeep Learning Computer Vision Projects:\nPyTorch & Keras CNN Tutorial MNIST\nPyTorch & Keras Misclassifications and Model Performance Analysis\nPyTorch & Keras Fashion-MNIST with and without Regularisation\nCNN Visualisation - Filter and Filter Activation Visualisation\nCNN Visualisation Filter and Class Maximisation\nCNN Visualisation GradCAM GradCAMplusplus and FasterScoreCAM\nReplicating LeNet and AlexNet in Tensorflow2.0 using Keras\nPyTorch & Keras Pretrained Models - 1 - VGG16, ResNet, Inceptionv3, MobileNetv2, SqueezeNet, WideResNet, DenseNet201, MobileMNASNet, EfficientNet and MNASNet\nRank-1 and Rank-5 Accuracy\nPyTorch and Keras Cats vs Dogs PyTorch - Train with your own data\nPyTorch Lightning Tutorial - Batch and LR Selection, Tensorboards, Callbacks, mGPU, TPU and more\nPyTorch Lightning - Transfer Learning\nPyTorch and Keras Transfer Learning and Fine Tuning\nPyTorch & Keras Using CNN's as a Feature Extractor\nPyTorch & Keras - Google Deep Dream\nPyTorch Keras - Neural Style Transfer + TF-HUB Models\nPyTorch & Keras Autoencoders using the Fashion-MNIST Dataset\nPyTorch & Keras - Generative Adversarial Networks - DCGAN - MNIST\nKeras - Super Resolution SRGAN\nProject - Generate_Anime_with_StyleGAN\nCycleGAN - Turn Horses into Zebras\nArcaneGAN inference\nPyTorch & Keras Siamese Networks\nFacial Recognition with VGGFace in Keras\nPyTorch Facial Similarity with FaceNet\nDeepFace - Age, Gender, Expression, Headpose and Recognition\nObject Detection - Gun, Pistol Detector - Scaled-YOLOv4\nObject Detection - Mask Detection - TensorFlow Object Detection - MobileNetV2 SSD\nObject Detection  - Sign Language Detection - TFODAPI - EfficientDetD0-D7\nObject Detection - Pot Hole Detection with TinyYOLOv4\nObject Detection - Mushroom Type Object Detection - Detectron 2\nObject Detection - Website Screenshot Region Detection - YOLOv4-Darknet\nObject Detection - Drone Maritime Detector - Tensorflow Object Detection Faster R-CNN\nObject Detection - Chess Pieces Detection - YOLOv3 PyTorch\nObject Detection - Hardhat Detection for Construction sites - EfficientDet-v2\nObject DetectionBlood Cell Object Detection - YOLOv5\nObject DetectionPlant Doctor Object Detection - YOLOv5\nImage Segmentation - Keras, U-Net and SegNet\nDeepLabV3 - PyTorch_Vision_Deeplabv3\nMask R-CNN Demo\nDetectron2 - Mask R-CNN\nTrain a Mask R-CNN - Shapes\nYolov5 DeepSort Pytorch tutorial\nDeepFakes - first-order-model-demo\nVision Transformer Tutorial PyTorch\nVision Transformer Classifier in Keras\nImage Classification using BigTransfer (BiT)\nDepth Estimation with Keras\nImage Similarity Search using Metric Learning with Keras\nImage Captioning with Keras\nVideo Classification with a CNN-RNN Architecture with Keras\nVideo Classification with Transformers with Keras\nPoint Cloud Classification - PointNet\nPoint Cloud Segmentation with PointNet\n3D Image Classification CT-Scan\nX-ray Pneumonia Classification using TPUs\nLow Light Image Enhancement using MIRNet\nCaptcha OCR Cracker\nFlask Rest API - Server and Flask Web App\nDetectron2 - BodyPose",
      "target_audience": [
        "College/University Students of all levels Undergrads to PhDs (very helpful for those doing projects)",
        "Software Developers and Engineers looking to transition into Computer Vision",
        "Start up founders lookng to learn how to implement thier big idea",
        "Hobbyist and even high schoolers looking to get started in Computer Vision"
      ]
    },
    {
      "title": "Computer Vision: Face Recognition Quick Starter in Python",
      "url": "https://www.udemy.com/course/computer-vision-face-recognition-quick-starter-in-python/",
      "bio": "Python Deep Learning based Face Detection, Recognition, Emotion , Gender and Age Classification using all popular models",
      "objectives": [
        "Face Detection from Images",
        "Face Detection from Realtime Videos",
        "Emotion Detection, Age-Gender Prediction",
        "Face Recognition from Images, Realtime Videos"
      ],
      "course_content": {
        "Course Introduction and Table of Contents": [
          "Course Introduction and Table of Contents"
        ],
        "Introduction to Face Recognition": [
          "Introduction to Face Recognition"
        ],
        "Environment Setup: Installing Anaconda Package": [
          "Environment Setup: Installing Anaconda Package"
        ],
        "Python Basics (Optional)": [
          "Python Basics - Assignment",
          "Python Basics - Flow Control",
          "Python Basics - Data Structures",
          "Python Basics - Functions"
        ],
        "Setting up Environment - Additional Dependencies (With DLib Fixes)": [
          "Setting up Environment - Additional Dependencies - Part 1",
          "Attempt 1 : Install Dlib for Anaconda the formal way",
          "Attempt 2: Installing DLib using Visual Studio Build Tools",
          "Setting up Environment - Additional Dependencies (With DLib Fixes) - Part 2"
        ],
        "(Optional) DLib Error : Downgrading Python and Fixing": [
          "(Optional) DLib Error : Downgrading Python and Fixing"
        ],
        "Introduction to Face Detectors": [
          "Introduction to Face Detectors"
        ],
        "Face Detection Implementation": [
          "Face Detection Implementation - Part 1",
          "Face Detection Implementation - Part 2"
        ],
        "Optional: cv2.imshow() Not Responding Issue Fix": [
          "Optional: cv2.imshow() Not Responding Issue Fix",
          "Optional Fix 2: cv2.imshow() Not Responding"
        ],
        "Realtime face detection from WebCam": [
          "Realtime face detection - Part 1",
          "Realtime face detection - Part 2"
        ]
      },
      "requirements": [
        "A decent configuration computer and an enthusiasm to dive into the world of computer vision based Face Recognition"
      ],
      "description": "DISCLAIMER:  This course requires you to download and install Anaconda from anaconda website and install packages and libraries within anaconda for face recognition. If you are a Udemy Business user, please check with your employer before downloading software.\n\n\nHi There!\nwelcome to my new course 'Face Recognition with Deep Learning using Python'. This is an updated course from my Computer Vision series which covers Python Deep Learning based Face Detection, Face Recognition, Emotion , Gender and Age Classification using all popular models including Haar Cascade, HOG, SSD, MMOD, MTCNN, EigenFace, FisherFace, VGGFace, FaceNet, OpenFace, DeepFace\n\n\nFace Detection and Face Recognition is the most used applications of Computer Vision. Using these techniques, the computer will be able to extract one or more faces in an image or video and then compare it with the existing data to identify the people in that image.\n\n\nFace Detection and Face Recognition is widely used by governments and organizations for surveillance and policing. We are also making use of it daily in many applications like face unlocking of cell phones etc.\n\n\nThis course will be a quick starter for people who wants to dive deep into face recognition using Python without having to deal with all the complexities and mathematics associated with typical Deep Learning process.\n\n\nWe will be using a python library called face-recognition which uses simple classes and methods to get the face recognition implemented with ease. We are also using OpenCV, Dlib and Pillow for python as supporting libraries.\n\n\nLet's now see the list of interesting topics that are included in this course.\n\n\nAt first we will have an introductory theory session about Face Detection and Face Recognition technology.\n\n\nAfter that, we are ready to proceed with preparing our computer for python coding by downloading and installing the anaconda package. Then we will install the rest of dependencies and libraries that we require including the dlib, face-recognition, opencv etc and will try a small program to see if everything is installed fine.\n\n\nMost of you may not be coming from a python based programming background. The next few sessions and examples will help you get the basic python programming skill to proceed with the sessions included in this course. The topics include Python assignment, flow-control, functions and data structures.\n\n\nThen we will have an introduction to the basics and working of face detectors which will detect human faces from a given media. We will try the python code to detect the faces from a given image and will extract the faces as separate images.\n\n\nThen we will go ahead with face detection from a video. We will be streaming the real-time live video from the computer's webcam and will try to detect faces from it. We will draw rectangle around each face detected in the live video.\n\n\nIn the next session, we will customize the face detection program to blur the detected faces dynamically from the webcam video stream.\n\n\nAfter that we will try facial expression recognition using pre-trained deep learning model and will identify the facial emotions from the real-time webcam video as well as static images\n\n\nAnd then we will try Age and Gender Prediction using pre-trained deep learning model and will identify the  Age and Gender from the real-time webcam video as well as static images\n\n\nAfter face detection, we will have an introduction to the basics and working of face recognition which will identify the faces already detected.\n\n\nIn the next session, We will try the python code to identify the names of people and their the faces from a given image and will draw a rectangle around the face with their names on it.\n\n\nThen, like as we did in face detection we will go ahead with face recognition from a video. We will be streaming the real-time live video from the computer's webcam and will try to identify and name the faces in it. We will draw rectangle around each face detected and beneath that their names in the live video.\n\n\nMost times during coding, along with the face matching decision, we may need to know how much matching the face is. For that we will get a parameter called face distance which is the magnitude of matching of two faces. We will later convert this face distance value to face matching percentage using simple mathematics.\n\n\nIn the coming two sessions, we will learn how to tweak the face landmark points used for face detection. We will draw line joining these face land mark points so that we can visualize the points in the face which the computer is used for evaluation.\n\n\nTaking the landmark points customization to the next level, we will use the landmark points to create a custom face make-up for the face image.\n\n\nTill now we were using the face-recognition third party library to achieve most of the functionality. But from now onwards we will try the face-recognition pipeline steps which includes face detection, face alignment, face feature extraction verification and classification separately one by one using popular libraries. We will have an introduction about these in this session.\n\n\nIn the next session, we will start with face detection. We will divide them into traditional face detection methods and modern methods which involves CNN.\n\n\nAt first we will try the Haar Cascade object detection algorithm for face detection. We will try it at first for still images and later we will implement it for saved videos as well as live web cam videos.\n\n\nAnother popular algorithm for face detection is HOG or Histogram of Oriented Gradients. At first we will have an introduction to the working of HOG algorithm and then we will try the HOG method for images, videos and real-time web cam stream.\n\n\nThe next face detection algorithm we will try is SSD or Single Shot Detection. We will repeat the same functionality exercises for SSD also. And then comes MMOD. We will repeat the same functionality exercises for SSD also.\n\n\nThen the MMOD, the Max-Margin Object Detection. We will repeat the same functionality exercises for MTCNN also.\n\n\nThen comes the next algorithm which is MTCNN, the Multi-task Cascaded Convolutional Networks. We will repeat the same functionality exercises including image, video and real time stream for MTCNN also.\n\n\nFinally we will have a quick comparison between the performance of these face detection algorithms.\n\n\nAfter face detection, we will go ahead with face alignment. We will use the popular Dlib library python implementation to perform the face alignment for image, video and video streams.\n\n\nAfter face alignment exercises, we will proceed with face verification and classification where the actual face recognition is happening. At first we will have an introduction about face classification. We will divide the techniques into traditional face recognition methods and modern methods which involves CNN.\n\n\nAt first we will try the techniques Eigenface Fisherface and LBPH, the Local Binary Pattern Histogram. We will have a short introduction about these algorithms and will then proceed with\npreparing the image dataset for these algorithms.\n\n\nThen we will set up the prerequisite for them. Later we will proceed with face detection using MTCNN and preprocessing of the detected face for recognition. Then the exercises involving training with the image dataset and trying prediction for images. We will then save this model so that we can load it later and do prediction without having to go through training again.\n\n\nWe will also try it for pre-saved videos and real time webcam stream. Once we are done with that we will have a quick comparison of the Eigenface Fisherface and LBPH algorithms.\n\n\nThat's all the traditional ways, now we will proceed with deep learning face recognition. At first using the popular VGGNet model for face recognition called VGGface. We will have an introduction to VGG face and then we will implement VGGface face verification for images. Later we will try VGGface face verification for videos as well as realtime streams.\n\n\nAnd then we have an introduction to FaceNet, OpenFace and DeepFace Models. We will use a popular easy to use open source python face recognition framework called deepface to implement the rest of popular deep learning techniques.\n\n\nWe will install deepface to our computer and then try it at first for face detection and face alignment. Then we will try deepface for face one to one verification. Later with few changes, we can use it for face classification which involves an one to many comparison. deepface can also be used for performing face analysis involving gender, age, emotion, ethnicity etc\n\n\nThat's all about the topics which are currently included in this quick course. The code, images and libraries used in this course has been uploaded and shared in a folder. I will include the link to download them in the last session or the resource section of this course. You are free to use the code in your projects with no questions asked.\n\n\nAlso after completing this course, you will be provided with a course completion certificate which will add value to your portfolio.\n\n\nSo that's all for now, see you soon in the class room. Happy learning and have a great time.",
      "target_audience": [
        "Beginners or who wants to start with Python based Face Recognition"
      ]
    },
    {
      "title": "ChatGPT for Business: Writing with a Generative AI Companion",
      "url": "https://www.udemy.com/course/writing-with-ai/",
      "bio": "Boost Your Writing with Generative AI Tools: ChatGPT or Google Gemini to Help You Write More Creatively & Effectively",
      "objectives": [
        "Learn how to use AI writing companions for your business needs",
        "ChatGPT: Learn how to quickly start using ChatGPT as your AI Writing assistant",
        "Best practices for using AI writing tools effectively",
        "How to craft great prompts (prompt engineering) to get the best results",
        "Know the capabilities & limitations to AI tools like ChatGPT & Google Bard, and when to use them",
        "Use AI writing software to save you and your business time",
        "Enhance creativity and collaboration with an AI writing tool",
        "Use AI writing tools to generate ideas for your work",
        "Revise, edit, and improve what you've already written",
        "Understand the ethical considerations and responsible use of AI writing tools",
        "Improve SEO (Search Engine Optimization) with AI writing tools",
        "Content optimization such as text summarization, question answering, and translation",
        "Google Bard: Learn how to quickly start using Google Bard as your AI Writing assistant (and the differences from ChatGPT)"
      ],
      "course_content": {
        "Introduction to Writing with AI": [
          "What is this Writing with AI Course? What Will You Be Learning?",
          "Get Started with ChatGPT | Let's See ChatGPT in Action",
          "Quick Tips for Improving Your ChatGPT Prompts (Prompt Engineering)"
        ],
        "Ethics of Using AI Writing Tools Like ChatGPT or Google Bard": [
          "Ethical Consideration: The Responsible Use of AI Writing Tools like ChatGPT",
          "Human vs. AI Writing (like ChatGPT), and When to Use Each"
        ],
        "AI Writing and Your Business": [
          "Adding AI Tools like ChatGPT to Your Writing Workflow",
          "Writing with ChatGPT | Case Study: Brainstorming and Outlining",
          "Case Study: Writing an Article with ChatGPT",
          "Case Study: Writing Catchy Subject Lines, Titles, Headlines with ChatGPT",
          "Case Study: Writing a Video Script from Scratch and From a Template",
          "Case Study: Writing Social Media Posts & Promotional Emails with ChatGPT"
        ],
        "Improving Your ChatGPT Prompts for Your Business Needs": [
          "Download the ChatGPT Prompt Examples",
          "Prompt Engineering - Writing Better ChatGPT Prompts to Get Better Results",
          "Case Study: Using Advanced Prompts to Get Amazing ChatGPT Results"
        ],
        "Optimizing Existing & Outdated Content with AI": [
          "Optimizing Existing Content with ChatGPT",
          "Training ChatGPT with Specific Industry Knowledge",
          "Case Study: Writing a Book from Your Video Script",
          "Matching ChatGPT Content to Your Brand & Tone",
          "Case Study: Using Existing Video Transcripts to Improve ChatGPT Writing",
          "Strategies for Efficient Editing & Decreasing Errors"
        ],
        "Future Trends in AI Writing": [
          "What to Look Out for in the Future of AI"
        ],
        "Conclusion": [
          "Thank You & What's Next?",
          "Bonus Lesson: Learn More from Us"
        ]
      },
      "requirements": [
        "There are no requirements to take this AI writing course. You should have access to the internet to follow along and practice the tools & techniques we teach in the class.",
        "To follow along & practice, you'll need to sign up for accounts with Open AI's ChatGPT and Google Bard."
      ],
      "description": "If you write for your business or job, this AI Writing course is a must-watch for you. Don't wait too long to master the use of modern AI tools like ChatGPT and Google Bard. These AI tools can benefit you as a business, and more specifically as a writer.\nIn this course, you'll learn how to use an AI writing companion to:\nEnhance Your Creativity\nSpeed Up Your Workflow\nWrite More & Write Better\nOptimize Text for SEO\nand so much more!\nWithin the first 15 minutes of class, you will have a clear understanding of:\nHow to sign up & start prompting ChatGPT to help you write\nBest practices for writing prompts (prompt engineering)\nWhy & how you can use an AI writing assistant\nHow can an AI Writing Companion help you?\nGenerate ideas that inspire you to write\nEdit your writing for grammar & spelling mistakes\nAutomatically rewrite your work in a different tone or for a different target audience\nCondense or expand your writing\nTurn your writing into a different format (i.e. social media post, email blast, article)\nTranslate your writing into another language\nSummarize long text into condensed notes\nOptimize your writing for search engines, making them keyword friendly\nGenerate catchy headlines, subject lines, and titles for your content\nWrite entire articles, posts, and other content for you\nWatch a free preview of this course to start learning, and to see if this is the right course for you.\nBy the end of this course, you'll have a comprehensive understanding of AI writing tools such as ChatGPT & Google Bard, and how to use them for your specific business needs.\n\n\nWhat can you write with ChatGPT?\nThe following list contains only the beginning of what you can use an AI writing assistant for:\nArticles\nSocial Media Posts\nEmails\nLanding Pages\nCover Letters\nResumes\nEssays\nVideo Scripts\nPodcast Scripts\n\n\nIf this sounds good, watch a free preview of the class and enroll!\n\n\nSee you in class!\nPhil",
      "target_audience": [
        "Anyone who writes, and wants to use AI (artificial intelligence) to speed up & improve their writing",
        "Business owners who want to improve their marketing, content creation, and customer engagement using AI writing tools.",
        "Marketers and content creators who want to streamline their workflow and improve their productivity using AI tools.",
        "Researchers who want to conduct effective research for academic or professional purposes using AI tools.",
        "Anyone who wants to stay up-to-date with technological advancements and gain a competitive edge in their field."
      ]
    },
    {
      "title": "Master Regression & Prediction with Pandas and Python [2025]",
      "url": "https://www.udemy.com/course/master-regression-prediction-with-pandas-and-python-2024/",
      "bio": "Learn to Master Regression and Prediction with Pandas and Python for Data Science, Data Analysis, and Machine Learning",
      "objectives": [
        "Master Regression, Regression analysis, and Prediction both in theory and practice",
        "Master Regression models from simple Regression models to Polynomial Multiple Regression models and advanced Multivariate Polynomial Multiple Regression models",
        "Use Machine Learning Automatic Model Creation and Feature Selection",
        "Use Regularization of Regression models with Lasso Regression and Ridge Regression",
        "Use Decision Tree, Random Forest, XGBoost, and Voting Regression models",
        "Use Feedforward Multilayer Networks and Advanced Regression model Structures",
        "Use effective advanced Residual analysis and tools to judge models goodness-of-fit plus residual distributions",
        "Use the Statsmodels and Scikit-learn libraries for Regression supported by Matplotlib, Seaborn, Pandas, and Python",
        "Master Python 3 programming with Python’s native data structures, data transformers, functions, object orientation, and logic",
        "Use and design advanced Python constructions and execute detailed Data Handling tasks with Python incl. File Handling",
        "Use Python’s advanced object-oriented programming and make your own custom objects, functions and how to generalize functions",
        "Manipulate data and use advanced multi-dimensional uneven data structures",
        "Master the Pandas 2 and 3 library for Advanced Data Handling",
        "Use the language and fundamental concepts of the Pandas library and to handle all aspects of creating, changing, modifying, and selecting Data from a Pandas D",
        "Use file handling with Pandas and how to combine Pandas DataFrames with Pandas concat, join, and merge functions/methods",
        "Perform advanced data preparation including advanced model-based imputation of missing data and the scaling and standardizing of data",
        "Make advanced data descriptions and statistics with Pandas. Rank, sort, cross-tabulate, pivot, melt, transpose, and group data",
        "[Bonus] Make advanced Data Visualizations with Pandas, Matplotlib, and Seaborn",
        "Cloud computing: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources",
        "Option: To use the Anaconda Distribution (for Windows, Mac, Linux)",
        "Option: Use Python environment fundamentals with the Conda package management system and command line installing/updating of libraries and packages"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Setup of the Anaconda Cloud Notebook",
          "Download and installation of the Anaconda Distribution (optional)",
          "The Conda Package Management System (optional)"
        ],
        "Master Python for Data Handling": [
          "Overview",
          "Python Integers",
          "Python Floats",
          "Python Strings",
          "Python String Methods",
          "Python Strings and DateTime Objects",
          "Python Data Storage Overview",
          "Python Set",
          "Python Tuple",
          "Python Dictionary",
          "Python List",
          "Data Transformers and Functions Overview",
          "Python While-loop",
          "Python For-loop",
          "Python Conditional Code Branching and Logic Operators",
          "Python Function Theory",
          "Python Functions: create your own functions",
          "Python Object Oriented Programming: Some theory",
          "Python Object Oriented Programming II: create your own custom objects",
          "Python Object Oriented Programming III: Files and Tables",
          "Python Object Oriented Programming IV: Recap and More"
        ],
        "Master Pandas for Data Handling": [
          "Master Pandas for Data Handling: Overview",
          "Pandas Theory and Terminology",
          "Creating a Pandas DataFrame from scratch",
          "Pandas File Handling: Overview",
          "Pandas File Handling: The .csv file format",
          "Pandas File Handling: The .xlsx file format",
          "Pandas File Handling: SQL-database files and Pandas DataFrame",
          "Pandas Operations & Techniques: Overview",
          "Pandas Operations & Techniques: Object Inspection",
          "Pandas Operations & Techniques: DataFrame Inspection",
          "Pandas Operations & Techniques: Column Selections",
          "Pandas Operations & Techniques: Row Selections",
          "Pandas Operations & Techniques: Conditional Selections",
          "Pandas Operations & Techniques: Scalers and Standardization",
          "Pandas Operations & Techniques: Concatenate DataFrames",
          "Pandas Operations & Techniques: Joining DataFrames",
          "Pandas Operations & Techniques: Merging DataFrames",
          "Pandas Operations & Techniques: Transpose & Pivot Functions",
          "Pandas Data Preparation: Overview & workflow",
          "Pandas Data Preparation II: Edit DataFrame labels",
          "Pandas Data Preparation III: Duplicates",
          "Pandas Data Preparation IV: Missing Data & Imputation",
          "Pandas Data Preparation V: Data Binnings [Extra Video]",
          "Pandas Data Preparation VI: Indicator Features [Extra Video]",
          "Pandas Data Description: Overview",
          "Pandas Data Description II: Sorting and Ranking",
          "Pandas Data Description III: Descriptive Statistics",
          "Pandas Data Description IV: Crosstabulations & Groupings",
          "Pandas Data Visualization: Overview",
          "Pandas Data Visualization II: Histograms",
          "Pandas Data Visualization III: Boxplots",
          "Pandas Data Visualization IV: Scatterplots",
          "Pandas Data Visualization V: Pie Charts",
          "Pandas Data Visualization VI: Line plots"
        ],
        "Master Regression Models for Prediction": [
          "Regression, Prediction, and Supervised Learning. Section Overview (I)",
          "The Traditional Simple Regression Model (II)",
          "The Traditional Simple Regression Model (III)",
          "Some practical and useful modelling concepts (IV)",
          "Some practical and useful modelling concepts (V)",
          "Linear Multiple Regression model (VI)",
          "Linear Multiple Regression model (VII)",
          "Multivariate Polynomial Multiple Regression models (VIII)",
          "Multivariate Polynomial Multiple Regression models (VIIII)",
          "Regression Regularization, Lasso and Ridge models (X)",
          "Decision Tree Regression models (XI)",
          "Random Forest Regression (XII)",
          "Voting Regression (XIII)"
        ],
        "Feedforward Networks and Advanced Regression Models": [
          "Overview",
          "Artificial Neural Networks, Feedforward Networks, and the Multi-Layer Perceptron",
          "Feedforward Multi-Layer Perceptrons for Prediction tasks",
          "eXtreme Gradient Boosting Regression (XGBoost)"
        ]
      },
      "requirements": [
        "Everyday experience using a computer with either Windows, MacOS, iOS, Android, ChromeOS, or Linux is recommended",
        "Access to a computer with an internet connection",
        "Programming experience is not needed and you will be taught everything you need",
        "The course only uses costless software",
        "Walk-you-through installation and setup videos for Cloud computing and Windows 10/11 is included"
      ],
      "description": "Welcome to the course Master Regression & Prediction with Pandas and Python!\nThis three-in-one master class video course will teach you to master Regression, Prediction, Python 3, Pandas 2 + 3, and advanced Data Handling.\nYou will learn to master Regression, Regression analysis, and Prediction with a large number of advanced Regression techniques for purposes of Prediction and Automatic Model Creation, or so-called true machine intelligence or AI. You will learn to handle advanced model structures and eXtreme Gradient Boosting Regression for prediction tasks.\nPython 3 is one of the most popular and useful programming languages in the world, and Pandas 2 and future version 3 is the most powerful, efficient, and useful Data Handling library in existence.\nYou will learn to master Python's native building blocks and powerful object-oriented programming. You will design your own advanced constructions of Python’s building blocks and execute detailed Data Handling tasks with Python.\nYou will learn to master the Pandas library and to use its powerful Data Handling techniques for advanced Data Science and Machine Learning Data Handling tasks. The Pandas library is a fast, powerful, flexible, and easy-to-use open-source data analysis and data manipulation tool, which is directly usable with the Python programming language.\n\n\nYou will learn to:\nMaster Regression, Regression analysis and Prediction both in theory and practice\nMaster Regression models from simple linear Regression models to Polynomial Multiple Regression models and advanced Multivariate Polynomial Multiple Regression models plus XGBoost Regression.\nUse Machine Learning Automatic Model Creation and Feature Selection\nUse Regularization of Regression models with Lasso Regression and Ridge Regression\nUse Decision Tree, Random Forest, XGBoost, and Voting Regression models\nUse Feedforward Multilayer Networks and Advanced Regression model Structures\nUse effective advanced Residual analysis and tools to judge models goodness-of-fit plus residual distributions.\nUse the Statsmodels and Scikit-learn libraries for Regression supported by Matplotlib, Seaborn, Pandas, and Python\nMaster Python 3 programming with Python’s native data structures, data transformers, functions, object orientation, and logic\nUse and design advanced Python constructions and execute detailed Data Handling tasks with Python incl. File Handling\nUse Python’s advanced object-oriented programming and make your own custom objects, functions and how to generalize functions\nManipulate data and use advanced multi-dimensional uneven data structures\nMaster the Pandas 2 and 3 library for Advanced Data Handling\nUse the language and fundamental concepts of the Pandas library and to handle all aspects of creating, changing, modifying, and selecting Data from a Pandas DataFrame object\nUse file handling with Pandas and how to combine Pandas DataFrames with Pandas concat, join, and merge functions/methods\nPerform advanced data preparation including advanced model-based imputation of missing data and the scaling and standardizing of data\nMake advanced data descriptions and statistics with Pandas. Rank, sort, cross-tabulate, pivot, melt, transpose, and group data\n[Bonus] Make advanced Data Visualizations with Pandas, Matplotlib, and Seaborn\nCloud computing: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources.\nOption: To use the Anaconda Distribution (for Windows, Mac, Linux)\nOption: Use Python environment fundamentals with the Conda package management system and command line installing/updating of libraries and packages – golden nuggets to improve your quality of work life.\nAnd much more…\n\n\nThis course is an excellent way to learn to master Regression, Prediction, Python, Pandas and Data Handling!\nRegression and Prediction are the most important and used tools for modeling, AI, and forecasting. Data Handling is the process of making data useful and usable for regression, prediction, and data analysis.\nMost Data Scientists and Machine Learning Engineers spends about 80% of their working efforts and time on Data Handling tasks. Being good at Python, Pandas, and Data Handling are extremely useful and time-saving skills that functions as a force multiplier for productivity.\n\n\nThis course is designed for everyone who wants to\nlearn to master Regression and Prediction\nlearn to Master Python 3 from scratch or the beginner level\nlearn to Master Python 3 and knows another programming language\nreach the Master - intermediate Python programmer level as required by many advanced Udemy courses in Python, Data Science, or Machine Learning\nlearn to Master the Pandas library\nlearn Data Handling skills that work as a force multiplier and that they will have use of in their entire career\nlearn advanced Data Handling and improve their capabilities and productivity\n\n\nRequirements:\nEveryday experience using a computer with either Windows, MacOS, iOS, Android, ChromeOS, or Linux is recommended\nAccess to a computer with an internet connection\nProgramming experience is not needed and you will be taught everything you need\nThe course only uses costless software\nWalk-you-through installation and setup videos for Cloud computing and Windows 10/11 is included\n\n\nThis course is the course we ourselves would want to be able to enroll in if we could time-travel and become new students. In our opinion, this course is the best course to learn to Master Regression, Prediction, Python, Pandas, and Data Handling.\n\n\nEnroll now to receive 30+ hours of video tutorials with manually edited English captions, and a certificate of completion after completing the course!",
      "target_audience": [
        "anyone who wants to learn to master Regression and Prediction",
        "anyone who wants to learn to Master Python 3 from scratch or the beginner level",
        "anyone who wants to learn to Master Python 3 and knows another programming language",
        "anyone who wants to reach the Master/intermediate Python programmer level as required by many advanced Udemy courses in Python, Data Science, or Machine Learning",
        "anyone who wants to learn to Master the Pandas library",
        "anyone who wants to learn Data Handling skills that work as a force multiplier and that they will have use of in their entire career",
        "anyone who wants to learn advanced Data Handling and improve their capabilities and productivity"
      ]
    },
    {
      "title": "Complete Python and Machine Learning in Financial Analysis",
      "url": "https://www.udemy.com/course/python-and-machine-learning-in-financial-analysis/",
      "bio": "Using Python, Machine Learning, and Deep Learning in Financial Analysis with step-by-step coding (with all codes)",
      "objectives": [
        "You will be able to use the functions provided to download financial data from a number of sources and preprocess it for further analysis",
        "You will be able to draw some insights into patterns emerging from a selection of the most commonly used metrics (such as MACD and RSI)",
        "Introduces the basics of time series modeling. Then, we look at exponential smoothing methods and ARIMA class models.",
        "shows you how to estimate various factor models in Python. one ,three-, four-, and five-factor models.",
        "Introduces you to the concept of volatility forecasting using (G)ARCH class models, how to choose the best-fitting model, and how to interpret your results.",
        "Introduces concept of Monte Carlo simulations and use them for simulating stock prices, the valuation of European/American options and calculating the VaR.",
        "Introduces the Modern Portfolio Theory and shows you how to obtain the Efficient Frontier in Python. how to evaluate the performance of such portfolios.",
        "Presents a case of using machine learning for predicting credit default. You will get to know tune the hyperparameters of the models and handle imbalances",
        "Introduces you to a selection of advanced classifiers (including stacking multiple models)and how to deal with class imbalance, use Bayesian optimization.",
        "Demonstrates how to use deep learning techniques for working with time series and tabular data. The networks will be trained using PyTorch."
      ],
      "course_content": {
        "Financial Data and Preprocessing": [
          "Introduction of Python Programming in Financial Analysis",
          "Introduction of Financial Analysis",
          "Introduction",
          "Getting data from Yahoo Finance",
          "Getting data from Quandl",
          "Converting prices to returns",
          "Changing frequency",
          "Visualizing time series data",
          "Identifying outliers",
          "Investigating stylized facts of asset returns",
          "Codes of Chapter 1"
        ],
        "Technical Analysis in Python": [
          "Introduction",
          "requirements of chapter 2",
          "Creating a candlestick chart",
          "Backtesting a strategy based on simple moving average",
          "Calculating Bollinger Bands and testing a buy/sell strategy",
          "Calculating the relative strength index and testing a long/short strategy",
          "Building an interactive dashboard for TA",
          "Codes of Chapter 2"
        ],
        "Time Series Modeling": [
          "Introduction",
          "requirements of chapter 3",
          "Decomposing time series",
          "Testing for stationarity in time series",
          "Correcting for stationarity in time series",
          "Modeling time series with exponential smoothing methods",
          "Modeling time series with ARIMA class models",
          "Forecasting using ARIMA class models",
          "Codes of Chapter 3"
        ],
        "Multi-Factor Models": [
          "Introduction",
          "requirements of chapter 4",
          "Implementing the CAPM in Python",
          "Implementing the Fama-French three-factor model in Python",
          "Implementing the rolling three-factor model on a portfolio of assets",
          "Implementing the four- and five-factor models in Python",
          "Codes of Chapter 4"
        ],
        "Modeling Volatility with GARCH Class Models": [
          "Introduction",
          "requirements of chapter 5",
          "Explaining stock returns' volatility with ARCH models",
          "Explaining stock returns' volatility with GARCH models",
          "Implementing a CCC-GARCH model for multivariate volatility forecasting",
          "Forecasting a conditional covariance matrix using DCC-GARCH",
          "Codes of Chapter 5"
        ],
        "Monte Carlo Simulations in Finance": [
          "Introduction",
          "requirements of chapter 6",
          "Simulating stock price dynamics using Geometric Brownian Motion",
          "Pricing European options using simulations",
          "Pricing American options with Least Squares Monte Carlo",
          "Pricing American options using Quantlib",
          "Estimating value-at-risk using Monte Carlo",
          "Codes of Chapter 6"
        ],
        "Asset Allocation in Python": [
          "Introduction",
          "Evaluating the performance of a basic 1/n portfolio",
          "Finding the Efficient Frontier using Monte Carlo simulations",
          "Finding the Efficient Frontier using optimization with scipy",
          "Codes of Chapter 7"
        ],
        "Identifying Credit Default with Machine Learning": [
          "Introduction",
          "requirements of chapter 8",
          "Loading data and managing data types",
          "Exploratory data analysis",
          "Splitting data into training and test sets",
          "Dealing with missing values",
          "Encoding categorical variables",
          "Fitting a decision tree classifier",
          "Implementing scikit-learn's pipelines",
          "Tuning hyperparameters using grid search and cross-validation",
          "Codes of Chapter 8"
        ],
        "Advanced Machine Learning Models in Finance": [
          "Introduction",
          "requirements of chapter 9",
          "Investigating advanced classifiers",
          "Theres more about use advanced classifiers to achieve better results",
          "Using stacking for improved performance",
          "Investigating the feature importance",
          "Investigating different approaches to handling imbalanced data",
          "Bayesian hyperparameter optimization",
          "Codes of Chapter 9"
        ],
        "Deep Learning in Finance": [
          "Introduction",
          "requirements of chapter 10",
          "Deep learning for tabular data",
          "Multilayer perceptrons for time series forecasting",
          "Convolutional neural networks for time series forecasting",
          "Recurrent neural networks for time series forecasting",
          "Codes of Chapter 10",
          "The End"
        ]
      },
      "requirements": [
        "Statistics and Basic Python"
      ],
      "description": "In this course, you will become familiar with a variety of up-to-date financial analysis content, as well as algorithms techniques of machine learning in the Python environment, where you can perform highly specialized financial analysis. You will get acquainted with technical and fundamental analysis and you will use different tools for your analysis. You will learn the Python environment completely. You will also learn deep learning algorithms and artificial neural networks that can greatly enhance your financial analysis skills and expertise.\nThis tutorial begins by exploring various ways of downloading financial data and preparing it for modeling. We check the basic statistical properties of asset prices and returns, and investigate the existence of so-called stylized facts. We then calculate popular indicators used in technical analysis (such as Bollinger Bands, Moving Average Convergence Divergence (MACD), and Relative Strength Index (RSI)) and backtest automatic trading strategies built on their basis.\nThe next section introduces time series analysis and explores popular models such as exponential smoothing, AutoRegressive Integrated Moving Average (ARIMA), and Generalized Autoregressive Conditional Heteroskedasticity (GARCH) (including multivariate specifications). We also introduce you to factor models, including the famous Capital Asset Pricing Model (CAPM) and the Fama-French three-factor model. We end this section by demonstrating different ways to optimize asset allocation, and we use Monte Carlo simulations for tasks such as calculating the price of American options or estimating the Value at Risk (VaR).\nIn the last part of the course, we carry out an entire data science project in the financial domain. We approach credit card fraud/default problems using advanced classifiers such as random forest, XGBoost, LightGBM, stacked models, and many more. We also tune the hyperparameters of the models (including Bayesian optimization) and handle class imbalance. We conclude the book by demonstrating how deep learning (using PyTorch) can solve numerous financial problems.",
      "target_audience": [
        "Developers",
        "Financial Analysts",
        "Data Analysts",
        "Data Scientists",
        "Stock and cryptocurrency traders",
        "Students",
        "Teachers",
        "Researchers"
      ]
    },
    {
      "title": "Data Analytics & Visualization: Using Excel and Python",
      "url": "https://www.udemy.com/course/data-analytics-visualization-acquire-demanded-tech-skills/",
      "bio": "Unlocking Insights through Data: Mastering Analytics and Visualization for In-Demand Tech Proficiency",
      "objectives": [
        "Real-world use cases of Python and its versatility.",
        "Installation of Python on both Mac and Windows operating systems.",
        "Fundamentals of programming with Python, including variables and data types.",
        "Working with various operators in Python to perform operations.",
        "Fundamental concepts and importance of statistics in various fields.",
        "How to use statistics for effective data analysis and decision-making.",
        "Introduction to Python for statistical analysis, including data manipulation and visualization."
      ],
      "course_content": {},
      "requirements": [
        "Students should have a general understanding of how to operate a computer.",
        "Be comfortable with common tasks like file management and using a web browser.",
        "No Prior Programming Experience Required.",
        "A basic understanding of mathematics, including algebra and arithmetic.",
        "Familiarity with fundamental concepts in data analysis and problem-solving."
      ],
      "description": "Embark on a transformative journey into the dynamic realm of Data Analytics and Visualization, where you will acquire essential and sought-after tech skills. This comprehensive course is designed to empower you with proficiency in key tools and methodologies, including Python programming, Excel, statistical analysis, data analysis, and data visualization.\n\n\nKey Learning Objectives:\n- Gain hands-on experience in Python, a powerful and versatile programming language widely used for data analysis and manipulation.\n- Learn to leverage Python libraries such as Pandas and NumPy for efficient data handling and manipulation.\n- Develop advanced skills in Excel, exploring its robust features for data organization, analysis, and visualization.\n- Harness the power of Excel functions and formulas to extract insights from complex datasets.\n- Acquire a solid foundation in statistical concepts and techniques essential for making informed decisions based on data.\n- Apply statistical methods to interpret and draw meaningful conclusions from data sets.\n- Explore the entire data analysis process, from data cleaning and preprocessing to exploratory data analysis (EDA) and feature engineering.\n- Learn how to identify patterns, outliers, and trends within datasets, enabling you to extract valuable insights.\n- Master the art of presenting data visually through a variety of visualization tools and techniques.\n- Use industry-standard tools like Matplotlib and Seaborn to create compelling and informative data visualizations.\n\n\nUpon completion, you will possess a well-rounded skill set in data analytics and visualization, equipping you to tackle real-world challenges and contribute meaningfully to data-driven decision-making in any professional setting. Join us on this journey to become a proficient and sought-after tech professional in the field of data analytics and visualization.",
      "target_audience": [
        "Beginners with no prior programming experience.",
        "Students or professionals in various fields, including business, science, social sciences, and healthcare, who want to enhance their data analysis skills.",
        "Anyone interested in automating tasks or data analysis.",
        "Data analysts, researchers, and scientists seeking to strengthen their statistical foundations and Python programming skills.",
        "Beginners with no prior statistical knowledge but with a curiosity to learn and apply statistical methods.",
        "Professionals looking to advance their career by acquiring valuable statistical and data analysis skills."
      ]
    },
    {
      "title": "Data Manipulation in Python: A Pandas Crash Course",
      "url": "https://www.udemy.com/course/data-manipulation-in-python/",
      "bio": "Learn how to use Python and Pandas for data analysis and data manipulation. Transform, clean and merge data with Python.",
      "objectives": [
        "Visualise data using methods from histograms to dimensionality reduction.",
        "Create, save and serialise data frames in and out of multiple formats.",
        "Clean and format data easily.",
        "Detect and intelligently fill missing values.",
        "Group, aggregate and summarise your data.",
        "Merge data sources into a beautiful whole.",
        "Pivot and cross-tabulate data like a pro.",
        "Intersplice, summarise and investigate time series data.",
        "Seamlessly work with data from different time zones.",
        "Learn the common pitfalls and traps that ensnare beginners and how to avoid them."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Who Am I? And how to get help",
          "EXTRA: Learning Path",
          "Setting up python and editors",
          "Live Install",
          "Get the materials"
        ],
        "Dataset Basics": [
          "Finding Datasets",
          "Jupyter Notebooks and Loading Data",
          "Pandas vs Numpy",
          "Creating DataFrames",
          "Saving and Serialising",
          "Inspecting DataFrames"
        ],
        "Visual exploration": [
          "Introduction and super basic plots",
          "Pandas vs Matplotlib",
          "Visualising 1D distributions",
          "Visualising 2D distributions",
          "Styling Pandas Table outputs",
          "Higher dimension visualisations",
          "Summary"
        ],
        "Basic Data Manipulations": [
          "Introduction, Labelling and Ordering",
          "Slicing and Filtering",
          "Replacing and Thresholding",
          "Removing and adding data",
          "Apply, map and vectorised functions",
          "Summary"
        ],
        "Grouping": [
          "Introduction and motivation",
          "Basic grouping syntax",
          "Intelligent imputation",
          "Grouping aggregation",
          "Summary"
        ],
        "Merging": [
          "Introduction and basic syntax",
          "Different types of merging",
          "Helpful merging functions",
          "Summary"
        ],
        "Advanced Manipulation - MultiIndex, Pivoting and more": [
          "Introduction and basic MultiIndexes",
          "MultiIndex II - MultiIndex Strikes Back",
          "Stacking and Unstacking",
          "Pivoting",
          "Pivot Margins",
          "Crosstab",
          "Melting",
          "Summary"
        ],
        "Time Series Data": [
          "Introduction and the Datetime Index",
          "Reindexing",
          "Resampling",
          "Rolling functions",
          "Time Zones",
          "Summary"
        ],
        "Conclusion": [
          "A recap and a thank you",
          "Extra - Customising Jupyter Notebooks",
          "Extra - Chapter 2 Data Runthrough",
          "Extra - Chapter 3 Visualisation Runthrough",
          "Extra - Chapter 4 Basics Runthrough",
          "Extra - Chapter 5 Grouping Runthrough",
          "Extra - Chapter 6 Merging Runthrough",
          "Extra - Chapter 7 Advanced Runthrough",
          "Extra - Chapter 8 TimeSeries Runthrough"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic knowledge of Python"
      ],
      "description": "In the real-world, data is anything but clean, which is why Python libraries like Pandas are so valuable.\n\n\nIf data manipulation is setting your data analysis workflow behind then this course is the key to taking your power back.\n\n\nOwn your data, don’t let your data own you!\n\n\nWhen data manipulation and preparation accounts for up to 80% of your work as a data scientist, learning data munging techniques that take raw data to a final product for analysis as efficiently as possible is essential for success.\n\n\nData analysis with Python library Pandas makes it easier for you to achieve better results, increase your productivity, spend more time problem-solving and less time data-wrangling, and communicate your insights more effectively.\n\n\nThis course prepares you to do just that!\n\n\nWith Pandas DataFrame, prepare to learn advanced data manipulation, preparation, sorting, blending, and data cleaning approaches to turn chaotic bits of data into a final pre-analysis product. This is exactly why Pandas is the most popular Python library in data science and why data scientists at Google, Facebook, JP Morgan, and nearly every other major company that analyzes data use Pandas.\n\n\nIf you want to learn how to efficiently utilize Pandas to manipulate, transform, pivot, stack, merge and aggregate your data for preparation of visualization, statistical analysis, or machine learning, then this course is for you.\n\n\nHere’s what you can expect when you enrolled with your instructor, Ph.D. Samuel Hinton:\n\n\nLearn common and advanced Pandas data manipulation techniques to take raw data to a final product for analysis as efficiently as possible.\nAchieve better results by spending more time problem-solving and less time data-wrangling.\nLearn how to shape and manipulate data to make statistical analysis and machine learning as simple as possible.\nUtilize the latest version of Python and the industry-standard Pandas library.\nPerforming data analysis with Python’s Pandas library can help you do a lot, but it does have its downsides. And this course helps you beat them head-on:\n\n\n1. Pandas has a steep learning curve: As you dive deeper into the Pandas library, the learning slope becomes steeper and steeper. This course guides beginners and intermediate users smoothly into every aspect of Pandas.\n\n\n2. Inadequate documentation: Without proper documentation, it’s difficult to learn a new library. When it comes to advanced functions, Pandas documentation is rarely helpful. This course helps you grasp advanced Pandas techniques easily and saves you time in searching for help.\n\n\nAfter this course, you will feel comfortable delving into complex and heterogeneous datasets knowing with absolute confidence that you can produce a useful result for the next stage of data analysis.\n\n\nHere’s a closer look at the curriculum:\nLoading and creating Pandas DataFrames\nDisplaying your data with basic plots, and 1D, 2D and multidimensional visualizations.\nPerforming basic DataFrame manipulations: indexing, labeling, ordering slicing, filtering and more.\nPerforming advanced Pandas DataFrame manipulations: multiIndexing, stacking, hierarchical indexing, pivoting, melting and more.\nCarrying out DataFrame grouping: aggregation, imputation, and more.\nMastering time series manipulations: reindexing, resampling, rolling functions, method chaining and filtering, and more.\nMerging Pandas DataFrames\nLastly, this course is packed with a cheatsheet and practical exercises that are based on real-life examples. So not only will you learn the theory, but you will also get some hands-on practice with Pandas too.",
      "target_audience": [
        "Python students that want to learn how to manipulate data professionally.",
        "Aspiring data analysts and scientists looking to upgrade their skillset.",
        "People who would prefer to spend more time solving interesting problems than formatting data.",
        "Old hands at programming that want to see what new methods and industry-leading tools are at their fingertips in the new decade."
      ]
    },
    {
      "title": "Data Architecture for Data Scientists",
      "url": "https://www.udemy.com/course/data-architecture-for-data-scientists/",
      "bio": "Datawarehouse, Data Lake, Data Lakehouse, Data Mesh, Kafka, Lambda & Kappa architecture, Feature Store, Vector DB & more",
      "objectives": [
        "Data Architecture in general, to be able to navigate your organizations data landscape",
        "Develop understanding of topics like Data Lake, Datawarehousing and even Data Lakehouse to be able to communicate with data engineering teams",
        "Understand the pricinciples of data governance topics like Data Mesh to better navigate the data governance paradigm",
        "Get introduced to technologies related to machine learning specific data infrastructure like feature stores and vector databases",
        "What is data architecture? What is a data warehouse (DWH) ? What is data lake? What is data lakehouse? What is data mesh?",
        "How is streaming data used in data science? What is a feature store? How is a feature store used in machine learning? What are vector databases??"
      ],
      "course_content": {
        "Introduction": [
          "Why enroll in this course?",
          "Course contents",
          "About the course creator",
          "Million dollar slide"
        ],
        "Data Types": [
          "Structured data",
          "Unstructured data",
          "Semi-structured data",
          "Short explanation of JSON and XML structures",
          "Semi-structured data in machine learning",
          "Module review quiz - Data Types",
          "Resources and Slides"
        ],
        "Datawarehouse": [
          "Introduction to datawarehousing",
          "Datawarehousing for data scientists",
          "Cloud datawarehousing",
          "Module review quiz - Datawarehousing",
          "Resources and Slides"
        ],
        "Data Lake": [
          "Introduction to a data lake",
          "The technology used to build a data lake",
          "Cloud Storage terminology - Buckets and blobs",
          "Module review quiz - Datalake",
          "Resources and Slides"
        ],
        "Data Lakehouse": [
          "Challenges with the data lake",
          "Introduction to the data lakehouse",
          "Module review quiz - Datalakehouse",
          "Resources and Slides"
        ],
        "Data Governance with the Data Mesh": [
          "Introduction to Data Mesh",
          "Data Mesh principles : Domain ownership and data as a product",
          "Data Mesh principles : Self service and federated governance",
          "Data Catalog",
          "Data Contracts",
          "Data Fabric",
          "Module review quiz - Data Governance with the Data Mesh",
          "Resources and Slides"
        ],
        "Streaming data in Data Science": [
          "Introduction to streaming data",
          "Kafka 101",
          "Lambda architecture",
          "Kappa architecture and comparison",
          "Word of caution and Resources",
          "Module review quiz - Streaming data in Data Science"
        ],
        "Data infrastructure for Machine Learning": [
          "Feature Store",
          "Vector Database",
          "Module review quiz - Data Infrastructure for Machine Learning"
        ],
        "Flowchart and Use case examples": [
          "Data Architecture decision making flowchart",
          "Use case examples and applying the decision flow"
        ]
      },
      "requirements": [
        "Basic understanding of data science project workflow like model training and model deployment",
        "Basic understanding of why data is needed for training and deploying models",
        "Understanding of the difference between batch and real time use cases"
      ],
      "description": "Machine learning models are only as good as the data they are trained on, which is why understanding data architecture is critical for data scientists building machine learning models.\nThis course will teach you:\nThe fundamentals of data architecture\nA refresher on data types, including structured, unstructured, and semi-structured data\nDataWarehouse Fundamentals\nData Lake Fundamentals\nThe differences between data warehouses and data lakes\nDataLakehouse Fundamentals\nData Mesh fundamentals for decentralized governance of data including topics like data catalog, data contracts and data fabric.\nThe challenges of incorporating streaming data in data science\nSome machine learning-specific data infrastructure, such as feature stores and vector databases\nThe course will help you:\nMake informed decisions about the architecture of your data infrastructure to improve the accuracy and effectiveness of your models\nAdopt modern technologies and practices to improve workflows\nDevelop a better understanding and empathy for data engineers\nImprove your reputation as an all-around data scientist\nThink of data architecture as the framework that supports the construction of a machine learning model. Just as a building needs a strong framework to support its structure, a machine learning model needs a solid data architecture to support its accuracy and effectiveness. Without a strong framework, the building is at risk of collapsing, and without a strong data architecture, machine learning models are at risk of producing inaccurate or biased results. By understanding the principles of data architecture, data scientists can ensure that their data infrastructure is robust, reliable, and capable of supporting the training and deployment of accurate and effective machine learning models.\nBy the end of this course, you'll have the knowledge to help guide your team and organization in creating the right data architecture for deploying data science use cases.",
      "target_audience": [
        "Data Scientists who are transitioning from academia or business domains",
        "Junior data scientists who would like to understand the topics surrounding data infrastructure",
        "Citizen data scientists who wish to deploy machine learning models in production",
        "Anyone who wishes to learn the basics of data architecture in a very short time",
        "BI Analysts and BI developers who would like a quick overview of the enterprise data landscape",
        "Folks who wish to get a quick overview of data architecture components in an enterprise."
      ]
    },
    {
      "title": "Machine Learning Practical: 6 Real-World Applications",
      "url": "https://www.udemy.com/course/machine-learning-practical/",
      "bio": "Machine Learning - Get Your Hands Dirty by Solving Real Industry Challenges with Python",
      "objectives": [
        "You will know how real data science project looks like",
        "You will be able to include these Case Studies in your resume",
        "You will be able better market yourself as a Machine Learning Practioneer",
        "You will feel confident during Data Science interview",
        "You will learn how to chain multiple ML algorithms together to achieve the goal",
        "You will learn most advanced Data Visualization techniques with Seaborn and Matplotlib",
        "You will learn Logistic Regression",
        "You will learn L1 Regularization (Lasso)",
        "You will learn Random Forest Classifier"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course!",
          "Learning Paths",
          "Where to get the materials"
        ],
        "Breast Cancer Classification": [
          "Introduction",
          "Business Challenge",
          "Challenge in Machine Learning Vocabulary",
          "Data Visualisation",
          "Model Training",
          "Model Evaluation",
          "Improving the Model",
          "Conclusion"
        ],
        "Fashion Class Classification": [
          "Business Challenge",
          "Challenge in Machine Learning Vocabulary",
          "Data Visualisation",
          "Model Training Part I",
          "Model Training Part II",
          "Model Training Part III",
          "Model Training Part IV",
          "Model Evaluation",
          "Improving the Model",
          "Conclusion"
        ],
        "Directing Customers to Subscription Through App Behavior Analysis": [
          "Fintech Case Studies Introduction",
          "Introduction",
          "Data",
          "Features Histograms",
          "Correlation Plot",
          "Correlation Matrix",
          "Feature Engineering - Response",
          "Feature Engineering - Screens",
          "Data Pre-Processing",
          "Model Building",
          "Model Conclusion",
          "Final Remarks"
        ],
        "Minimizing Churn Rate Through Analysis of Financial Habits": [
          "Introduction",
          "Data",
          "Data Cleaning",
          "Features Histograms",
          "Pie Chart Distributions",
          "Correlation Plot",
          "Correlation Matrix",
          "One-Hot Encoding",
          "Feature Scaling & Balancing",
          "Model Building",
          "K-Fold Cross Validation",
          "Feature Selection",
          "Model Conclusion",
          "Final Remarks"
        ],
        "Predicting the Likelihood of E-Signing a Loan Based on Financial History": [
          "Introduction",
          "Data",
          "Data Housekeeping",
          "Histograms",
          "Correlation Plot",
          "Correlation Matrix",
          "Feature Engineering",
          "Data Preprocessing",
          "Model Building Part 1",
          "Model Building Part 2",
          "Grid Search Part 1",
          "Grid Search Part 2",
          "Model Conclusion",
          "Final Remarks"
        ],
        "Credit Card Fraud Detection": [
          "Case Study",
          "Machine Learning Vocabulary",
          "Set Up",
          "Data Visualization",
          "Data Preprocessing",
          "Deep Learning Part 1",
          "Deep Learning Part 2",
          "Splitting the Data",
          "Training",
          "Metrics",
          "Confusion Matrix",
          "Machine Learning Classifiers",
          "Random Forest",
          "Decision Trees",
          "Sampling",
          "Undersampling",
          "Smote",
          "Final remarks",
          "THANK YOU Video"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "You need to know Python (Machine Learning A-Z level is enough) in order to complete this course.",
        "You need to know how to set up your working environment (Anaconda, Jupyter Notebook, Spyder)",
        "This should not be your first Machine Learning course. You need to understand main concepts."
      ],
      "description": "So you know the theory of Machine Learning and know how to create your first algorithms. Now what?\nThere are tons of courses out there about the underlying theory of Machine Learning which don’t go any deeper – into the applications.\n\nThis course is not one of them.\nAre you ready to apply all of the theory and knowledge to real life Machine Learning challenges?\nThen welcome to “Machine Learning Practical”.\n\n\nWe gathered best industry professionals with tons of completed projects behind.\nEach presenter has a unique style, which is determined by his experience, and like in a real world, you will need adjust to it if you want successfully complete this course. We will leave no one behind!\n\n\nThis course will demystify how real Data Science project looks like. Time to move away from these polished examples which are only introducing you to the matter, but not giving any real experience.\n\n\nIf you are still dreaming where to learn Machine Learning through practice, where to take real-life projects for your CV, how to not look like a noob in the recruiter's eyes, then you came to the right place!\n\n\nThis course provides a hands-on approach to real-life challenges and covers exactly what you need to succeed in the real world of Data Science.\nThere are most exciting case studies including:\n●      diagnosing diabetes in the early stages\n●      directing customers to subscription products with app usage analysis\n●      minimizing churn rate in finance\n●      predicting customer location with GPS data\n●      forecasting future currency exchange rates\n●      classifying fashion\n●      predicting breast cancer\n●      and much more!\nAll real.\nAll true.\nAll helpful and applicable.\nAnd another extra:\nIn this course we will also cover Deep Learning Techniques and their practical applications.\nSo as you can see, our goal here is to really build the World’s leading practical machine learning course.\nIf your goal is to become a Machine Learning expert, you know how valuable these real-life examples really are.\nThey will determine the difference between Data Scientists who just know the theory and Machine Learning experts who have gotten their hands dirty.\nSo if you want to get hands-on experience which you can add to your portfolio, then this course is for you.\nEnroll now and we’ll see you inside.",
      "target_audience": [
        "Data Science and Machine Learning enthusiasts who want to understand how real data science projects look like.",
        "Anyone with Machine Learning and Python knowledge who wants to practice their skills"
      ]
    },
    {
      "title": "Clustering & Classification With Machine Learning In Python",
      "url": "https://www.udemy.com/course/clustering-classification-with-machine-learning-in-python/",
      "bio": "Harness The Power Of Machine Learning For Unsupervised & Supervised Learning In Python",
      "objectives": [
        "Harness The Power Of Anaconda/iPython For Practical Data Science",
        "Read In Data Into The Python Environment From Different Sources",
        "Carry Out Basic Data Pre-processing & Wrangling In Python",
        "Implement Unsupervised/Clustering Techniques Such As k-means Clustering",
        "Implement Dimensional Reduction Techniques (PCA) & Feature Selection",
        "Implement Supervised Learning Techniques/Classification Such As Random Forests In Python",
        "Neural Network & Deep Learning Based Classification"
      ],
      "course_content": {
        "INTRODUCTION TO THE COURSE: The Key Concepts and Software Tools": [
          "Welcome to Clustering & Classification with Machine Learning in Python",
          "What is Machine Learning?",
          "Data and Scripts For the Course",
          "Python Data Science Environment",
          "For Mac Users",
          "Introduction to IPython",
          "IPython in Browser",
          "Python Data Science Packages To Be Used"
        ],
        "Read in Data From Different Sources With Pandas": [
          "What are Pandas?",
          "Read in Data from CSV",
          "Read in Online CSV",
          "Read in Excel Data",
          "Read in HTML Data",
          "Read in Data from Databases"
        ],
        "Data Cleaning & Munging": [
          "Remove Missing Values",
          "Conditional Data Selection",
          "Data Grouping",
          "Data Subsetting",
          "Ranking & Sorting",
          "Concatenate",
          "Merging & Joining Data Frames"
        ],
        "Unsupervised Learning in Python": [
          "Unsupervised Classification- Some Basic Concepts",
          "K-Means Clustering:Theory",
          "Implement K-Means on the Iris Data",
          "Quantifying K-Means Clustering Performance",
          "K-Means Clustering with Real Data",
          "How To Select the Optimal Number of Clusters?",
          "Gaussian Mixture Modelling (GMM)",
          "Hierarchical Clustering-theory",
          "Hierarchical Clustering-practical"
        ],
        "Dimension Reduction & Feature Selection for Machine Learning": [
          "Principal Component Analysis (PCA)-Theory",
          "Principal Component Analysis (PCA)-Case Study 1",
          "Principal Component Analysis (PCA)-Case Study 2",
          "Linear Discriminant Analysis(LDA) for Dimension Reduction",
          "t-SNE Dimension Reduction",
          "Feature Selection to Select the Most Relevant Predictors",
          "Recursive Feature Elimination (RFE)"
        ],
        "Supervised Learning: Classification": [
          "Concepts Behind Supervised Learning",
          "Data Preparation for Supervised Learning",
          "Pointers on Evaluating the Accuracy of Classification Modelling",
          "Using Logistic Regression as a Classification Model",
          "kNN- Classification",
          "Naive Bayes Classification",
          "Linear Discriminant Analysis",
          "SVM- Linear Classification",
          "Non-Linear SVM Classification",
          "RF-Classification",
          "Gradient Boosting Machine (GBM)",
          "Voting Classifier"
        ],
        "Neural Networks and Deep Learning Based Classification Techniques": [
          "Perceptrons for Binary Classification",
          "Artificial Neural Networks (ANN) for Binary Classification",
          "Multi-class Classification With MLP",
          "Introduction to H20",
          "Use H20 for Deep Learning Classification",
          "Specify the Activation Function",
          "H20 Deep Learning for Classification"
        ],
        "Miscellaneous Information": [
          "Using Colabs for Online Jupyter Notebooks",
          "Colab GPU",
          "Github",
          "What Is Data Science?"
        ]
      },
      "requirements": [
        "Be Able To Operate & Install Software On A Computer",
        "Prior Exposure To Common Machine Learning Terms Such As Unsupervised & Supervised Learning"
      ],
      "description": "HERE IS WHY YOU SHOULD TAKE THIS COURSE:\nThis course your complete guide to both supervised & unsupervised learning using Python. This means, this course covers all the main aspects of practical data science and if you take this course, you can do away with taking other courses or buying books on Python based data science.\nIn this age of big data, companies across the globe use Python to sift through the avalanche of information at their disposal..\nBy becoming proficient in unsupervised & supervised learning in Python, you can give your company a competitive edge and boost your career to the next level.\nLEARN FROM AN EXPERT DATA SCIENTIST WITH +5 YEARS OF EXPERIENCE:\nMy name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I also just recently finished a PhD at Cambridge University.\nI have several years of experience in analyzing real life data from different sources  using data science techniques and producing publications for international peer reviewed journals.\nOver the course of my research I realized almost all the Python data science courses and books out there do not account for the multidimensional nature of the topic .\nThis course will give you a robust grounding in the main aspects of machine learning- clustering & classification.\nUnlike other Python instructors, I dig deep into the machine learning features of Python and gives you a one-of-a-kind grounding in Python Data Science!\nYou will go all the way from carrying out data reading & cleaning  to machine learning to finally implementing simple deep learning based models using Python\nTHE COURSE COMPOSES OF 7 SECTIONS TO HELP YOU MASTER PYTHON MACHINE LEARNING:\n• A full introduction to Python Data Science and powerful Python driven framework for data science, Anaconda • Getting started with Jupyter notebooks for implementing data science techniques in Python  • Data Structures and Reading in Pandas, including CSV, Excel and HTML data • How to Pre-Process and “Wrangle” your Python data by removing NAs/No data, handling conditional data, grouping by attributes, etc.\n• Machine Learning, Supervised Learning, Unsupervised Learning in Python\n• Artificial neural networks (ANN) and Deep Learning. You’ll even discover how to use artificial neural networks and deep learning structures for classification!\nWith such a rigorous grounding in so many topics, you will be an unbeatable data scientist by the end of the course.\nNO PRIOR PYTHON OR STATISTICS OR MACHINE LEARNING KNOWLEDGE IS REQUIRED:\nYou’ll start by absorbing the most valuable Python Data Science basics and techniques.\nI use easy-to-understand, hands-on methods to simplify and address even the most difficult concepts in Python.\nMy course will help you implement the methods using real data obtained from different sources.\nAfter taking this course, you’ll easily use packages like Numpy, Pandas, and Matplotlib to work with real data in Python..\nYou’ll even understand concepts like unsupervised learning, dimension reduction and supervised learning.. I will even introduce you to deep learning and neural networks using the powerful H2o framework!\nMost importantly, you will learn to implement these techniques practically using Python. You will have access to all the data and scripts used in this course. Remember, I am always around to support my students!\nJOIN MY COURSE NOW!",
      "target_audience": [
        "Students Interested In Getting Started With Data Science Applications In The Python Environment",
        "People Wanting To Master The Anaconda iPython Environment For Data Science & Scientific Computations",
        "Students Wishing To Learn The Implementation Of Unsupervised Learning On Real Data Using Python",
        "Students Wishing To Learn The Implementation Of Supervised Learning (Classification) On Real Data Using Python",
        "Students Looking To Get Started With Artificial Neural Networks & Deep Learning"
      ]
    },
    {
      "title": "The AI Engineer Course 2025: Complete AI Engineer Bootcamp",
      "url": "https://www.udemy.com/course/the-ai-engineer-course-complete-ai-engineer-bootcamp/",
      "bio": "Complete AI Engineer Training: Python, NLP, Transformers, LLMs, LangChain, Hugging Face, APIs",
      "objectives": [
        "The course provides the entire toolbox you need to become an AI Engineer",
        "Understand key Artificial Intelligence concepts and build a solid foundation",
        "Start coding in Python and learn how to use it for NLP and AI",
        "Impress interviewers by showing an understanding of the AI field",
        "Apply your skills to real-life business cases",
        "Harness the power of Large Language Models",
        "Leverage LangChain for seamless development of AI-driven applications by chaining interoperable components",
        "Become familiar with Hugging Face and the AI tools it offers",
        "Use APIs and connect to powerful foundation models",
        "Utilize Transformers for advanced speech-to-text"
      ],
      "course_content": {
        "Intro to AI Module: Getting started": [
          "Building an AI tool in 5 minutes: A quick demo",
          "What does the course cover",
          "Natural vs Artificial Intelligence",
          "Brief history of AI",
          "Demystifying AI, Data science, Machine learning, and Deep learning",
          "Weak vs Strong AI",
          "Quiz 1"
        ],
        "Intro to AI Module: Data is essential for building AI": [
          "Structured vs unstructured data",
          "How we collect data",
          "Labelled and unlabelled data",
          "Metadata: Data that describes data",
          "Quiz 2"
        ],
        "Intro to AI Module: Key AI techniques": [
          "Machine learning",
          "Supervised, Unsupervised, and Reinforcement learning",
          "Deep learning",
          "Quiz 3"
        ],
        "Intro to AI Module: Important AI branches": [
          "Robotics",
          "Computer vision",
          "Traditional ML",
          "Generative AI",
          "Quiz 4"
        ],
        "Intro to AI Module: Understanding Generative AI": [
          "The rise of Gen AI: Introducing ChatGPT",
          "Early approaches to Natural Language Processing (NLP)",
          "Recent NLP advancements",
          "From Language Models to Large Language Models (LLMs)",
          "The efficiency of LLM training. Supervised vs Semi-supervised learning",
          "From N-Grams to RNNs to Transformers: The Evolution of NLP",
          "Phases in building LLMs",
          "Prompt engineering vs Fine-tuning vs RAG: Techniques for AI optimization",
          "The importance of foundation models",
          "Buy vs Make: foundation models vs private models"
        ],
        "Intro to AI Module: Practical challenges in Generative AI": [
          "Inconsistency and hallucination",
          "Budgeting and API costs",
          "Latency",
          "Running out of data"
        ],
        "Intro to AI Module: The AI tech stack": [
          "Python programming",
          "Working with APIs",
          "Vector databases",
          "The importance of open source",
          "Hugging Face",
          "LangChain",
          "AI evaluation tools"
        ],
        "Intro to AI Module: AI job positions": [
          "AI strategist",
          "AI developer",
          "AI engineer"
        ],
        "Intro to AI Module: Looking ahead": [
          "AI ethics",
          "Future of AI"
        ],
        "Python Module: Why Python?": [
          "Programming Explained in a Few Minutes",
          "Why Python"
        ]
      },
      "requirements": [
        "No prior experience is required. We will start from the very basics",
        "You’ll need to install Anaconda. We will show you how to do that step by step"
      ],
      "description": "The Problem\nAI Engineers are best suited to thrive in the age of AI. It helps businesses utilize Generative AI by building AI-driven applications on top of their existing websites, apps, and databases. Therefore, it’s no surprise that the demand for AI Engineers has been surging in the job marketplace.\nSupply, however, has been minimal, and acquiring the skills necessary to be hired as an AI Engineer can be challenging.\nSo, how is this achievable?\nUniversities have been slow to create specialized programs focused on practical AI Engineering skills. The few attempts that exist tend to be costly and time-consuming.\nMost online courses offer ChatGPT hacks and isolated technical skills, yet integrating these skills remains challenging.\nThe Solution\nAI Engineering is a multidisciplinary field covering:\nAI principles and practical applications\nPython programming\nNatural Language Processing in Python\nLarge Language Models and Transformers\nDeveloping apps with orchestration tools like LangChain\nVector databases using PineCone\nCreating AI-driven applications\nEach topic builds on the previous one, and skipping steps can lead to confusion. For instance, applying large language models requires familiarity with Langchain—just as studying natural language processing can be overwhelming without basic Python coding skills.\nSo, we created the AI Engineer Bootcamp 2024 to provide the most effective, time-efficient, and structured AI engineering training available online.\nThis pioneering training program overcomes the most significant barrier to entering the AI Engineering field by consolidating all essential resources in one place.\nOur course is designed to teach interconnected topics seamlessly—providing all you need to become an AI Engineer at a significantly lower cost and time investment than traditional programs.\nThe Skills\n1. Intro to Artificial Intelligence\nStructured and unstructured data, supervised and unsupervised machine learning, Generative AI, and foundational models—these familiar AI buzzwords; what exactly do they mean?\nWhy study AI? Gain deep insights into the field through a guided exploration that covers AI fundamentals, the significance of quality data, essential techniques, Generative AI, and the development of advanced models like GPT, Llama, Gemini, and Claude.\n2. Python Programming\nMastering Python programming is essential to becoming a skilled AI developer—no-code tools are insufficient.\nPython is a modern, general-purpose programming language suited for creating web applications, computer games, and data science tasks. Its extensive library ecosystem makes it ideal for developing AI models.\nWhy study Python programming?\nPython programming will become your essential tool for communicating with AI models and integrating their capabilities into your products.\n3. Intro to NLP in Python\nExplore Natural Language Processing (NLP) and learn techniques that empower computers to comprehend, generate, and categorize human language.\nWhy study NLP?\nNLP forms the basis of cutting-edge Generative AI models. This program equips you with essential skills to develop AI systems that meaningfully interact with human language.\n4. Introduction to Large Language Models\nThis program section enhances your natural language processing skills by teaching you to utilize the powerful capabilities of Large Language Models (LLMs). Learn critical tools like Transformers Architecture, GPT, Langchain, HuggingFace, BERT, and XLNet.\nWhy study LLMs?\nThis module is your gateway to understanding how large language models work and how they can be applied to solve complex language-related tasks that require deep contextual understanding.\n5. Building Applications with LangChain\nLangChain is a framework that allows for seamless development of AI-driven applications by chaining interoperable components.\nWhy study LangChain?\nLearn how to create applications that can reason. LangChain facilitates the creation of systems where individual pieces—such as language models, databases, and reasoning algorithms—can be interconnected to enhance overall functionality.\n6. Vector Databases\nWith emerging AI technologies, the importance of vectorization and vector databases is set to increase significantly. In this Vector Databases with Pinecone module, you’ll have the opportunity to explore the Pinecone database—a leading vector database solution.\nWhy study vector databases?\nLearning about vector databases is crucial because it equips you to efficiently manage and query large volumes of high-dimensional data—typical in machine learning and AI applications. These technical skills allow you to deploy performance-optimized AI-driven applications.\n7. Speech Recognition with Python\nDive into the fascinating field of Speech Recognition and discover how AI systems transform spoken language into actionable insights. This module covers foundational concepts such as audio processing, acoustic modeling, and advanced techniques for building speech-to-text applications using Python.\nWhy study speech recognition?\nSpeech Recognition is at the core of voice assistants, automated transcription tools, and voice-driven interfaces. Mastering this skill enables you to create applications that interact with users naturally and unlock the full potential of audio data in AI solutions.\nWhat You Get\n$1,250 AI Engineering training program\nActive Q&A support\nEssential skills for AI engineering employment\nAI learner community access\nCompletion certificate\nFuture updates\nReal-world business case solutions for job readiness\nWe're excited to help you become an AI Engineer from scratch—offering an unconditional 30-day full money-back guarantee.\nWith excellent course content and no risk involved, we're confident you'll love it.\nWhy delay? Each day is a lost opportunity. Click the ‘Buy Now’ button and join our AI Engineer program today.",
      "target_audience": [
        "You should take this course if you want to become an AI Engineer or if you want to learn about the field",
        "This course is for you if you want a great career",
        "The course is also ideal for beginners, as it starts from the fundamentals and gradually builds up your skills"
      ]
    },
    {
      "title": "Zero to Hero in Ollama: Create Local LLM Applications",
      "url": "https://www.udemy.com/course/ollama-starttech/",
      "bio": "Run customized LLM models on your system privately | Use ChatGPT like interface | Build local applications using Python",
      "objectives": [
        "Install and configure Ollama on your local system to run large language models privately.",
        "Customize LLM models to suit specific needs using Ollama’s options and command-line tools.",
        "Execute all terminal commands necessary to control, monitor, and troubleshoot Ollama models",
        "Set up and manage a ChatGPT-like interface using Open WebUI, allowing you to interact with models locally",
        "Deploy Docker and Open WebUI for running, customizing, and sharing LLM models in a private environment.",
        "Utilize different model types, including text, vision, and code-generating models, for various applications.",
        "Create custom LLM models from a gguf file and integrate them into your applications.",
        "Build Python applications that interface with Ollama models using its native library and OpenAI API compatibility.",
        "Develop a RAG (Retrieval-Augmented Generation) application by integrating Ollama models with LangChain.",
        "Implement tools and agents to enhance model interactions in both Open WebUI and LangChain environments for advanced workflows."
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Ollama course",
          "Installing and Setting up Ollama",
          "This is a milestone",
          "Model customizations and other options",
          "All Ollama Command Prompt/ Terminal commands",
          "Quiz"
        ],
        "Open WebUI - ChatGPT like interface for Ollama models": [
          "Introduction to Open WebUI",
          "Setting up Docker and Open WebUI",
          "Open WebUI features and functionalities",
          "Getting response based on documents and websites",
          "Open WebUI user access control",
          "Quiz"
        ],
        "Types of Ollama Models and their capabilities": [
          "Types of Ollama models",
          "Text models available in Ollama",
          "Vision models available in Ollama",
          "Code generating models available in Ollama",
          "Create custom model from gguf file"
        ],
        "Using Ollama with Python": [
          "Installing and Setting up Python environment",
          "Using Ollama in Python using Ollama library",
          "Calling Ollama Model using API and OpenAI compatibility"
        ],
        "Using Ollama with LangChain in Python": [
          "What is LangChain and why are we using it?",
          "Basic modules of Langchain",
          "Quiz"
        ],
        "Creating RAG application using Ollama and LangChain": [
          "Understanding the concept of RAG (Retrieval Augmented Generation)",
          "Loading, Chunking and Embedding document using LangChain and Ollama",
          "Answering user question with retrieved information",
          "Quiz"
        ],
        "Using Tools and Agents with Ollama models": [
          "Understanding Tools and Agents",
          "Tools and Agents using LangChain and Llama3.1",
          "Quiz",
          "The final milestone!",
          "About the upcoming roleplay",
          "Interview Practice"
        ],
        "Conclusion": [
          "About your certificate",
          "Bonus lecture"
        ]
      },
      "requirements": [
        "Basic Python knowledge and a computer capable of running Docker and Ollama are recommended, but no prior AI experience is required."
      ],
      "description": "Are you looking to build and run customized large language models (LLMs) right on your own system, without depending on cloud solutions? Do you want to maintain privacy while leveraging powerful models similar to ChatGPT? If you're a developer, data scientist, or an AI enthusiast wanting to create local LLM applications, this course is for you!\nThis hands-on course will take you from beginner to expert in using Ollama, a platform designed for running local LLM models. You’ll learn how to set up and customize models, create a ChatGPT-like interface, and build private applications using Python—all from the comfort of your own system.\nIn this course, you will:\nInstall and customize Ollama for local LLM model execution\nMaster all command-line tools to effectively control Ollama\nRun a ChatGPT-like interface on your system using Open WebUI\nIntegrate various models (text, vision, code generation) and even create your own custom models\nBuild Python applications using Ollama and its library, with OpenAI API compatibility\nLeverage LangChain to enhance your LLM capabilities, including Retrieval-Augmented Generation (RAG)\nDeploy tools and agents to interact with Ollama models in both terminal and LangChain environments\nWhy is this course important? In a world where privacy is becoming a greater concern, running LLMs locally ensures your data stays on your machine. This not only improves data security but also allows you to customize models for specialized tasks without external dependencies.\nYou’ll complete activities like building custom models, setting up Docker for web interfaces, and developing RAG applications that retrieve and respond to user queries based on your data. Each section is packed with real-world applications to give you the experience and confidence to build your own local LLM solutions.\nWhy this course? I specialize in making advanced AI topics practical and accessible, with hands-on projects that ensure you’re not just learning but actually building real solutions. Whether you’re new to LLMs or looking to deepen your skills, this course will equip you with everything you need.\nReady to build your own LLM-powered applications privately? Enroll now and take full control of your AI journey with Ollama!",
      "target_audience": [
        "AI enthusiasts who want to build and run customized LLM models privately on their local systems.",
        "Python developers seeking to integrate large language models into local applications for enhanced functionality.",
        "Data scientists who aim to create secure, private LLM-powered tools without relying on cloud-based solutions.",
        "Machine learning engineers looking to explore and customize open-source models using Ollama and LangChain.",
        "Tech professionals who want to develop RAG (Retrieval-Augmented Generation) applications using local data.",
        "Privacy-conscious developers interested in running AI models with full control over data and environment."
      ]
    },
    {
      "title": "Deploy Machine Learning Models on GCP + AWS Lambda (Docker)",
      "url": "https://www.udemy.com/course/deploy-machine-learning-model/",
      "bio": "How to Serialize - Deserialize model with scikit-learn & Deployment on Heroku, AWS Lambda, ECS, Docker and Google Cloud",
      "objectives": [
        "Model Deployment Process",
        "Different option available for Model Deployment",
        "Deploy Scikit-learn, Tensorflow 2.0 Model with Flask Web Framework",
        "Deploy Model on Google cloud function, App engine",
        "Serve model through Google AI Platform",
        "Run Prediction API on Heroku Cloud",
        "Serialize and Deserialize model through Scikit-learn and Tensorflow",
        "Deploying model on Amazon AWS Lambda",
        "Install Flower prediction model with Docker",
        "Deploy Docker Container on Amazon Container Services (ECS)"
      ],
      "course_content": {
        "Introduction": [
          "Disclaimer",
          "Deployment Overview",
          "reviews",
          "Course - FAQ",
          "Join Online Classroom",
          "Machine Learning Workflow",
          "Different Model Deployment Option"
        ],
        "Code Download": [
          "Code Download"
        ],
        "Flask Basics": [
          "Introduction to Flask & Setup environment",
          "Download and Install Anaconda",
          "Create Virtual environment",
          "Install Library",
          "Spyder IDE",
          "Flask Introduction",
          "(Hands-on) Flask Hello World",
          "(Hands-on) Flask Web app - With parameter",
          "Quiz"
        ],
        "Deploying machine learning (Sci-kit Learn) model to Flask": [
          "Section : Introduction",
          "Data Preparation & Create Model",
          "(Hands-on) Serialize & Deserialize Scikit-learn Model",
          "(Hands-on) Deploying model to Flask Web application",
          "Test Webservice through Postman +Python requests"
        ],
        "Model Serialization with Tensorflow 2.0": [
          "Build Neural Network Model - keras (Tensorflow 2.0)",
          "(Hands-on) Serialize and Deserialize model"
        ],
        "-------- Deploy model on Heroku Cloud --------": [
          "(Hands-on) Deploy Flower Classification Model on Heroku",
          "Heroku"
        ],
        "------- Deploy Model on Google Cloud ----------": [
          "Section : Introduction",
          "Google cloud Introduction",
          "(Hands-on) Upload Model on Google Cloud Storage",
          "(Hands-on) Deploy model on Google app engine",
          "(Hands-on) Deploy model on Google cloud Functions",
          "(Hands-on) Deploy Model on Google AI cloud",
          "Quiz"
        ],
        "----- Deploy Model on AWS Lambda -----": [
          "AWS Lambda : ML Model Deployment"
        ],
        "From Windows Machine": [
          "AWS Lambda : Hello World Function Part - 1",
          "AWS Lambda Introduction : Hello World Part - 2",
          "Model Packaging",
          "Corrections",
          "Upload Package to Amazon S3",
          "Deploy Package on AWS Lambda and Test"
        ],
        "From Linux Machine with serverless": [
          "Section : Introduction",
          "Linux (UBUNTU) installation",
          "Install Serverless Framework",
          "Creating AWS user Credentials",
          "Install Miniconda",
          "Create serverless Project",
          "Deploy artifacts on AWS Lambda and Test"
        ]
      },
      "requirements": [
        "Basics of Python Programming",
        "Basic knowledge of Web development"
      ],
      "description": "Disclaimer :\nThis course requires you to download Anaconda and Docker Desktop from their official websites. If you are a Udemy Business user, please check with your employer before downloading any software to ensure compliance with your organization’s policies.\n\n\nHello everyone, welcome to one of the most practical course on Machine learning and Deep learning model deployment production level.\nWhat is model deployment :\nLet's say you have a model after doing some rigorous training on your data set. But now what to do with this model. You have tested your model with testing data set that's fine. You got very good accuracy also with this model. But real test will come when live data will hit your model. So This course is about How to serialize your model and deployed on server.\nAfter attending this course :\nyou will be able to deploy a model on a cloud server.\nYou will be ahead one step in a machine learning journey.\nYou will be able to add one more machine learning skill in your resume.\nWhat is going to cover in this course?\n1.  Course Introduction\nIn this section I will teach you about what is model deployment basic idea about machine learning system design workflow and different deployment options are available at a cloud level.\n2. Flask Crash course\nIn this section you will learn about crash course on flask for those of you who is not familiar with flask framework as we are going to deploy model with the help of this flask web development framework available in Python.\n3.  Model Deployment with Flask\nIn this section you will learn how to Serialize and Deserialize scikit-learn model and will deploy owner flask based Web services. For testing Web API we will use Postman API testing tool and Python requests module.\n4. Serialize Deep Learning Tensorflow Model\nIn this section you will learn how to serialize and deserialize keras model on Fashion MNIST Dataset.\n5.  Deploy on Heroku cloud\nIn this section you will learn how to deploy already serialized flower classification data set model which we have created in a last section will deploy on Heroku cloud - Pass solution.\n6.  Deploy on Google cloud\nIn this section you will learn how to deploy model on different Google cloud services like Google Cloud function, Google app engine and Google managed AI cloud.\n7.  Deploy on Amazon AWS Lambda\nIn this section, you will learn how to deploy flower classification model on AWS lambda function.\n8.  Deploy on Amazon AWS ECS with Docker Container\nIn This section, we will see how to put application inside docker container and deploy it inside Amazon ECS (Elastic Container Services)\n\n\nThis course comes with 30 days money back guarantee. No question ask. So what are you waiting for just enroll it today.\nI will see you inside class.\nHappy learning\nAnkit Mistry",
      "target_audience": [
        "Anyone who knows ML and want to move towards Model deployment",
        "Anyone who want to know how to put Machine Learning app into production"
      ]
    },
    {
      "title": "Tensorflow Deep Learning - Data Science in Python",
      "url": "https://www.udemy.com/course/tensorflow-bootcamp-for-data-science-in-python/",
      "bio": "Tensorflow Deep Learning Python : Tensorflow Neural Network Training : Tensorflow Models - Android Java : Tensorflow C#",
      "objectives": [
        "Harness The Power Of Anaconda/iPython For Practical Data Science",
        "Learn How To Install & Use Tensorflow Within Anaconda",
        "Implement Statistical & Machine Learning With Tensorflow",
        "Implement Neural Network Modelling With Tensorflow",
        "Implement Deep Learning Based Unsupervised Learning With Tensorflow",
        "Implement Deep Learning Based Supervised Learning With Tensorflow"
      ],
      "course_content": {
        "INTRODUCTION TO TENSORFLOW : The Key Concepts and Software Tools": [
          "Welcome to the World of TensorFlow",
          "Introduction to the Course",
          "Data and Scripts For the Course",
          "What is Artificial Intelligence?",
          "Python Data Science Environment",
          "For Mac Users",
          "Introduction to IPython",
          "IPython in Browser",
          "Install Tensorflow",
          "Written Tensorflow Installation Instructions"
        ],
        "Introduction to TensorFlow": [
          "A Brief Touchdown",
          "A Brief Touchdown: Computational Graphs",
          "Common Mathematical Operators in Tensorflow",
          "A Tensorflow Session",
          "Interactive Tensorflow Session",
          "Constants and Variables in Tensorflow",
          "Placeholders in Tensorflow",
          "TensorBoard: Visualize Graphs in TensorFlow",
          "Access TensorBoard Graphs"
        ],
        "Other Python Packages and Their Interaction with Tensorflow": [
          "Miscellaneous Python Packages for Data Science",
          "Introduction to Numpy",
          "Create Numpy Arrays",
          "Numpy Operations",
          "Numpy for Statistical Operation",
          "Introduction to Pandas",
          "Read in Data from CSV",
          "Read in Excel Data",
          "Basic Data Cleaning",
          "Convert to Tensor Objects"
        ],
        "Statistical Modelling with Tensorflow": [
          "Correlation Analysis",
          "Linear Regression-Theory",
          "Linear Regression (From First Principles) With Tensorflow",
          "Visualize the Results of OLS",
          "Multiple Regression With Tensorflow-Part 1",
          "Multiple Regression With Tensorflow-Machine Learning Approach",
          "Estimate With Tensorflow Estimators",
          "Multiple Regression With Tensorflow Estimators",
          "More on Linear Regressor Estimator",
          "GLM: Generalized Linear Model",
          "Linear Classifier For Binary Classification",
          "Accuracy Assessment For Binary Classification",
          "Linear Classification with Binary Classification With Mixed Predictors"
        ],
        "Introduction to Machine Learning": [
          "Introduction",
          "What is Machine Learning?"
        ],
        "Unsupervised Learning": [
          "What is Unsupervised Learning?",
          "K-Means Clustering:Theory",
          "Implement K-Means on Real Data"
        ],
        "Supervised Learning": [
          "Softmax Classification",
          "Random Forest (RF) for Binary Classification",
          "Random Forest (RF) for Multiclass Classification",
          "kNN- Classification"
        ],
        "Artificial Neural Networks and Deep Learning with Tensorflow": [
          "Introduction to Artificial Neural Networks (ANN)",
          "Multi Layer Perceptron (MLP)",
          "Deep Neural Network (DNN) Classifier",
          "Deep Neural Network (DNN) Classifier With Mixed Predictors",
          "Deep Neural Network (DNN) Regression",
          "Wide and Deep Learning",
          "Autoencoders Theory",
          "Autoencoders for Credit Card Fraud Detection",
          "Autoencoders for Multiple Classes"
        ],
        "Convolution Neural Network (CNN) For Image Analysis": [
          "Introduction to CNN",
          "Implement a CNN for Multi-Class Supervised Classification",
          "Activation Functions",
          "More on CNN",
          "Pre-Requisite For Working With Imagery Data",
          "CNN on Image Data",
          "More on TFLearn",
          "Autoencoders with CNN"
        ],
        "Miscellaneous Section": [
          "Use Colabs for Jupyter Data Science",
          "Colab GPU",
          "Introduction To Github"
        ]
      },
      "requirements": [
        "Be Able To Operate & Install Software On A Computer",
        "Prior Exposure To Python Programming Will Be Beneficial",
        "Have Prior Exposure To Common Machine Learning Terms",
        "Prior Exposure To Basic Statistical Concepts Will be Useful"
      ],
      "description": "Complete Tensorflow Mastery For Machine Learning & Deep Learning in Python\nTHIS IS A COMPLETE DATA SCIENCE TRAINING WITH TENSORFLOW IN PYTHON!\nIt is a full 7-Hour Python Tensorflow Data Science Boot Camp that will help you learn statistical modelling, data visualization, machine learning and basic deep learning  using the Tensorflow framework in Python..\nHERE IS WHY YOU SHOULD ENROLL IN THIS COURSE:\nThis course is your complete guide to practical data science using the Tensorflow framework in Python..\nThis means, this course covers all the aspects of practical data science with Tensorflow (Google's powerful Deep Learning framework) and if you take this course, you can do away with taking other courses or buying books on Python Tensorflow based data science.\nIn this age of big data, companies across the globe use Python to sift through the avalanche of information at their disposal and advent of Tensorflow is revolutionizing Deep Learning...\nBy storing, filtering, managing, and manipulating data in Python and Tensorflow, you can give your company a competitive edge and boost your career to the next level.\nTHIS IS MY PROMISE TO YOU: COMPLETE THIS ONE COURSE & BECOME A PRO IN PRACTICAL PYTHON TENSORFLOW BASED DATA SCIENCE!\nBut first things first. My name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation).\nI have several years of experience in analyzing real life data from different sources  using data science related techniques and producing publications for international peer reviewed journals.\nOver the course of my research I realized almost all the Python data science courses and books out there do not account for the multidimensional nature of the topic and use data science interchangeably with machine learning..\nThis gives students an incomplete knowledge of the subject. My course, on the other hand, will give you a robust grounding in all aspects of data science within the Tensorflow framework.\nUnlike other Python courses, we dig deep into the statistical modeling features of Tensorflow and give you a one-of-a-kind grounding in Python based Tensorflow Data Science!\nDISCOVER 8 COMPLETE SECTIONS ADDRESSING EVERY ASPECT OF PYTHON BASED TENSORFLOW DATA SCIENCE:\n• A full introduction to Python Data Science and powerful Python driven framework for data science, Anaconda\n• Getting started with Jupyter notebooks for implementing data science techniques in Python\n• A comprehensive presentation about Tensorflow installation and a brief introduction to the other Python data science packages\n• Brief introduction to the working of Pandas and Numpy\n• The basics of the Tensorflow syntax and graphing environment\n• Statistical modelling with Tensorflow\n• Machine Learning, Supervised Learning, Unsupervised Learning in the Tensorflow framework\n• You’ll even discover how to create artificial neural networks and deep learning structures with Tensorflow\nBUT,  WAIT! THIS ISN'T JUST ANY OTHER DATA SCIENCE COURSE:\nYou’ll start by absorbing the most valuable Python Tensorflow Data Science basics and techniques.\nI use easy-to-understand, hands-on methods to simplify and address even the most difficult concepts.\nMy course will help you implement the methods using real data obtained from different sources. Many courses use made-up data that does not empower students to implement Python based data science in real -life.\nAfter taking this course, you’ll easily use packages like Numpy, Pandas, and Matplotlib to work with real data in Python along with gaining fluency in Tensorflow. I will even introduce you to deep learning models such as Convolution Neural network (CNN) !!\nThe underlying motivation for the course is to ensure you can apply Python based data science on real data into practice today, start analyzing  data for your own projects whatever your skill level, and impress your potential employers with actual examples of your data science abilities.\nThis course will take students without a prior Python and/or statistics background background from a basic level to performing some of the most common advanced data science techniques using the powerful Python based Jupyter notebooks\nIt is a practical, hands-on course, i.e. we will spend some time dealing with some of the theoretical concepts related to data science. However, majority of the course will focus on implementing different  techniques on real data and interpret the results..\nAfter each video you will learn a new concept or technique which you may apply to your own projects!\nJOIN THE COURSE NOW!\n#tensorflow #python #deeplearning #android #java #neuralnetwork  #models",
      "target_audience": [
        "People Interested In Learning Python Based Tensorflow For Data Science Applications",
        "People With Prior Exposure To Python Programming &/Or Data Science Concepts",
        "People Interested In Carrying Out Data Science In Jupyter Notebook Environment",
        "People Interested In Implementing Statistical and Machine Learning Models With Tensorflow",
        "People Interested In Implementing Deep Learning Models With Tensorflow"
      ]
    },
    {
      "title": "Data Science: Deep Learning and Neural Networks in Python",
      "url": "https://www.udemy.com/course/data-science-deep-learning-in-python/",
      "bio": "The MOST in-depth look at neural network theory for machine learning, with both pure Python and Tensorflow code",
      "objectives": [
        "Learn how Deep Learning REALLY works (not just some diagrams and magical black box code)",
        "Learn how a neural network is built from basic building blocks (the neuron)",
        "Code a neural network from scratch in Python and numpy",
        "Code a neural network using Google's TensorFlow",
        "Describe different types of neural networks and the different types of problems they are used for",
        "Derive the backpropagation rule from first principles",
        "Create a neural network with an output that has K > 2 classes using softmax",
        "Describe the various terms related to neural networks, such as \"activation\", \"backpropagation\" and \"feedforward\"",
        "Install TensorFlow",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "Where to get the code",
          "How to Succeed in this Course"
        ],
        "Review": [
          "Review Section Introduction",
          "What does machine learning do?",
          "Neuron Predictions",
          "Neuron Training",
          "Deep Learning Readiness Test",
          "Review Section Summary"
        ],
        "Preliminaries: From Neurons to Neural Networks": [
          "Neural Networks with No Math",
          "Introduction to the E-Commerce Course Project"
        ],
        "Classifying more than 2 things at a time": [
          "Prediction: Section Introduction and Outline",
          "From Logistic Regression to Neural Networks",
          "Interpreting the Weights of a Neural Network",
          "Softmax",
          "Sigmoid vs. Softmax",
          "Feedforward in Slow-Mo (part 1)",
          "Feedforward in Slow-Mo (part 2)",
          "Where to get the code for this course",
          "Softmax in Code",
          "Building an entire feedforward neural network in Python",
          "E-Commerce Course Project: Pre-Processing the Data",
          "E-Commerce Course Project: Making Predictions",
          "Prediction Quizzes",
          "Prediction: Section Summary",
          "Suggestion Box"
        ],
        "Training a neural network": [
          "Training: Section Introduction and Outline",
          "What do all these symbols and letters mean?",
          "What does it mean to \"train\" a neural network?",
          "How to Brace Yourself to Learn Backpropagation",
          "Categorical Cross-Entropy Loss Function",
          "Training Logistic Regression with Softmax (part 1)",
          "Training Logistic Regression with Softmax (part 2)",
          "Backpropagation (part 1)",
          "Backpropagation (part 2)",
          "Backpropagation in code",
          "Backpropagation (part 3)",
          "The WRONG Way to Learn Backpropagation",
          "E-Commerce Course Project: Training Logistic Regression with Softmax",
          "E-Commerce Course Project: Training a Neural Network",
          "Training Quiz",
          "Training: Section Summary"
        ],
        "Practical Machine Learning": [
          "Practical Issues: Section Introduction and Outline",
          "Donut and XOR Review",
          "Donut and XOR Revisited",
          "Neural Networks for Regression",
          "Common nonlinearities and their derivatives",
          "Practical Considerations for Choosing Activation Functions",
          "Hyperparameters and Cross-Validation",
          "Manually Choosing Learning Rate and Regularization Penalty",
          "Why Divide by Square Root of D?",
          "Practical Issues: Section Summary"
        ],
        "TensorFlow, exercises, practice, and what to learn next": [
          "TensorFlow plug-and-play example",
          "Visualizing what a neural network has learned using TensorFlow Playground",
          "Where to go from here",
          "You know more than you think you know",
          "How to get good at deep learning + exercises",
          "Deep neural networks in just 3 lines of code with Sci-Kit Learn"
        ],
        "Project: Facial Expression Recognition": [
          "Facial Expression Recognition Project Introduction",
          "Facial Expression Recognition Problem Description",
          "The class imbalance problem",
          "Utilities walkthrough",
          "Facial Expression Recognition in Code (Binary / Sigmoid)",
          "Facial Expression Recognition in Code (Logistic Regression Softmax)",
          "Facial Expression Recognition in Code (ANN Softmax)",
          "Facial Expression Recognition Project Summary"
        ],
        "Backpropagation Supplementary Lectures": [
          "Backpropagation Supplementary Lectures Introduction",
          "Why Learn the Ins and Outs of Backpropagation?",
          "Gradient Descent Tutorial",
          "Help with Softmax Derivative",
          "Backpropagation with Softmax Troubleshooting"
        ],
        "Higher-Level Discussion": [
          "What's the difference between \"neural networks\" and \"deep learning\"?",
          "Who should take this course in 2020 and beyond?",
          "Who should learn backpropagation in 2020 and beyond?",
          "Where does this course fit into your deep learning studies?"
        ]
      },
      "requirements": [
        "Basic math (calculus derivatives, matrix arithmetic, probability)",
        "Install Numpy and Python",
        "Don't worry about installing TensorFlow, we will do that in the lectures.",
        "Being familiar with the content of my logistic regression course (cross-entropy cost, gradient descent, neurons, XOR, donut) will give you the proper context for this course"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis course will get you started in building your FIRST artificial neural network using deep learning techniques. Following my previous course on logistic regression, we take this basic building block, and build full-on non-linear neural networks right out of the gate using Python and Numpy. All the materials for this course are FREE.\nWe extend the previous binary classification model to multiple classes using the softmax function, and we derive the very important training method called \"backpropagation\" using first principles. I show you how to code backpropagation in Numpy, first \"the slow way\", and then \"the fast way\" using Numpy features.\nNext, we implement a neural network using Google's new TensorFlow library.\nYou should take this course if you are interested in starting your journey toward becoming a master at deep learning, or if you are interested in machine learning and data science in general. We go beyond basic models like logistic regression and linear regression and I show you something that automatically learns features.\nThis course provides you with many practical examples so that you can really see how deep learning can be used on anything. Throughout the course, we'll do a course project, which will show you how to predict user actions on a website given user data like whether or not that user is on a mobile device, the number of products they viewed, how long they stayed on your site, whether or not they are a returning visitor, and what time of day they visited.\nAnother project at the end of the course shows you how you can use deep learning for facial expression recognition. Imagine being able to predict someone's emotions just based on a picture!\nAfter getting your feet wet with the fundamentals, I provide a brief overview of some of the newest developments in neural networks - slightly modified architectures and what they are used for.\nNOTE:\n\nIf you already know about softmax and backpropagation, and you want to skip over the theory and speed things up using more advanced techniques along with GPU-optimization, check out my follow-up course on this topic, Data Science: Practical Deep Learning Concepts in Theano and TensorFlow.\nI have other courses that cover more advanced topics, such as Convolutional Neural Networks, Restricted Boltzmann Machines, Autoencoders, and more! But you want to be very comfortable with the material in this course before moving on to more advanced subjects.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus (taking derivatives)\nmatrix arithmetic\nprobability\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\nBe familiar with basic linear models such as linear regression and logistic regression\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "Students interested in machine learning - you'll get all the tidbits you need to do well in a neural networks course",
        "Professionals who want to use neural networks in their machine learning and data science pipeline. Be able to apply more powerful models, and know its drawbacks."
      ]
    },
    {
      "title": "Artificial Neural Network and Machine Learning using MATLAB",
      "url": "https://www.udemy.com/course/artificial-neural-network-and-machine-learning-using-matlab/",
      "bio": "Learn to Create Neural Network with Matlab Toolbox and Easy to Follow Codes; with Comprehensive Theoretical Concepts",
      "objectives": [
        "Develop a multilayer perceptron neural networks or MLP in MATLAB using Toolbox",
        "Apply Artificial Neural Networks in practice",
        "Building Artificial Neural Network Model",
        "Knowledge on Fundamentals of Machine Learning and Artificial Neural Network",
        "Understand Optimization methods",
        "Understand the Mathematical Model of a Neural Network",
        "Understand Function approximation methodology",
        "Make powerful analysis",
        "Knowledge on Performance Functions",
        "Knowledge on Training Methods for Machine Learning"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Artificial Intelligence and Machine Learning": [
          "History of A.I.",
          "Artificial Intelligence",
          "Machine Learning",
          "Learning Methods",
          "Supervised Learning",
          "Unsupervised Learning",
          "Reinforcement Learning",
          "Fundamentals of Machine Learning"
        ],
        "Fundamentals of Artificial Neural Network": [
          "Biological Model of Neural Network",
          "How our Brain makes decisions",
          "Classification theory",
          "What is Artificial Neuron",
          "Combinational Logic Gate: AND",
          "Logical Disjunction Gate: OR",
          "Artificial Neuron Model",
          "Exclusive Logical Gate: XOR",
          "Complementary Lecture on \"Exclusive Logical Gate: XOR\"",
          "Multilayer Perceptron Concepts",
          "Mathematical Model of Net function",
          "Mathematical Model of MPL",
          "Hardware Implementation of a Neuron",
          "MLP Structure",
          "Neural Network's Logic",
          "Calculating Mean Square Error",
          "Optimization Methods",
          "Techniques of Neural Networks",
          "Neural Network Architectures",
          "Fundamentals of Neural Network Models"
        ],
        "MATLAB: Neural Net Fitting Tool": [
          "MATLAB overview",
          "Function Approximation Example",
          "Generate Data in MATLAB",
          "Data Selection",
          "Validation and Test Data",
          "Train Network",
          "Plots",
          "Retrain the Neural Network",
          "Effect of increasing the number of samples on NN",
          "Activation Function",
          "Activation Function Complementary Lecture",
          "Affect of Neurons in Hidden Layer on NN",
          "Calculate Maximum Number of Neurons per Layer"
        ],
        "MATLAB: Scripts": [
          "Save Data to Work Space",
          "Create Simple Script",
          "Create Advanced Script"
        ],
        "MATLAB: Modified Advance Script": [
          "Fitting Network",
          "Train Function",
          "Performance Function",
          "Train Parameters",
          "Recalculating Training, Validation and Test Performance",
          "Modify Plots"
        ],
        "MATLAB: Engine Data Set (Multiple Targets)": [
          "Create a NN for Engine Data Set",
          "Create a Function",
          "Two Targets in the Same NN",
          "Types of Data"
        ]
      },
      "requirements": [
        "Basics of Mathematics",
        "No programming experience is needed. You will learn everything you need to know."
      ],
      "description": "This course is uniquely designed to be suitable for both experienced developers seeking to make that jump to Machine learning or complete beginners who don't understand machine learning and Artificial Neural Network from the ground up.\nIn this course, we introduce a comprehensive training of multilayer perceptron neural networks or MLP in MATLAB, in which, in addition to reviewing the theories related to MLP neural networks, the practical implementation of this type of network in MATLAB environment is also fully covered.\nMATLAB offers specialized toolboxes and functions for working with Machine Learning and Artificial Neural Networks which makes it a lot easier and faster for you to develop a NN.\nAt the end of this course, you'll be able to create a Neural Network for applications such as classification, clustering, pattern recognition, function approximation, control, prediction, and optimization.",
      "target_audience": [
        "Anyone with passion for learning!",
        "Anyone who wants to develop a Neural Network with no programming skills!",
        "Students who want to learn and apply machine learning for their projects.",
        "Professionals in the field of Electronic, Computer, IT and Industry.",
        "Researchers who want to explore and conduct a research with Machine Learning techniques.",
        "Any business owner who wants to understand how to leverage the Machine Learning in their business",
        "Anyone who is looking for salary growth and new opportunities in the field of Data Science and Machine Learning"
      ]
    },
    {
      "title": "Data Visualization with Power BI Simplified",
      "url": "https://www.udemy.com/course/data-visualization-with-power-bi-simplified/",
      "bio": "\"Unlocking the Power of Data: A Comprehensive Guide to Building Dynamic Dashboards and Visualizations with Power BI\"",
      "objectives": [
        "How to use Power Query Editor",
        "How to design Dashboards in Power BI",
        "How to design Dashboards using Microsoft PowerPoint",
        "How to publish Dashboards to their websites"
      ],
      "course_content": {},
      "requirements": [
        "No Power BI experienced needed"
      ],
      "description": "Data visualization is an essential skill for anyone who wants to understand and communicate insights from data. In today's data-driven world, organizations need to quickly and accurately make informed decisions based on vast amounts of data. Power BI is a powerful data visualization tool that allows users to create interactive dashboards and reports that help them analyze and communicate insights from data. This short course is designed to teach you the basics of data visualization with Power BI, and provide hands-on experience with the tool.\nThe course will start by introducing you to the Power BI interface and its various components. You will learn how to create a data model, import data from various sources, and transform data to make it suitable for analysis. You will also learn how to create relationships between tables in your data model, and how to manage data from multiple sources.\nOnce you have a solid foundation in the basics of Power BI, you will learn how to create visually compelling dashboards, reports, and visualizations. You will learn how to use Power BI's built-in visualizations, and how to customize them to meet your specific needs. You will also learn how to use advanced features like drill-through and drill-down, to help you explore your data in more detail.\nThroughout the course, you will be provided with real-world examples and exercises, which will allow you to apply what you have learned to real-world scenarios. You will also learn how to publish your reports and dashboards to the Power BI service, so you can share your insights with others.\nBy the end of the course, you will have a solid understanding of how to use Power BI to effectively analyze and visualize data, and be able to create visually compelling dashboards and reports that help you communicate insights to others.",
      "target_audience": [
        "Beginner interested in learning Data Visualization with Power BI"
      ]
    },
    {
      "title": "Exploratory Data Analysis in Python",
      "url": "https://www.udemy.com/course/exploratory-data-analysis-in-python/",
      "bio": "A course about how to approach a dataset for the first time",
      "objectives": [
        "Exploring a dataset for calculating overall statistics",
        "Visualize the correlations between the features",
        "Visualize the predictive power of the features",
        "Create useful insights from a dataset"
      ],
      "course_content": {
        "Introduction": [
          "Introduction to the course",
          "What is EDA?",
          "The dataset",
          "Required Python packages",
          "Jupyter notebooks"
        ],
        "Univariate analysis": [
          "A first sight to our dataset",
          "Summarization",
          "Histograms",
          "Boxplots"
        ],
        "Multivariate analysis": [
          "Pairplots",
          "Correlation matrix and histograms",
          "Stacked histograms"
        ],
        "Some useful libraries": [
          "Sweetviz",
          "Pandas profiling"
        ],
        "General guidelines": [
          "Practical suggestions"
        ]
      },
      "requirements": [
        "Python programming language"
      ],
      "description": "When we put our hands on a dataset for the first time, we can’t wait to test several models and algorithms. This is wrong because if we don’t know the information before feeding our model, the results will be unreliable and the model itself will surely fail. Moreover, if we don’t select the best features in advance, the training phase becomes slow and the model won’t learn anything useful.\nSo, the first approach we must have is to take a look at our dataset and visualize the information it contains. In other words, we have to explore it.\nThat’s the purpose of the Exploratory Data Analysis.\nEDA is an important step of data science and machine learning. It helps us explore the information hidden inside a dataset before applying any model or algorithm. It makes heavy use of data visualization, it’s bias-free.\nMoreover, it lets us figure out whether our features have predictive power or not, determining if the machine learning project we are working on has chances to be successful. Without EDA, we may give the wrong data to a model without reaching any success.\nWith this course, the student will learn:\nHow to visualize information that is hidden inside the dataset\nHow to visualize the correlation and the importance of the columns of a dataset\nSome useful Python libraries\nAll the lessons are practical and made using Python programming language and Jupyter notebooks. All the notebooks are downloadable.",
      "target_audience": [
        "Python developers",
        "Data scientists"
      ]
    },
    {
      "title": "Mastering Databricks & Apache spark -Build ETL data pipeline",
      "url": "https://www.udemy.com/course/mastering-databricks-apache-spark-build-etl-data-pipeline/",
      "bio": "Learn fundamental concept about databricks and process big data by building your first data pipeline on Azure",
      "objectives": [
        "Databricks",
        "Build your first data pipeline to process CSV, JSON, XML",
        "Orchestrate data pipeline on Azure data factory",
        "Spin up spark cluster",
        "Delta tables",
        "Concept of time travel and vacuum on delta tables",
        "Apache Spark SQL",
        "Filtering Dataframe",
        "Renaming, drop, Select, Cast",
        "Aggregation operations SUM, AVERAGE, MAX, MIN",
        "Rank, Row Number, Dense Rank",
        "Building dashboards",
        "Build Complete project",
        "Build End to End data pipeline"
      ],
      "course_content": {},
      "requirements": [
        "There are no pre requisites with this course"
      ],
      "description": "Welcome to the course on Mastering Databricks & Apache spark -Build ETL data pipeline\nDatabricks combines the best of data warehouses and data lakes into a lakehouse architecture. In this course we will be learning how to perform various operations in Scala, Python and Spark SQL. This will help every student in building solutions which will create value and mindset to build batch process in any of the language. This course will help in writing same commands in different language and based on your client needs we can adopt and deliver world class solution. We will be building end to end solution in azure databricks.\n\n\nKey Learning Points\nWe will be building our own cluster which will process our data and with one click operation we will load different sources data to Azure SQL and Delta tables\nAfter that we will be leveraging databricks notebook to prepare dashboard to answer business questions\nBased on the needs we will be deploying infrastructure on Azure cloud\nThese scenarios will give student 360 degree exposure on cloud platform and how to step up various resources\nAll activities are performed in Azure Databricks\n\n\nFundamentals\nDatabricks\nDelta tables\nConcept of versions and vacuum on delta tables\nApache Spark SQL\nFiltering Dataframe\nRenaming, drop, Select, Cast\nAggregation operations SUM, AVERAGE, MAX, MIN\nRank, Row Number, Dense Rank\nBuilding dashboards\nAnalytics\nThis course is suitable for Data engineers, BI architect, Data Analyst, ETL developer, BI Manager",
      "target_audience": [
        "Data engineer",
        "People who are interested in build End to End ETL data pipeline",
        "Learn fundamentals commands in Python, Apache Spark SQL, Scala"
      ]
    },
    {
      "title": "LangChain- Develop AI Agents with LangChain & LangGraph",
      "url": "https://www.udemy.com/course/langchain/",
      "bio": "Learn LangChain and LangGraph by building real world AI Agents (Python, Latest Version 0.3.0+)",
      "objectives": [
        "Become proficient in LangChain",
        "Have 3 end to end working LangChain based generative AI applications",
        "Prompt Engineering Theory: Chain of Thought, ReAct, Few Shot prompting and understand how LangChain is build under the hood",
        "Understand how to navigate inside the LangChain opensource codebase",
        "Large Language Models theory for software engineers",
        "LangChain: Lots of chains Chains, Agents, DocumentLoader, TextSplitter, OutputParser, Memory",
        "RAG, Vectorestores/ Vector Databasrs (Pinecone, FAISS)",
        "Model Context Protocol",
        "LangGraph"
      ],
      "course_content": {},
      "requirements": [
        "This is not a beginner course. Basic software engineering concepts are needed",
        "I assume students will be familiar software engineering subjects such as: git, python, pipenv, environment variables, classes, testing and debugging",
        "No Machine Learning experience is needed."
      ],
      "description": "COURSE WAS RE-RECORDED and supports- LangChain Version 0.3+\n**Ideal students are software developers / data scientists / AI/ML Engineers**\n\nWelcome to the AI Agents with LangChain and LangGraph Udemy course - Unleashing the Power of Agentic AI!\nThis  course is designed to teach you how to QUICKLY harness the power the LangChain & LangGraph libraries for LLM applications and Agentic AI.\nThis course will equip you with the skills and knowledge necessary to develop cutting-edge LLM solutions for a diverse range of topics.\nPlease note that this is not a course for beginners. This course assumes that you have a background in software engineering and are proficient in Python. I will be using Pycharm IDE but you can use any editor you'd like since we only use basic feature of the IDE like debugging and running scripts .\n\nWhat You’ll Build:  No fluff. No toy examples. You’ll build:\nSearch Agent\nDocumentation Helper – A chatbot over Python package docs (and any data you choose), using advanced retrieval and RAG.\nSlim ChatGPT Code Interpreter – A lightweight code execution assistant.\nPrompt Engineering Theory Section\nIntroduction to LangGraph\nIntroduction to Model Context Protocol (MCP)\nIce Breaker Agent – An AI agent that searches Google, finds LinkedIn and Twitter profiles, scrapes public info, and generates personalized icebreakers.\n\nThe topics covered in this course include:\nAI Agents\nAgentic AI\nAI Engineering\nLangChain, LangGraph\nLLM + GenAI History\nPrompt Engineering: Few shots prompting, Chain of Thought, ReAct prompting\nContext Engineering\nChat Models\nOpen Source Models\nPrompts, PromptTemplates, langchainub\nOutput Parsers, Pydantic Output Parsers\nChains: create_retrieval_chain, create_stuff_documents_chain\nAgents, Custom Agents, Python Agents, CSV Agents, Agent Routers\nOpenAI Functions, Tool Calling\nTools, Toolkits\nMemory\nVectorstores (Pinecone, FAISS, Chroma)\nRAG (Retrieval Augmentation Generation)\nDocumentLoaders, TextSplitters\nStreamlit (for UI), Copilotkit\nLCEL\nLangSmith\nLangGraph\nFireCrawl\nGIST of Cursor IDE\nCursor Composter\nCurser Chat\nMCP - Model Context Protocol & LangChain Ecosystem\nIntroduction To LangGraph\n\n\nThroughout the course, you will work on hands-on exercises and real-world projects to reinforce your understanding of the concepts and techniques covered. By the end of the course, you will be proficient in using LangChain to create powerful, efficient, and versatile LLM applications for a wide array of usages.\n\nWhy This Course?\nUp-to-date: Covers LangChain v0.3+ and the latest LangGraph ecosystem.\nPractical: Real projects, real APIs, real-world skills.\nCareer-boosting: Stay ahead in the LLM and GenAI job market.\nStep-by-step guidance: Clear, concise, no wasted time.\nFlexible: Use any Python IDE (Pycharm shown, but not required).\n\n\nDISCLAIMERS\nPlease note that this is not a course for beginners. This course assumes that you have a background in software engineering and are proficient in Python.\nI will be using Pycharm IDE but you can use any editor you'd like since we only use basic feature of the IDE like debugging and running scripts.\nThe Ice-Breaker project requires usage of 3rd party APIs-\nScrapin, Tavily, Twitter API  which are generally paid services.\nAll of those 3rd parties have a free tier we will use to create stub responses development and testing.",
      "target_audience": [
        "Software Engineers that want to learn how to build Generative AI based applications with LangChain and LangGraph",
        "Developers that want to learn how to build Generative AI based applications with LangChain and LangGraph",
        "Engineers that want to learn how to build Generative AI based applications with LangChain and LangGraph"
      ]
    },
    {
      "title": "AI Mastery Bootcamp: Complete Guide with 1000 Projects",
      "url": "https://www.udemy.com/course/ai-engineering-complete-bootcamp-masterclass/",
      "bio": "AI Algorithms, AI Models, AI Agents, Python to 1000 Real-World AI Projects, AI Agents, MCP, Google A2A, more(AI)",
      "objectives": [
        "Master Python for Artificial Intelligence: Write efficient Python code, essential for AI and ML programming tasks.",
        "Data Preprocessing Skills for Artificial Intelligence: Prepare, clean, and transform data to enhance model performance.",
        "Statistical Knowledge for Artificial Intelligence: Apply core statistics to understand data patterns and inform decisions.",
        "Build Machine Learning Models for Artificial Intelligence: Develop and fine-tune ML models for classification, regression, and clustering.",
        "Deep Learning Proficiency: Design and train neural networks, including CNNs and RNNs, for image and sequence tasks.",
        "Utilize Transfer Learning: Adapt pre-trained models to new tasks, saving time and resources.",
        "Deploy ML Models with APIs: Create scalable APIs to serve ML models in real-world applications.",
        "Containerize with Docker: Package models for portable deployment across environments.",
        "Monitor and Maintain Models: Track model performance, detect drift, and implement retraining pipelines.",
        "Complete ML Lifecycle: Master end-to-end AI project skills, from data to deployment and ongoing maintenance."
      ],
      "course_content": {},
      "requirements": [
        "Foundational Math Skills: Understanding of algebra and basic calculus concepts (derivatives, functions) for ML.",
        "Interest in AI and ML: A passion for learning AI, machine learning, and data-driven technologies.",
        "Laptop/Computer: A device capable of running data processing and ML libraries like TensorFlow, PyTorch, and Docker.",
        "Curiosity and Perseverance: Willingness to solve problems, experiment with data, and work through challenges."
      ],
      "description": "With tools like ChatGPT, DeepSeek AI, Mistral, Claude (Anthropic AI), Perplexity AI, Google Gemini, Microsoft Copilot, Jasper AI, Meta AI, Chatsonic, GitHub Copilot, YouChat, and Writesonic on the rise everyone needs to put Artificial Intelligence on their Radar.\n\nWelcome to the Artificial Intelligence Mastery: Complete AI Bootcamp 2025! This comprehensive Artificial Intelligence Bootcamp is your ultimate guide to becoming a skilled AI Engineer, empowering you to master Artificial Intelligence and apply it to real-world problems. Over an intensive 16-week Artificial Intelligence training program, you'll gain hands-on experience in building, training, and deploying AI models using the latest AI tools and frameworks.\nIn this Artificial Intelligence Bootcamp, you'll start with the fundamentals of Artificial Intelligence, including Python programming, data preprocessing, and an introduction to machine learning. As you progress, you'll explore advanced Artificial Intelligence concepts such as neural networks, deep learning, natural language processing (NLP), and computer vision. You'll also master industry-standard AI frameworks like TensorFlow, PyTorch, and Hugging Face, essential for modern AI development and deployment. This is the #1 Course on Udemy for AI.\nThis AI Bootcamp 2025 focuses heavily on practical AI skills, ensuring that every module comes with real-world projects to strengthen your understanding. Whether you're an AI beginner or someone looking to expand their AI expertise, this course is designed for you.\nBy the end of the AI Mastery Bootcamp, you'll have the AI skills, confidence, and hands-on experience to build and deploy AI solutions from scratch. You’ll be fully prepared to tackle industry AI challenges, contribute to AI research, or innovate in your own AI-driven projects.\nKey Highlights of the AI Mastery Bootcamp:\nComprehensive AI Curriculum covering Python, Machine Learning, Deep Learning, NLP, and AI Frameworks\nHands-On AI Projects to build practical AI skills\nReal-World AI Applications and case studies\nIndustry-Standard AI Tools: TensorFlow, PyTorch, Hugging Face\nBeginner-Friendly AI Program with step-by-step guidance\nWhether you're aiming to become an AI Engineer, AI Researcher, or a leader in the AI industry, this Artificial Intelligence Bootcamp will equip you with the tools, knowledge, and experience you need.\nJoin the AI Revolution Today – Enroll in the Artificial Intelligence Mastery: Complete AI Bootcamp 2025 and become a leader in the world of AI!",
      "target_audience": [
        "Aspiring AI Engineers: Those looking to build a career in AI and gain hands-on, production-ready skills.",
        "Data Scientists and Analysts: Professionals who want to expand their expertise to include machine learning, deep learning, and model deployment.",
        "Software Engineers: Developers interested in applying programming skills to AI and machine learning projects.",
        "Career Changers: Individuals from non-technical backgrounds with foundational Python knowledge, eager to transition into AI.",
        "Graduate Students: Students in data science, computer science, or related fields wanting a practical, job-ready experience in AI engineering.",
        "Tech Entrepreneurs: Founders and CTOs interested in understanding AI for building AI-driven products or managing AI teams."
      ]
    },
    {
      "title": "SQL and Data Visualization - The Complete Bootcamp",
      "url": "https://www.udemy.com/course/sql-data-visualization-the-complete-course/",
      "bio": "Become an expert in SQL, Data Analysis, Business Intelligence & Data Visualization. Learn SQL from A to Z",
      "objectives": [
        "SQL",
        "PostgreSQL",
        "sql",
        "SQL Language",
        "Business Intelligence",
        "Data Analysis",
        "Data Visualization",
        "Metabase"
      ],
      "course_content": {},
      "requirements": [
        "Have a computer"
      ],
      "description": "In this course you will learn how to write and execute SQL queries on your own database. Starting with basic SQL, we will learn how to use PostgreSQL to make advanced data analysis, graphs, reports and tables.\n\n\nWho should attend this SQL and Data Viz course?\n\nYou who are dependent on Business Analysts to create Reports\nYou who are limited to Excel or Spreadsheet for your data analyses\nYou who would like to do some in-depth analysis and blow your boss or client away\nYou're the one who always said, \"I need to learn SQL!\"\nYou who find your business analysis dull and graphless.\n\n\nWhy learn SQL?\n\nWhether you are a Marketer, Product Marketer, Developer, Sales or Manager, SQL is one of the most sought-after skills in the job market. This extraordinary skill takes your CV into another world, the world of autonomy and rigor in your reporting.\nTo learn SQL and Data Viz, you don't need to have a \"Business Analyst\" career plan. This skill will serve you in all areas, just like some tools such as Excel or Google Spreadsheet have served you well. The only difference? SQL is much more advanced than these!\n\n\nBecome an expert in data visualization\n\nYou'll learn how to become a business analyst and produce reports that are as visual as they are effective. In just a few clicks, create your own PostgreSQL database and connect the best tools to make your analyses dynamic, attractive and orderly. You can then create your own charts, histograms, curves and world maps for specific business situations.\n\n\nLearn SQL from A to Z\nSQL will have no secrets from you. Of course, we'll go over the basics. Then, you'll even become an expert as you get more and more powerful as you watch the videos.\nCASE\nWHEN 'become better in analysis' THEN 'join this course'\nELSE  'keep working on Excel'\nEND AS expert_sql_and_data_viz\n\n\nThe tools you will master\n- Metabase, the perfect data visualization tool to run your data analysis\n- Heroku, to host your database\n- PGAdmin, to create your database and data set\n\n\nWhy is this course different from the others?\n1- You will apply SQL in real business cases that you will be able to reuse\n2- You will work on your own database\n3- You will create your own graphs in a few minutes\n4- You will progress step by step in the SQL language\n5- I will answer all your questions in a few minutes\nAnyway, I just wanted to tell you that I'm not from the Data world. I'm a Marketing Manager who got tired of doing his reports in Excel and wanted to take it to the next level by learning SQL! So, if I did it, you can do it.\nSo, join me in this course \"SQL & Data Visualization: the complete guide\", and don't hesitate to leave a note if you like this course ✌️\n\n\nRaffi",
      "target_audience": [
        "People who want to improve their reporting and analysis",
        "People who want to learn SQL",
        "Marketing Manager who wants to improve business analysis",
        "Product Marketer who wants to learn SQL to improve a Product",
        "Anyone who wants to get rid of Excel or Spreadsheet to do an analysis"
      ]
    },
    {
      "title": "Data Science: Transformers for Natural Language Processing",
      "url": "https://www.udemy.com/course/data-science-transformers-nlp/",
      "bio": "ChatGPT, GPT-4, BERT, Deep Learning, Machine Learning & NLP with Hugging Face, Attention in Python, Tensorflow, PyTorch",
      "objectives": [
        "Apply transformers to real-world tasks with just a few lines of code",
        "Fine-tune transformers on your own datasets with transfer learning",
        "Sentiment analysis, spam detection, text classification",
        "NER (named entity recognition), parts-of-speech tagging",
        "Build your own article spinner for SEO",
        "Generate believable human-like text",
        "Neural machine translation and text summarization",
        "Question-answering (e.g. SQuAD)",
        "Zero-shot classification",
        "Understand self-attention and in-depth theory behind transformers",
        "Implement transformers from scratch",
        "Use transformers with both Tensorflow and PyTorch",
        "Understand BERT, GPT, GPT-2, and GPT-3, and where to apply them",
        "Understand encoder, decoder, and seq2seq architectures",
        "Master the Hugging Face Python library",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {},
      "requirements": [
        "Install Python, it's free!",
        "Beginner and intermediate level content: Decent Python programming skills",
        "Expert level content: Good understanding of CNNs and RNNs and ability to code in PyTorch or Tensorflow"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, Gemini Pro, Llama 3, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\n\n\nHello friends!\nWelcome to Data Science: Transformers for Natural Language Processing.\n\n\nEver since Transformers arrived on the scene, deep learning hasn't been the same.\nMachine learning is able to generate text essentially indistinguishable from that created by humans\nWe've reached new state-of-the-art performance in many NLP tasks, such as machine translation, question-answering, entailment, named entity recognition, and more\nWe've created multi-modal (text and image) models that can generate amazing art using only a text prompt\nWe've solved a longstanding problem in molecular biology known as \"protein structure prediction\"\n\n\nIn this course, you will learn very practical skills for applying transformers, and if you want, detailed theory behind how transformers and attention work.\nThis is different from most other resources, which only cover the former.\n\n\nThe course is split into 3 major parts:\nUsing Transformers\nFine-Tuning Transformers\nTransformers In-Depth\n\n\nPART 1: Using Transformers\n\n\nIn this section, you will learn how to use transformers which were trained for you. This costs millions of dollars to do, so it's not something you want to try by yourself!\nWe'll see how these prebuilt models can already be used for a wide array of tasks, including:\ntext classification (e.g. spam detection, sentiment analysis, document categorization)\nnamed entity recognition\ntext summarization\nmachine translation\nquestion-answering\ngenerating (believable) text\nmasked language modeling (article spinning)\nzero-shot classification\nThis is already very practical.\nIf you need to do sentiment analysis, document categorization, entity recognition, translation, summarization, etc. on documents at your workplace or for your clients - you already have the most powerful state-of-the-art models at your fingertips with very few lines of code.\nOne of the most amazing applications is \"zero-shot classification\", where you will observe that a pretrained model can categorize your documents, even without any training at all.\n\n\nPART 2: Fine-Tuning Transformers\n\n\nIn this section, you will learn how to improve the performance of transformers on your own custom datasets. By using \"transfer learning\", you can leverage the millions of dollars of training that have already gone into making transformers work very well.\nYou'll see that you can fine-tune a transformer with relatively little work (and little cost).\nWe'll cover how to fine-tune transformers for the most practical tasks in the real-world, like text classification (sentiment analysis, spam detection), entity recognition, and machine translation.\n\n\nPART 3: Transformers In-Depth\n\n\nIn this section, you will learn how transformers really work. The previous sections are nice, but a little too nice. Libraries are OK for people who just want to get the job done, but they don't work if you want to do anything new or interesting.\nLet's be clear: this is very practical.\nHow practical, you might ask?\nWell, this is where the big bucks are.\nThose who have a deep understanding of these models and can do things no one has ever done before are in a position to command higher salaries and prestigious titles. Machine learning is a competitive field, and a deep understanding of how things work can be the edge you need to come out on top.\nWe'll look at the inner workings of encoders, decoders, encoder-decoders, BERT, GPT, GPT-2, GPT-3, GPT-3.5, ChatGPT, and GPT-4 (for the latter, we are limited to what OpenAI has revealed).\nWe'll also look at how to implement transformers from scratch.\nAs the great Richard Feynman once said, \"what I cannot create, I do not understand\".\n\n\nSUGGESTED PREREQUISITES:\nDecent Python coding skills\nDeep learning with CNNs and RNNs useful but not required\nDeep learning with Seq2Seq models useful but not required\nFor the in-depth section: understanding the theory behind CNNs, RNNs, and seq2seq is very useful\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out\n\n\nThank you for reading and I hope to see you soon!",
      "target_audience": [
        "Anyone who wants to master natural language processing (NLP)",
        "Anyone who loves deep learning and wants to learn about the most powerful neural network (transformers)",
        "Anyone who wants to go beyond typical beginner-only courses on Udemy"
      ]
    },
    {
      "title": "RA: Data Science and Supply Chain analytics. A-Z with Python",
      "url": "https://www.udemy.com/course/ra-data-science-and-supply-chain-analytics-a-z-with-python/",
      "bio": "Learn Python, Supply Chain Data Science ,Linear Programming, Forecasting, Pricing and Inventory Management.",
      "objectives": [
        "A-Z Guide to Mastering Python for Data Science.",
        "Work as A demand Planner.",
        "Become a data driven supply chain manager.",
        "Use linear Programming in python for logistics optimization and Production scheduling.",
        "Set stock policies and safety stocks for all of your Business products.",
        "Revenue management",
        "Segment Customers, Products and suppliers to maximize service levels and reduce costs.",
        "Learn simulations to make informed supply chain decisions.",
        "Become a supply chain data scientist.",
        "Learn Supply chain techniques you will only find in this course. Guaranteed!"
      ],
      "course_content": {
        "Introduction": [
          "Intro",
          "Why we need to learn coding?",
          "Curriculum",
          "Plan of attack",
          "Supply chain visualization",
          "Cost and service Dynamics",
          "Service level and product characteristics",
          "Customer and supplier characteristics",
          "Supply chain Views",
          "The Financial flow",
          "Why is supply chain complicated"
        ],
        "Supply chain Data": [
          "intro",
          "Types of Data in supply chain",
          "Data From suppliers",
          "Data from production",
          "Data from stocks",
          "Data from sales and customers",
          "Why we need to learn Data Science",
          "Analytics Types"
        ],
        "Welcome to the world of Python": [
          "Python",
          "downloading Anaconda",
          "Installing Anaconda",
          "Spyder overview",
          "Jupiter Notebook overview",
          "Python Libraries",
          "Inventorize Package"
        ],
        "Python Programming Fundamentals": [
          "Intro",
          "Dataframes",
          "Arithmetic Calculations with Python",
          "Lists",
          "Dictionaries",
          "Arrays",
          "Importing data in Python",
          "Subsetting Data Frames",
          "Conditions",
          "Writing functions",
          "mapping",
          "for loops",
          "for looping a function",
          "Mapping On a data frame",
          "for looping on a data frame",
          "Summary",
          "Assignment",
          "Assignment answer 1",
          "Assignment answer 2"
        ],
        "Supply chain statistical analysis": [
          "Intro",
          "Measures of centrality and Spread",
          "Calculating the mean",
          "Calculating the median",
          "Measures of spread",
          "Percentiles",
          "Correlations: subsetting Cars dataset",
          "Correlations of continuous variables",
          "Correlation plots",
          "Correlation thresholds",
          "Detecting outliers",
          "Outliers in python",
          "linear regression",
          "intro to linear regression",
          "Linear Regression in python",
          "Fitting the linear model",
          "Importance of distributions in supply chain",
          "Chi- Square tests",
          "Distributions in Excel",
          "Distributions Chi-square tests",
          "cover for 90% of demand",
          "Assignment",
          "Assignment Answer",
          "Distributions in python",
          "Testing for several distributions",
          "Summary",
          "Assignment",
          "Assignment answer"
        ],
        "Manipulation and Data cleaning": [
          "Manipulation Intro",
          "Dropping Duplicates and NAs",
          "Conversions lecture",
          "Conversions",
          "Filterations",
          "Imputations",
          "Indexing tutorial",
          "slicing index",
          "Manipulation Lecture",
          "Groupby",
          "Slicing the group by",
          "Dropping levels",
          "The proper form",
          "Pivot Tables",
          "Aggregate function in pivot table",
          "Melting the data",
          "Left Join",
          "inner and outer join",
          "Joining in python",
          "Inner, left join and full join (outer)",
          "Summary",
          "Assignment",
          "Assignment answer 1",
          "Assignment answer 2",
          "Assignment answer 3",
          "Assignment answer 4",
          "Assignment answer 5"
        ],
        "Working with dates in Python": [
          "Date Intro",
          "datetime",
          "Last purchase date and recency",
          "recency Histogram",
          "Modeling inter-arrival time",
          "Modeling inter_arrival time 2",
          "Modeling inter arrival time 3",
          "Resampling",
          "rolling time series",
          "rolling Time series 2",
          "Summary",
          "Assignment",
          "Assignment Answer"
        ],
        "Visualization with matplotlib and seaborn": [
          "Intro",
          "Line plot",
          "Line Plot part 2",
          "scatter plot",
          "Countplot",
          "Barplot",
          "Distribution Plots",
          "Boxplots",
          "Histograms",
          "Pairplots",
          "Visualization Summary",
          "Assignment",
          "Assignment answer 1",
          "Assignment Answer 2"
        ],
        "Segmentation": [
          "Intro",
          "Pareto Law",
          "Importance of ABC analysis",
          "Multi-criteria segmentation",
          "Transforming the data for excel",
          "ABC_analysis in Excel",
          "Assignment",
          "ABC in python",
          "Multi-Criteria ABC analysis",
          "Multi-Criteria ABC analysis with store or department level",
          "Supplier segmentation 1",
          "Supplier segmentation 2",
          "Supplier Segmentation In python",
          "Value_index",
          "Visualizing Krajic",
          "Summary",
          "Assignment ABC",
          "Assignment answer"
        ],
        "Forecasting Basics": [
          "Why we need forecasts",
          "Qualitative and Quantitative Forecasting",
          "Optimistic and Pessimistic Forecasting",
          "Time Components",
          "Preparing the Data for Regression",
          "Forecasting in Excel",
          "Forecasting in excel 2",
          "Assignment",
          "Regression in python",
          "Regression in python part2",
          "Initializing a date range for forecasting",
          "Forecasting",
          "Summary",
          "Assignment Questions",
          "Assignment",
          "Assignment2"
        ]
      },
      "requirements": [
        "Excel",
        "Anaconda",
        "spyder"
      ],
      "description": "\"I attended this course with high expectations. And I was not disappointed. It´s incredible to see what is possible with Python in terms of supply chain planning and optimization. Haytham is doing a great job as a trainer. Starting with explanation of basics and ending with presentation of advanced techniques supply chain managers can apply in real life.\"\nLarsen Block\nDirector Supply Chain Management at Freudenberg Home and Cleaning Solutions GmbH\n\n\nNew update : Forecast for OTB calculation with AutoML is added (Aug 2023)\nAfter our Data Science and supply chain analytics with R course being dubbed the highest rated course in supply chain on Udemy, we are pleased to Introduce Data Science and supply chain analytics. A-Z with Python !!\n\" 60000 Professionals are using inventorize across R & Python. Know how to use it only in this course\"\nIt's been seven years since I moved from Excel to data science and since then I have never looked back! With eleven years between working in Procurement, lecturing in universities, training over 70000 professionals in supply chain and data science with R and python, and finally opening my own business in consulting for five years now. I am extremely excited to share with you this course and learn with you through this unique rewarding course. My goal is that all of you become experts in data science and supply-chain. I have put all the techniques I have learned and practiced in this one sweet bundle of data science and supply chain.\nAs a consultancy, we develop algorithms for retailers and supply chains to make aggregate and item controllable forecasting, optimize stocks, plan assortment and Maximize profit margin by optimizing prices. 20000 people are already using our free package for supply chain analysis \"Inventorize\" and we can't wait to share its capabilities with you so you can start dissecting supply chain problems...for free!\nThe motivation behind this project is filling the gap of finding a comprehensive course that tackles supply chains using data science. there are courses for data science, forecasting, revenue management, inventory management, and simulation modeling. but here we tackle all of them as a bundle. Lectures, Concepts, codes, exercises, and spreadsheets. and we don't present the code, we do the code with you, step by step.\nthe abundance of the data from customers, suppliers, products, and transactions have opened the way for making informed business decisions on a bigger and more dynamic scale that can no longer be achieved by spreadsheets. In this course, we learn data science from a supply chain mindset.\nDon't worry If you don't know how to code, we learn step by step by applying supply chain analysis!\n*NOTE: Full course includes downloadable resources and Python project files, homework and course quizzes, lifetime access, and a 30-day money-back guarantee.\nWho this course is for:\n· If you are an absolute beginner at coding, then take this course.\n· If you work in a supply-chain and want to make data-driven decisions, this course will equip you with what you need.\n· If you are an inventory manager and want to optimize inventory for 1000000 products at once, then this course is for you.\n· If you work in finance and want to forecast your budget by taking trends, seasonality, and other factors into account then this course is just what you need.\n· If you are a seasoned python user, then take this course to get up to speed quickly with python capabilities. You will become a regular python user in no time.\n· If you want to take a deep dive (not just talking) in supply chain management, then take this course.\n· If you want to apply machine learning techniques for supply -chain, we will walk you through the methods of supervised and unsupervised learning.\n· If you are switching from Excel to a data science language. then this course will fast track your goal.\n· If you are tired of doing the same analysis again and again on spreadsheets and want to find ways to automate it, this course is for you.\n· If you are frustrated about the limitations of data loading and available modules in excel, then Moving to python will make our lives a whole lot easier.\n\n\nCourse Design\nthe course is designed as experiential learning Modules, the first couple of modules are for supply chain fundamentals followed by Python programming fundamentals, this is to level all of the takers of this course to the same pace. and the third part is supply chain applications using Data science which is using the knowledge of the first two modules to apply. while the course delivery method will be a mix of me explaining the concepts on a whiteboard, Presentations, and Python-coding sessions where you do the coding with me step by step. there will be assessments in most of the sections to strengthen your newly acquired skills. all the practice and assessments are real supply chain use cases.\nSupply chain Fundamentals Module includes:\n1- Introduction to supply chain.\n2- Supply chain Flows.\n3- Data produced by supply chains.\nPython Programming Fundamentals Module includes:\n1- Basics of Python\n2- Data cleaning and Manipulation.\n3- Statistical analysis.\n4- Data Visualization.\n5- Advanced Programming.\nSupply chain Applications Module include :\n1- Product segmentations single and Multi-criteria.\n2- Supplier segmentations.\n3- Customers segmentations.\n4- Forecasting techniques and accuracy testing.\n5- Linear Programming and logistics optimizations.\n6- Pricing and Markdowns optimization Techniques.\n7- Inventory Policy and Safety stock Calculations.\n8- Inventory simulations.\n9- Machine Learning for supply-chain.\n10- Simulations for optimizing Capacity and Resources.\n\n\n*NOTE: Many of the concepts and analysis I explain first in excel as I find excel the best way to first explain a concept and then we scale up, improve and generalize with Python. By the end of this course, you will have an exciting set of skills and a toolbox you can always rely on when tackling supply chain challenges. The course may take from 12-16 weeks to finish, 4-5 hours of lectures, and practice every week.\n\n\n\n\nHappy Supply Chain mining!\nHaytham\nRescale Analytics\n\n\nFeedback from Clients and Training:\n\n\n\"In Q4 2018, I was fortunate to find an opportunity to learn R in Dubai, after hearing about it from indirect references in UK.\n\n\nI attended a Supply Chain Forecasting & Demand Planning Masterclass conducted by Haitham Omar and the possibilities seemed endless. So, we requested Haitham to conduct a 5-day workshop in our office to train 8 staff members, which opened us up as a team to deeper data analysis. Today, we have gone a step further and retained Haitham, as a consultant, to take our data analysis to the next level and to help us implement inventory guidelines for our business. The above progression of our actions is a clear indication of the capabilities of Haitham as a specialist in R and in data analytics, demand planning, and inventory management.\"\n\n\nShailesh Mendonca\nCommercial lead-in Adventure AHQ- Sharaf Group\n\n\n\n\n“ Haytham mentored me in my Role of Head of Supply Chain efficiency. He is extremely knowledgebase about the supply concepts, latest trends, and benchmarks in the supply chain world. Haytham’s analytics-driven approach was very helpful for me to recommend and implement significant changes to our supply chain at Aster group”\nSaify Naqvi\nHead of Supply Chain Efficiency\n\n\n\n\n“I participated in the training session called \"Supply Chain Forecasting & Management\" on December 22nd 2018. This training helped me a lot in my daily work since I am working in Purchase Dpt. Haytham have the pedagogy to explain us very difficult calculations and formula in a simple way. I highly recommend this training.”\nDjamel BOUREMIZ\nPurchasing Manager at Mineral Circles Bearings",
      "target_audience": [
        "Python",
        "Supply chain.",
        "Analytics",
        "Forecasting",
        "Inventory",
        "Pricing",
        "Machine learning",
        "Revenue Mangement",
        "Linear Programming",
        "Simulations"
      ]
    },
    {
      "title": "YOLO: Automatic License Plate Detection & Extract text App",
      "url": "https://www.udemy.com/course/deep-learning-web-app-project-number-plate-detection-ocr/",
      "bio": "Learn to Develop License Plate Object Detection, OCR and Create Web App Project using Deep Learning, TensorFlow 2, Flask",
      "objectives": [
        "Object Detection from Scratch",
        "License Plate Detection",
        "Extract text from Image using Tesseract",
        "Train InceptionResnet V2 in TensorFlow 2 for Object Detection",
        "Flask Based Web API",
        "Labeling Object Detection Data using Image Annotation Tool",
        "Train custom YOLO model from scratch",
        "Real time license plate detection with YOLO"
      ],
      "course_content": {},
      "requirements": [
        "Basic knowledge on Python",
        "Knowledge on Deep learning with TensorFlow",
        "Basics on HTML"
      ],
      "description": "Welcome to NUMBER PLATE DETECTION AND OCR: A DEEP LEARNING WEB APP PROJECT from scratch\nImage Processing and Object Detection is one of the areas of Data Science and has a wide variety of applications in the industries in the current world. Many industries looking for a Data Scientist with these skills. This course covers modeling techniques including labeling Object Detection data (images), data preprocessing, Deep Learning Model building (InceptionResNet V2), evaluation, and production (Web App)\nWe start this course Project Architecture that was followed to Develop this App in Python. Then I will show how to gather data and label images for object detection for Licence Plate or Number Plate using Image Annotation Tool which is open-source software developed in python GUI (pyQT).\nThen after we label the image we will work on data preprocessing, build and train deep learning object detection model (InceptionResnet V2) in TensorFlow 2. Once the model is trained with the best loss, we will evaluate the model. I will show you how to calculate the\nIntersection Over Union (IoU)\nThe precision of the object detection model.\nOnce we have done with the Object Detection model, then using this model we will crop the image which contains the license plate which is also called the region of interest (ROI), and pass the ROI to Optical Character Recognition API Tesseract in Python (Pytesseract). In this model, I will show you how to extract text from images.  Now, we will put it all together and build a Pipeline Deep Learning model.\nIn the final module, we will learn to create a web app project using FLASK Python. Initially, we will learn basics concepts in Flask like URL routing, render the template, template inheritance, etc. Then we will create our website using HTML, Bootstrap. With that we are finally ready with our App.\nWHAT YOU WILL LEARN?\nBuilding Project in Python Programming\nLabeling Image for Object Detection\nTrain Object Detection model (InceptionResNet V2) in TensorFlow 2.x\nModel Evaluation\nOptical Character Recognition with Pytesseract\nFlask API\nFlask Web App Development in HTML, Boostrap, Python\nTrain YOLO model with Custom data\nDevelop web application and integrate YOLO Model\n\n\nWe know that Computer Vision-Based Web App is one of those topics that always leaves some doubts. Feel free to ask questions in Q & A and we are very happy to answer all your questions.\nWe also provided all Notebooks, py files in the resources which will useful for reference.",
      "target_audience": [
        "Anyone who want to build deep learning project from sctrach",
        "A python developer who want to develop Number Plate OCR Project",
        "Anyone who want to learn end to end Deep Learning Project",
        "Who are curious in developing Web App project in TensorFlow 2"
      ]
    },
    {
      "title": "13 Python Data Analytics Real World Hands-on Projects",
      "url": "https://www.udemy.com/course/bigdata-analysis-python/",
      "bio": "First step towards Data Science in this competitive job market",
      "objectives": [
        "Master Big Data Analytics utilizing Python programming language",
        "Acquire proficiency in completing data analysis tasks using Python",
        "Apply Python Pandas Library to solve real-time analytical questions",
        "Enhance analytical skills through hands-on projects",
        "Explore core Python programming language concepts relevant to data analysis",
        "Gain insights into basic Data Science methodologies and practices",
        "Access downloadable source codes and datasets for all projects",
        "Utilize Python libraries such as Pandas and Matplotlib to perform advanced data analysis",
        "Understand fundamental data manipulation techniques using Pandas",
        "Visualize data effectively using Matplotlib",
        "Learn to handle diverse datasets efficiently",
        "Develop a solid foundation in Python for data analytics purposes",
        "Experience an engaging learning journey",
        "Analyze various datasets effectively - Weather Data, Netflix Data, Covid-19 Data, Cars Data, Police Data, London Housing Data, Census Data, Udemy Data"
      ],
      "course_content": {
        "Data Analysis with Python": [
          "Project 1 : Weather Data Analysis",
          "Project 1 : Weather Data Analysis",
          "Project 2 : Cars Data Analysis",
          "Project 2 : Cars Data Analysis",
          "Project 3 : Police Data Analysis",
          "Project 3 : Police Data Analysis",
          "Project 4 : Covid-19 Data Analysis",
          "Project 4 : Covid-19 Data Analysis",
          "Project 5 : London Housing Data Analysis",
          "Project 6 : Census Data Analysis",
          "Project 5 : London Housing Data Analysis",
          "Project 6 : Census Data Analysis",
          "Project 7 : Udemy Data Analysis",
          "Project 7 : Udemy Data Analysis",
          "Project 8 : Netflix Data Analysis",
          "Project 8 : Netflix Data Analysis",
          "Project 9 : Sales Data Analysis",
          "Project 10 : Spotify & YouTube Data Analysis",
          "Project 11 : Airlines' Flights Data Analysis",
          "Project 12 : AI Financial Market Data Analysis",
          "Project 13 : HR Data Analysis"
        ]
      },
      "requirements": [
        "Only Basic Python Programming knowledge is required",
        "You can use any IDE like Jupyter Notebook or Google Colab etc for coding",
        "All source codes and datasets are freely available to download",
        "Interest in Data Analytics / Data Science / Python"
      ],
      "description": "In this comprehensive course, we present to you 13 Data Analytics projects solved using Python, a language renowned for its versatility and effectiveness in the realm of data analysis.\nThese projects serve as an invaluable resource for individuals embarking on their journey towards a career in Data Science domain, offering practical insights and hands-on experience essential for success in the field.\nMoreover, for those contemplating a transition into the dynamic and rewarding domain of data analytics, these projects provide a solid foundation, equipping learners with the requisite skills and knowledge.\nDesigned with students in mind, these projects are not only educational but also serve as potential submissions for academic institutions.\nAs part of our commitment to fostering a supportive learning environment, we provide access to the source codes and datasets for all projects.\nEach project is accompanied by clear and concise explanations, ensuring accessibility for learners of all levels.\nCentral to the completion of these projects is the utilization of the Python Pandas Library, a powerful toolset for data manipulation and analysis.\n\n\nNow, let's delve into the diverse array of projects awaiting you:\nProject 1 - Weather Data Analysis\nProject 2 - Cars Data Analysis\nProject 3 - Police Data Analysis\nProject 4 - Covid Data Analysis\nProject 5 - London Housing Data Analysis\nProject 6 - Census Data Analysis\nProject 7 - Udemy Data Analysis\nProject 8 - Netflix Data Analysis\nProject 9 - Sales Data Analysis\nProject 10 - Spotify & YouTube Data Analysis\nProject 11 - Airlines' Flights Data Analysis\nProject 12 - AI Financial Market Data Analysis\nProject 13 - HR Data Analysis\n\n\nSome examples of commands used in these projects are :\n* reset_index() - To convert the index of a Series into a column to form a DataFrame.\n* loc[ ] - To show any row's values.\n* info() - To provide the basic information about the dataframe.\n* drop() - To drop any column or row from the dataframe.\n* str.strip().str.replace(r'\\s+', ' ', regex=True) - To remove extra spaces in any text column.\n* duplicated() - To show all the duplicate records from a dataframe.\n* drop_duplicates(inplace=True) - To remove the duplicate records from the dataframe.\n* round() - To round-off the values of a numerical column.\n* to_datetime() - To convert the datatype of date column into datetime format.\n* groupby() - To make the group of all unique values of a column.\n* std()  - To check the standard deviation of any numerical column.\n* var()  - To check the variance of any numerical column.\n* mean()  - To check the mean of any numerical column.\n* agg() - Using agg() with groupby().\n* head() - It shows the first N rows in the data (by default, N=5).\n* columns - To show all the column names of the dataframe.\n* unique() - In a column, it shows all the unique values. It can be applied on a single column only, not on the whole dataframe.\n* nunique() - It shows the total no. of unique values in each column. It can be applied on a single column as well as on the whole dataframe.\n* describe() - To show some summary about the columns.\n* astype() - To change the datatype of any column.\n* dtype - To check the datatype of any column.\n* value_counts - In a column, it shows all the unique values with their count. It can be applied on a single column only.\n* plot(kind='bar') - To draw the bar graph.\n* type() - To the type of any variable.\n* plt.figure(figsize = ()) - To set the size of any figure.\n* plt.title(), plt.xlabel(), plt.ylabel() - To set the Title, x-axis label, y-axis label.\n* sort_values(ascending = False) - To sort the values in descending order.\n* dt.month - To create a new column showing Month only.\n* shape - It shows the total no. of rows and no. of columns of the dataframe\n* index - This attribute provides the index of the dataframe\n* dtypes - It shows the data-type of each column\n* count - It shows the total no. of non-null values in each column. It can be applied on a single column as well as on the whole dataframe.\n* isnull( ) - To show where Null value is present.\n* dropna( ) - It drops the rows that contains all missing values.\n* isin( ) - To show all records including particular elements.\n* str.contains( ) - To get all records that contains a given string.\n* str.split( ) - It splits a column's string into different columns.\n* dt.year.value_counts( ) - It counts the occurrence of all individual years in Time column.\n* sns.countplot(df['Col_name']) - To show the count of all unique values of any column in the form of bar graph.\n* max( ), min( ) - It shows the maximum/minimum value of the series\n\n\nThrough these projects and commands, learners will not only acquire essential skills in data analysis but also gain a deeper understanding of the underlying principles and methodologies driving the field of data analytics.\nWhether you're pursuing a career as a Data Analyst, seeking to enhance your academic portfolio, or simply eager to expand your knowledge and skills in Python-based data analysis, this course is tailored to meet your needs and aspirations.",
      "target_audience": [
        "Anyone looking for Data Analyst job",
        "Students looking for Data Analytics Projects",
        "Beginner & Intermediate Python Programmers",
        "Anyone wants to enhance big data analysis skills"
      ]
    },
    {
      "title": "Deep Learning with PyTorch for Medical Image Analysis",
      "url": "https://www.udemy.com/course/deep-learning-with-pytorch-for-medical-image-analysis/",
      "bio": "Learn how to use Pytorch-Lightning to solve real world medical imaging tasks!",
      "objectives": [
        "Learn how to use NumPy",
        "Learn classic machine learning theory principals",
        "Foundations of Medical Imaging",
        "Data Formats in Medical Imaging",
        "Creating Artificial Neural Networks with PyTorch",
        "Use PyTorch-Lightning for state of the art training",
        "Visualize the decision of a CNN",
        "2D & 3D data handling",
        "Automatic Cancer Segmentation"
      ],
      "course_content": {
        "Introduction": [
          "COURSE OVERVIEW LECTURE - PLEASE DO NOT SKIP!",
          "Link to Download the Course Files",
          "Installation and Environment Setup",
          "Installation without yml file",
          "Course Curriculum"
        ],
        "Crash Course: NumPy": [
          "Introduction to NumPy",
          "NumPy Arrays",
          "NumPy Arrays Part Two",
          "NumPy Index Selection",
          "NumPy Operations",
          "NumPy Exercises",
          "NumPy Exercise - Solutions"
        ],
        "Machine Learning Concepts Overview": [
          "What is Machine Learning",
          "Supervised Learning",
          "Overfitting",
          "Evaluating Performance - Classification Error Metrics",
          "Evaluating Performance - Regression Error Metrics"
        ],
        "PyTorch Basics": [
          "PyTorch Basics Introduction",
          "Tensor Basics",
          "Tensor Basics-Part Two",
          "Tensor Operations",
          "Tensor Operations-Part Two",
          "PyTorch Basics - Exercise",
          "PyTorch Basics - Exercise Solutions"
        ],
        "CNN - Convolutional Neural Networks": [
          "Introduction to CNNs",
          "Understanding the MNIST data set",
          "ANN with MNIST - Part One - Data",
          "ANN with MNIST - Part Two - Creating the Network",
          "IMPORTANT: Library Difference between video and notebook",
          "ANN with MNIST - Part Three - Training",
          "ANN with MNIST - Part Four - Evaluation",
          "Image Filters and Kernels",
          "Convolutional Layers",
          "Pooling Layers",
          "MNIST Data Revisited",
          "MNIST with CNN - Code Along - Part One",
          "MNIST with CNN - Code Along - Part Two",
          "MNIST with CNN - Code Along - Part Three",
          "Why do we need GPUs?",
          "Using GPUs for PyTorch"
        ],
        "Medical Imaging - A short Introduction": [
          "Introduction",
          "Overview: X-RAY",
          "Overview: CT",
          "Overview: MRI",
          "Overview: PET"
        ],
        "Data Formats in Medical Imaging": [
          "Introduction",
          "DICOM",
          "DICOM-in-Python",
          "NIfTI",
          "NIfTI-in-Python",
          "Preprocessing",
          "Preprocessing-in-Python-Part-1",
          "Preprocessing-in-Python-Part-2"
        ],
        "Pneumonia-Classification": [
          "Introduction",
          "Preprocessing",
          "Train-01-Data-Loading",
          "Train-02-Model-Creation",
          "Train-03-Trainer",
          "Train-04-Evaluation",
          "Interpretability"
        ],
        "Cardiac-Detection": [
          "01-Introduction",
          "02-Preprocessing",
          "03-Dataset-Part-1",
          "04-Dataset-Part-2",
          "Train-01-Data-Loading",
          "Train-02-Model-Creation",
          "Train-03-Evaluation"
        ],
        "Atrium-Segmentation": [
          "01-Introduction",
          "Preprocessing-01-Visualization",
          "Preprocessing-02-Processing",
          "Dataset-01-Dataset-Creation",
          "Dataset-02-Dataset-Validation",
          "UNet",
          "Train-01-Data-Loading-and-Loss",
          "Train-02-Model-Creation",
          "Train-03-Evaluation"
        ]
      },
      "requirements": [
        "Understanding of Python Basic Topics (data types,loops,functions) also Python OOP recommended",
        "Ideally PyTorch, but not necessarily required"
      ],
      "description": "Did you ever want to apply Deep Neural Networks to more than MNIST, CIFAR10 or cats vs dogs?\nDo you want to learn about state of the art Machine Learning frameworks while segmenting cancer in CT-images?\nThen this is the right course for you!\nWelcome to one of the most comprehensive courses on  Deep Learning in medical imaging!\nThis course focuses on the application of state of the art Deep Learning architectures to various medical imaging challenges.\nYou will tackle several different tasks, including cancer segmentation, pneumonia classification, cardiac detection, Interpretability and many more.\nThe following topics are covered:\nNumPy\nMachine Learning Theory\nTest/Train/Validation Data Splits\nModel Evaluation - Regression and Classification Tasks\nTensors with PyTorch\nConvolutional Neural Networks\nMedical Imaging\nInterpretability of a network's decision - Why does the network do what it does?\nA state of the art high level pytorch library: pytorch-lightning\nTumor Segmentation\nThree-dimensional data\nand many more\nWhy choose this specific Deep Learning with PyTorch for Medical Image Analysis course ?\nThis course provides unique knowledge on the application of deep learning to highly complex and  non-standard (medical) problems (in 2D and 3D)\nAll lessons include clearly summarized theory and code-along examples, so that you can understand and follow every step.\nPowerful online community with our QA Forums with thousands of students and dedicated Teaching Assistants, as well as student interaction on our Discord Server.\nYou will learn skills and techniques that the vast majority of AI engineers do not have!\n--------------\nJose, Marcel, Sergios & Tobias",
      "target_audience": [
        "Python developers and Machine Learning engineers who want to learn how to tackle real world problems occurring on a daily basis in the field of medical imaging with the help of Deep Convolutional Neural Networks.",
        "Everybody who wants to learn more about the joint field of AI and Medical Imaging & how it works",
        "Developers familiar with basic Deep Learning knowledge who want to apply their skills to more than toy problems",
        "Medical professionals interested in how AI actually works in medicine"
      ]
    },
    {
      "title": "Linear Regression and Logistic Regression in Python",
      "url": "https://www.udemy.com/course/linear-regression-and-logistic-regression-in-python-starttech/",
      "bio": "Build predictive ML models with no coding or maths background. Linear Regression and Logistic Regression for beginners",
      "objectives": [
        "Learn how to solve real life problem using the Linear and Logistic Regression technique",
        "Preliminary analysis of data using Univariate and Bivariate analysis before running regression analysis",
        "Understand how to interpret the result of Linear and Logistic Regression model and translate them into actionable insight",
        "Indepth knowledge of data collection and data preprocessing for Linear and Logistic Regression problem",
        "Basic statistics using Numpy library in Python",
        "Data representation using Seaborn library in Python",
        "Linear Regression technique of Machine Learning using Scikit Learn and Statsmodel libraries of Python"
      ],
      "course_content": {},
      "requirements": [
        "This course starts from basics and you do not even need coding background to build these models in Python",
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Linear Regression and Logistic Regression course that teaches you everything you need to create a Linear or Logistic Regression model in Python, right?\nYou've found the right Linear Regression course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using linear and logistic regression technique of Machine Learning.\nCreate a linear regression and logistic regression model in Python and analyze its result.\nConfidently model and solve regression and classification problems\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Linear Regression model, which is the most popular Machine Learning model, to solve business problems.\nBelow are the course contents of this course on Linear Regression:\nSection 1 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics\nthen graphical representations to describe the data and then a lecture on measures of center like mean\nmedian and mode and lastly measures of dispersion like range and standard deviation\nSection 2 - Python basic\nThis section gets you started with Python.\nThis section will help you set up the python and Jupyter environment on your system and it'll teach\nyou how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nSection 3 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\nSection 4 - Data Preprocessing\nIn this section you will learn what actions you need to take a step by step to get the data and then\nprepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment, missing value imputation, variable transformation and correlation.\nSection 5 - Regression Model\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you\nunderstand where the concept is coming from and how it is important. But even if you don't understand\nit,  it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results, what are other variations to the ordinary least squared method and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a regression model in Python will soar. You'll have a thorough understanding of how to use regression modelling to create predictive models and solve business problems.\n\n\nHow this course will help you?\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you the most popular techniques of machine learning, which is Linear Regression and Logistic Regregression\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear and logistic regression.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhat is the Linear regression technique of Machine learning?\nLinear Regression is a simple machine learning model for regression problems, i.e., when the target variable is a real value.\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\nWhen there is a single input variable (x), the method is referred to as simple linear regression.\nWhen there are multiple input variables, the method is known as multiple linear regression.\nWhy learn Linear regression technique of Machine learning?\nThere are four reasons to learn Linear regression technique of Machine learning:\n1. Linear Regression is the most popular machine learning technique\n2. Linear Regression has fairly good prediction accuracy\n3. Linear Regression is simple to implement and easy to interpret\n4. It gives you a firm base to start learning other advanced techniques of Machine Learning\nHow much time does it take to learn Linear regression technique of machine learning?\nLinear Regression is easy but no one can determine the learning time it takes. It totally depends on you. The method we adopted to help you learn Linear regression starts from the basics and takes you to advanced level within hours. You can follow the same, but remember you can learn nothing without practicing it. Practice is the only way to remember whatever you have learnt. Therefore, we have also provided you with another data set to work on as a separate project of Linear regression.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 4 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of Linear and Logistic Regression modelling - Having a good knowledge of Linear and Logistic Regression gives you a solid understanding of how machine learning works. Even though Linear regression is the simplest technique of Machine learning, it is still the most popular one with fairly good prediction ability. Fifth and sixth section cover Linear regression topic end-to-end and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.\nWhy use Python for data Machine Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Machine Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nMachine Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master Linear and Logistic Regression from beginner to advanced level in a short span of time"
      ]
    },
    {
      "title": "PyTorch: Deep Learning and Artificial Intelligence",
      "url": "https://www.udemy.com/course/pytorch-deep-learning/",
      "bio": "Neural Networks for Computer Vision, Time Series Forecasting, NLP, GANs, Reinforcement Learning, and More!",
      "objectives": [
        "Artificial Neural Networks (ANNs) / Deep Neural Networks (DNNs)",
        "Predict Stock Returns",
        "Time Series Forecasting",
        "Computer Vision",
        "How to build a Deep Reinforcement Learning Stock Trading Bot",
        "GANs (Generative Adversarial Networks)",
        "Recommender Systems",
        "Image Recognition",
        "Convolutional Neural Networks (CNNs)",
        "Recurrent Neural Networks (RNNs)",
        "Natural Language Processing (NLP) with Deep Learning",
        "Demonstrate Moore's Law using Code",
        "Transfer Learning to create state-of-the-art image classifiers",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {},
      "requirements": [
        "Know how to code in Python and Numpy",
        "For the theoretical parts (optional), understand derivatives and probability"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\n\n\nWelcome to PyTorch: Deep Learning and Artificial Intelligence!\n\n\nAlthough Google's Deep Learning library Tensorflow has gained massive popularity over the past few years, PyTorch has been the library of choice for professionals and researchers around the globe for deep learning and artificial intelligence.\nIs it possible that Tensorflow is popular only because Google is popular and used effective marketing?\nWhy did Tensorflow change so significantly between version 1 and version 2? Was there something deeply flawed with it, and are there still potential problems?\nIt is less well-known that PyTorch is backed by another Internet giant, Facebook (specifically, the Facebook AI Research Lab - FAIR). So if you want a popular deep learning library backed by billion dollar companies and lots of community support, you can't go wrong with PyTorch. And maybe it's a bonus that the library won't completely ruin all your old code when it advances to the next version. ;)\nOn the flip side, it is very well-known that all the top AI shops (ex. OpenAI, Apple, and JPMorgan Chase) use PyTorch. OpenAI just recently switched to PyTorch in 2020, a strong sign that PyTorch is picking up steam.\nIf you are a professional, you will quickly recognize that building and testing new ideas is extremely easy with PyTorch, while it can be pretty hard in other libraries that try to do everything for you. Oh, and it's faster.\n\n\nDeep Learning has been responsible for some amazing achievements recently, such as:\nGenerating beautiful, photo-realistic images of people and things that never existed (GANs)\nBeating world champions in the strategy game Go, and complex video games like CS:GO and Dota 2 (Deep Reinforcement Learning)\nSelf-driving cars (Computer Vision)\nSpeech recognition (e.g. Siri) and machine translation (Natural Language Processing)\nEven creating videos of people doing and saying things they never did (DeepFakes - a potentially nefarious application of deep learning)\n\n\nThis course is for beginner-level students all the way up to expert-level students. How can this be?\nIf you've just taken my free Numpy prerequisite, then you know everything you need to jump right in. We will start with some very basic machine learning models and advance to state of the art concepts.\nAlong the way, you will learn about all of the major deep learning architectures, such as Deep Neural Networks, Convolutional Neural Networks (image processing), and Recurrent Neural Networks (sequence data).\nCurrent projects include:\nNatural Language Processing (NLP)\nRecommender Systems\nTransfer Learning for Computer Vision\nGenerative Adversarial Networks (GANs)\nDeep Reinforcement Learning Stock Trading Bot\nEven if you've taken all of my previous courses already, you will still learn about how to convert your previous code so that it uses PyTorch, and there are all-new and never-before-seen projects in this course such as time series forecasting and how to do stock predictions.\nThis course is designed for students who want to learn fast, but there are also \"in-depth\" sections in case you want to dig a little deeper into the theory (like what is a loss function, and what are the different types of gradient descent approaches).\nI'm taking the approach that even if you are not 100% comfortable with the mathematical concepts, you can still do this! In this course, we focus more on the PyTorch library, rather than deriving any mathematical equations. I have tons of courses for that already, so there is no need to repeat that here.\n\n\nInstructor's Note: This course focuses on breadth rather than depth, with less theory in favor of building more cool stuff. If you are looking for a more theory-dense course, this is not it. Generally, for each of these topics (recommender systems, natural language processing, reinforcement learning, computer vision, GANs, etc.) I already have courses singularly focused on those topics.\n\n\nThanks for reading, and I’ll see you in class!\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Beginners to advanced students who want to learn about deep learning and AI in PyTorch"
      ]
    },
    {
      "title": "Data Science Project Planning",
      "url": "https://www.udemy.com/course/data-science-project-planning/",
      "bio": "Fundamental Concepts for Beginners",
      "objectives": [
        "Fundamental concepts underlying core planning activities that are critical for a data science project's success.",
        "PLEASE NOTE: This course will not cover technical topics like programming , statistics and algorithms."
      ],
      "course_content": {},
      "requirements": [
        "Willingness to look beyond the technical aspects and learn about the crucial planning activities involved in a data science project.",
        "Familiarity with high school level mathematics"
      ],
      "description": "Success of any project depends highly on how well it has been planned. Data science projects are no exception.\nLarge number of data science projects in industrial settings fail to meet the expectations due to lack of proper planning at their inception stage.\nThis course will provide a overview of core planning activities that are critical to the success of any data science project.\nWe will discuss the concepts underlying  - Business Problem Definition; Data Science Problem Definition; Situation Assessment; Scheduling Tasks and Deliveries.\nThe concepts learned will help the students in:\nA) Framing the business problem\nB) Getting buy-in from the stakeholders\nC) Identifying appropriate data science solution that can solve the business problem\nD) Defining success criteria and metrics to evaluate the key project deliverables  viz;  models, data flow pipeline and documentation.\nE) Assessing the prevailing situation impacting the project. For e.g. availability of data and resources; risks; estimated costs and perceived benefits.\nF) Preparing delivery schedules that enable early and continuously incremental valuable actionable insights to the customers\nG) Understanding the desired team attributes and communication needs",
      "target_audience": [
        "Managers or Leads who are going to plan their first data science project in a real life business environment",
        "Members of a data science team who want to build awareness about crucial planning activities required for making their project successful",
        "Senior Executives requiring a bird’s eye view of activities involved in planning a data science project"
      ]
    },
    {
      "title": "Machine Learning- From Basics to Advanced",
      "url": "https://www.udemy.com/course/step-by-step-guide-to-machine-learning-course/",
      "bio": "A beginners guide to learn Machine Learning (including Hands-on projects - From Basic to Advance Level)",
      "objectives": [
        "Learn how to use NumPy, to do fast mathematical calculations in machine learning.",
        "Learn what is Machine Learning and Data Wrangling in machine learning.",
        "Learn how to use scikit-learn for data-preprocessing in machine learning.",
        "Learn different model selection and feature selections techniques in machine learning.",
        "Learn about cluster analysis and anomaly detection in machine learning.",
        "Learn about SVMs for classification, regression and outliers detection in machine learning."
      ],
      "course_content": {},
      "requirements": [
        "Basic knowledge of scripting and programming",
        "Basic knowledge of python programming"
      ],
      "description": "If you are looking to start your career in Machine learning then this is the course for you.\nThis is a course designed in such a way that you will learn all the concepts of machine learning right from basic to advanced levels.\nThis course has 5 parts as given below:\nIntroduction & Data Wrangling in machine learning\nLinear Models, Trees & Preprocessing in machine learning\nModel Evaluation, Feature Selection & Pipelining in machine learning\nBayes, Nearest Neighbors & Clustering in machine learning\nSVM, Anomalies, Imbalanced Classes, Ensemble Methods in machine learning\nFor the code explained in each lecture, you can find a GitHub link in the resources section.\n\n\nWho's teaching you in this course?\nI am Professional Trainer and consultant for Languages C, C++, Python, Java, Scala, Big Data Technologies - PySpark, Spark using Scala Machine Learning & Deep Learning- sci-kit-learn, TensorFlow, TFLearn, Keras, h2o and delivered at corporates like GE, SCIO Health Analytics, Impetus, IBM Bangalore & Hyderabad, Redbus, Schnider, JP Morgan - Singapore & HongKong, CISCO, Flipkart, MindTree, DataGenic, CTS - Chennai, HappiestMinds, Mphasis, Hexaware, Kabbage. I have shared my knowledge that will guide you to understand the holistic approach towards ML.\n\n\nMachine learning is the fuel we need to power robots, alongside AI. With Machine Learning, we can power programs that can be easily updated and modified to adapt to new environments and tasks to get things done quickly and efficiently.\nHere are a few reasons for you to pursue a career in Machine Learning:\n1) Machine learning is a skill of the future – Despite the exponential growth in Machine Learning, the field faces skill shortage. If you can meet the demands of large companies by gaining expertise in Machine Learning, you will have a secure career in a technology that is on the rise.\n2) Work on real challenges – Businesses in this digital age face a lot of issues that Machine learning promises to solve. As a Machine Learning Engineer, you will work on real-life challenges and develop solutions that have a deep impact on how businesses and people thrive. Needless to say, a job that allows you to work and solve real-world struggles gives high satisfaction.\n3) Learn and grow – Since Machine Learning is on the boom, by entering into the field early on, you can witness trends firsthand and keep on increasing your relevance in the marketplace, thus augmenting your value to your employer.\n4) An exponential career graph – All said and done, Machine learning is still in its nascent stage. And as the technology matures and advances, you will have the experience and expertise to follow an upward career graph and approach your ideal employers.\n5) Build a lucrative career– The average salary of a Machine Learning engineer is one of the top reasons why Machine Learning seems a lucrative career to a lot of us. Since the industry is on the rise, this figure can be expected to grow further as the years pass by.\n6) Side-step into data science – Machine learning skills help you expand avenues in your career. Machine Learning skills can endow you with two hats- the other of a data scientist. Become a hot resource by gaining expertise in both fields simultaneously and embark on an exciting journey filled with challenges, opportunities, and knowledge.\n\nMachine learning is happening right now. So, you want to have an early bird advantage of toying with solutions and technologies that support it. This way, when the time comes, you will find your skills in much higher demand and will be able to secure a career path that’s always on the rise.\n\nEnroll Now!! See You in Class.\n\nHappy learning\nTeam Edyoda",
      "target_audience": [
        "Beginners who want to become a data scientist",
        "Software developers who want to learn machine learning from scratch",
        "Python developers who are interested to learn machine learning",
        "Professionals who want to start their career in Machine Leaning"
      ]
    },
    {
      "title": "Introduction to Machine Learning for Data Science",
      "url": "https://www.udemy.com/course/machine-learning-for-data-science/",
      "bio": "A primer on Machine Learning for Data Science. Revealed for everyday people, by the Backyard Data Scientist.",
      "objectives": [
        "Genuinely understand what Computer Science, Algorithms, Programming, Data, Big Data, Artificial Intelligence, Machine Learning, and Data Science is.",
        "To understand how these different domains fit together, how they are different, and how to avoid the marketing fluff.",
        "The Impacts Machine Learning and Data Science is having on society.",
        "To really understand computer technology has changed the world, with an appreciation of scale.",
        "To know what problems Machine Learning can solve, and how the Machine Learning Process works.",
        "How to avoid problems with Machine Learning, to successfully implement it without losing your mind!"
      ],
      "course_content": {
        "Introduction": [
          "Course Promotion Video",
          "A special message for hard of hearing and ESL students",
          "Thank you for investing in this Course!",
          "Course Overview",
          "Secret sauce inside!: How to get the most out of this course.",
          "Course Links Reference Guide and Lecture Resources",
          "Course Survey"
        ],
        "Core Concepts": [
          "Core Concepts Overview",
          "Computer Science - the `Train Wreck' Definition",
          "What's Data / \"I can see data everywhere!\"",
          "Structured vs Unstructured Data",
          "Structured and Unstructured Data",
          "Computer Science - Definition Revisited & The Greatest \"lie\" ever SOLD....",
          "What's big data?",
          "Big Data - Quiz",
          "What is Artificial Intelligence (AI)",
          "What is Machine Learning? - Part 1 - The ideas",
          "What is Machine Learning? - Part 2 - An Example",
          "What is data science?",
          "Recap & How do these relate to each other?"
        ],
        "Impacts, Importance and examples": [
          "Impacts, Importance and examples - Overview",
          "Why is this important now?",
          "Computers exploding! - The explosive growth of computer power explained.",
          "What problems does Machine Learning Solve?",
          "Where it's transforming our lives"
        ],
        "The Machine Learning Process": [
          "The Machine Learning Process - Overview",
          "5 Step Machine Learning Process Overview",
          "1 - Asking the right question",
          "2 - Identifying, obtaining, and preparing the right data",
          "3 - Identifying and applying a ML Algorithm",
          "4 - Evaluating the performance of the model and adjusting",
          "5 - Using and presenting the model",
          "Machine Learning - Process"
        ],
        "How to apply Machine Learning for Data Science": [
          "How to apply Machine Learning for Data Science - Overview",
          "Where to begin your journey",
          "Common platforms and tools for Data Science",
          "Data Science using - R",
          "Data Science using - Python",
          "Data Science using SQL",
          "Data Science using Excel",
          "Data Science using RapidMiner",
          "Cautionary Tales"
        ],
        "Conclusion": [
          "All done! What's next?"
        ],
        "Section 1 -Bonus course - Machine Learning in Python and Jupyter for Beginners": [
          "Introduction and Anaconda Installation",
          "What will we cover!",
          "Introduction and Setup"
        ],
        "Section 2 -Bonus course - Machine Learning in Python and Jupyter for Beginners": [
          "Crash course in Python - Beginning concepts",
          "Crash course in Python - Strings, Slices and Lists!",
          "Crash course in Python - Expressions, Operators, Conditions and Loops",
          "Crash course in Python - Functions, Scope, Dictionaries and more!"
        ],
        "Section 3 - Bonus course - Machine Learning in Python and Jupyter for Beginners": [
          "Hands on Running Python"
        ],
        "Section 4 - Bonus course - Machine Learning in Python and Jupyter for Beginners": [
          "Foundations of Machine Learning and Data Science - Definitions and concepts.",
          "Foundations of Machine Learning and Data Science - Machine Learning Workflow",
          "Foundations of Machine Learning and Data Science - Algorithms, concepts and more"
        ]
      },
      "requirements": [
        "A passion to learn, and basic computer skills!",
        "Students should understand basic high-school level mathematics, but Statistics is not required to understand this course."
      ],
      "description": "Disclaimer:\nThe second of this course demonstrates techniques using Jupyter Notebooks from Anaconda.  You are welcome to follow along (however), it is not required to do these exercises to complete this course. If you are a Udemy Business user, please check with your employer before downloading software.\n\n\nWelcome!:\nThank you all for the huge response to this emerging course!  We are delighted to have over 20,000 students in over 160 different countries.  I'm genuinely touched by the overwhelmingly positive and thoughtful reviews.  It's such a privilege to share and introduce this important topic with everyday people in a clear and understandable way.\nI'm also excited to announce that I have created real closed captions for all course material, so weather you need them due to a hearing impairment, or find it easier to follow long (great for ESL students!)... I've got you covered.\nMost importantly:\nTo make this course \"real\", we've expanded.  In November of 2018, the course went from 41 lectures and 8 sections, to 62 lectures and 15 sections!  We hope you enjoy the new content!\n\n\nUnlock the secrets of understanding Machine Learning for Data Science!\nIn this introductory course, the “Backyard Data Scientist” will guide you through wilderness of Machine Learning for Data Science.  Accessible to everyone, this introductory course not only explains Machine Learning, but where it fits in the “techno sphere around us”, why it’s important now, and how it will dramatically change our world today and for days to come.\n\n\nOur exotic journey will include the core concepts of:\nThe train wreck definition of computer science and one that will actually instead make sense.\nAn explanation of data that will have you seeing data everywhere that you look!\nOne of the “greatest lies” ever sold about the future computer science.\nA genuine explanation of Big Data, and how to avoid falling into the marketing hype.\nWhat is Artificial intelligence?  Can a computer actually think?  How do computers do things like navigate like a GPS or play games anyway?\nWhat is Machine Learning?  And if a computer can think – can it learn?\nWhat is Data Science, and how it relates to magical unicorns!\nHow Computer Science, Artificial Intelligence, Machine Learning, Big Data and Data Science interrelate to one another.\nWe’ll then explore the past and the future while touching on the importance, impacts and examples of Machine Learning for Data Science:\nHow a perfect storm of data, computer and Machine Learning algorithms have combined together to make this important right now.\nWe’ll actually make sense of how computer technology has changed over time while covering off a journey from 1956 to 2014.  Do you have a super computer in your home?  You might be surprised to learn the truth.\nWe’ll discuss the kinds of problems Machine Learning solves, and visually explain regression, clustering and classification in a way that will intuitively make sense.\nMost importantly we’ll show how this is changing our lives.  Not just the lives of business leaders, but most importantly…you too!\nTo make sense of the Machine part of Machine Learning, we’ll explore the Machine Learning process:\nHow do you solve problems with Machine Learning and what are five things you must do to be successful?\nHow to ask the right question, to be solved by Machine Learning.\nIdentifying, obtaining and preparing the right data … and dealing with dirty data!\nHow every mess is “unique” but that tidy data is like families!\nHow to identify and apply Machine Learning algorithms, with exotic names like “Decision Trees”, “Neural Networks” “K’s Nearest Neighbors” and “Naive Bayesian Classifiers”\nAnd the biggest pitfalls to avoid and how to tune your Machine Learning models to help ensure a successful result for Data Science.\nOur final section of the course will prepare you to begin your future journey into Machine Learning for Data Science after the course is complete.  We’ll explore:\nHow to start applying Machine Learning without losing your mind.\nWhat equipment Data Scientists use, (the answer might surprise you!)\nThe top five tools Used for data science, including some surprising ones.\nAnd for each of the top five tools – we’ll explain what they are, and how to get started using them.\nAnd we’ll close off with some cautionary tales, so you can be the most successful you can be in applying Machine Learning to Data Science problems.\nBonus Course!  To make this “really real”, I’ve included a bonus course!\nMost importantly in the bonus course I’ll include information at the end of every section titled “Further Magic to Explore” which will help you to continue your learning experience.\nIn this bonus course we’ll explore:\nCreating a real live Machine Learning Example of Titanic proportions.  That’s right – we are going to predict survivability onboard the Titanic!\nUse Anaconda Jupyter and python 3.x\nA crash course in python - covering all the core concepts of Python you need to make sense of code examples that follow. See the included free cheat sheet!\nHands on running Python! (Interactively, with scripts, and with Jupyter)\nBasics of how to use Jupyter Notebooks\nReviewing and reinforcing core concepts of Machine Learning (that we’ll soon apply!)\nFoundations of essential Machine Learning and Data Science modules:\nNumPy – An Array Implementation\nPandas – The Python Data Analysis Library\nMatplotlib – A plotting library which produces quality figures in a variety of formats\nSciPy – The fundamental Package for scientific computing in Python\nScikit-Learn – Simple and efficient tools data mining, data analysis, and Machine Learning\nIn the titanic hands on example we’ll follow all the steps of the Machine Learning workflow throughout:\nAsking the right question.\nIdentifying, obtaining, and preparing the right data\nIdentifying and applying a Machine Learning algorithm\nEvaluating the performance of the model and adjusting\nUsing and presenting the model\nWe’ll also see a real world example of problems in Machine learning, including underfit and overfit.\nThe bonus course finishes with a conclusion and further resources to continue your Machine Learning journey.\nSo I invite you to join me, the Backyard Data Scientist on an exquisite journey into unlocking the secrets of Machine Learning for Data Science - for you know - everyday people like you!\nSign up right now, and we'll see you – on the other side.",
      "target_audience": [
        "Before you load Python, Before you start R - you need this course. This introductory course will introduce you to the Fundamentals, that you need before you start getting \"Hands on\".",
        "Anyone interested in understanding how Machine Learning is used for Data Science.",
        "Including business leaders, managers, app developers, consumers - you!",
        "Adventurous folks, whom are ready to strap themselves into the exotic world of Data Science and Machine Learning."
      ]
    },
    {
      "title": "PySpark Essentials for Data Scientists (Big Data + Python)",
      "url": "https://www.udemy.com/course/pyspark-essentials-for-data-scientists-big-data-python/",
      "bio": "Learn how to wrangle Big Data for Machine Learning using Python in PySpark taught by an industry expert!",
      "objectives": [
        "Use Python with Big Data on a distributed framework (Apache Spark)",
        "Work with REAL datasets on realistic consulting projects",
        "How to streaming LIVE data from Twitter using Spark Structured Streaming",
        "Learn how to create a \"Pandora Like\" app that classifies songs into genres using machine learning",
        "Flag suspicious job postings using Natural Language Processing",
        "Use machine learning to predict optimal cement strength and the factors that affect it",
        "Classify Christmas cooking recipes using Topic Modeling (LDA)",
        "Customer Segmentation using Gaussian Mixture Modeling (Clustering)",
        "Use cluster analysis to develop a strategy designed to increase college graduation rates for under-priveleged populations",
        "How to use the k-means clustering algorithm to define a marketing outreach strategy",
        "Integrate a UI to monitor your model training and development process with MLflow",
        "Theory and application of cutting edge data science algorithms",
        "Manipulate, Join and Aggregate Dataframes in Spark with Python",
        "Learn how to apply Spark's machine learning techniques on distributed Dataframes",
        "Cross Validation & Hyperparameter Tuning",
        "Frequent Pattern Mining Techniques",
        "Classification & Regression Techniques",
        "Data Wrangling for Natural Language Processing",
        "How to write SQL Queries in Spark"
      ],
      "course_content": {},
      "requirements": [
        "Familiarity with Python is helpful but not required",
        "Some background in data science is helpful but not required",
        "A hunger to LEARN"
      ],
      "description": "This course is for data scientists (or aspiring data scientists) who want to get PRACTICAL training in PySpark (Python for Apache Spark) using REAL WORLD datasets and APPLICABLE coding knowledge that you’ll use everyday as a data scientist! By enrolling in this course, you’ll gain access to over 100 lectures, hundreds of example problems and quizzes and over 100,000 lines of code!\nI’m going to provide the essentials for what you need to know to be an expert in Pyspark by the end of this course, that I’ve designed based on my EXTENSIVE experience consulting as a data scientist for clients like the IRS, the US Department of Labor and United States Veterans Affairs.\nI’ve structured the lectures and coding exercises for real world application, so you can understand how PySpark is actually used on the job. We are also going to dive into my custom functions that I wrote MYSELF to get you up and running in the MLlib API fast and make getting started building machine learning models a breeze! We will also touch on MLflow which will help us manage and track our model training and evaluation process in a custom user interface that will make you even more competitive on the job market!\nEach section will have a concept review lecture as well as code along activities structured problem sets for you to work through to help you put what you have learned into action, as well as the solutions to each problem in case you get stuck. Additionally, real world consulting projects have been provided in every section with AUTHENTIC datasets to help you think through how to apply each of the concepts we have covered.\nLastly, I’ve written up some condensed review notebooks and handouts of all the course content to make it super easy for you to reference later on. This will be super helpful once you land your first job programming in PySpark!\nI can’t wait to see you in the lectures! And I really hope you enjoy the course! I’ll see you in the first lecture!",
      "target_audience": [
        "Data Scientists interested in learning PySpark",
        "PySpark developers looking to strengthen their coding skills",
        "Python developers who need to work with big data",
        "Data Scientists who want to learn to work with big data"
      ]
    },
    {
      "title": "Statistics for Data Science and Business Analysis",
      "url": "https://www.udemy.com/course/statistics-for-data-science-and-business-analysis/",
      "bio": "Statistics you need in the office: Descriptive & Inferential statistics, Hypothesis testing, Regression analysis",
      "objectives": [
        "Understand the fundamentals of statistics",
        "Learn how to work with different types of data",
        "How to plot different types of data",
        "Calculate the measures of central tendency, asymmetry, and variability",
        "Calculate correlation and covariance",
        "Distinguish and work with different types of distributions",
        "Estimate confidence intervals",
        "Perform hypothesis testing",
        "Make data driven decisions",
        "Understand the mechanics of regression analysis",
        "Carry out regression analysis",
        "Use and understand dummy variables",
        "Understand the concepts needed for data science even with Python and R!"
      ],
      "course_content": {},
      "requirements": [
        "Absolutely no experience is required. We will start from the basics and gradually build up your knowledge. Everything is in the course.",
        "A willingness to learn and practice"
      ],
      "description": "Do you want to work as a Marketing Analyst, a Business Intelligence Analyst, a Data Analyst, or a Data Scientist?\nAnd you want to acquire the quantitative skills needed for the job?\nWell then, you’ve come to the right place!\nStatistics for Data Science and Business Analysis is here for you! (with TEMPLATES in Excel included)\nThis is where you start. And it is the perfect beginning!\nIn no time, you will acquire the fundamental skills that will enable you to understand complicated statistical analysis directly applicable to real-life situations. We have created a course that is:\nEasy to understand\nComprehensive\nPractical\nTo the point\nPacked with plenty of exercises and resources\nData-driven\nIntroduces you to the statistical scientific lingo\nTeaches you about data visualization\nShows you the main pillars of quant research\nIt is no secret that a lot of these topics have been explained online. Thousands of times. However, it is next to impossible to find a structured program that gives you an understanding of why certain statistical tests are being used so often. Modern software packages and programming languages are automating most of these activities, but this course gives you something more valuable – critical thinking abilities. Computers and programming languages are like ships at sea. They are fine vessels that will carry you to the desired destination, but it is up to you, the aspiring data scientist or BI analyst, to navigate and point them in the right direction.\nTeaching is our passion\nWe worked full-time for several months to create the best possible Statistics course, which would deliver the most value to you. We want you to succeed, which is why the course aims to be as engaging as possible. High-quality animations, superb course materials, quiz questions, handouts and course notes, as well as a glossary with all new terms you will learn, are just some of the perks you will get by subscribing.\nWhat makes this course different from the rest of the Statistics courses out there?\nHigh-quality production – HD video and animations (This isn’t a collection of boring lectures!)\nKnowledgeable instructor (An adept mathematician and statistician who has competed at an international level)\nComplete training – we will cover all major statistical topics and skills you need to become a marketing analyst, a business intelligence analyst, a data analyst, or a data scientist\nExtensive case studies that will help you reinforce everything you’ve learned\nExcellent support - if you don’t understand a concept or you simply want to drop us a line, you’ll receive an answer within 1 business day\nDynamic - we don’t want to waste your time! The instructor sets a very good pace throughout the whole course\nWhy do you need these skills?\nSalary/Income – careers in the field of data science are some of the most popular in the corporate world today. And, given that most businesses are starting to realize the advantages of working with the data at their disposal, this trend will only continue to grow\nPromotions – If you understand Statistics well, you will be able to back up your business ideas with quantitative evidence, which is an easy path to career growth\nSecure Future – as we said, the demand for people who understand numbers and data, and can interpret it, is growing exponentially; you’ve probably heard of the number of jobs that will be automated soon, right? Well, data science careers are the ones doing the automating, not getting automated\nGrowth - this isn’t a boring job. Every day, you will face different challenges that will test your existing skills and require you to learn something new\nPlease bear in mind that the course comes with Udemy’s 30-day unconditional money-back guarantee. And why not give such a guarantee? We are certain this course will provide a ton of value for you.\nClick 'Buy now' and let's start learning together today!",
      "target_audience": [
        "People who want a career in Data Science",
        "People who want a career in Business Intelligence",
        "Business analysts",
        "Business executives",
        "Individuals who are passionate about numbers and quant analysis",
        "Anyone who wants to learn the subtleties of statistics and how it is used in the business world",
        "People who want to start learning statistics",
        "People who want to learn the fundamentals of statistics"
      ]
    },
    {
      "title": "Text Mining and Natural Language Processing in R",
      "url": "https://www.udemy.com/course/text-mining-and-natural-language-processing-in-r/",
      "bio": "Hands-on text mining and natural language processing (NLP) training for data science applications in R",
      "objectives": [
        "Students will be able to read in data from different sources- including databases",
        "Basic webscraping- extracting text and tabular data from HTML pages",
        "Social media mining from Facebook and Twitter",
        "Extract information relating to tweets and posts",
        "Analyze text data for emotions",
        "Carry out Sentiment analysis",
        "Implement natural language processing (NLP) on different types of text data"
      ],
      "course_content": {},
      "requirements": [
        "Should have prior experience of R and RStudio",
        "Prior experience of statistical and machine learning techniques will be beneficial",
        "Should have an interest in learning practical text mining and natural language processing (NLP)",
        "Should have an interest in deriving insights from social media and text data"
      ],
      "description": "Do You Want to Gain an Edge by Gleaning Novel Insights from Social Media?\nDo You Want to Harness the Power of Unstructured Text and Social Media to Predict Trends?\nOver the past decade there has been an explosion in social media sites and now sites like Facebook and Twitter are used for everything from sharing information to distributing news. Social media both captures and sets trends. Mining unstructured text data and social media is the latest frontier of machine learning and data science.\nLEARN FROM AN EXPERT DATA SCIENTIST  WITH +5 YEARS OF EXPERIENCE:\nMy name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation). I have several years of experience in analyzing real-life data from different sources using data science-related techniques and producing publications for international peer-reviewed journals. Unlike other courses out there, which focus on theory and outdated methods, this course will teach you practical techniques to harness the power of both text data and social media to build powerful predictive models. We will cover web-scraping, text mining and natural language processing along with mining social media sites like Twitter and Facebook for text data. Additionally, you will learn to apply both exploratory data analysis and machine learning techniques to gain actionable insights from text and social media data.\nTAKE YOUR DATA SCIENCE CAREER TO THE NEXT LEVEL\nBECOME AN EXPERT IN TEXT  MINING & NATURAL LANGUAGE PROCESSING :\nMy course will help you implement the methods using real data obtained from different sources. Many courses use made-up data that does not empower students to implement R based data science in real life. After taking this course, you’ll easily use packages like the caret, dplyr to work with real data in R. You will also learn to use the common social media mining and natural language processing packages to extract insights from text data.   I will even introduce you to some very important practical case studies - such as identifying important words in a text and predicting movie sentiments based on textual reviews. You will also extract tweets pertaining to trending topics analyze their underlying sentiments and identify topics with Latent Dirichlet allocation. With this Powerful course, you’ll know it all:  extracting text data from websites, extracting data from social media sites and carrying out analysis of these using visualization, stats, machine learning, and deep learning!\nStart analyzing data for your own projects, whatever your skill level and Impress your potential employers with actual examples of your data science projects.\nHERE IS WHAT YOU WILL GET:\nData Structures and Reading in R, including CSV, Excel, JSON, HTML data.\nWeb-Scraping using R\nExtracting text data from Twitter and Facebook using APIs\nExtract and clean data from the FourSquare app\nExploratory data analysis of textual data\nCommon Natural Language Processing techniques such as sentiment analysis and topic modelling\nImplement machine learning techniques such as clustering, regression and classification on textual data\nNetwork analysis\nPlus you will apply your newly gained skills and complete a practical text analysis assignment\nWe will spend some time dealing with some of the theoretical concepts. However, the majority of the course will focus on implementing different techniques on real data and interpreting the results.\nAfter each video, you will learn a new concept or technique which you may apply to your own projects.\nAll the data and code used in the course has been made available free of charge and you can use it as you like. You will also have access to additional lectures that are added in the future for FREE.\nJOIN THE COURSE NOW!",
      "target_audience": [
        "People who wish to learn practical text mining and natural language processing",
        "People with prior experience of using RStudio",
        "People with some prior experience of implementing machine learning techniques in R",
        "People who were previously enrolled for my Data Science:Data Mining and Natural Language Processing course",
        "People who wish to derive insights from textual and social media data"
      ]
    },
    {
      "title": "NumPy, Pandas, & Python for Data Analysis: A Complete Guide",
      "url": "https://www.udemy.com/course/numpy-pandas-python-for-data-analysis-a-complete-guide/",
      "bio": "Learn Data Analysis Techniques with Python, NumPy, and Pandas: From Data Cleaning to Advanced Visualization",
      "objectives": [
        "Introduction to Jupyter Notebook",
        "Basic Python programming concepts",
        "Installing NumPy & Pandas",
        "Creating NumPy arrays from Python lists",
        "Mathematical functions in NumPy",
        "Reading and writing files with NumPy",
        "Creating and understanding DataFrames",
        "DataFrame indexing and selection",
        "Adding, removing, and updating data",
        "Data filtering, sorting, and grouping",
        "Time series analysis and manipulation",
        "Identifying and handling missing data",
        "Merging, joining, and concatenating DataFrames",
        "Applying functions to DataFrames",
        "Customizing plots (titles, labels, colors)",
        "Creating complex visualizations (histograms, scatter plots, box plots)",
        "Memory optimization techniques"
      ],
      "course_content": {
        "Module 1: Introduction to Python for Data Analysis": [
          "Basic Python programming concepts",
          "Understanding data analysis"
        ],
        "Module 2: Introduction to NumPy": [
          "Creating NumPy arrays from Python lists",
          "Array indexing, slicing, and reshaping",
          "Basic operations with NumPy arrays",
          "Mathematical functions in NumPy",
          "Working with multidimensional arrays",
          "Random number generation",
          "Reading and writing files with NumPy",
          "Numpy Operations"
        ],
        "Module 3: Introduction to Pandas": [
          "Creating and understanding DataFrames",
          "Importing and exporting data (CSV, Excel, JSON)",
          "DataFrame indexing and selection",
          "Adding, removing, and updating data",
          "Handling missing data",
          "Data filtering, sorting, and grouping",
          "Understanding time series data",
          "Working with dates and times in Pandas",
          "Time series analysis and manipulation"
        ],
        "Module 4: Data Cleaning and Preparation": [
          "Identifying and handling missing data",
          "Removing duplicates",
          "Data transformation and normalization",
          "Merging, joining, and concatenating DataFrames",
          "Reshaping and pivoting data",
          "Applying functions to DataFrames"
        ],
        "Module 5: Data Visualization with Pandas": [
          "Importance of data visualization",
          "Basic plotting with Pandas",
          "Customizing plots (titles, labels, colors)",
          "Creating complex visualizations (histograms, scatter plots, box plots)"
        ],
        "Module 7: Advanced Topics in Data Analysis": [
          "Memory optimization techniques",
          "Efficient computation with large datasets",
          "Preparing data for machine learning",
          "Working with dates and times in Pandas",
          "Basic machine learning algorithms using Pandas and NumPy"
        ]
      },
      "requirements": [
        "No prior knowledge is required."
      ],
      "description": "Unlock the full potential of data analysis with NumPy, Pandas, and Python in this comprehensive, hands-on course! Whether you're a beginner or looking to sharpen your skills, this course will guide you through everything you need to master data analysis using Python's most powerful libraries.\n\n\nYou will learn to:\n\n\nPython for Data Analysis: Master the fundamentals of Python, the most popular language for data science, including core programming concepts and essential libraries.\nNumPy Essentials: Dive deep into NumPy for fast numerical computations, array manipulation, and performance optimization.\nPandas Mastery: Learn how to efficiently work with large datasets using Pandas, the powerful data manipulation library. Handle, clean, transform, and analyze real-world data with ease.\nData Visualization: Understand how to represent your data visually to gain insights using Python libraries like Matplotlib and Seaborn.\nReal-World Projects: Apply your knowledge to real-world datasets, tackling data challenges from start to finish—exploring, cleaning, and drawing insights.\n\n\nWhat you'll learn:\n\n\nFundamentals of Python programming for data analysis\nIntroduction to NumPy: Arrays, operations, and performance techniques\nDeep dive into Pandas: DataFrames, Series, and advanced data manipulation\nData cleaning and preprocessing techniques\nExploratory data analysis (EDA) with Pandas\nReal-world case studies and hands-on projects\n\n\nEnroll today and take the first step toward mastering data analysis with Python, NumPy, and Pandas!",
      "target_audience": [
        "Anyone looking to enhance their data analysis skills using Python",
        "Beginners interested in data analysis with Python",
        "Data enthusiasts looking to gain in-demand skills"
      ]
    },
    {
      "title": "iOS Machine Learning with Core ML 2 and Swift 5",
      "url": "https://www.udemy.com/course/machine-learning-with-core-ml-2-and-swift/",
      "bio": "Learn how to integrate machine learning into iOS apps. Hands-on Swift 5 coding using CoreML 2, Vision, NLP and CreateML.",
      "objectives": [
        "Learn from a software engineer with 20+ years of professional experience",
        "Get a practical introduction to machine learning in the context of iOS and macOS development",
        "Get the companion e-book for FREE! (sells for $28.80 on Amazon)",
        "Learn how to integrate natural language text analysis into your apps",
        "Build apps that can recognize and classify objects in images and video streams",
        "Use Core ML to perform sentiment analysis on Amazon product reviews",
        "Gain a working knowledge of Core ML 2, Vision, Natural Language Processing and Create ML"
      ],
      "course_content": {
        "Introduction": [
          "What is Machine Learning?",
          "What You Should Know",
          "Join the Official Student Group"
        ],
        "Machine Learning: the Big Picture": [
          "Supervised and Unsupervised Machine Learning",
          "The Machine Learning Model",
          "iOS and Machine Learning",
          "Exercise Files",
          "Test Your Skills"
        ],
        "iOS Machine Learning Architecture": [
          "A High-Level View",
          "The CoreML Framework",
          "The NaturalLanguage Framework",
          "The Vision Framework",
          "The GamePlayKit Framework"
        ],
        "Natural Language Text Analysis": [
          "Natural Language Text Analysis - What Are We Going to Build?",
          "Recognizing the Dominant Language of a Text",
          "Tokenizing a String",
          "Identifying Parts of Speech",
          "Identifying People, Places and Organizations"
        ],
        "Image Analysis with the Vision Framework": [
          "Image Analysis with the Vision Framework - What Are We Going to Build?",
          "The Starter App",
          "Analyzing Still Images using Vision",
          "Implementing the Image Request Handler",
          "Implementing the Image Analysis Request",
          "Converting Coordinates Between Quartz 2D and UIKit",
          "Visualizing the Detected Rectangles",
          "Recognizing Text, Faces and Barcodes in Still Images"
        ],
        "Training a Flower Classifier on Your Computer using Create ML": [
          "Training a Flower Classifier on Your Computer - What Are We Going to Build?",
          "Recognizing Flowers - Preparing the Training Data",
          "Training an Image Classifier in a Playground",
          "Recognizing Flowers - the Starter App",
          "Integrating the Flower Classifier Model",
          "Displaying Predictions",
          "Picking an Image",
          "Performing the Image Analysis Request",
          "The Flower Recognizer App in Action"
        ],
        "Determining the Tonality of a Review": [
          "Review Sentiment Analysis - What Are We Going to Build?",
          "Preparing the Training Data for the Review Sentiment Classifier",
          "Training a Text Classifier in a Playground",
          "Creating the MLTextClassifier",
          "Saving the Core ML Model",
          "Laying Out the User Interface of the Review Classifier App",
          "Integrating the Review Classifier Model",
          "Testing the Review Classifier App"
        ],
        "Next Steps + Bonuses": [
          "Goodbye!",
          "Companion eBook",
          "Bonus Lecture: Learn More from Karoly + Useful Links"
        ]
      },
      "requirements": [
        "You need a Mac with macOS Mojave and Xcode 10 or later",
        "You should have basic Swift programming skills",
        "You should definitely go ahead if you know how Xcode works"
      ],
      "description": "A practical and concise Core ML 2 course you can complete in less than three hours. Companion eBook included!\nWouldn't it be great to integrate features like synthetic vision, natural language processing, or sentiment analysis into your apps? In this course, I'll teach you how to unleash the power of machine learning using Apple Core ML 2. I'll show you how to train and deploy models for natural language and visual recognition using Create ML.\nI'm going to familiarize you with common machine learning tasks. We'll focus on practical applications, using hands-on Swift coding.\nWe're going to demystify what machine learning is by investigating how it works. And no worries, I introduce each concept using simple terms, avoiding confusing jargon.\nWe'll delve into advanced topics like synthetic vision and natural language processing. You'll apply what you've learned by building iOS applications capable of identifying faces, barcodes, text, and rectangular areas in photos in real-time.\nYou'll learn how to train machine learning models on your computer. You're going to develop several smart apps, including a flower recognizer and an Amazon review sentiment analyzer.\nAnd there's a lot more!\nWhat qualifies me to teach you?\nI have more than 25 years of software development expertise. I've worked for companies like Apple, Siemens, and SAP.\nAs a software architect, I have designed and built several enterprise systems and frameworks, including core components of Siemens Healthcare's syngo image processing system. I'm one of the senior software architects behind the SAP Cloud Platform SDK for iOS, a framework built by Apple and SAP. I currently hold twelve patents related to inventions in the field of mobile computing.\nTopics include:\n- Understanding the machine learning frameworks provided by Apple\n- Natural language text processing using the NaturalLanguage framework\n- Setting up a Core ML project in Xcode\n- Image analysis using Vision\n- Training an image classifier on your computer using CreateML\n- Determining the tonality of an Amazon product review\n\"Machine Learning with CoreML 2 and Swift 5\" is the perfect course for you if you're interested in machine learning.\n\n\nSTUDENT REVIEWS\n“Thank you Karoly, you have delivered another excellent course, with detailed explanations and real world examples of machine learning that any app developer will be able to put into practice with their app development.Excellent course.” - Jim McMillan\n“This course is the best introduction to Machine Learning with Swift. It is going to familiarize you with common machine learning tasks and is very helpful for beginners.” - Zbyszek Pietras\n“I've been looking for a course that teaches CoreML2 with natural language processing and CreateML. I found this course very useful and gets directly to the important topics. I also appreciated the Vision CoreML section as well.” - Dan Gray\n\n\nMORE THAN AN ONLINE COURSE. WITH THIS CLASS, YOU ALSO RECEIVE:\nPersonalized support\nAs a student of this course, you’ll get access to the course's private forum, where I answer questions and provide support if necessary.\nThe companion eBook\nDownloadable resources\nYou get downloadable demo projects you can use to follow along.\nContinuous updates\nI keep enhancing this course to provide fresh and up-to-date content.\n\nOUR 30-DAY MONEY-BACK GUARANTEE\nIf you aren't satisfied with your purchase, we'll refund you your money. We want to make sure you're completely satisfied with the course. That's why we're happy to offer you this money-back guarantee.\nGo ahead and click the enroll button. See you in the first lesson!",
      "target_audience": [
        "Take this course if you want to get started with Core ML, Vision, and/or Natural Language Processing",
        "This course is for you if you want to bring your game to the next level by integrating machine learning into your apps"
      ]
    },
    {
      "title": "Complete Time Series Analysis With Python",
      "url": "https://www.udemy.com/course/complete-time-series-data-analysis-bootcamp-with-python/",
      "bio": "Python Time Series: How To Use Data Science, Statistics & Machine Learning For Modelling Time Series Data in Python",
      "objectives": [
        "Master Time Series Data In Python & Become Proficient In Time Series Data Analysis",
        "LEARN How To Use Python-based Packages For Time Series Analysis",
        "APPLY Python Data Science Techniques To REAL LIFE Data",
        "IMPLEMENT Common Data Processing And Visualisation Techniques For Time Series Data in Python",
        "BE ABLE To Read In, Pre-process & Visualize Time Series Data",
        "The Basic Conditions Time Series Data Must Fulfill & How To Check For These",
        "MODEL Time Series Data To Forecast Future Values",
        "USE Machine Learning Regression For Forecasting Future Values"
      ],
      "course_content": {
        "Introduction to the Course": [
          "Welcome to Time Series Analysis with Python",
          "Data and Scripts Used in the Course",
          "Introduction to the Python Data Science Environment",
          "Installation Instructions For Mac"
        ],
        "Read in Data From External Data Sources": [
          "Introduction to Pandas",
          "Read in CSV Data",
          "Read in Excel Data",
          "Read in HTML Data",
          "Read in JSON Data"
        ],
        "Preprocessing & Visualising Time Series Data in Python": [
          "Some Date Specific Python Functions",
          "An Example of Time Series Data in Python",
          "More Details on Datetime",
          "Basic Operations on Time Series Data",
          "Theory Behind Exploratory Data Analysis (EDA)",
          "Principles of Data Visualization",
          "Prep Up the Time Series Data",
          "Line Charts For Examining Temporal Data",
          "Multiple Lines in the Same Chart",
          "Aggregating & Visualising Data Summary",
          "Using Multiple Line Plots For Discerning Specific Information",
          "Histograms",
          "Plot the Temporal Variations of Two Entities"
        ],
        "Characteristics & Conditions of Time Series Data": [
          "Moving Average (MA) Forecast Example",
          "Classical Time Series Data",
          "Different Components of Time Series Data",
          "Seasonal Part of Time Series",
          "Of Multiplicative and Additive Seasonality",
          "Testing for Stationarity: ADF Test",
          "Make Time Series Stationary: Take Log",
          "First Order Differencing to Make Time Series Stationary",
          "Log Based Differencing",
          "Linear Regression For Detrending"
        ],
        "Basic Time Series Forecasting": [
          "Rolling Mean For Detecting Temporal Variation",
          "Simple Exponential Smoothing (SES)",
          "Holt extended simple exponential smoothing",
          "Holt Winters",
          "Auto Regression Model (AR): Consider Previous Time Steps",
          "Implement a Basic ARIMA Model",
          "Automated ARIMA & Account for Seasonality (SARIMA)"
        ],
        "Machine Learning For Time Series": [
          "Random Forest For Identifying Important Time Periods",
          "\"Prophetic\" Time Series Forecasting",
          "Using Prophet For Predicting Values for a Future Time Frame"
        ],
        "Use Deep Learning For Time Series Data": [
          "What is Keras?",
          "Install Keras on Windows",
          "Install Keras on Mac",
          "Theory Behind ANN and DNN",
          "MLP For Time Series",
          "LSTM For Time Series Data",
          "LSTM For Predicting Stock Prices",
          "Univariate LSTM For Stock Prediction",
          "Unseen Values"
        ],
        "Miscellaneous Lectures": [
          "Bitcoin Price Forecasting",
          "POSIT",
          "Distributed Computing"
        ]
      },
      "requirements": [
        "Prior Familiarity With The Interface Of Jupiter Notebooks and Package Installation",
        "Prior Exposure to Basic Statistical Techniques (Such As p-Values, Mean, Variance)",
        "Be Able To Carry Out Data Reading And Pre-Processing Tasks Such As Data Cleaning In Python",
        "Interest In Working With Time Series Data Or Data With A Time Component To Them"
      ],
      "description": "THIS IS YOUR COMPLETE GUIDE TO TIME SERIES DATA ANALYSIS IN PYTHON!\nThis course is your complete guide to time series analysis using Python. So, all the  main aspects of analyzing temporal data will be covered n depth..\nIf you take this course, you can do away with taking other courses or buying books on Python based data analysis.\nIn this age of big data, companies across the globe use Python to sift through the avalanche of information at their disposal. By becoming proficient in in analysing time series data in Python, you can give your company a competitive edge and boost your career to the next level.\nLEARN FROM AN EXPERT DATA SCIENTIST  WITH +5 YEARS OF EXPERIENCE:\nHey, my name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University.\nI have +5 years of experience in analyzing real life data from different sources  using data science related techniques and i have produced many publications for international peer reviewed journals.\nOver the course of my research I realised almost all the Python data science courses and books out there do not account for the multidimensional nature of the topic .\nSo, unlike other instructors, I dig deep into the data science features of R and gives you a one-of-a-kind grounding in data science related topics!\nYou will go all the way from carrying out data reading & cleaning  to to finally implementing powerful statistical and machine learning algorithms for analyzing time series data.\nAmong other things:\nYou will be introduced to powerful Python-based packages for time series analysis.\nYou will be introduced to both the commonly used techniques, visualization methods and machine/deep learning techniques that can be implemented for time series data.\n& you will learn to apply these frameworks to real life data including temporal stocks and financial data.\nNO PRIOR PYTHON OR STATISTICS/MACHINE LEARNING KNOWLEDGE IS REQUIRED!\nYou’ll start by absorbing the most valuable Python Data Science basics and techniques. I use easy-to-understand, hands-on methods to simplify and address even the most difficult concepts in Python.\nMy course will help you implement the methods using REAL DATA obtained from different sources. Many courses use made-up data that does not empower students to implement Python based data science in real-life.\nAfter taking this course, you’ll easily use the common time series packages in Python...\nYou’ll even understand the underlying concepts to understand what algorithms and methods are best suited for your data.\nWe will work with real data and you will have access to all the code and data used in the course.\nJOIN MY COURSE NOW!",
      "target_audience": [
        "Anyone Who Wants Master Time Series Data In Python",
        "Anyone Who Wants To Become Proficient In Time Series Data Analysis Working With Real Life Data",
        "People Interested in Applying Machine Learning Techniques to Time Series Data",
        "Anyone Who Wants To Become An Expert Data Scientist"
      ]
    },
    {
      "title": "AI Essentials: Introduction to Artificial Intelligence",
      "url": "https://www.udemy.com/course/ai-essentials-introduction-to-artificial-intelligence/",
      "bio": "AI Essentials: A Simple Introduction to Artificial Intelligence Technologies by MTF Institute",
      "objectives": [
        "AI, its Modern History and current Implementations",
        "Machine Learning",
        "Neural Networks and Deep Learning",
        "The relationship between Machine Learning, Neural Networks and Deep Learning",
        "The theory of Artificial General Intelligence (AGI)",
        "Intelligent agents",
        "Natural Language Processing (NLP)",
        "Computer Vision",
        "AI Ethics",
        "AI In Robotics",
        "Search Algorithms",
        "Knowledge Representation and Reasoning (KRR)",
        "Explain-ability and Transparency in AI (XAI)",
        "Generative Artificial Intelligence (GenAI)",
        "Large Language Models (LLMs)",
        "AI and IT",
        "AI for Good"
      ],
      "course_content": {
        "Introduction": [
          "Meet Your Instructor",
          "Welcome to MTF",
          "Onboarding to learning process"
        ],
        "AI Essentials for Everyone": [
          "AI, it's Modern History and current Implementations",
          "What is Machine Learning",
          "Neural Networks and Deep Learning",
          "The relationship between Machine learning, Neural Networks and Deep learning",
          "The theory of Artificial General Intelligence (AGI)",
          "Intelligent agents",
          "Natural Language Processing (NLP)",
          "Listen to AI: How Text-to-Speech is Changing Communication",
          "Computer Vision",
          "AI Ethics",
          "AI in Robotics",
          "Search Algorithms",
          "Knowledge Representation and Reasoning",
          "Explain-ability and Transparency in AI",
          "Generative AI, LLMs, AI & IT, AI for Good"
        ],
        "Hands-On AI Exploration and Practice": [
          "Practical Computer Vision with AI",
          "Through the Eyes of AI: A Hands-On Image Analysis Journey",
          "Exploring Different Types of Flowers with Microsoft Copilot",
          "AI-Powered Visual Content Creation with Microsoft Copilot",
          "Exploring AI-Powered Voice-to-Text with Bing"
        ],
        "AI Essentials: Challenge Yourself": [
          "AI Essentials - Quiz"
        ],
        "Interactive Part, Next Steps and Answers to Questions": [
          "Congratulations with finishing from MTF",
          "Interactive Part",
          "Bonus Section: Next Steps"
        ]
      },
      "requirements": [
        "For a better learning experience, we suggest you use a laptop / mobile phone / pen and paper for taking notes, highlighting important points, and making summaries to reinforce your learning."
      ],
      "description": "Welcome to course: AI Essentials: A Simple Introduction to Artificial Intelligence Technologies by MTF Institute\n\n\nCourse provided by MTF Institute of Management, Technology and Finance\nMTF is the global educational and research institute with HQ at Lisbon, Portugal, focused on business & professional hybrid (on-campus and online) education at areas: Business & Administration, Science & Technology, Banking & Finance.\nMTF R&D center focused on research activities at areas: Artificial Intelligence, Machine Learning, Data Science, Big Data, WEB3, Blockchain, Cryptocurrency & Digital Assets, Metaverses, Digital Transformation, Fintech, Electronic Commerce, Internet of Things.\nMTF is the official partner of: IBM, Intel, Microsoft, member of the Portuguese Chamber of Commerce and Industry.\nMTF is present in 208 countries and has been chosen by more than 380,000 students.\n\n\n\n\nHello everyone, and welcome to our course in Artificial Intelligence (AI)! My name is Mohamed Elfateh, I have been working in the Information Technology field for over a decade. I am interested in learning about modern technologies and sharing my knowledge with others.\n\n\nWhat is AI?\nAI is a field of computer science that studies how to create machines that can process information, make decisions, and perform specific tasks.\n\n\nThe History of modern AI\nThe field of artificial intelligence (AI) has a long and rich history, dating back to the early days of computing. However, the field as we know it today was founded in 1956 at a conference at Dartmouth College in New Hampshire. This conference brought together the leading researchers in AI at the time, including Alan Turing, John McCarthy, and Marvin Minsky. The conference is credited with helping to define the field of AI and to set the agenda for future research.\n\n\nArtificial Intelligence “AI” is a complex field that needs the ability of people from diverse disciplines to work together. As an example, manufacturing autonomous vehicles like self-driving cars requires the work of people from different fields, such as AI Researchers, Automotive Engineers, and Computer Vision Researchers.\nAI Researchers develop algorithms that enable self-driving cars to perceive their surroundings, make decisions, and control their movements. Automotive Engineers develop the hardware and software systems that are needed to implement these algorithms, and Computer Vision Researchers develop new techniques for enabling self-driving cars to see and understand the world.\n\n\nWhat is Machine Learning?\nMachine Learning is a type of Artificial Intelligence (AI) that allows computers to learn from data without explicit programming. In other words, machine learning algorithms can recognize patterns and make predictions based on data, without getting a command. This enables computers to learn new tasks and improve their performance over time without human intervention.\n\n\nWhere is Machine Learning used?\nMachine learning is in use by a wide range of applications, including email filtering, Social Media Personalization, Image Recognition, Speech Recognition, fraud detection, Text prediction, Product recommendation, medical diagnosis, Healthcare Personalization, Traffic Prediction.\n\n\nNeural networks are a type of machine learning algorithm that is inspired by the structure and function of the human brain.\nNeural networks can learn complex patterns from data.\nNeural networks have been successfully used in areas such as natural language processing.\n\n\nDeep Learning is a type of machine learning that uses neural networks with multiple layers.\nEach layer consists of multiple nodes that can perform diverse tasks. This allows deep learning models to learn more complex patterns from data.\nDeep learning has been used to achieve significant results in a wide range of tasks, including image recognition, speech recognition, and machine translation.",
      "target_audience": [
        "No specific requirements. The course is for any individual who want to build a career in AI and data science or improve their knowledge.",
        "What is AI? AI is a field of computer science that studies how to create machines that can process information, make decisions, and perform specific tasks.",
        "The History of modern AI. The field of artificial intelligence (AI) has a long and rich history, dating back to the early days of computing. However, the field as we know it today was founded in 1956 at a conference at Dartmouth College in New Hampshire. This conference brought together the leading researchers in AI at the time, including Alan Turing, John McCarthy, and Marvin Minsky. The conference is credited with helping to define the field of AI and to set the agenda for future research.",
        "Artificial Intelligence “AI” is a complex field that needs the ability of people from diverse disciplines to work together. As an example, manufacturing autonomous vehicles like self-driving cars requires the work of people from different fields, such as AI Researchers, Automotive Engineers, and Computer Vision Researchers. AI Researchers develop algorithms that enable self-driving cars to perceive their surroundings, make decisions, and control their movements. Automotive Engineers develop the hardware and software systems that are needed to implement these algorithms, and Computer Vision Researchers develop new techniques for enabling self-driving cars to see and understand the world.",
        "What is Machine Learning? Machine Learning is a type of Artificial Intelligence (AI) that allows computers to learn from data without explicit programming. In other words, machine learning algorithms can recognize patterns and make predictions based on data, without getting a command. This enables computers to learn new tasks and improve their performance over time without human intervention.",
        "Where is Machine Learning used? Machine learning is in use by a wide range of applications, including email filtering, Social Media Personalization, Image Recognition, Speech Recognition, fraud detection, Text prediction, Product recommendation, medical diagnosis, Healthcare Personalization, Traffic Prediction.",
        "Neural networks are a type of machine learning algorithm that is inspired by the structure and function of the human brain. Neural networks can learn complex patterns from data. Neural networks have been successfully used in areas such as natural language processing.",
        "Deep Learning is a type of machine learning that uses neural networks with multiple layers. Each layer consists of multiple nodes that can perform diverse tasks. This allows deep learning models to learn more complex patterns from data. Deep learning has been used to achieve significant results in a wide range of tasks, including image recognition, speech recognition, and machine translation."
      ]
    },
    {
      "title": "Learn Apache Cassandra in 2 hours | NoSQL Database",
      "url": "https://www.udemy.com/course/learn-cassandra-from-scratch/",
      "bio": "A complete guide to learn most popular NoSQL Database Cassandra with hands on",
      "objectives": [
        "A complete understanding about Apache Cassandra basics and its Architecture."
      ],
      "course_content": {
        "Apache Cassandra Overview": [
          "Apache Cassandra Introduction",
          "Cluster Setup",
          "Understanding Data Model",
          "CRUD hands on",
          "Partitioning/Clustering",
          "Understanding Partitioning/Clustering Keys",
          "Partition/Clustering Key Hands on",
          "Data Types Intro",
          "Data Types Hands on"
        ],
        "Cassandra Architecture": [
          "Replication",
          "Write Consistency",
          "Read Consistency",
          "Replication/Consistency",
          "Gossip Protocol",
          "How write happens?",
          "Cassandra Storage & SSTable",
          "How Read happens?",
          "Write/Read",
          "Compaction"
        ],
        "Repair Mechanism": [
          "Repair Mechanism Intro",
          "Read Repair",
          "Hinted Hands off",
          "Anti-Entrophy"
        ],
        "Cassandra Configuration & Managing Cluster": [
          "Intro",
          "Local Setup",
          "Cassandra Configuration",
          "Managing cluster using NodeTool"
        ],
        "Cassandra with Java": [
          "Using Java Connector"
        ],
        "Cassandra With Spark": [
          "Creating Spark RDD from Cassandra Table",
          "Processing Cassandra Data in Spark"
        ],
        "Best Practices & Anti Patterns": [
          "Cassandra Data Modelling",
          "Cassandra Anti Patterns"
        ],
        "Cassandra on Cloud": [
          "Cassandra on AWS"
        ]
      },
      "requirements": [
        "MAC or PC"
      ],
      "description": "This Apache Cassandra training course  teaches you working with Cassandra. This course is intended for complete beginners in Cassandra.\nThis is most concise, efficient and bestseller course on Apache Cassandra.\nIn this course, we will\nwhat is Cassandra\nhow to install Cassandra\nunderstand Cassandra data model with some hands on exercise which will teach you how to create a keyspace, create a table,insert and read the data .\ndifferent data types in Cassandra with exercise.\nAfter this you will learn about the partition key and clustering key and understand how data is distributed across the nodes in a cluster.T\nI will covers the Cassandra Architecture in details in which we will cover replication, consistency, gossip protocol, write path, read path, Cassandra storage and compaction.\nUnderstanding anti patterns and data modeling goals.\nUnderstand Cassandra configuration files\nWorking with nodetools to manage cluster\nIntegrate with Cassandra java driver to write and run Cassandra from java program.\nIntegrate Spark with Cassandra to perform analytics .\nCassandra on AWS\n\nOnce you have completed this video based training course, you will have a solid understanding of Cassandra, and be able to use Cassandra for your own development projects. Working files are included, allowing you to follow along with the author throughout the lessons.",
      "target_audience": [
        "Beginners who want to learn no sql database Apache Cassandra."
      ]
    },
    {
      "title": "Data Science Career Path",
      "url": "https://www.udemy.com/course/data-science-learning/",
      "bio": "Tackle Big Data Challenges with Specialized Courses",
      "objectives": [
        "Solve Real-World Problems: Gain the ability to extract valuable insights from data, helping organizations make informed decisions and solve complex challenges.",
        "High Earning Potential: Data scientists are among the highest-paid professionals. Learn how to command competitive salaries and enjoy financial stability.",
        "Innovation and Automation: Be at the forefront of technological innovation, as data science drives automation, machine learning, and AI advancements.",
        "Global Relevance: Data science skills are sought after worldwide, providing you with the flexibility to work in India or abroad.",
        "Data science is in high demand across various industries. By mastering data science, you open doors to exciting and well-paying career opportunities."
      ],
      "course_content": {},
      "requirements": [
        "To enroll in our Data Science Learning Course, you'll need: A basic understanding of mathematics and statistics. Familiarity with programming languages like Python. A laptop or computer with internet access. Dedication and a passion for learning."
      ],
      "description": "Overview:\nWelcome to the Data Science Learning Course, your gateway to the fascinating world of data-driven decision-making and analytics. In today's digital age, data is the currency that drives industries and innovation. Our comprehensive program is designed to equip you with the knowledge and skills needed to harness the power of data science. Whether you are a novice or a seasoned professional looking to upskill, our course will provide you with a solid foundation in this dynamic field.\nBenefits of Learning Data Science:\nUnlock Career Opportunities: Data science is in high demand across various industries. By mastering data science, you open doors to exciting and well-paying career opportunities.\nSolve Real-World Problems: Gain the ability to extract valuable insights from data, helping organizations make informed decisions and solve complex challenges.\nHigh Earning Potential: Data scientists are among the highest-paid professionals. Learn how to command competitive salaries and enjoy financial stability.\nInnovation and Automation: Be at the forefront of technological innovation, as data science drives automation, machine learning, and AI advancements.\nGlobal Relevance: Data science skills are sought after worldwide, providing you with the flexibility to work in India or abroad.\nWho Can Learn:\nOur Data Science Learning Course is suitable for:\nStudents: Kickstart your career with a strong foundation in data science.\nProfessionals: Enhance your skill set and stay relevant in a rapidly evolving job market.\nEntrepreneurs: Leverage data to make data-driven decisions and gain a competitive edge.\nCareer Changers: Switch to a rewarding career in data science regardless of your background.\nCareer Scope:\nData science offers an extensive range of career opportunities, including but not limited to:\nData Scientist: Analyze data to extract actionable insights.\nMachine Learning Engineer: Develop algorithms and models.\nData Analyst: Interpret data and create reports.\nBusiness Analyst: Help organizations make data-driven decisions.\nBig Data Engineer: Manage and process vast datasets.\nAI Researcher: Pioneer cutting-edge AI solutions.\nSalary Package (India and Foreign):\nIn India, data science professionals can earn competitive salaries based on experience and expertise. Entry-level positions often start at 5-10 lakhs per annum, while experienced data scientists can command salaries exceeding 20 lakhs per annum. In foreign countries, such as the United States, data scientists can earn even more, with salaries ranging from $80,000 to 150,000 per year, depending on location and experience.\nRequirements To Study:\nTo enroll in our Data Science Learning Course, you'll need:\nA basic understanding of mathematics and statistics.\nFamiliarity with programming languages like Python.\nA laptop or computer with internet access.\nDedication and a passion for learning.",
      "target_audience": [
        "Students: Kickstart your career with a strong foundation in data science.",
        "Career Changers: Switch to a rewarding career in data science regardless of your",
        "Professionals: Enhance your skill set and stay relevant in a rapidly evolving job market.",
        "Entrepreneurs: Leverage data to make data-driven decisions and gain a competitive edge."
      ]
    },
    {
      "title": "Learn & Deploy Data Science Web Apps with Streamlit",
      "url": "https://www.udemy.com/course/streamlit-for-datascience/",
      "bio": "Learn, Develop and Deploy Streamlit web app for Data Science application using just Python",
      "objectives": [
        "Create powerful streamlit apps",
        "Create beautiful web app in minutes",
        "Build Web App without knowing anything on HTML, CSS, Javascrip",
        "Develop Web Apps in Python",
        "Develop data science web app"
      ],
      "course_content": {},
      "requirements": [
        "Beginner to Python",
        "Must know Pandas for Data Analysis"
      ],
      "description": "Welcome to the course Learn Streamlit for Data Science\nStreamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science that can be used to share analytics results, build complex interactive experiences, and illustrate new machine learning models. In just a few minutes you can build and deploy powerful data apps.\nOn top of that, developing and deploying Streamlit apps is incredibly fast and flexible, often turning application development time from days into hours.\nIn this course, we start out with the Streamlit basics. We will learn how to download and run demo Streamlit apps, how to edit demo apps using our own text editor, how to organize our Streamlit apps, and finally, how to make our very own. Then, we will explore the basics of data visualization in Streamlit. We will learn how to accept some initial user input, and then add some finishing touches to our own apps with text. At the end of this course, you should be comfortable starting to make your own Streamlit applications.\nIn particular, we will cover the following topics:\nWhy Streamlit?\nInstalling Streamlit\nOrganizing Streamlit apps\nStreamlit\nText Elements\nDisplay Data\nLayouts\nWidgets\nData Visualization\nIntegrating Widgets to Visualizations\nPlotly\nBokeh\nStreamlit\nData Science Project\nDeploy Data Science Web App in Cloud",
      "target_audience": [
        "Data Scientist who want to present Data Analysis and machine learning models"
      ]
    },
    {
      "title": "Survival Analysis in R",
      "url": "https://www.udemy.com/course/survival-analysis-in-r/",
      "bio": "Use R to master survival analysis, duration analysis, event time analysis or reliability analysis.",
      "objectives": [
        "The general concepts of survival analysis",
        "How to use R for survival analysis",
        "Identify the best packages for survival data",
        "The best data structure of a survival dataset and how to clean it",
        "Visualizing survival models with different charting tools: ggplot2, ggfortify, R Base",
        "Kaplan-Meier estimator",
        "Logrank test",
        "Cox proportional hazards model",
        "Parametric models",
        "Survival trees",
        "Missing data imputation",
        "Outlier detection",
        "Date and time data handling with lubridate"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Course: Survival Analysis in R",
          "Course Structure and Content: Managing Expectations",
          "The Survival Analysis Task View",
          "Survival Analysis Background",
          "Understanding Censored Data",
          "Course Script: Survival Analysis Models",
          "The Optimal Survival Dataset Structure and Our Main Course Dataset for Download"
        ],
        "General Survival Analysis Models": [
          "Welcome to the Section: Non-Parametric Models for Survival Data",
          "The Survival Function",
          "The Survival Object",
          "The Kaplan-Meier Estimator",
          "Kaplan-Meier Plot",
          "Kaplan-Meier Plot with 'ggfortify'",
          "The Logrank Test",
          "Implementation of the Logrank Test in R",
          "Exercise: Kaplan-Meier Estimator and Logrank Test",
          "Solution: Kaplan-Meier Estimator and Logrank Test"
        ],
        "Cox Proportional Hazards Model and Parametric Models": [
          "The Cox Proportional Hazards Model",
          "Implementation of the Cox Proportional Hazards Model in R",
          "Interpretation of the Model Result",
          "Aalen's Additive Regression Model",
          "Parametric Models in Survival Analysis",
          "Parametric Regression Models in Survival Analysis",
          "Exercise: Cox Proportional Hazards Model",
          "Solution: Cox Proportional Hazards Model"
        ],
        "Tree Based Models": [
          "Survival Trees",
          "Survival Trees in R with Ranger",
          "Survival Tree Setup",
          "Visualizing the Survival Model",
          "Comparison Plot"
        ],
        "Managing the Time Variable in a Survival Dataset": [
          "Tools for Date and Time Data in R",
          "Course Script: Managing the Time Variable",
          "Working with Dates and Time in R",
          "Format Conversion from Strings to Date/ Time",
          "The Lubridate Package",
          "Exercise",
          "Calculations with Lubridate",
          "Calculating Interval Length"
        ],
        "Outlier Detection and Missing Value Imputation in Survival Analysis": [
          "Outlier Detection and Missing Data Imputation",
          "Missing Data Handling",
          "Course Script: Missing Data Handling and Outlier Detection",
          "Simple Methods for Missing Data Handling",
          "Missing Data Implementation with Machine Learning",
          "Statistical Outliers",
          "Detecting Outliers in Univariate Datasets",
          "Detecting Outliers in Multivariate Datasets",
          "Exercise: Missing Data Imputation and Outlier Detection",
          "Solution: Missing Data Imputation and Outlier Detection"
        ]
      },
      "requirements": [
        "Required programs: R and RStudio",
        "Basic R skills",
        "Interest in survival analysis"
      ],
      "description": "Survival Analysis is a sub discipline of statistics. It actually has several names. In some fields it is called event-time analysis, reliability analysis or duration analysis. R is one of the main tools to perform this sort of analysis thanks to the survival package.\nIn this course you will learn how to use R to perform survival analysis. To check out the course content it is recommended to take a look at the course curriculum. There are also videos available for free preview.\nThe course structure is as follows:\nWe will start out with course orientation, background on which packages are primarily used for survival analysis and how to find them, the course datasets as well as general survival analysis concepts.\nAfter that we will dive right in and create our first survival models. We will use the Kaplan Meier estimator as well as the logrank test as our first standard survival analysis tools.\nWhen we talk about survival analysis there is one model type which is an absolute cornerstone of survival analysis: the Cox proportional hazards model. You will learn how to create such a model, how to add covariates and how to interpret the results.\nYou will also learn about survival trees. These rather new machine learning tools are more and more popular in survival analysis. In R you have several functions available to fit such a survival tree.\nThe last 2 sections of the course are designed to get your dataset ready for analysis. In many scenarios you will find that date-time data needs to be properly formatted to even work with it. Therefore, I added a dedicated section on date-time handling with a focus on the lubridate package. And you will also learn how to detect and replace missing values as well as outliers. These problematic pieces of data can totally destroy your analysis, therefore it is crucial to understand how to manage it.\nBesides the videos, the code and the datasets, you also get access to a vivid discussion board dedicated to survival analysis.\nBy the way, this course is part of a whole data science course portfolio. Check out the R-Tutorials instructor page to see all the other available course.\nWell over 100.000 people around the world did already use our classes to master data science. Why don´t you try it out yourself? With a Udemy 30-day money back guarantee there is nothing you can lose, you can only gain precious skills to come out ahead in today’s job market.",
      "target_audience": [
        "Analysts working with survival data",
        "Data scientists interested in this sub discipline of statistics",
        "Medical researches and clinical trials personnel",
        "Engineers and people in academia working with time event data",
        "Students taking classes in survival analysis or related topics"
      ]
    },
    {
      "title": "The Complete Artificial Intelligence for Cyber Security 2024",
      "url": "https://www.udemy.com/course/the-complete-artificial-intelligence-for-cyber-security-2021/",
      "bio": "Combine the power of Data Science, Machine Learning and Deep Learning to create powerful AI for Real-World applications",
      "objectives": [
        "Isolation Forest",
        "Markov Chains",
        "Statsmodels",
        "NLP (Natural Language Processing)",
        "Linear Regression",
        "Logistic Regression",
        "Naïve Bayes",
        "ANN (Artificial Intelligence)",
        "Random Forest",
        "K-means",
        "HMM",
        "Eigenfaces and Eigenvalues",
        "SVM (Support Vector Machine)",
        "XGBOOST",
        "Pandas",
        "Numpy",
        "matplotlib",
        "IF-IDF",
        "Tensorflow",
        "Scikit-Learn",
        "Cyber security",
        "Google Colab",
        "Data Pre-processing.",
        "Analysing Data.",
        "Data standardization.",
        "Splitting Data into Training Set and Test Set.",
        "One-hot Encoding.",
        "Understanding Machine Learning Algorithm.",
        "Training Neural Network.",
        "Model building.",
        "Analysing Results.",
        "Model compilation.",
        "A Comparison Of Categorical And Binary Problem.",
        "Make a Prediction.",
        "Testing Accuracy.",
        "Confusion Matrix.",
        "Keras."
      ],
      "course_content": {
        "Introduction to Artificial Intelligence and Cybersecurity (Updated in 2024)": [
          "Course structure",
          "Important note about tools in this course",
          "How to make the most out of this course",
          "UPDATED CONTENT",
          "Basic concepts of machine learning (Updated on 2025)",
          "Introduction to cybersecurity and common cyber threats",
          "What is the Role of AI in cybersecurity"
        ],
        "Fundamentals of Machine Learning for Cybersecurity (UPDATED 2024)": [
          "Basics of supervised, unsupervised, and reinforcement learning (Updated on 2025)",
          "What is scikit-learn? (Updated on 2025)",
          "what is pandas? (Updated on 2025)",
          "What is numpy (updated on 2025)",
          "Implementation of basic machine learning in cyber security",
          "what is standardization ? (Updated on 2025)",
          "Implementation of standardization",
          "what is principal component analysis (pca)?",
          "Implementation of principal component analysis",
          "What is markov chains? (Updated on 2025)",
          "Implementation of markov chains",
          "what is clustering (Updated on 2025)",
          "what is plotly (Updated on 2025)",
          "Implementation of clustering",
          "What is XGBOOST classifier (Updated on 2025)",
          "Implementation of XGBOOST classifier",
          "what is scipy? (Updated on 2025)",
          "what is matplotlib? (Updated on 2025)",
          "What is isolation forest (Updated on 2025)?",
          "Implementation of Isolation forest?",
          "What is K-nearest Neighbors (Updated on 2025)",
          "what is hashing vectorizer (Updated on 2025)",
          "what is tf-idf?",
          "Implementation of hashing vectorizer and tf-idf with scikit-learn"
        ],
        "Introduction to phishing attack detectors (updated 2024)": [
          "what is logistic regression? (Updated on 2025)",
          "what is decision tree? (Updated on 2025)",
          "what is phishing attack",
          "What is spam detection",
          "What is perceptrons?",
          "Detecting spams using perceptrons",
          "What is SVM",
          "Implementation of spam detection using SVM",
          "Implementation of Phishing detection with logistic regression",
          "Implementation of phishing detection using decision trees"
        ],
        "Malware threat detection using machine learning method (updated 2024)": [
          "What is malware",
          "what is malware static and dymanic analysis",
          "what is obfuscated JavaScript?",
          "What is N-gram",
          "what is Markov process",
          "Implementation of N-grams",
          "what is metamorphic malware?",
          "What is K-means",
          "What is HMMs",
          "Implementation of malware detection using K-means",
          "Implementation of Malware detection using decision tree",
          "Implementation of malware detection using random forest"
        ],
        "Automatic Intrusion Detection (updated 2024)": [
          "what is automatic instrusion detection??",
          "what is spam email and spam filtering?",
          "What is phishing URL?",
          "What is network?",
          "How to classify network?",
          "what is Network behavior anomaly detection?",
          "what is Credit card fraud detection?",
          "What is confusion matrix",
          "what is Counterfeit bank note detection?",
          "what is Ad blocking?",
          "what is Wireless indoor localization?",
          "What is botnet?",
          "How to detect botnet",
          "What is Gaussian Naive Bayes",
          "implementation of DDos attacks",
          "Implementation of botnet detection",
          "Implementation of Counterfeit bank note detection",
          "Implementation of ad-blocking",
          "Implementation of phishing URL",
          "Assignment: Implementation of wireless indoor localization",
          "Implementation of spam detection"
        ],
        "Securing and Attacking Data with Machine Learning (updated 2024)": [
          "What is password security?",
          "what is XG-Boost",
          "what is artificial neural network",
          "what are Variance, covariance, and the covariance matrix?",
          "What are Eigenvectors and Eigenvalues",
          "What is MLPClassifier?",
          "What is XGBClassifier?",
          "what is PUFs",
          "what is Challenge-Response Pairs (CRPs)?",
          "Implementation of assessing password security",
          "Assignment on ML attacks on PUFs",
          "Implementation of keystroke detection",
          "Implementation of facial recognition"
        ],
        "(OLD CONTENT) Introduction": [
          "Course structure",
          "How To Make The Most Out Of This Course",
          "Who is this course for????",
          "How does the course work?",
          "Type of Machine learning",
          "AI in the context of cybersecurity"
        ],
        "Basic machine learning for cyber security (OLD CONTENT)": [
          "Introduction",
          "Train test splitting the data Introduction",
          "Train test splitting the data Implemetation",
          "Standardizing your data",
          "Summarizing large data using principal component analysis",
          "Generating text using Markov chains",
          "Performing clustering using scikit-learn",
          "Training an XGBoost classifier",
          "Analyzing time series using statsmodels",
          "Analyzing time series using statsmodels Explanation",
          "Anomaly detection with Isolation Forest introduction",
          "Anomaly detection with Isolation Forest Implementation",
          "Anomaly detection with Isolation Forest Explanation",
          "Natural language processing using a hashing vectorizer and tf-idf Introduction",
          "Natural language processing using a hashing vectorizer and tf-idf Implementation"
        ],
        "(OLD Content) Detecting Email Cybersecurity Threats with AI": [
          "Introduction",
          "Introduction to detect spam with Perceptrons",
          "Introduction to Perceptrons",
          "Introduction to spam filters",
          "Spam filter in action",
          "Detecting spam with linear classifiers",
          "How the Perceptron learns",
          "A simple Perceptron-based spam filter",
          "Pros and cons of Perceptrons",
          "Introduction to Spam detection with SVMs",
          "SVM spam filter example",
          "Introduction to Phishing detection with logistic regression and decision trees",
          "Linear regression for spam detection",
          "introduction to Logistic regression",
          "Logistic Regression Implementation",
          "Introduction to making decisions with trees",
          "Phishing detection with decision trees",
          "Spam detection with Naive Bayes",
          "NLP with Naive Bayes Implementation",
          "Summary of the project"
        ],
        "(OLD COTENT) Malware Threat Detection": [
          "Introduction to Malware detection",
          "Malware goes by many names",
          "Malware analysis tools of the trade",
          "Static malware analysis",
          "Dynamic malware analysis",
          "Hacking the PE file format",
          "Introduction of Decision tree malware detectors",
          "Malware detection with decision trees",
          "Random Forest Malware classifier",
          "Clustering malware with K-Means",
          "K-Means steps and its advantages and disadvantages",
          "Detecting metamorphic malware with HMMs Introductions",
          "Polymorphic malware detection strategies",
          "HMM Implementation",
          "Summary of the section"
        ]
      },
      "requirements": [
        "There will be no Prerequisites.",
        "Basic knowledge of Python will be good.",
        "But everything will be taught from the round up."
      ],
      "description": "*** AS SEEN ON KICKSTARTER ***\nLearn key AI concepts and intuition training to get you quickly up to speed with all things AI. Covering:\n\nHow to start building AI with no previous coding experience using Python.\nHow to solve AI problems in cyber security field.\nHere is what you will get with this course:\n\n\n1. Complete beginner to expert AI skills – Learn to code self-improving AI for a range of purposes. In fact, I will code together with you. Every tutorial starts with a blank page and we write up the code from scratch. This way you can follow along and understand exactly how the code comes together and what each line means.\n2. Coding step– Plus, you’ll get a template which shows all the steps and all detailed explanations on each step.\n3. Intuition Tutorials – Where most courses simply bombard you with dense theory and set you on your way, you will develop a deep understanding for not only what you’re doing, but why you’re doing it. That’s why I don’t throw complex theories at you, but focus on building up your intuition in coding AI making for infinitely better results down the line.\n4. Real-world solutions – You’ll achieve your goal in not only 1 project but in more than 10. Each module is comprised of varying structures and difficulties, meaning you’ll be skilled enough to build AI adaptable to any projects in real life, rather than just passing a glorified memory “test and forget” like most other courses. Practice truly does make perfect.\n5. In-course support – I fully committed to making this the most accessible and results-driven AI course on the planet. This requires me to be there when you need my help. That’s why I will support you in your journey, meaning you’ll get a response from me within 72 hours maximum.",
      "target_audience": [
        "Anyone interested in Artificial Intelligence, Machine Learning or Deep Learning in Cyber Security",
        "Any people who are not satisfied with their job and who want to become a Data Scientist.",
        "Any data analysts who want to level up in Machine Learning, Deep Learning and Artificial Intelligence.",
        "Any people who are not that comfortable with coding but who are interested in Machine Learning, Deep Learning, Artificial Intelligence and want to apply it easily on datasets.",
        "Any students in college who want to start a career in Data Science.",
        "Anyone passionate about Artificial Intelligence.",
        "Data Scientists who want to take their AI Skills to the next level.",
        "AI experts who want to expand on the field of applications.",
        "Any people who want to create added value to their business by using powerful Machine Learning, Artificial Intelligence and Deep Learning tools. Any people who want to work in a Car company as a Data Scientist, Machine Learning, Deep Learning and Artificial Intelligence engineer.",
        "Any people who are not satisfied with their job and who want to become a Data Scientist."
      ]
    },
    {
      "title": "Complete Machine Learning & Data Science with Python| ML A-Z",
      "url": "https://www.udemy.com/course/complete-machine-learning-data-science-libraries-with-python/",
      "bio": "Learn Numpy, Pandas, Matplotlib, Seaborn, Scipy, Supervised & Unsupervised Machine Learning A-Z and feature engineering",
      "objectives": [
        "Data Science libraries like Numpy , Pandas , Matplotlib, Scipy, Scikit Learn, Seaborn , Plotly and many more",
        "Machine learning Concept and Different types of Machine Learning",
        "Machine Learning Algorithms like Regression, Classification, Naive Bayes Classifier, Decision Tree, Support Vector Machine Algorithm etc..",
        "Feature engineering",
        "Python Basics"
      ],
      "course_content": {},
      "requirements": [
        "No previous programming experience needed."
      ],
      "description": "Artificial Intelligence is the next digital frontier, with profound implications for business and society. The global AI market size is projected to reach $202.57 billion by 2026, according to Fortune Business Insights.\nThis Data Science & Machine Learning (ML) course is not only ‘Hands-On’ practical based but also includes several use cases so that students can understand actual Industrial requirements, and work culture. These are the requirements to develop any high level application in AI.\nIn this course several Machine Learning (ML) projects are included.\n1) Project - Customer Segmentation Using K Means Clustering\n2) Project - Fake News Detection using Machine Learning (Python)\n3) Project COVID-19: Coronavirus Infection Probability using Machine Learning\n4) Project - Image compression using K-means clustering | Color Quantization using K-Means\nThis course include topics ---\nWhat is Data Science\nDescribe Artificial Intelligence and Machine Learning and Deep Learning\nConcept of Machine Learning - Supervised Machine Learning , Unsupervised Machine Learning and Reinforcement Learning\nPython for Data Analysis- Numpy\nWorking envirnment-\nGoogle Colab\nAnaconda Installation\nJupyter Notebook\nData analysis-Pandas\nMatplotlib\nWhat is Supervised Machine Learning\nRegression\nClassification\nMultilinear Regression Use Case- Boston Housing Price Prediction\nSave Model\nLogistic Regression on Iris Flower Dataset\nNaive Bayes Classifier on Wine Dataset\nNaive Bayes Classifier for Text Classification\nDecision Tree\nK-Nearest Neighbor(KNN) Algorithm\nSupport Vector Machine Algorithm\nRandom Forest Algorithm I\nWhat is UnSupervised Machine Learning\nTypes of Unsupervised Learning\nAdvantages and Disadvantages of Unsupervised Learning\nWhat is clustering?\nK-means Clustering\nImage compression using K-means clustering | Color Quantization using K-Means\nUnderfitting, Over-fitting and best fitting in Machine Learning\nHow to avoid Overfitting in Machine Learning\nFeature Engineering\nTeachable Machine\nPython Basics\nIn the recent years, self-driving vehicles, digital assistants, robotic factory staff, and smart cities have proven that intelligent machines are possible. AI has transformed most industry sectors like retail, manufacturing, finance, healthcare, and media and continues to invade new territories. Everyday a new app, product or service unveils that it is using machine learning to get smarter and better.\nNOTE :- In description reference notes also provided , open reference notes , there is Download link. You can download datasets there.",
      "target_audience": [
        "Anyone interested in Machine Learning.",
        "Any students in college who want to start a career in Data Science."
      ]
    },
    {
      "title": "Data Visualization in Python Masterclass for Beginners",
      "url": "https://www.udemy.com/course/complete-data-visualization-in-python/",
      "bio": "Visualisation in matplotlib, Seaborn, Plotly & Cufflinks, EDA on Boston Housing, Titanic, IPL, FIFA, Covid-19 Data.",
      "objectives": [
        "Learn Complete Exploratory Data Analysis on the Latest Covid-19 Dataset",
        "Learn EDA on Kaggle's Boston Housing and Titanic Datasets",
        "Learn IPL Cricket Matches and FIFA World Cup Matches Analysis and Visualization",
        "Learn Data Visualization by Plotly and Cufflinks, Seaborn, matplotlib, and Pandas",
        "Learn Interactive plots and visualization",
        "Installation of python and related libraries.",
        "Covid-19 Data Visualization",
        "Covid-19 Dataset Analysis and Visualization in Python",
        "Data Science Visualization with Covid-19",
        "Use the Numpy and Pandas in data manipulation",
        "Learn Complete Text Data EDA",
        "Create a variety of charts, Bar Charts, Line Charts, Stacked Charts, Pie Charts, Histograms, KDE plots, Violinplots, Boxplots, Auto Correlation plots",
        "Learn Data Analysis by Pandas.",
        "Use the Pandas module with Python to create and structure data.",
        "Customize graphs, modifying colors, lines, fonts, and more",
        "Basic concepts of data visualization and its importance in data analysis",
        "How to use Python libraries such as Matplotlib, Seaborn, and Plotly to create various types of charts and plots"
      ],
      "course_content": {},
      "requirements": [
        "No introductory skill level of Python programming required",
        "Have a computer (either Mac, Windows, or Linux)",
        "Desire to learn!"
      ],
      "description": "Are you ready to start your path to becoming a Data Scientist!\nKGP Talkie brings you all in one course. Learn all kinds of Data Visualization with practical datasets.\nThis comprehensive course will be your guide to learning how to use the power of Python to analyze data, create beautiful visualizations!\nThis is a very unique course where you will learn EDA on Kaggle's Boston Housing, Titanic and Latest Covid-19 Datasets, Text Dataset, IPL Cricket Matches of all seasons, and FIFA world cup matches with real and practical examples.\nData Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $110,000 in the United States and all over the World according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems!\nThis course is designed for both beginners with some programming experience or experienced developers looking to make the jump to Data Science!\nThis comprehensive course is comparable to other Data Science bootcamps that usually cost thousands of dollars, but now you can learn all that information at a fraction of the cost! With over 200+ Full HD video lectures and detailed code notebooks for every lecture this is one of the most comprehensive courses on Complete Data Visualization in Python.\nWe'll teach you how to program with Python, how to analyze and create amazing data visualizations with Python! You can use this course as your ready-to-go reference for your own project.\n\n\nHere just a few of the topics we will be learning:\nProgramming with Python\nNumPy with Python\nUsing Pandas Data Frames to solve complex tasks\nUse Pandas to Files\nUse matplotlib and Seaborn for data visualizations\nUse Plotly and Cufflinks for interactive visualizations\nExploratory Data Analysis (EDA) of Boston Housing Dataset\nExploratory Data Analysis (EDA) of Titanic Dataset\nExploratory Data Analysis (EDA) of the Latest Covid-19 Dataset\nand much, much more!\n\n\nThis Data Visualization in Python Masterclass can help data scientists in several ways:\nIt can help them gain a deeper understanding of how to effectively communicate data insights using visualizations.\nIt can teach them how to use Python libraries specifically designed for data visualization, making it easier for them to create visualizations in their own data analysis projects.\nIt can also provide them with hands-on experience working with real-world data sets, allowing them to practice creating visualizations and improve their skills.\nIt can also teach them to create interactive visualizations which can be used to create dashboards and reports, which can be shared with stakeholders.\nIt can also help them to create visualizations that can convey more information in less space, making it more effective and efficient.\n\n\nOverall, this course can help data scientists to become more proficient in creating effective and engaging data visualizations, which can be used to communicate their data insights more effectively.",
      "target_audience": [
        "Beginners python programmers.",
        "Beginners Data Science programmers.",
        "Students of Data Science and Machine Learning.",
        "Anyone interested in learning more about python, data science, or data visualizations.",
        "Anyone interested about the rapidly expanding world of data science!",
        "Developers who want to work in analytics and visualization project.",
        "Anyone who wants to explore and understand data before applying machine learning."
      ]
    },
    {
      "title": "Machine Learning with Python from Scratch",
      "url": "https://www.udemy.com/course/python-machine-learning/",
      "bio": "Mastering Machine Learning Algorithms including Neural Networks with Numpy, Pandas, Matplotlib, Seaborn and Scikit-Learn",
      "objectives": [
        "Have an understand of Machine Learning and how to apply it in your own programs",
        "Understand and be able to use Pythons main scientific libraries for Data analysis - Numpy, Pandas, Matplotlib and Seaborn.",
        "Understand and be able to use artificial neural networks",
        "Obtain a solid understand of machine learning in general",
        "Potential for a new job in the future."
      ],
      "course_content": {
        "Environment Setup": [
          "L1-Anaconda",
          "L2-Jupyter Notebook"
        ],
        "Data Analysis": [
          "L1-Introduction",
          "L2-Numpy: Array Concept and Math Operations",
          "L3-Numpy: Indexing, Slicing and Iterating",
          "L4-Numpy: Shape Manipulation",
          "L5-Numpy: Linear Algebra",
          "L6-Pandas: Data structures and properties",
          "L7-Pandas: Operations",
          "L8-Pandas: Applying Functions",
          "L9-Pandas: Importing and Exporting data",
          "L10-Pandas: Merge-Join-Concat-Group by",
          "L11-Pandas: Statistics with Pandas",
          "L12-Time Series with Pandas",
          "L13-Matplotlib basics",
          "L14-Matplotlib Subplots and Axes",
          "L15-Matplotlib: Object Oriented Method",
          "L16-Matplotlib: Color Maps",
          "L17-Matplotlib: Statistical Graphs part1",
          "L18-Matplotlib: Statistical Graphs part2",
          "L19-Seaborn: Basics",
          "L20-Seaborn: Color Palette",
          "L21-Seaborn: Categorical Data",
          "L22-Seaborn: Numerical Data"
        ],
        "Machine Learning": [
          "L1-Introduction to Machine Learning",
          "L2-Overfitting and Underfitting",
          "L3-KFold Cross Validation",
          "L4-Classification Metrics",
          "L5-Logistic Regression",
          "L6-Plotting Decision Boundaries",
          "L7-Naive Bayes Classifier",
          "L8-Suppor Vector Machines for Classification",
          "L9-Decision Trees",
          "L10-Random Forest",
          "L11-KNN",
          "L12-GridSearchCV",
          "L13-K-Means",
          "L14-Principal Component Analysis(PCA)",
          "L15-Linear Discriminant Analysis(LDA)",
          "L16-Kernel Principal Component Analysis(KPCA)",
          "L17-Ensemble Methods(Bagging)",
          "L18-AdaBoost",
          "L19-Regression Model and Metrics",
          "L20-Linear Regression",
          "L21-Regularization with Lasso, Ridge and ElasticNet",
          "L22-Polynomial Regression",
          "L23-SVM, KNN and Random Forest for Regression",
          "L24-RANSAC Regression"
        ],
        "Neural Networks": [
          "L1-Neural Networks Concepts-Part 1",
          "L2-Neural Networks Concepts-Part 2",
          "L3-Loss Functions",
          "L4-Activation Functions",
          "L5-Optimization of ANNs",
          "L6-Constructing an ANN with Python-part1",
          "L7-Constructing an ANN with Python-part2",
          "L8-Constructing an ANN with Python-part3",
          "L9-Perceptron with Scikit Learn",
          "L10-Multilayer Perceptron with Scikit Learn"
        ],
        "Applications": [
          "L1-Datasets",
          "L2-ANN for Regression Part 1",
          "L3-ANN for Regression Part 2",
          "L4-Recognizing Handwritten Digits"
        ],
        "Extra Information - Source code, and other stuff": [
          "Source Code",
          "Bonus Lecture and Information"
        ]
      },
      "requirements": [
        "Basic knowledge of Python",
        "Basic knowledge of Linear Algebra",
        "No previous experience in Machine learning, or any of the various libraries are needed."
      ],
      "description": "Machine Learning is a hot topic!  Python Developers who understand how to work with Machine Learning are in high demand.\nBut how do you get started?\nMaybe you tried to get started with Machine Learning, but couldn’t find decent tutorials online to bring you up to speed, fast.\nMaybe the information you found was too basic, and didn’t give you the real-world Machine learning skills using Python that you needed.\nOr maybe the information got bogged down in complex math explanations and was too difficult to relate to.\nWhatever the reason, you are in the right place if you want to progress your skills in Machine Language using Python.\nThis course will help you to understand the main machine learning algorithms using Python, and how to apply them in your own projects.\nBut what exactly is Machine Learning?\nIt’s a field of computer science that gives computers the ability to “learn” – e.g. continually improve performance on a specific task, with data, without being explicitly programmed.\nWhy is it important?\nMachine learning is often used to solve tasks considered too complex for humans to solve.  We create algorithms and apply a bunch of data to that algorithm and let the computer process (execute) the algorithm and search for a model (solution).\nBecause of the practical applications of machine learning, such as self driving cars (one example) there is huge interest from companies and government in Machine learning, and as a result, there are a a lot of opportunities for Python developers who are skilled in this field.\nIf you want to increase your career options, then understanding and being able to work with Machine Learning with your own Python programs should be high on your list of priorities.\nWhat will you learn in this course?\nFor starters, you will learn about the main scientific libraries in Python for data analysis such as Numpy, Pandas, Matplotlib and Seaborn. You’ll then learn about artificial neural networks and how to work with machine learning models using them.\nYou obtain a solid background in machine learning and be able to apply that knowledge directly in your own programs.\nWhat are the Main topics included in the course?\nData Analysis with Numpy, Pandas, Matplotlib and Seaborn.\nThe machine learning schema.\nOverfitting and Underfitting\nK Fold Cross Validation\nClassification metrics\nRegularization: Lasso, Ridge and ElasticNet\nLogistic Regression\nSupport Vector Machines for Regression and Classification\n\nNaive Bayes Classifier\nDecision Trees and Random Forest\nKNN classifier\nHyperparameter Optimization: GridSearchCV\nPrincipal Component Analysis (PCA)\nLinear Discriminant Analysis (LDA)\nKernel Principal Component Analysis (KPCA)\nEnsemble methods: Bagging\nAdaBoost\nK means clustering analysis\nRegression model and evaluation\nLinear and Polynomial Regression\nSVM, KNN, and Random Forest for Regression\nRANSAC Regression\nNeural Networks: Constructing our own MLP.\nPerceptron and Multilayer Perceptron\nAnd don’t worry if you do not understand some, or all of these terms. By the end of the course you will know what they are and how to use them.\nWhy enrolling in this course is the best decision you can make.\nThis course helps you to understand the difficult concepts of Machine learning in a unique way. Rather than just focusing on complex maths explanaitons, simpler explanations with charts, and info displays are included.\nMany examples and genuinely useful code snippets are also included to make it even easier to learn and understand.\nAfter completing this course, you will have the necessary skills to apply Machine learning in your own projects.\nThe sooner you sign up for this course, the sooner you will have the skills and knowledge you need to increase your job or consulting opportunities.    Your new job or consulting opportunity awaits!\nWhy not get started today?\nClick the Signup button to sign up for the course!",
      "target_audience": [
        "Students who wish to take their basic Python skills to the next level by mastering Pythons various scientific libraries",
        "Students who want to understand and apply Machine Learning into their own programs",
        "Students wanting to empower themselves with machine learning."
      ]
    },
    {
      "title": "Data Science: Natural Language Processing (NLP) in Python",
      "url": "https://www.udemy.com/course/data-science-natural-language-processing-in-python/",
      "bio": "Applications: decrypting ciphers, spam detection, sentiment analysis, article spinners, and latent semantic analysis.",
      "objectives": [
        "Write your own cipher decryption algorithm using genetic algorithms and language modeling with Markov models",
        "Write your own spam detection code in Python",
        "Write your own sentiment analysis code in Python",
        "Perform latent semantic analysis or latent semantic indexing in Python",
        "Have an idea of how to write your own article spinner in Python",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {},
      "requirements": [
        "Install Python, it's free!",
        "You should be at least somewhat comfortable writing Python code",
        "Know how to install numerical libraries for Python such as Numpy, Scipy, Scikit-learn, Matplotlib, and BeautifulSoup",
        "Take my free Numpy prerequisites course (it's FREE, no excuses!) to learn about Numpy, Matplotlib, Pandas, and Scikit-Learn, as well as Machine Learning basics",
        "Optional: If you want to understand the math parts, linear algebra and probability are helpful"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nIn this course you will build MULTIPLE practical systems using natural language processing, or NLP - the branch of machine learning and data science that deals with text and speech. This course is not part of my deep learning series, so it doesn't contain any hard math - just straight up coding in Python. All the materials for this course are FREE.\nAfter a brief discussion about what NLP is and what it can do, we will begin building very useful stuff. The first thing we'll build is a cipher decryption algorithm. These have applications in warfare and espionage. We will learn how to build and apply several useful NLP tools in this section, namely, character-level language models (using the Markov principle), and genetic algorithms.\nThe second project, where we begin to use more traditional \"machine learning\", is to build a spam detector. You likely get very little spam these days, compared to say, the early 2000s, because of systems like these.\nNext we'll build a model for sentiment analysis in Python. This is something that allows us to assign a score to a block of text that tells us how positive or negative it is. People have used sentiment analysis on Twitter to predict the stock market.\nWe'll go over some practical tools and techniques like the NLTK (natural language toolkit) library and latent semantic analysis or LSA.\nFinally, we end the course by building an article spinner. This is a very hard problem and even the most popular products out there these days don't get it right. These lectures are designed to just get you started and to give you ideas for how you might improve on them yourself. Once mastered, you can use it as an SEO, or search engine optimization tool. Internet marketers everywhere will love you if you can do this for them!\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...",
      "target_audience": [
        "Students who are comfortable writing Python code, using loops, lists, dictionaries, etc.",
        "Students who want to learn more about machine learning but don't want to do a lot of math",
        "Professionals who are interested in applying machine learning and NLP to practical problems like spam detection, Internet marketing, and sentiment analysis",
        "This course is NOT for those who find the tasks and methods listed in the curriculum too basic.",
        "This course is NOT for those who don't already have a basic understanding of machine learning and Python coding (but you can learn these from my FREE Numpy course).",
        "This course is NOT for those who don't know (given the section titles) what the purpose of each task is. E.g. if you don't know what \"spam detection\" might be useful for, you are too far behind to take this course."
      ]
    },
    {
      "title": "Prompt Engineering: Getting Future Ready (1000+ Prompts inc)",
      "url": "https://www.udemy.com/course/prompt-engineering-course/",
      "bio": "Prompt Engineering for beginners in 2023. 1000+ prompts, resources, and templates for ChatGPT and Image-to-Image & text",
      "objectives": [
        "Create thousands of unique prompts for ChatGPT, Stable Diffusion, DALL-E, and MidJourney.",
        "Generate text and image-based prompts tailored to your writing style and genre with Leonardo and DALL-E.",
        "Get access to resources, templates and guides worth 1000+ prompts",
        "Generate Hyper-realistic images using our Prompt templates to supercharge your work & life",
        "Automate your prompt writing process with AI-powered prompt engineering tools",
        "Get future ready by mastering Prompt Engineering as a beginners in 2023",
        "Learn how to leverage Perplexity AI for Research and Writing"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course outline",
          "Join your free AI Newsletter"
        ],
        "The Basics": [
          "What is Prompt Engineering?"
        ],
        "2024 Prompt ENG Update": [
          "Strategy 1: Clear Instructions",
          "Strategy 2: Reference Text",
          "Strategy 3: Splitting Complex Tasks",
          "Strategy 4: Let Model Think",
          "Strategy 5: External Tools",
          "Strategy 6: Testing Changes Systematically",
          "Places you can find Prompts",
          "Practice Test"
        ],
        "2024 Tool Intros": [
          "Intro",
          "ChatGPT Demo",
          "Bard Intro",
          "Massive GPT-4o Image Generation Update (2025)",
          "Claude-2 Intro",
          "Bing Image Creator Intro",
          "Midjourney Intro",
          "RunwayML (GEN-2) Video Generation Intro",
          "PIKA Video Generation Intro",
          "Elevenlabs for Audio Generation",
          "D-ID for Digital Avatars",
          "Olly Social (AI Chrome Extension for Social Media Engagement)",
          "Claude Introduces Prompt Caching"
        ],
        "2 Hour Masterclass on AI": [
          "(Update) New ChatGPT UI",
          "AI Masterclass"
        ],
        "Engineering Text to Text Prompts": [
          "What is Text to Text Prompting?",
          "What is ChatGPT? (Optional)",
          "ChatGPT Walkthrough (Optional)",
          "Prompt Engineering: Text to Text",
          "Playing with Text Prompts in ChatGPT",
          "Repository of Prompts (Part-1)",
          "Bypassing AI Content Detection"
        ],
        "Text to Image Tool setups (Skippable)": [
          "Leonardo AI Setup",
          "Midjourney setup",
          "Two approaches to setup Stable Diffusion",
          "Approach 1: Offline Setup",
          "Approach 1: Part 2",
          "Inpainting in Stable Diffusion",
          "Outpainting in Stable Diffusion",
          "Approach 2: Online Setup"
        ],
        "Engineering Text to Image Prompts": [
          "How to write Image Prompts?",
          "Repository of prompts (Part-1)",
          "Repository of prompts (Part-2)",
          "AI tools that generate image prompts",
          "Hyper-realistic prompts in Leonardo AI",
          "20+ Prompts in Leonardo for Realistic generation"
        ],
        "More about AI & Prompting": [
          "Leonardo vs Midjourney vs Stable Diffusion",
          "2024 AI 5 Things to stay up to date",
          "Learn how to leverage Perplexity AI for Research and Writing",
          "GPT-4o",
          "Claude 3.5 (Visual PDFs)",
          "2025 AI Roadmap",
          "GPT-5"
        ],
        "BONUS": [
          "Bonus"
        ]
      },
      "requirements": [
        "No Programming Expertise needed",
        "A Functioning Computer",
        "An OpenAI account",
        "Willingness to learn"
      ],
      "description": "Welcome to \"Prompt Engineering: Getting Future Ready,\" the perfect course for beginners looking to dive into the world of prompt engineering in 2023. This comprehensive course offers over 1000 prompts, resources, and templates that cover the major tools used in prompt engineering, including ChatGPT, Stable Diffusion, Leonardo AI, and Midjourney.\nThrough this course, you will gain a deep understanding of prompt engineering and how it works. You will explore the key differences between text-to-text generation and image-to-image generation, and learn how to use each of the tools covered in the course effectively. Whether you're interested in generating text or images, you'll have the skills you need to create stunning and hyper-realistic results.\nThe course also features a hyper-realism prompt guide that will take you through the steps of creating incredibly realistic images and text that are almost indistinguishable from real life. With practical examples and hands-on exercises, you'll gain the skills you need to become an expert in prompt engineering.\nBy the end of this course, you will be well equipped to use the latest tools and techniques in prompt engineering, setting you up for success in a field that is constantly evolving. So, enroll now and join us on this exciting journey towards becoming a prompt engineering expert!",
      "target_audience": [
        "Beginners interested in prompt engineering",
        "Experienced AI practitioners looking to expand their skills",
        "Individuals who want to learn how to generate hyper-realistic text and images",
        "Those who want to stay up-to-date with the latest tools and techniques in AI",
        "Students who are interested in AI and want to learn more about prompt engineering",
        "Professionals who want to incorporate prompt engineering into their work"
      ]
    },
    {
      "title": "Mastering Ollama: Build Private Local LLM Apps with Python",
      "url": "https://www.udemy.com/course/master-ollama-python/",
      "bio": "Run custom Ollama LLMs privately on your system—Use ChatGPT-like UI—Hands-on projects—No cloud or extra costs required",
      "objectives": [
        "Install and configure Ollama on your local system to run large language models privately.",
        "Customize LLM models to suit specific needs using Ollama’s options and command-line tools.",
        "Execute all terminal commands necessary to control, monitor, and troubleshoot Ollama models.",
        "Set up and manage a ChatGPT-like interface, allowing you to interact with models locally.",
        "Utilize different model types—including text, vision, and code-generating models—for various applications.",
        "Create custom LLM models from a Modelfile file and integrate them into your applications.",
        "Build Python applications that interface with Ollama models using its native library and OpenAI API compatibility.",
        "Develop Retrieval-Augmented Generation (RAG) applications by integrating Ollama models with LangChain.",
        "Implement tools and function calling to enhance model interactions for advanced workflows.",
        "Set up a user-friendly UI frontend to allow users to interface and chat with different Ollama models."
      ],
      "course_content": {
        "Introduction": [
          "Introduction & What Will Your Learn",
          "Course Prerequisites",
          "Please WATCH this DEMO"
        ],
        "Development Environment Setup": [
          "Development Environment Setup",
          "Udemy 101 - Tips for A Better Learning Experience"
        ],
        "Download Code and Resources": [
          "How to Get Source Code",
          "Download Source code and Resources"
        ],
        "Ollama Deep Dive - Introduction to Ollama and Setup": [
          "Ollama Deep Dive - Ollama Overview - What is Ollama and Advantages",
          "Ollama Key Features and Use Cases",
          "System Requirements & Ollama Setup - Overview",
          "Download and Setup Ollama and Llam3.2 Model - Hands-on & Testing",
          "Ollama Models Page - Full Overview",
          "Ollama Model Parameters Deep Dive",
          "Understanding Parameters and Disk Size and Computational Resources Needed"
        ],
        "Ollama CLI Commands and the REST API - Hands-on": [
          "Ollama Commands - Pull and Testing a Model",
          "Pull in the Llava Multimodal Model and Caption an Image",
          "Summarization and Sentiment Analysis & Customizing Our Model with the Modelfile",
          "Ollama REST API - Generate and Chat Endpoints",
          "Ollama REST API - Request JSON Mode",
          "Ollama Models Support Different Tasks - Summary"
        ],
        "Ollama - User Interfaces for Ollama Models": [
          "Different Ways to Interact with Ollama Models - Overview",
          "Ollama Model Running Under Msty App - Frontend Tool - RAG System Chat with Docs"
        ],
        "Ollama Python Library - Using Python to Interact with Ollama Models": [
          "The Ollama Python Library for Building LLM Local Applications - Overview",
          "Interact with Llama3 in Python Using Ollama REST API - Hands-on",
          "Ollama Python Library - Chatting with a Model",
          "Chat Example with Streaming",
          "Using Ollama show Function",
          "Create a Custom Model in Code"
        ],
        "Ollama Building LLM Applications with Ollama Models": [
          "Hands-on: Build a LLM App - Grocery List Categorizer",
          "Building RAG Systems with Ollama - RAG & LangChain Overview",
          "Deep Dive into Vectorstore and Embeddings - The Whole Picture - Crash course",
          "PDF RAG System Overview - What we'll Build",
          "Setup RAG System - Document Ingestion & Vector Database Creation and Embeddings",
          "RAG System - Retrieval and Querying",
          "RAG System - Cleaner Code",
          "RAG System - Streamlit UI"
        ],
        "Ollama Tool Function Calling - Hands-on": [
          "Function Calling (Tools) Overview",
          "Setup Tool Function Calling Application",
          "Categorize Items Using the Model and Setup the Tools List",
          "Tools Calling LLM Application - Final Product"
        ],
        "Final RAG System with Ollama and Voice Response": [
          "Voice RAG System - Overview",
          "Setup EleveLabs API Key and Load and Summarize the Document",
          "Ollama Voice RAG System - Working!",
          "Adding ElevenLab Voice Generated Reading the Response Back to Us"
        ]
      },
      "requirements": [
        "Basic Python Programming Knowledge",
        "Comfort with Command Line Interface (CLI)"
      ],
      "description": "Are you concerned about data privacy and the high costs associated with using Large Language Models (LLMs)?\nIf so, this course is the perfect fit for you. \"Mastering Ollama: Build Private LLM Applications with Python\" empowers you to run powerful AI models directly on your own system, ensuring complete data privacy and eliminating the need for expensive cloud services.\nBy learning to deploy and customize local LLMs with Ollama, you'll maintain full control over your data and applications while avoiding the ongoing expenses and potential risks of cloud-based solutions.\n\n\nThis hands-on course will take you from beginner to expert in using Ollama, a platform designed for running local LLM models. You'll learn how to set up and customize models, create a ChatGPT-like interface, and build private applications using Python—all from the comfort of your system.\nIn this course, you will:\nInstall and configure Ollama for local LLM model execution.\nCustomize LLM models to suit your specific needs using Ollama’s tools.\nMaster command-line tools to control, monitor, and troubleshoot Ollama models.\nIntegrate various models, including text, vision, and code-generating models, and even create your custom models.\nBuild Python applications that interface with Ollama models using its native library and OpenAI API compatibility.\nDevelop Retrieval-Augmented Generation (RAG) applications by integrating Ollama models with LangChain.\nImplement tools and function calling to enhance model interactions in terminal and LangChain environments.\nSet up a user-friendly UI frontend to allow users to chat with different Ollama models.\nWhy is this course important?\nIn a world where data privacy is growing, running LLMs locally ensures your data stays on your machine. This enhances data security and allows you to customize models for specialized tasks without external dependencies or additional costs.\nYou'll engage in practical activities like building custom models, developing RAG applications that retrieve and respond to user queries based on your data, and creating interactive interfaces.\nEach section has real-world applications to give you the experience and confidence to build your local LLM solutions.\nWhy choose this course?\nThis course is uniquely crafted to make advanced AI concepts approachable and actionable. We focus on practical, hands-on learning, enabling you to build real-world solutions from day one. You'll dive deep into projects that bridge theory and practice, ensuring you gain tangible skills in developing local LLM applications. Whether you're new to large language models or seeking to enhance your existing abilities, this course provides all the guidance and tools you need to create private AI applications using Ollama and Python confidently.\n\n\nReady to develop powerful AI applications while keeping your data completely private?\nEnroll today and seize full control of your AI journey with Ollama.\nHarness the capabilities of local LLMs on your own system and take your skills to the next level!",
      "target_audience": [
        "Python Developers looking to expand their skill set by integrating Large Language Models (LLMs) into their applications.",
        "AI Enthusiasts and Practitioners interested in running and customizing local LLMs privately without relying on cloud services.",
        "Data Scientists and Machine Learning Engineers who want to understand and implement local AI models using Ollama and LangChain.",
        "Software Engineers aiming to develop secure AI applications on their own systems, maintaining full control over data and infrastructure.",
        "Students and Researchers exploring the capabilities of local LLMs and seeking hands-on experience with advanced AI technologies.",
        "Professionals Concerned with Data Privacy who need to process sensitive information without sending data to external servers or cloud platforms.",
        "Anyone Interested in Building ChatGPT-like Applications Locally, and wants to gain practical experience through real-world projects.",
        "Beginners to LLMs and Ollama who have basic Python knowledge and are eager to learn about AI application development.",
        "Beginners to LLMs and Ollama who have basic Python knowledge and are eager to learn about AI application development.",
        "Educators and Trainers seeking to incorporate AI and LLMs into their curriculum or training programs without relying on external services."
      ]
    },
    {
      "title": "Python for Machine Learning: The Complete Beginner's Course",
      "url": "https://www.udemy.com/course/python-for-machine-learning-beginners/",
      "bio": "Learn to create machine learning algorithms in Python for students and professionals",
      "objectives": [
        "Learn Python programming and Scikit learn applied to machine learning regression",
        "Understand the underlying theory behind simple and multiple linear regression techniques",
        "Learn to solve regression problems (linear regression and logistic regression)",
        "Learn the theory and the practical implementation of logistic regression using sklearn",
        "Learn the mathematics behind decision trees",
        "Learn about the different algorithms for clustering"
      ],
      "course_content": {
        "Introduction to Machine Learning": [
          "What is Machine Learning?",
          "Applications of Machine Learning",
          "Machine learning Methods",
          "What is Supervised learning?",
          "What is Unsupervised learning?",
          "Supervised learning vs Unsupervised learning",
          "Course Materials"
        ],
        "Optional: Setting Up Python & ML Algorithms Implementation": [
          "Introduction",
          "Python libraries for Machine Learning",
          "Setting up Python",
          "What is Jupyter?",
          "Anaconda Installation Windows Mac and Ubuntu",
          "Implementing Python in Jupyter",
          "Managing Directories in Jupyter Notebook"
        ],
        "Simple Linear Regression": [
          "Introduction to regression",
          "How Does Linear Regression Work?",
          "Line representation",
          "Implementation in python: Importing libraries & datasets",
          "Implementation in python: Distribution of the data",
          "Implementation in python: Creating a linear regression object"
        ],
        "Multiple Linear Regression": [
          "Understanding Multiple linear regression",
          "Implementation in python: Exploring the dataset",
          "Implementation in python: Encoding Categorical Data",
          "Implementation in python: Splitting data into Train and Test Sets",
          "Implementation in python: Training the model on the Training set",
          "Implementation in python: Predicting the Test Set results",
          "Evaluating the performance of the regression model",
          "Root Mean Squared Error in Python"
        ],
        "Classification Algorithms: K-Nearest Neighbors": [
          "Introduction to classification",
          "K-Nearest Neighbors algorithm",
          "Example of KNN",
          "K-Nearest Neighbours (KNN) using python",
          "Implementation in python: Importing required libraries",
          "Implementation in python: Importing the dataset",
          "Implementation in python: Splitting data into Train and Test Sets",
          "Implementation in python: Feature Scaling",
          "Implementation in python: Importing the KNN classifier",
          "Implementation in python: Results prediction & Confusion matrix"
        ],
        "Classification Algorithms: Decision Tree": [
          "Introduction to decision trees",
          "What is Entropy?",
          "Exploring the dataset",
          "Decision tree structure",
          "Implementation in python: Importing libraries & datasets",
          "Implementation in python: Encoding Categorical Data",
          "Implementation in python: Splitting data into Train and Test Sets",
          "Implementation in python: Results prediction & Accuracy"
        ],
        "Classification Algorithms: Logistic regression": [
          "Introduction",
          "Implementation steps",
          "Implementation in python: Importing libraries & datasets",
          "Implementation in python: Splitting data into Train and Test Sets",
          "Implementation in python: Pre-processing",
          "Implementation in python: Training the model",
          "Implementation in python: Results prediction & Confusion matrix",
          "Logistic Regression vs Linear Regression"
        ],
        "Clustering": [
          "Introduction to clustering",
          "Use cases",
          "K-Means Clustering Algorithm",
          "Elbow method",
          "Steps of the Elbow method",
          "Implementation in python",
          "Hierarchical clustering",
          "Density-based clustering",
          "Implementation of k-means clustering in python",
          "Importing the dataset",
          "Visualizing the dataset",
          "Defining the classifier",
          "3D Visualization of the clusters",
          "3D Visualization of the predicted values",
          "Number of predicted clusters"
        ],
        "Recommender System": [
          "Introduction",
          "Collaborative Filtering in Recommender Systems",
          "Content-based Recommender System",
          "Implementation in python: Importing libraries & datasets",
          "Merging datasets into one dataframe",
          "Sorting by title and rating",
          "Histogram showing number of ratings",
          "Frequency distribution",
          "Jointplot of the ratings and number of ratings",
          "Data pre-processing",
          "Sorting the most-rated movies",
          "Grabbing the ratings for two movies",
          "Correlation between the most-rated movies",
          "Sorting the data by correlation",
          "Filtering out movies",
          "Sorting values",
          "Repeating the process for another movie",
          "Quiz Time"
        ],
        "Conclusion": [
          "Conclusion"
        ]
      },
      "requirements": [
        "Experience with the basics of Python",
        "Readiness, flexibility, and passion for learning",
        "Basic mathematical skills"
      ],
      "description": "To understand how organizations like Google, Amazon, and even Udemy use machine learning and artificial intelligence (AI) to extract meaning and insights from enormous data sets, this machine learning course will provide you with the essentials. According to Glassdoor and Indeed, data scientists earn an average income of $120,000, and that is just the norm!\nWhen it comes to being attractive, data scientists are already there. In a highly competitive job market, it is tough to keep them after they have been hired. People with a unique mix of scientific training, computer expertise, and analytical abilities are hard to find.\nLike the Wall Street \"quants\" of the 1980s and 1990s, modern-day data scientists are expected to have a similar skill set. People with a background in physics and mathematics flocked to investment banks and hedge funds in those days because they could come up with novel algorithms and data methods.\nThat being said, data science is becoming one of the most well-suited occupations for success in the twenty-first century. It is computerized, programming-driven, and analytical in nature. Consequently, it comes as no surprise that the need for data scientists has been increasing in the employment market over the last several years.\nThe supply, on the other hand, has been quite restricted. It is challenging to get the knowledge and abilities required to be recruited as a data scientist.\nIn this course, mathematical notations and jargon are minimized, each topic is explained in simple English, making it easier to understand. Once you've gotten your hands on the code, you'll be able to play with it and build on it. The emphasis of this course is on understanding and using these algorithms in the real world, not in a theoretical or academic context.\nYou'll walk away from each video with a fresh idea that you can put to use right away!\nAll skill levels are welcome in this course, and even if you have no prior statistical experience, you will be able to succeed!",
      "target_audience": [
        "Anyone who want to pursue a career in Machine Learning",
        "Any Python programming enthusiast willing to add machine learning proficiency to their portfolio",
        "Technologists who are curious about how Machine Learning works in the real world",
        "Programmers who are looking to add machine learning to their skillset"
      ]
    },
    {
      "title": "[2025] Tensorflow 2: Deep Learning & Artificial Intelligence",
      "url": "https://www.udemy.com/course/deep-learning-tensorflow-2/",
      "bio": "Machine Learning & Neural Networks for Computer Vision, Time Series Analysis, NLP, GANs, Reinforcement Learning, +More!",
      "objectives": [
        "Artificial Neural Networks (ANNs) / Deep Neural Networks (DNNs)",
        "Predict Stock Returns",
        "Time Series Forecasting",
        "Computer Vision",
        "How to build a Deep Reinforcement Learning Stock Trading Bot",
        "GANs (Generative Adversarial Networks)",
        "Recommender Systems",
        "Image Recognition",
        "Convolutional Neural Networks (CNNs)",
        "Recurrent Neural Networks (RNNs)",
        "Use Tensorflow Serving to serve your model using a RESTful API",
        "Use Tensorflow Lite to export your model for mobile (Android, iOS) and embedded devices",
        "Use Tensorflow's Distribution Strategies to parallelize learning",
        "Low-level Tensorflow, gradient tape, and how to build your own custom models",
        "Natural Language Processing (NLP) with Deep Learning",
        "Demonstrate Moore's Law using Code",
        "Transfer Learning to create state-of-the-art image classifiers",
        "Earn the Tensorflow Developer Certificate",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {},
      "requirements": [
        "Know how to code in Python and Numpy",
        "For the theoretical parts (optional), understand derivatives and probability"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\n\n\nWelcome to Tensorflow 2.0!\n\n\nWhat an exciting time. It's been nearly 4 years since Tensorflow was released, and the library has evolved to its official second version.\nTensorflow is Google's library for deep learning and artificial intelligence.\nDeep Learning has been responsible for some amazing achievements recently, such as:\nGenerating beautiful, photo-realistic images of people and things that never existed (GANs)\nBeating world champions in the strategy game Go, and complex video games like CS:GO and Dota 2 (Deep Reinforcement Learning)\nSelf-driving cars (Computer Vision)\nSpeech recognition (e.g. Siri) and machine translation (Natural Language Processing)\nEven creating videos of people doing and saying things they never did (DeepFakes - a potentially nefarious application of deep learning)\n\n\nTensorflow is the world's most popular library for deep learning, and it's built by Google, whose parent Alphabet recently became the most cash-rich company in the world (just a few days before I wrote this). It is the library of choice for many companies doing AI and machine learning.\nIn other words, if you want to do deep learning, you gotta know Tensorflow.\n\n\nThis course is for beginner-level students all the way up to expert-level students. How can this be?\nIf you've just taken my free Numpy prerequisite, then you know everything you need to jump right in. We will start with some very basic machine learning models and advance to state of the art concepts.\nAlong the way, you will learn about all of the major deep learning architectures, such as Deep Neural Networks, Convolutional Neural Networks (image processing), and Recurrent Neural Networks (sequence data).\nCurrent projects include:\nNatural Language Processing (NLP)\nRecommender Systems\nTransfer Learning for Computer Vision\nGenerative Adversarial Networks (GANs)\nDeep Reinforcement Learning Stock Trading Bot\nEven if you've taken all of my previous courses already, you will still learn about how to convert your previous code so that it uses Tensorflow 2.0, and there are all-new and never-before-seen projects in this course such as time series forecasting and how to do stock predictions.\nThis course is designed for students who want to learn fast, but there are also \"in-depth\" sections in case you want to dig a little deeper into the theory (like what is a loss function, and what are the different types of gradient descent approaches).\n\n\nAdvanced Tensorflow topics include:\nDeploying a model with Tensorflow Serving (Tensorflow in the cloud)\nDeploying a model with Tensorflow Lite (mobile and embedded applications)\nDistributed Tensorflow training with Distribution Strategies\nWriting your own custom Tensorflow model\nConverting Tensorflow 1.x code to Tensorflow 2.0\nConstants, Variables, and Tensors\nEager execution\nGradient tape\n\n\nInstructor's Note: This course focuses on breadth rather than depth, with less theory in favor of building more cool stuff. If you are looking for a more theory-dense course, this is not it. Generally, for each of these topics (recommender systems, natural language processing, reinforcement learning, computer vision, GANs, etc.) I already have courses singularly focused on those topics.\n\n\nThanks for reading, and I’ll see you in class!\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Beginners to advanced students who want to learn about deep learning and AI in Tensorflow 2.0"
      ]
    },
    {
      "title": "ChatGPT Productivity + Time Management. ChatGPT Productivity",
      "url": "https://www.udemy.com/course/chatgpt-foundation-course-complete-chatgpt-course-chatgpt-new/",
      "bio": "ChatGPT To Dramatically Improve Productivity + Unusual Time Mgmt. Tips. ChatGPT Work, Chat GPT Automation, ChatGPT 2023!",
      "objectives": [
        "How to leverage ChatGPT AI in your everyday work life.",
        "Hot Tips so you can get the most out of using ChatGpt that many are not aware of.",
        "Learn some unique ways you can increase your productivity and improve your time management.",
        "Full understanding of the ChatGPT interface.",
        "Dramatically increase your productivity by using ChatGPT",
        "Using ChatGTP For Hard Skills like programming, debugging and excel.",
        "Using ChatGTP For Soft Skills like communication, problem solving, adaptability, and collaboration.",
        "Writing emails and other correspondence for a variety of use cases."
      ],
      "course_content": {},
      "requirements": [
        "No prerequisites except desire to learn how emerging technology like ChatGPT AI can be used in everyday life.",
        "Free ChatGPT account to apply what you learn to your unique situation."
      ],
      "description": "Curious about all the talk about ChatGPT and how can you use it to your own benefit? Want to learn some unusual tips & best practices to dramatically increase your productivity?  Then this course which combines two courses into one is for you!\nWhat students are saying:\n\"I have already been introduced to ChatGPT but I have learned so much in the first few videos that I didn't even know!\" -Shaista R\n\"Lot's of useful examples in the course. I must admit that besides learning this was also a bit of entertainment. Nice job Steve \" -Bernd E\nSteve is a great teacher. I am looking forward to hearing all his 'crazy' time-saving ideas. He already gave me a good one. I wish I had heard some of these great ideas before I retired. I could have used them in my job. -Deborah H\n\"Practical and to the point\"-Igor V\n\"The instructor is knowledgeable, engaging and easy to follow.\" -Carmen R\n\n\nYOU GET TWO COURSES IN ONE ALL FOCUSED ON INCREASING YOUR PRODUCTIVTY FAST!\n\n\nCourse #1 that is included. ChatGPT to dramatically increase your productivity and save you a tremendous amount if time.\nReady to use the incredible power of Artificial Intelligence (AI) in an easy manner with ChatGPT?\nDo you want to learn how to automate repetitive tasks, streamline workflows, and make data-driven decisions using ChatGPT?\nIf yes, then this is the course for you!\nKey ChatGPT Course Outcomes:\nGain a comprehensive understanding of ChatGPT and its architecture\nLearn how to interact with ChatGPT and customize its outputs\nDiscover the potential applications of ChatGPT in business and beyond\nAcquire the skills needed to automate repetitive tasks and streamline your workflows\nDevelop soft skills, such as problem-solving and communication, through the use of ChatGPT\nUse ChatGPT for hard skills like learning Excel or debugging code.\n\n\nThe their is the 2nd course that is included on Unusual Tips To Increase Your Productivity!\nGoal of the course is to give you some practical ideas you can implement immediately, even if some are a little scary.\nThe best practices cover many areas including:\nEmail\nMeetings\nWorking with others\nWorking with your boss\nPhone conversations\nand more!\nSome of the lessons include:\nMaybe #1 Most Crazy: Your Outgoing Voicemail Message\nShort Emails Save You In Many Ways\n#1 Way To Reduce New Emails In Your Inbox\nThe One Most Important Task To Do Everyday\nEat That Frog (Yummy)\nThe 80/20 Rule Is So 1980. Do This Instead\nBatching Is Like Bathing. You Feel Great When Done\nNo Multi-Tasking Except.\nNo \"Email At Zero\" Punishment Free Zone\nI am Feeling Sleepy. Time For A Power Nap\nand more!\nAgain, I use every technique in this course and rest assured, there will be some you will think that is impossible for you to do but may be worth a try.\nThe next step? Just click the button to enroll and I look forward to seeing you in your first lesson!\nThanks.\n-Steve B\n\n\nFor fun, I asked ChatGPT some questions about itself and here are the answers.\nWhat is ChatGPT?\nI am ChatGPT, a large language model developed by OpenAI. I use deep learning algorithms to understand natural language and generate responses to questions or prompts. Essentially, I'm a computer program designed to mimic human conversation and provide helpful answers and insights on a wide range of topics.\nWhat Are The Benefits Of ChatGPT?\nThere are several benefits of using ChatGPT, including:\nAvailability: ChatGPT is available 24/7, so users can get answers to their questions or engage in conversation at any time of day.\nSpeed: ChatGPT is capable of processing large amounts of data quickly, so users can get responses to their queries in a matter of seconds.\nConvenience: ChatGPT can be accessed from anywhere with an internet connection, making it a convenient tool for people who need information or assistance on-the-go.\nConsistency: ChatGPT's responses are consistent and accurate, regardless of the user's mood, tone, or location. This helps to ensure that users receive reliable information every time they use the service.\nPersonalization: ChatGPT can be trained on specific topics or industries, allowing it to provide personalized answers and insights tailored to the user's needs.\nOverall, ChatGPT is a powerful tool that can help individuals and businesses save time and effort by providing quick, reliable, and personalized responses to their questions and queries.",
      "target_audience": [
        "Increase Your Productivity: Automation of repetitive tasks, increased accuracy, streamline workflows, and enhance your decision making.",
        "Leverage Emerging Technology: Gain an edge as you apply emerging concepts such as Artificial Intelligence, AI, and best practices.",
        "Those who like unusual, some would say odd, ways to improve your productivity & time management.",
        "Content Creators: This course can help writers and content creators who want to understand how ChatGPT can be used to generate high-quality content and save time.",
        "Everyday Usage: Use ChatGPT in your life from work to home life use."
      ]
    },
    {
      "title": "Automated Machine Learning for Beginners (Google & Apple)",
      "url": "https://www.udemy.com/course/automl-for-ai-powered-professionals/",
      "bio": "Learn AI: Computer Vision, NLP, Tabular Data - build powerful models with Google AutoML & Apple CreateML",
      "objectives": [
        "Master Automated Machine Learning with Google Cloud AutoML & Apple Create ML",
        "Train meaningful Machine Learning Models without a single line of code",
        "Tackle problems in Computer Vision, Natural Language Processing and Regression",
        "Identify & Solve Real-Life Machine Learning Problems",
        "Build a Simple AI-Powered App for Android, iOS or both",
        "Ideate your own Product as part of your Course Project"
      ],
      "course_content": {
        "Introduction to the Course": [
          "Welcome to the Course",
          "Introduction Video",
          "The AIBrain Community",
          "Choose your Machine Learning Platform"
        ],
        "Introduction to Automated Machine Learning": [
          "Why Automated Machine Learning?",
          "The 5-Step Machine Learning Pipeline",
          "Term Project Announcement"
        ],
        "Vision AI with Google AutoML": [
          "Before You Begin",
          "Guidelines for AutoML Labs",
          "Image Classification - Introduction",
          "Image Classification - 5 Step ML Pipeline",
          "Image Classification - Preparing Dataset & Train Model",
          "Image Classification - Deploy & Test",
          "Image Classification - Takeaways",
          "Image Classification - Assignment",
          "Object Detection - Introduction",
          "Object Detection - 5 Step ML Pipeline",
          "Object Detection - Preparing Dataset",
          "Object Detection - Train & Evaluate",
          "Object Detection - Takeaways",
          "Object Detection - Assignment",
          "Vision AI - Quiz"
        ],
        "Vision AI with Apple Create ML": [
          "Before You Begin",
          "Image Classification - Introduction",
          "Image Classification - 5 Step ML Pipeline",
          "Image Classification - Getting Started",
          "Image Classification - Train & Evaluate Model",
          "Image Classification - Takeaways",
          "Image Classification - Assignment",
          "Object Detection - Introduction",
          "Object Detection - 5 Step ML Pipeline",
          "Object Detection - Getting Started",
          "Object Detection - Dataset Annotation",
          "Object Detection - Evaluate Model",
          "Object Detection - Takeaways",
          "Object Detection - Assignment",
          "Vision AI - Quiz"
        ],
        "Natural Language Processing with Google AutoML": [
          "Sentiment Analysis - Introduction",
          "Sentiment Analysis - 5 Step ML Pipeline",
          "Sentiment Analysis - Obtaining Dataset & Train Model",
          "Sentiment Analysis - Deploy & Evaluate",
          "Sentiment Analysis - Takeaways",
          "Sentiment Analysis - Assignment",
          "Document Classification - Introduction",
          "Document Classification - 5 Step Pipeline",
          "Document Classification - Train Model",
          "Document Classification - Evaluate & Test Model",
          "Document Classification - Takeaways",
          "Document Classification - Assignment",
          "NLP - Quiz"
        ],
        "Natural Language Processing with Create ML": [
          "Sentiment Analysis - Introduction",
          "Sentiment Analysis - 5 Step Pipeline",
          "Sentiment Analysis - Getting Started",
          "Sentiment Analysis - Dataset Preparation",
          "Sentiment Analysis - Training & Evaluation",
          "Sentiment Analysis - Takeaways",
          "Sentiment Analysis - Assignment",
          "Document Classification - Introduction",
          "Document Classification - 5 Step ML Pipeline",
          "Document Classification - Getting Started",
          "Document Classification - Dataset Preparation",
          "Document Classification - Training & Testing",
          "Document Classification - Takeaways",
          "Document Classification - Assignment",
          "NLP - Quiz"
        ],
        "Tabular Prediction with Google AutoML": [
          "Tabular Prediction - Introduction",
          "Tabular Prediction - The 5-Step ML Pipeline",
          "Tabular Prediction - Train & Evaluate",
          "Tabular Prediction - Takeaways",
          "Tabular Prediction - Assignment",
          "Tabular Prediction - Quiz"
        ],
        "Tabular Prediction with CreateML": [
          "Tabular Prediction - Introduction",
          "Tabular Prediction - 5-Step ML Pipeline",
          "Tabular Data - House Price Prediction",
          "Tabular Prediction - Takeaways",
          "Tabular Prediction - Assignment",
          "Tabular Prediction - Assignment",
          "Tabular Prediction - Quiz"
        ],
        "Term Project 1: Digital Prototyping": [
          "Digital Prototyping with Figma",
          "Why Prototyping?",
          "Getting Started with Figma",
          "Populating your Prototype",
          "Takeaways",
          "Product Idea & Digital Prototype"
        ],
        "Deploy AI in Android App": [
          "Introduction",
          "The 5-Step Machine Learning Pipeline",
          "Prerequisites",
          "Set Up Android Studio",
          "Brief Introduction to Android Studio",
          "Deploy ML Model in Sample Andoid Application",
          "Takeaways"
        ]
      },
      "requirements": [
        "No prerequisites whatsoever: this course is ideal for beginners",
        "Access to a computer with internet connection"
      ],
      "description": "Begin your AI journey with Automated Machine Learning!\nThis course focuses on giving you the big picture of Artificial Intelligence: the streamlined process of creating AI machine learning models. Leave the mathematical equations and Python coding aside and concentrate on what really matters!\n\n\nCourse Architecture guarantees Best Cognitive Learning Outcomes\nRichard Shinn, PhD in AI and CEO of AIBrain, is the Chief Architect behind this course. He designed the entire course structure to help learners acquire cognitive skills; learning by examples first, then learning based on denotational semantics and operational semantics. This two-step approach is particularly useful when we deal with the complexity of real-world problems.\n\n\nLearn Hands-On\nFrom day 1, you will learn hands-on skills. You will follow our lab instructors to build Machine Learning models with AutoML. By the end of the course, you will be able to build a working AI-powered mobile app. Following Richard Feynman’s principle “What I cannot create, I do not understand” we teach practical knowledge from day one.\nDeepen your Skills\nTo make sure that you will get a deep understanding of all the topics, you will have practical homework assignments accompanying all labs. Each lab has its own homework assignments to help you deepen the skills learned during the course.\nGain a Certificate from a Leading AI Company\nKick-start your career in AI with an official certificate from a leading AI company! Upon completion of the course, you will be awarded with a verified certificate of completion.\nComplete a Term Project: Build your own ML Model\nAs part of this course, you will ideate your own AI-powered product that leverages Machine Learning. You will build a simple low-fidelity prototype of an AI-powered application before building a custom Machine Learning model with AutoML for your idea.\nGet 1-1 Advice and Guidance\nWe have lab instructors available for 1-on-1 assistance in case you need help with your homework assignments. Reach out to our lab instructors any time in our AI School Community.\nJoin the AI School Community\nUse our vivid AISchool Community and Discord channel to discuss with your peers and leverage the power of community. Share your questions and answers with lab instructors and fellow students.\nEvaluate your Progress\nDuring your journey, you will always have the possibility to evaluate your progress and understanding of the materials through quizzes. This will help you to constantly measure your success and help to check your understanding of core concepts.\nBuild your Professional Network\nStay in touch with your peers and leverage a professional network of like-minded AI practitioners. In our community, we provide a dedicated alumni channel for all successful graduates of our course.\n\n\n\n\n\n\nThe course takes you through 4 carefully structured topics.\nTopic 1\nIntroduction to Automated Machine Learning\nFind out how AutoML is transforming the data science game by enabling anyone to build machine learning models without a single line of code. Familiarize yourself with the 5-Step ML Pipeline to solve any Machine Learning problem, before building your very first AI-powered smartphone application in 30 minutes.\n\n\nTopic 2\nVision AI with Google AutoML & Apple CreateML\nAI can help computers to interpret and understand digital images. In this segment, you will train models that can accurately locate and classify objects.\n\n\nTopic 3\nNLP & Tables with Google AutoML & Apple CreateML\nAI can help computers understand text and spoken words just like humans. Use SNS posts and new articles to train models to classify emotion and content.\nAI can analyze patterns across data with numerous variables. Use tabular data to predict house prices.\n\n\nTopic 4\nDeploy AI in an Application\nEasily deploy models on to your device and test it in real world settings.",
      "target_audience": [
        "Beginners without any background in programming or computer science",
        "Anyone who wants to automate the Machine Learning process",
        "Students who want to start a career in Artificial Intelligence and Machine Learning",
        "Professionals who want to learn how to apply Machine Learning in their respective jobs",
        "Anyone beginning their journey in Machine Learning"
      ]
    },
    {
      "title": "ML for Business Managers: Build Regression model in R Studio",
      "url": "https://www.udemy.com/course/machine-learning-basics-building-a-regression-model-in-r/",
      "bio": "Simple Regression & Multiple Regression| must-know for Machine Learning & Econometrics | Linear Regression in R studio",
      "objectives": [
        "Learn how to solve real life problem using the Linear Regression technique",
        "Preliminary analysis of data using Univariate and Bivariate analysis before running Linear regression",
        "Predict future outcomes basis past data by implementing Simplest Machine Learning algorithm",
        "Understand how to interpret the result of Linear Regression model and translate them into actionable insight",
        "Understanding of basics of statistics and concepts of Machine Learning",
        "Indepth knowledge of data collection and data preprocessing for Machine Learning Linear Regression problem",
        "Learn advanced variations of OLS method of Linear Regression",
        "Course contains a end-to-end DIY project to implement your learnings from the lectures",
        "How to convert business problem into a Machine learning Linear Regression problem",
        "How to do basic statistical operations in R",
        "Advanced Linear regression techniques using GLMNET package of R",
        "Graphically representing data in R before and after analysis"
      ],
      "course_content": {},
      "requirements": [
        "Students will need to install R and R studio software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Linear Regression course that teaches you everything you need to create a Linear Regression model in R, right?\nYou've found the right Linear Regression course!\nAfter completing this course you will be able to:\n· Identify the business problem which can be solved using linear regression technique of Machine Learning.\n· Create a linear regression model in R and analyze its result.\n· Confidently practice, discuss and understand Machine Learning concepts\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nHow this course will help you?\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you the most popular technique of machine learning, which is Linear Regression\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear regression.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Linear Regression model, which is the most popular Machine Learning model, to solve business problems.\nBelow are the course contents of this course on Linear Regression:\n· Section 1 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics\nthen graphical representations to describe the data and then a lecture on measures of center like mean\nmedian and mode and lastly measures of dispersion like range and standard deviation\n· Section 2 - R basic\nThis section will help you set up the R and R studio on your system and it'll teach you how to perform some basic operations in R.\n· Section 3 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\n· Section 4 - Data Preprocessing\nIn this section you will learn what actions you need to take a step by step to get the data and then\nprepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment, missing value imputation, variable transformation and correlation.\n· Section 5 - Regression Model\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you understand where the concept is coming from and how it is important. But even if you don't understand it, it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures. We also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results, what are other variations to the ordinary least squared method and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a regression model in R will soar. You'll have a thorough understanding of how to use regression modelling to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhat is the Linear regression technique of Machine learning?\nLinear Regression is a simple machine learning model for regression problems, i.e., when the target variable is a real value.\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\nWhen there is a single input variable (x), the method is referred to as simple linear regression.\nWhen there are multiple input variables, the method is known as multiple linear regression.\nWhy learn Linear regression technique of Machine learning?\nThere are four reasons to learn Linear regression technique of Machine learning:\n1. Linear Regression is the most popular machine learning technique\n2. Linear Regression has fairly good prediction accuracy\n3. Linear Regression is simple to implement and easy to interpret\n4. It gives you a firm base to start learning other advanced techniques of Machine Learning\nHow much time does it take to learn Linear regression technique of machine learning?\nLinear Regression is easy but no one can determine the learning time it takes. It totally depends on you. The method we adopted to help you learn Linear regression starts from the basics and takes you to advanced level within hours. You can follow the same, but remember you can learn nothing without practicing it. Practice is the only way to remember whatever you have learnt. Therefore, we have also provided you with another data set to work on as a separate project of Linear regression.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 4 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the R environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in R\nUnderstanding of Linear Regression modelling - Having a good knowledge of Linear Regression gives you a solid understanding of how machine learning works. Even though Linear regression is the simplest technique of Machine learning, it is still the most popular one with fairly good prediction ability. Fifth and sixth section cover Linear regression topic end-to-end and with each theory lecture comes a corresponding practical lecture in R where we actually run each query with you.\nWhy use R for data Machine Learning?\nUnderstanding R is one of the valuable skills needed for a career in Machine Learning. Below are some reasons why you should learn Machine learning in R\n1. It’s a popular language for Machine Learning at top tech firms. Almost all of them hire data scientists who use R. Facebook, for example, uses R to do behavioral analysis with user post data. Google uses R to assess ad effectiveness and make economic forecasts. And by the way, it’s not just tech firms: R is in use at analysis and consulting firms, banks and other financial institutions, academic institutions and research labs, and pretty much everywhere else data needs analyzing and visualizing.\n2. Learning the data science basics is arguably easier in R. R has a big advantage: it was designed specifically with data manipulation and analysis in mind.\n3. Amazing packages that make your life easier. Because R was designed with statistical analysis in mind, it has a fantastic ecosystem of packages and other resources that are great for data science.\n4. Robust, growing community of data scientists and statisticians. As the field of data science has exploded, R has exploded with it, becoming one of the fastest-growing languages in the world (as measured by StackOverflow). That means it’s easy to find answers to questions and community guidance as you work your way through projects in R.\n5. Put another tool in your toolkit. No one language is going to be the right tool for every job. Adding R to your repertoire will make some projects easier – and of course, it’ll also make you a more flexible and marketable employee when you’re looking for jobs in data science.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master Linear Regression from beginner to advanced in short span of time"
      ]
    },
    {
      "title": "Data Science and Machine Learning Basic to Advanced",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-basic-to-advanced/",
      "bio": "Complete Introduction to Data Science and Machine Learning from Basic to Advanced.",
      "objectives": [
        "Students will develop understanding of libraries used for Data Analysis like Pandas and Numpy.",
        "Learn to create impactful visualizations using Matplotlib and Seaborn. By creating these visualizations you will be able to derive better conclusions from data.",
        "After this course you will learn to build complete Data Science Pipeline from Data preparation to building the best Machine Learning Model.",
        "The course contains practical section after every new concept discussed and the course also has two projects at the end."
      ],
      "course_content": {
        "Welcome and Course Overview": [
          "Welcome",
          "Course Overview"
        ],
        "Numpy": [
          "Numpy Introduction and Installation",
          "Creating Arrays in Numpy",
          "Array Shape and Reshape",
          "Array Indexing",
          "Array Iterating",
          "Array Slicing",
          "Searching and Sorting"
        ],
        "Pandas": [
          "Pandas Introduction and Installation",
          "Pandas Series",
          "Pandas DataFrame",
          "Pandas ReadCSV",
          "Pandas Analyzing DataFrames"
        ],
        "Data Visualization": [
          "Matplotlib Introduction",
          "Different types of plots in Matplotlib",
          "Seaborn"
        ],
        "Data Preparation": [
          "Handling Missing Values",
          "Feature Encoding",
          "Feature Scaling"
        ],
        "Machine Learning": [
          "Machine Learning Introduction",
          "Supervised Machine Learning",
          "Unsupervised Machine Learning",
          "Train Test Split",
          "Regression Analysis",
          "Linear Regression",
          "Logistic Regression",
          "KNN",
          "SVM",
          "Decision Tree",
          "Random Forest",
          "K Means Clustering",
          "GridSearch CV"
        ],
        "Machine Learning Pipeline": [
          "Machine Learning Pipeline"
        ],
        "Projects": [
          "Diabetes Prediction",
          "Insurance Cost Prediction"
        ]
      },
      "requirements": [
        "Basic understanding of Python Programming Language."
      ],
      "description": "Learn how to use Numpy and Pandas for Data Analysis. This will cover all basic concepts of Numpy and Pandas that are useful in data analysis.\nLearn to create impactful visualizations using Matplotlib and Seaborn. Creating impactful visualizations is a crucial step in developing a better understanding about your data.\nThis course covers all Data Preprocessing steps like working with missing values, Feature Encoding and Feature Scaling.\nLearn about different Machine Learning Models like Random Forest, Decision Trees, KNN, SVM, Linear Regression, Logistic regression etc... All the video sessions will first discuss the basic theory concept behind these algorithms followed by the practical implementation.\nLearn to how to choose the best hyper parameters for your Machine Learning Model using GridSearch CV. Choosing the best hyper parameters is an important step in increasing the accuracy of your Machine Learning Model.\nYou will learn to build a complete Machine Learning Pipeline from Data collection to Data Preprocessing to Model Building. ML Pipeline is an important concept that is extensively used while building large scale ML projects.\nThis course has two projects at the end that will be built using all concepts taught in this course. The first project is about Diabetes Prediction using a classification machine learning algorithm and second is about prediciting the insurance premium using a regression machine learning algorithm.",
      "target_audience": [
        "Anyone who is looking to start his or her Data Science and Machine Learning Journey. People who are at intermediate level and already have some basic understanding of Data Science will also find this course helpful."
      ]
    },
    {
      "title": "Microsoft Azure Cognitive Services Crash Course",
      "url": "https://www.udemy.com/course/azure-cognitive-services-crash-course/",
      "bio": "Build Smart Applications in Minutes with Azure Cognitive Vision, Language, Speech, Decision and Search services",
      "objectives": [
        "Create smart software with Azure Cognitive Services.",
        "Prepare for Microsoft Exam AI-100: Designing and Implementing an Azure AI Solution",
        "Understand when to use Azure Cognitive Service",
        "Understand when Azure Cognitive Services shines comparing to custom Machine Learning",
        "You don't need any prior knowledge of AI or Machine Learning!"
      ],
      "course_content": {},
      "requirements": [
        "Basic familiarity with the Azure Portal is helpful",
        "Experience with one programming language",
        "High level understanding of HTTP REST APIs",
        "We will use Postman, a popular HTTP client for the demos, You can use other clients"
      ],
      "description": "This course is a one-stop shop to gain a solid understanding of Azure Cognitive Services.\nKnow all services under Azure Cognitive Services. You will list them all.\nDecide if any of these APIs can help with your business scenario. If not, know where to look next.\nUnderstand what each of these APIs do.  Microsoft documentation is referenced for further research.\nGain hands-on experience following the demos.\nTaking this course will help with the Microsoft Exam AI-100: Designing and Implementing an Azure AI Solution.",
      "target_audience": [
        "Software Developers",
        "Software Architects",
        "Project managers",
        "IT Managers",
        "Chief Technology Officers (CTOs)",
        "Machine Learning Experts"
      ]
    },
    {
      "title": "Data Science & Machine Learning: Naive Bayes in Python",
      "url": "https://www.udemy.com/course/data-science-machine-learning-naive-bayes-in-python/",
      "bio": "Master a crucial artificial intelligence algorithm and skyrocket your Python programming skills",
      "objectives": [
        "Apply Naive Bayes to image classification (Computer Vision)",
        "Apply Naive Bayes to text classification (NLP)",
        "Apply Naive Bayes to Disease Prediction, Genomics, and Financial Analysis",
        "Understand Naive Bayes concepts and algorithm",
        "Implement multiple Naive Bayes models from scratch"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "Where to get the Code",
          "Are You Beginner, Intermediate, or Advanced? All are OK!",
          "How to Succeed in this Course"
        ],
        "Naive Bayes Concepts (Beginner)": [
          "Concepts Section Introduction",
          "Classification Review",
          "Bayes' Rule Review",
          "Naive Bayes Intuition",
          "Concepts Section Summary",
          "Suggestion Box"
        ],
        "Naive Bayes Applications (Beginner-Intermediate)": [
          "Applications Section Introduction",
          "Strategy and Approach",
          "Disease Prediction with Naive Bayes",
          "Disease Prediction with Naive Bayes in Python (pt 1)",
          "Disease Prediction with Naive Bayes in Python (pt 2)",
          "Finance with Naive Bayes",
          "Finance with Naive Bayes in Python (pt 1)",
          "Finance with Naive Bayes in Python (pt 2)",
          "Genomics with Naive Bayes",
          "Genomics with Naive Bayes in Python",
          "Image Classification with Naive Bayes",
          "Image Classification with Naive Bayes in Python",
          "Text Classification with Naive Bayes (pt 1)",
          "Text Classification with Naive Bayes (pt 2)",
          "Text Classification with Naive Bayes in Python",
          "Applications Section Summary",
          "Application Exercise"
        ],
        "Naive Bayes In-Depth (Advanced)": [
          "Gaussian Naive Bayes Theory",
          "Gaussian Naive Bayes in Python",
          "Bernoulli Naive Bayes Theory",
          "Multinomial Naive Bayes Theory",
          "Exercises: Test Your Might!"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (Appendix/FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, TensorFlow, +More"
        ],
        "Extra Help With Python Coding for Beginners (Appendix/FAQ by Student Request)": [
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Temporary 403 Errors"
        ],
        "Effective Learning Strategies for Machine Learning (Appendix/FAQ by Student Requ": [
          "How to Succeed in this Course (Long Version)",
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "Machine Learning and AI Prerequisite Roadmap (pt 1)",
          "Machine Learning and AI Prerequisite Roadmap (pt 2)"
        ],
        "Appendix / FAQ Finale": [
          "BONUS"
        ]
      },
      "requirements": [
        "Decent Python programming skills",
        "Experience with Numpy, Matplotlib, and Pandas (we'll be using these)",
        "For advanced portions: know probability"
      ],
      "description": "In this self-paced course, you will learn how to apply Naive Bayes to many real-world datasets in a wide variety of areas, such as:\ncomputer vision\nnatural language processing\nfinancial analysis\nhealthcare\ngenomics\nWhy should you take this course? Naive Bayes is one of the fundamental algorithms in machine learning, data science, and artificial intelligence. No practitioner is complete without mastering it.\nThis course is designed to be appropriate for all levels of students, whether you are beginner, intermediate, or advanced. You'll learn both the intuition for how Naive Bayes works and how to apply it effectively while accounting for the unique characteristics of the Naive Bayes algorithm. You'll learn about when and why to use the different versions of Naive Bayes included in Scikit-Learn, including GaussianNB, BernoulliNB, and MultinomialNB.\nIn the advanced section of the course, you will learn about how Naive Bayes really works under the hood. You will also learn how to implement several variants of Naive Bayes from scratch, including Gaussian Naive Bayes, Bernoulli Naive Bayes, and Multinomial Naive Bayes. The advanced section will require knowledge of probability, so be prepared!\nThank you for reading and I hope to see you soon!\n\n\nSuggested Prerequisites:\nDecent Python programming skill\nComfortable with data science libraries like Numpy and Matplotlib\nFor the advanced section, probability knowledge is required\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including my free course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nLess than 24 hour response time on Q&A on average\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Beginner Python developers curious about data science and machine learning",
        "Students and professionals interested in machine learning fundamentals"
      ]
    },
    {
      "title": "AI & ML Made Easy: From Basic to Advanced (2025)",
      "url": "https://www.udemy.com/course/aritificial-intelligence-machine-learning-comprehensive-guide-2024/",
      "bio": "Learn AI, Machine Learning & Deep Learning from Scratch with Real Projects, NLP, and Industry-Focused Applications",
      "objectives": [
        "Understand the fundamentals of Artificial Intelligence (AI) and Machine Learning (ML)",
        "Learn how machines learn using Supervised, Unsupervised, and Reinforcement Learning methods",
        "Grasp the role of statistics, data preprocessing, and feature selection in building accurate models",
        "Build a solid foundation in machine learning algorithms, including regression, classification, and clustering",
        "Explore advanced topics like Deep Learning, Natural Language Processing (NLP), and Computer Vision",
        "Identify and address bias and ensure ethical AI implementation",
        "Gain hands-on experience through real-world AI & ML projects across business and industry use cases",
        "Understand how AI is applied in different sectors, from automation to decision-making systems",
        "Develop familiarity with industry tools, libraries, and languages used in AI & ML",
        "Prepare for real-world roles by understanding AI economics, business value, and practical implementation"
      ],
      "course_content": {
        "Unfolding the AI Universe": [
          "Kickstart Your Journey: Introduction to the Course",
          "Quiz: Kickstart Your Journey: Introduction to the Course",
          "Unlock the Mystery: Understanding Intelligence",
          "Quiz: Unlock the Mystery: Understanding Intelligence",
          "AI Simplified: Defining Artificial Intelligence",
          "Quiz: AI Simplified: Defining Artificial Intelligence",
          "Travel Through Time: The Evolution of AI",
          "Quiz: Travel Through Time: The Evolution of AI",
          "AI and Beyond: Exploring the Philosophy",
          "Quiz: AI and Beyond: Exploring the Philosophy",
          "Behind the Scenes: The Science of AI",
          "Quiz: Behind the Scenes: The Science of AI",
          "Why AI? Decoding its Current Popularity",
          "Quiz: Why AI? Decoding its Current Popularity",
          "Dive Deeper: Exploring Different Areas of AI",
          "Quiz: Dive Deeper: Exploring Different Areas of AI"
        ],
        "Understanding and Applying Machine Learning": [
          "Demystifying the Process: How Machines Learn",
          "Quiz: Demystifying the Process: How Machines Learn",
          "AI Revolution: Creating a Paradigm Shift",
          "Quiz: AI Revolution: Creating a Paradigm Shift",
          "Machine Learning in Action: Real-World Examples",
          "Quiz: Machine Learning in Action: Real-World Examples",
          "Everyday AI: Common Applications of Machine Learning",
          "Quiz: Everyday AI: Common Applications of Machine Learning"
        ],
        "Machine Learning Mastery: From Basics to Advanced Concepts": [
          "Machine Learning Foundations – Understanding the Basics",
          "Quiz: Machine Learning Uncovered: An Overview",
          "Core Principles – The Theory Behind Machine Learning",
          "Quiz: The Backbone: Fundamental Theory Behind Machine Learning",
          "Machine Learning Glossary – Essential Terminology Explained",
          "Quiz: Deciphering the Jargon: Machine Learning Terminology",
          "Blueprint of Machine Learning – Steps from Data to Model",
          "Quiz: The Machine Learning Blueprint: Understanding the Process",
          "The Machine Learning Spectrum – Exploring Different Approaches",
          "Quiz: Diverse Paths: Exploring Machine Learning Approaches",
          "Foundations of Data & Statistics in ML",
          "Quiz: Role of Statistics & Computer Science in Machine Learning",
          "Why data preprocessing is crucial in AI/ML",
          "Handling missing values, categorical data, and outliers",
          "Feature scaling (Normalization vs. Standardization)",
          "Feature selection techniques",
          "Building Your First Machine Learning Model"
        ],
        "Deep Dive into Supervised, Unsupervised, and Reinforcement Learning": [
          "Guided Learning: An Introduction to Supervised Learning",
          "Quiz: Guided Learning: An Introduction to Supervised Learning",
          "Unveiling the Mechanism: How Supervised Machine Learning Works",
          "Quiz: Unveiling the Mechanism: How Supervised Machine Learning Works",
          "Mastering Regression – Simple vs. Multiple Regression (Practical Implementation)",
          "Logistic Regression – Classification Made Simple",
          "Decision Trees & Random Forest – Building Smarter Models",
          "Supervised Learning in Action: A Practical Example",
          "Quiz: Supervised Learning in Action: A Practical Example",
          "Autonomous Learning: Unsupervised Machine Learning Overview",
          "Quiz: Autonomous Learning: Unsupervised Machine Learning Overview",
          "The Underlying Mechanism: How Unsupervised Machine Learning Works",
          "Quiz: The Underlying Mechanism: How Unsupervised Machine Learning Works",
          "Spotlight on Unsupervised Learning: A Practical Example",
          "Quiz: Spotlight on Unsupervised Learning: A Practical Example",
          "Learning by Doing: An Insight into Reinforcement Learning",
          "Quiz: Learning by Doing: An Insight into Reinforcement Learning",
          "Crunching Numbers: Exploring Statistical Algorithms",
          "Quiz: Crunching Numbers: Exploring Statistical Algorithms",
          "Building Your First Machine Learning Model"
        ],
        "Practice Test": [
          "Know your knowledge"
        ],
        "Navigating the Business and Economic Aspects of AI and Machine Learning": [
          "From App to Solution: Transforming Problem Solving",
          "Quiz: From App to Solution: Transforming Problem Solving",
          "Understanding the Economics of AI",
          "Quiz: Understanding the Economics of AI"
        ],
        "Navigating the AI Landscape: From Concepts to Practical Implementation": [
          "The Standard: General Machine Learning Process",
          "Quiz: The Standard: General Machine Learning Process",
          "The Human Element: Understanding Bias in Machine Learning",
          "Quiz: The Human Element: Understanding Bias in Machine Learning",
          "The AI Artisans: Who Implements AI",
          "Quiz: The AI Artisans: Who Implements AI",
          "Exploring Languages for Implementing Machine Learning",
          "Quiz: Exploring Languages for Implementing Machine Learning",
          "Using Machine Learning for Business Problem Solving",
          "Quiz: Using Machine Learning for Business Problem Solving"
        ],
        "Deep Learning, Natural Language Processing, and Computer Vision": [
          "Diving Deep: An Introduction to Deep Learning",
          "Quiz: Diving Deep: An Introduction to Deep Learning",
          "Understanding Natural Language Processing (NLP)",
          "Quiz: Understanding Natural Language Processing (NLP)",
          "Creating Realities: Generative AI & Overview",
          "Quiz: Creating Realities: Generative AI & Overview"
        ],
        "Stages, Types, and Ethical Considerations": [
          "The AI Spectrum: Exploring Types of AI",
          "Quiz: The AI Spectrum: Exploring Types of AI",
          "Understanding Stages of AI",
          "Quiz 38: Understanding Stages of AI",
          "AI with Conscience: Responsible & Ethical AI",
          "Quiz: AI with Conscience: Responsible & Ethical AI",
          "Ethical Implications of AI in Business"
        ],
        "Practice test": [
          "Test your IQ"
        ]
      },
      "requirements": [
        "No prior knowledge of AI or Machine Learning required",
        "A computer with internet access and a willingness to learn through hands-on practice",
        "Curiosity to explore how intelligent systems are built and applied in the real world"
      ],
      "description": "AI & ML Made Easy: From Basic to Advanced (2025) is a beginner-friendly yet comprehensive course designed to take you from the fundamentals of Artificial Intelligence (AI) and Machine Learning (ML) to advanced concepts like Deep Learning, Natural Language Processing (NLP), and real-world applications.\nYou'll explore how machines learn using supervised, unsupervised, and reinforcement learning techniques. Through real-world case studies and guided projects, you’ll understand how AI is transforming industries and how to apply ML models to solve meaningful problems.\nThis course doesn't just teach you theory. It walks you through the full AI & ML process—from understanding data and preprocessing it, to choosing algorithms, tuning models, and interpreting results. You’ll also explore the role of statistics, ethical considerations, and how AI is implemented in business environments.\n\nWhat This Course Offers:\nStep-by-step explanations of AI and ML concepts for learners at all levels\nHands-on projects covering Deep Learning, NLP, and real-world AI use cases\nPractical understanding of model building, data preprocessing, and evaluation techniques\nExposure to how AI is applied in business, healthcare, finance, and other sectors\nDiscussions around AI bias, ethics, and responsible implementation\nGuidance on tools, libraries, and languages used in AI/ML development\nIndustry insights to help you explore career paths in AI and Machine Learning\n\nWhether you're a student, developer, researcher, or just curious about AI, this course will equip you with the practical knowledge and confidence to understand and build intelligent systems from the ground up.",
      "target_audience": [
        "Beginners interested in learning the basics of AI and Machine Learning",
        "Tech professionals looking to upskill and delve deeper into AI and ML",
        "Researchers and academics who want to stay updated with the latest developments in AI and ML",
        "Anyone curious about the future of technology and its impact on various sectors"
      ]
    },
    {
      "title": "Cluster Analysis and Unsupervised Machine Learning in Python",
      "url": "https://www.udemy.com/course/cluster-analysis-unsupervised-machine-learning-python/",
      "bio": "Data science techniques for pattern recognition, data mining, k-means clustering, and hierarchical clustering, and KDE.",
      "objectives": [
        "Understand the regular K-Means algorithm",
        "Understand and enumerate the disadvantages of K-Means Clustering",
        "Understand the soft or fuzzy K-Means Clustering algorithm",
        "Implement Soft K-Means Clustering in Code",
        "Understand Hierarchical Clustering",
        "Explain algorithmically how Hierarchical Agglomerative Clustering works",
        "Apply Scipy's Hierarchical Clustering library to data",
        "Understand how to read a dendrogram",
        "Understand the different distance metrics used in clustering",
        "Understand the difference between single linkage, complete linkage, Ward linkage, and UPGMA",
        "Understand the Gaussian mixture model and how to use it for density estimation",
        "Write a GMM in Python code",
        "Explain when GMM is equivalent to K-Means Clustering",
        "Explain the expectation-maximization algorithm",
        "Understand how GMM overcomes some disadvantages of K-Means",
        "Understand the Singular Covariance problem and how to fix it"
      ],
      "course_content": {},
      "requirements": [
        "Know how to code in Python and Numpy",
        "Install Numpy and Scipy",
        "Matrix arithmetic, probability"
      ],
      "description": "Cluster analysis is a staple of unsupervised machine learning and data science.\nIt is very useful for data mining and big data because it automatically finds patterns in the data, without the need for labels, unlike supervised machine learning.\nIn a real-world environment, you can imagine that a robot or an artificial intelligence won’t always have access to the optimal answer, or maybe there isn’t an optimal correct answer. You’d want that robot to be able to explore the world on its own, and learn things just by looking for patterns.\nDo you ever wonder how we get the data that we use in our supervised machine learning algorithms?\nWe always seem to have a nice CSV or a table, complete with Xs and corresponding Ys.\nIf you haven’t been involved in acquiring data yourself, you might not have thought about this, but someone has to make this data!\nThose “Y”s have to come from somewhere, and a lot of the time that involves manual labor.\nSometimes, you don’t have access to this kind of information or it is infeasible or costly to acquire.\nBut you still want to have some idea of the structure of the data. If you're doing data analytics automating pattern recognition in your data would be invaluable.\nThis is where unsupervised machine learning comes into play.\nIn this course we are first going to talk about clustering. This is where instead of training on labels, we try to create our own labels! We’ll do this by grouping together data that looks alike.\nThere are 2 methods of clustering we’ll talk about: k-means clustering and hierarchical clustering.\nNext, because in machine learning we like to talk about probability distributions, we’ll go into Gaussian mixture models and kernel density estimation, where we talk about how to \"learn\" the probability distribution of a set of data.\nOne interesting fact is that under certain conditions, Gaussian mixture models and k-means clustering are exactly the same! We’ll prove how this is the case.\nAll the algorithms we’ll talk about in this course are staples in machine learning and data science, so if you want to know how to automatically find patterns in your data with data mining and pattern extraction, without needing someone to put in manual work to label that data, then this course is for you.\nAll the materials for this course are FREE. You can download and install Python, Numpy, and Scipy with simple commands on Windows, Linux, or Mac.\n\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nmatrix addition, multiplication\nprobability\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "Students and professionals interested in machine learning and data science",
        "People who want an introduction to unsupervised machine learning and cluster analysis",
        "People who want to know how to write their own clustering code",
        "Professionals interested in data mining big data sets to look for patterns automatically"
      ]
    },
    {
      "title": "SQL for Tech and Data Science Interviews",
      "url": "https://www.udemy.com/course/sql-for-tech-and-data-science-interviews/",
      "bio": "Do you want to ace your SQL tech or data science interview? This Is the perfect resource for you!",
      "objectives": [
        "Build a winning strategy for the tech/data science interview",
        "How to focus on mock interviews",
        "A guided walkthrough to a set of interview questions",
        "Work on your communication skills and interview style",
        "Tina’s 5-step framework to acing your SQL interview",
        "Know what SQL interviewers want to hear from you",
        "Anticipate follow-up questions",
        "Be able to demonstrate how you think and reason",
        "Reduce interview stress by doing the 10 mock interviews provided in the course"
      ],
      "course_content": {
        "Introduction": [
          "Why Take This Course and What Does It Cover"
        ],
        "SQL Overview and Environment Setup": [
          "Section Intro",
          "What is SQL?",
          "SQL Setup"
        ],
        "Fundamental SQL Components": [
          "Section Intro",
          "Structure of a SQL Query",
          "SQL Functions to Master"
        ],
        "Full Mock Interviews": [
          "Section Intro",
          "5 Step Framework for SQL Interviews",
          "What Comes Next",
          "Mock Interview 1",
          "Mock Interview 2",
          "Mock Interview 3",
          "Mock Interview 4",
          "Mock Interview 5",
          "Mock Interview 6",
          "Mock Interview 7",
          "Mock Interview 8",
          "Mock Interview 9",
          "Mock Interview 10"
        ],
        "Bonus": [
          "How to Recover from Mistakes",
          "How to Anticipate Followups",
          "How to Remain Calm"
        ]
      },
      "requirements": [
        "Intro to SQL covered. This is not an Intro to SQL course."
      ],
      "description": "You want to make sure you are ready for your tech/data science interview?\nDo you want to learn from an instructor who works for one of the FAANG companies?\nGreat! Perhaps it was chance that brought you here, but now that you have found this resource, you are a step closer to building a deliberate strategy. You will learn how to focus on doing mock interviews with real interview questions and in a real interview style.\nPlease bear in mind that this is not an Intro to SQL course that goes in depth about every single concept and function available for you to work with. Instead, this is a resource that will help you ace the job interview and get hired, provided that you have already learned the SQL basics.\nIf you are looking for a guided walkthrough and coaching through 10 mock interviews – this is the right course for you.\nTina’s 5-step framework will prepare you to tackle any SQL interview question. You will find out what interviewers want to hear from you. Learning how to interact with them is a fundamental skill you will need to master. Oftentimes, potential employers will challenge your assumptions and ask for your thoughts simply because they want to see how you deal with unexpected situations and challenges. You must find a way to keep the conversation going and know how to shrug off any mistakes you make throughout the interview!\nThe goal of this course is for you to have done the SQL interview at least 10 times – so, when the real interview comes, it just feels like another practice round.\nAbout the author: Tina Huang is a data scientist at one of the FAANG companies. She is also a popular YouTuber with over 60k subscribers. Tina is proud she taught herself SQL from scratch in 11 days to pass her FAANG SQL interview.\nThis course is one of the best resources you can choose to prepare yourself for the SQL interview you need, to land a job in tech and data science!",
      "target_audience": [
        "Aspiring data scientists",
        "Aspiring data analysts",
        "Aspiring business intelligence analysts",
        "Anyone who needs to get through a SQL interview to land their dream job"
      ]
    },
    {
      "title": "Master AWS with Python and Boto3",
      "url": "https://www.udemy.com/course/master-aws-with-python-and-boto3/",
      "bio": "Learn to use Python to connect to AWS and launch services with Boto3, such as S3, EC2, DynamoDB, and more!",
      "objectives": [
        "Understand the fundamental concepts and architecture of AWS and its services.",
        "Learn how to set up and configure the Boto3 library in a Python environment.",
        "Create and manage AWS resources such as EC2 instances, S3 buckets, and DynamoDB tables using Boto3.",
        "Develop robust and scalable automation scripts for AWS tasks and operations.",
        "Implement security best practices, including using IAM roles and policies with Boto3.",
        "Optimize AWS costs and resource usage by leveraging Boto3's capabilities for monitoring and management."
      ],
      "course_content": {
        "Introduction and AWS Setup": [
          "Course FAQs and File Downloads",
          "Course Overview",
          "Installation and Set-Up"
        ],
        "Boto3 Basics and S3": [
          "Introduction to Boto3",
          "Client",
          "Resources",
          "Paginators and Waiters",
          "Advanced Topics related to S3",
          "Exercise Tasks",
          "Exercise Solutions"
        ],
        "IAM Policy with Boto3": [
          "Introduction to IAM Policy",
          "Users and Groups"
        ],
        "Pricing and Billing with Boto3": [
          "Introduction to Pricing and Billing",
          "Billing",
          "Pricing"
        ],
        "EC2 on AWS with Boto3 and Python": [
          "Introduction to EC2 with AWS and Boto3",
          "EC2 Instances with Boto3",
          "SSH Connection in Python with Paramiko"
        ],
        "DynamoDB with Boto3": [
          "Introduction to DynamoDB Section",
          "DynamoDB CRUD Operations",
          "DynamoDB Batch Operations and Scan and Filtering",
          "DynamoDB - Global Secondary Index",
          "DynamoDB - Exercise",
          "DynamoDB - Exercise Solution"
        ],
        "Lambda on AWS with Boto3": [
          "Introduction to Lambda with Boto3",
          "Lambda - Hello World Example",
          "Lambda Triggers and AWS Console"
        ],
        "Capstone Project - Media Server on AWS with Python and Boto3": [
          "Capstone Project Overview",
          "Capstone Project Solutions Walkthrough"
        ],
        "Rekognition": [
          "Introduction to Rekognition",
          "Image Analysis and Detection"
        ],
        "Audio - AWS Polly, Transcribe, and Translate": [
          "Introduction to Audio and Text Services",
          "AWS Polly - Synthesize Speech",
          "AWS Transcribe - Speech to Text",
          "AWS Translate"
        ]
      },
      "requirements": [
        "General Python Programming Experience",
        "Experience with AWS and general knowledge of AWS services and resources",
        "AWS Permissions to read/write/create resourced"
      ],
      "description": "Dive Deep into the World of AWS and Python\nEmbark on a transformative journey as you unravel the immense capabilities of AWS, seamlessly integrated with the power of Python using Boto3. This comprehensive course promises not just to educate but to empower, giving you the skills to harness the true potential of cloud-based solutions.\nBegin with the Basics, Rise to Mastery\nFor the uninitiated, we'll start with an enriching introduction to AWS and Boto3, setting the foundation for what lies ahead. Following which, you’ll acquire hands-on expertise on fundamental Boto3 operations - your first step towards AWS proficiency.\nAs we delve deeper, you'll discover the intricacies of IAM (Identity and Access Management), ensuring your applications and data remain secure in the cloud. No more surprises on your monthly invoice! Get to grips with AWS Pricing and Billing to effectively manage your cloud expenditure.\nUnlock the Power of AWS Services\nDive into S3, AWS's flagship storage solution, mastering the art of storing and retrieving vast amounts of data with ease. Transition smoothly into EC2, understanding virtual servers and scalability. The world of databases awaits as you explore both DynamoDB and RDS, offering insights into NoSQL and relational databases respectively.\nOur journey doesn't stop there. Grasp the marvel of Lambda, AWS's serverless compute service, enabling you to run code without managing servers.\nExperience AI with AWS\nVenture into the fascinating realm of artificial intelligence as you master services like Rekognition for image and video analysis, Transcribe and Translate for speech-to-text and language translation capabilities, Polly for turning text into lifelike speech, and TextExtract for extracting printed text from documents.\nBring It All Together\nTo ensure you're fully equipped to apply what you've learned, you'll be challenged with a Capstone Project. This endeavor will tie together your newfound skills, culminating in a fully functional AWS-powered application.\nWhy Enroll?\nMastering AWS with Python and Boto3 promises more than just learning; it offers empowerment in the digital age. With meticulously crafted content and hands-on projects, you'll be poised to make your mark in the cloud world. So, are you ready to ascend to AWS mastery?\nJoin us today and transform your future!",
      "target_audience": [
        "Python developers looking to learn the AWS Boto3 SDK for Python"
      ]
    },
    {
      "title": "Deep Learning: Recurrent Neural Networks in Python",
      "url": "https://www.udemy.com/course/deep-learning-recurrent-neural-networks-in-python/",
      "bio": "GRU, LSTM, Time Series Forecasting, Stock Predictions, Natural Language Processing (NLP) using Artificial Intelligence",
      "objectives": [
        "Apply RNNs to Time Series Forecasting (tackle the ubiquitous \"Stock Prediction\" problem)",
        "Apply RNNs to Natural Language Processing (NLP) and Text Classification (Spam Detection)",
        "Apply RNNs to Image Classification",
        "Understand the simple recurrent unit (Elman unit), GRU, and LSTM (long short-term memory unit)",
        "Write various recurrent networks in Tensorflow 2",
        "Understand how to mitigate the vanishing gradient problem",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "Get Your Hands Dirty, Practical Coding Experience, Data Links",
          "Where to get the code",
          "How to Succeed in this Course"
        ],
        "Google Colab": [
          "Intro to Google Colab, how to use a GPU or TPU for free",
          "Uploading your own data to Google Colab",
          "Where can I learn about Numpy, Scipy, Matplotlib, Pandas, and Scikit-Learn?",
          "Temporary 403 Errors"
        ],
        "Machine Learning and Neurons": [
          "Review Section Introduction",
          "What is Machine Learning?",
          "Code Preparation (Classification Theory)",
          "Classification Notebook",
          "Code Preparation (Regression Theory)",
          "Regression Notebook",
          "The Neuron",
          "How does a model \"learn\"?",
          "Making Predictions",
          "Saving and Loading a Model",
          "Suggestion Box"
        ],
        "Feedforward Artificial Neural Networks": [
          "Artificial Neural Networks Section Introduction",
          "Forward Propagation",
          "The Geometrical Picture",
          "Activation Functions",
          "Multiclass Classification",
          "How to Represent Images",
          "Color Mixing Clarification",
          "Code Preparation (ANN)",
          "ANN for Image Classification",
          "ANN for Regression"
        ],
        "Recurrent Neural Networks, Time Series, and Sequence Data": [
          "Sequence Data",
          "Forecasting",
          "Autoregressive Linear Model for Time Series Prediction",
          "Proof that the Linear Model Works",
          "Recurrent Neural Networks",
          "RNN Code Preparation",
          "RNN for Time Series Prediction",
          "Paying Attention to Shapes",
          "GRU and LSTM (pt 1)",
          "GRU and LSTM (pt 2)",
          "A More Challenging Sequence",
          "Demo of the Long Distance Problem",
          "RNN for Image Classification (Theory)",
          "RNN for Image Classification (Code)",
          "Stock Return Predictions using LSTMs (pt 1)",
          "Stock Return Predictions using LSTMs (pt 2)",
          "Stock Return Predictions using LSTMs (pt 3)",
          "Other Ways to Forecast"
        ],
        "Natural Language Processing (NLP)": [
          "Embeddings",
          "Code Preparation (NLP)",
          "Text Preprocessing",
          "Text Classification with LSTMs"
        ],
        "In-Depth: Loss Functions": [
          "Mean Squared Error",
          "Binary Cross Entropy",
          "Categorical Cross Entropy"
        ],
        "In-Depth: Gradient Descent": [
          "Gradient Descent",
          "Stochastic Gradient Descent",
          "Momentum",
          "Variable and Adaptive Learning Rates",
          "Adam (pt 1)",
          "Adam (pt 2)"
        ],
        "Extras": [
          "Data Links"
        ],
        "Appendix / FAQ Finale": [
          "What is the Appendix?"
        ]
      },
      "requirements": [
        "Basic math (taking derivatives, matrix arithmetic, probability) is helpful",
        "Python, Numpy, Matplotlib"
      ],
      "description": "*** NOW IN TENSORFLOW 2 and PYTHON 3 ***\nEver wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nLearn about one of the most powerful Deep Learning architectures yet!\nThe Recurrent Neural Network (RNN) has been used to obtain state-of-the-art results in sequence modeling.\nThis includes time series analysis, forecasting and natural language processing (NLP).\nLearn about why RNNs beat old-school machine learning algorithms like Hidden Markov Models.\nThis course will teach you:\nThe basics of machine learning and neurons (just a review to get you warmed up!)\nNeural networks for classification and regression (just a review to get you warmed up!)\nHow to model sequence data\nHow to model time series data\nHow to model text data for NLP (including preprocessing steps for text)\nHow to build an RNN using Tensorflow 2\nHow to use a GRU and LSTM in Tensorflow 2\nHow to do time series forecasting with Tensorflow 2\nHow to predict stock prices and stock returns with LSTMs in Tensorflow 2 (hint: it's not what you think!)\nHow to use Embeddings in Tensorflow 2 for NLP\nHow to build a Text Classification RNN for NLP (examples: spam detection, sentiment analysis, parts-of-speech tagging, named entity recognition)\nAll of the materials required for this course can be downloaded and installed for FREE. We will do most of our work in Numpy, Matplotlib, and Tensorflow. I am always available to answer your questions and help you along your data science journey.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\nSee you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nmatrix addition, multiplication\nbasic probability (conditional and joint distributions)\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students, professionals, and anyone else interested in Deep Learning, Time Series Forecasting, Sequence Data, or NLP",
        "Software Engineers and Data Scientists who want to level up their career"
      ]
    },
    {
      "title": "Data Science and Machine Learning For Beginners with Python",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-for-beginners-with-python-c/",
      "bio": "Learn to Analyse , Make Predictions, Explore data Frames,Clean and Visualize Data",
      "objectives": [
        "Install Jupyter Notebook Server",
        "Create a new notebook",
        "Explore Components of Jupyter Notebook",
        "Understand Data Science Life Cycle",
        "Use Kaggle Data Sets",
        "Perform Probability Sampling",
        "Explore and use Tabular Data",
        "Explore Pandas DataFrame",
        "Manipulate Pandas DataFrame",
        "Perform Data Cleaning",
        "Perform Data Visualization",
        "Visualize Qualitative Data",
        "Explore Machine Learning Frameworks",
        "Understand Supervised Machine Learning",
        "Use machine learning to predict value of a house",
        "Use Scikit-Learn",
        "Load datasets",
        "Make Predictions using machine learning",
        "Understand Python Expressions and Statements",
        "Understand Python Data Types and how to cast data types",
        "Understand Python Variables and Data Structures",
        "Understand Python Conditional Flow and Functions",
        "Learn SQL with PostgreSQL",
        "Perform SQL CRUD Operations on PostgreSQL Database",
        "Filter and Sort Data using SQL",
        "Understand Big Data Terminologies."
      ],
      "course_content": {
        "Introduction and Setup": [
          "Introduction",
          "What is Jupyter Notebook",
          "Installing Jupyter Notebook Server",
          "Running Jupyter Notebook Server",
          "Common Jupyter Notebook Commands",
          "Jupyter Notebook Components",
          "Jupyter Notebook Dashboard",
          "Jupyter Notebook User Interface",
          "Creating a new Notebook"
        ],
        "Python Fundamentals": [
          "What is Python",
          "Python Expressions",
          "Python Statements",
          "Python Comments",
          "Python Data Types",
          "Casting Data Type",
          "Python Variables",
          "Python List",
          "Python Tuple",
          "Python Dictionaries",
          "Python Operators",
          "Python Conditional Statements",
          "Python Loops",
          "Python Functions"
        ],
        "Data Science": [
          "What is Data Science",
          "Impact of Data Science",
          "Data Science life cycle",
          "Data Science Terminologies",
          "Kaggle Data Sets",
          "Probability Sampling",
          "Tabular Data",
          "Exploring Pandas DataFrame",
          "Manipulating a Pandas DataFrame",
          "What is Data Cleaning",
          "Basic Data Cleaning Process",
          "What is Data Visualization",
          "Visualizing Qualitative Data : Part 1",
          "Visualizing Qualitative Data : Part 2"
        ],
        "Introduction to Machine Learning with Python": [
          "Installing Python",
          "Installing Pycharm on Windows",
          "Installing Pycharm on Macs",
          "Installing Anaconda",
          "What is Machine Learning",
          "Machine Learning Frameworks",
          "Machine Learning Vocabulary",
          "Supervised machine learning",
          "Where Machine Learning is used",
          "Creating a basic house value estimator",
          "Using Scikit-Learn",
          "Loading a dataset part 1",
          "Loading a dataset part 2",
          "Making Predictions part 1",
          "Making Predictions part 2"
        ],
        "SQL and Data Science with PostgreSQL": [
          "What is SQL",
          "What is PostgreSQL",
          "Installing PostgreSQL on windows",
          "Installing PostgreSQL on Mac",
          "Connecting to a PostgreSQL Database",
          "Database Concepts",
          "Install Sample Database",
          "What is CRUD",
          "Data Types",
          "SQL CREATE TABLE Statement",
          "SQL INSERT Statement",
          "SQL SELECT Statement",
          "SQL UPDATE Statement",
          "SQL WHERE clause",
          "SQL ORDER BY Clause"
        ],
        "Introduction to Big Data Terminology": [
          "What is Big Data",
          "What is high volume",
          "What is high variety",
          "What is high velocity",
          "Google's Big Data Approach",
          "What is a cluster",
          "What is a Node",
          "Google File System",
          "Google's Big Table",
          "What is MapReduce",
          "Apache Hadoop",
          "Thank You"
        ]
      },
      "requirements": [
        "No Prior experience is required.",
        "You'll need to install some software. We will show you how to do that step by step."
      ],
      "description": "Data science is the study of data. It involves developing methods of recording, storing, and analyzing data to effectively extract useful information . Data is a fundamental part of our everyday work, whether it be in the form of valuable insights about our customers, or information to guide product,policy or systems development.   Big business, social media, finance and the public sector all rely on data scientists to analyse their data and draw out business-boosting insights.\nPython is a dynamic modern object -oriented programming language that is easy to learn and can be used to do a lot of things both big and small. Python is what is referred to as a high level language. That means it is a language that is closer to humans than computer.It is also known as a general purpose programming language due to it's flexibility. Python is used a lot in data science.\nMachine learning relates to many different ideas, programming languages, frameworks. Machine learning is difficult to define in just a sentence or two. But essentially, machine learning is giving a computer the ability to write its own rules or algorithms and learn about new things, on its own. In this course, we'll explore some basic machine learning concepts and load data to make predictions.\nWe will also be using SQL to interact with data inside a PostgreSQL Database.\n\n\nWhat you'll learn\nUnderstand Data Science Life Cycle\nUse Kaggle Data Sets\nPerform Probability Sampling\nExplore and use Tabular Data\nExplore Pandas DataFrame\nManipulate Pandas DataFrame\nPerform Data Cleaning\nPerform Data Visualization\nVisualize Qualitative Data\nExplore Machine Learning Frameworks\nUnderstand Supervised Machine Learning\nUse machine learning to predict value of a house\nUse Scikit-Learn\nLoad datasets\nMake Predictions using machine learning\nUnderstand Python Expressions and Statements\nUnderstand Python Data Types and how to cast data types\nUnderstand Python Variables and Data Structures\nUnderstand Python Conditional Flow and Functions\nLearn SQL with PostgreSQL\nPerform SQL CRUD Operations on PostgreSQL Database\nFilter and Sort Data using SQL\nUnderstand Big Data Terminologies\n\n\nA Data Scientist can work as the following:\ndata analyst.\nmachine learning engineer.\nbusiness analyst.\ndata engineer.\nIT system analyst.\ndata analytics consultant.\ndigital marketing manager.",
      "target_audience": [
        "Beginners to Data Science",
        "Beginners to Machine Learning",
        "Beginners to Python",
        "Beginners to SQL"
      ]
    },
    {
      "title": "Computer Vision in Python for Beginners (Theory & Projects)",
      "url": "https://www.udemy.com/course/mastering-computer-vision-theory-projects-in-python/",
      "bio": "Computer Vision-Become an ace of Computer Vision, Computer Vision for Apps using Python, OpenCV, TensorFlow, etc.",
      "objectives": [
        "• The introduction and importance of Computer Vision (CV).",
        "• Why is CV such a popular field nowadays?",
        "• The fundamental concepts from the absolute beginning with comprehensive unfolding with examples in Python.",
        "• Practical explanation and live coding with Python.",
        "• The concept of colored and black and white images with practice.",
        "• Deep details of Computer Vision with examples of every concept from scratch.",
        "• TensorFlow (Deep learning framework by Google).",
        "• The use and applications of state-of-the-art Computer Vision (with implementations in state-of-the-art framework Numpy and TensorFlow).",
        "• Theory and implementation of Panoramic images.",
        "• Geometric transformations.",
        "• Image Filtering with implementation in Python.",
        "• Edge Detection, Shape Detection, and Corner Detection.",
        "• Object Tracking and Object detection.",
        "• 3D images.",
        "• Building your own applications for change detection in the live feed of cameras by using Computer Vision Techniques using Python.",
        "• Developing a complete project to make a very intelligent and efficient DVR using Python."
      ],
      "course_content": {},
      "requirements": [
        "• No prior knowledge is needed. You will start from the basics and slowly build your knowledge in computer vision.",
        "• A willingness to learn and practice.",
        "• Knowledge of Python will be a plus.",
        "• Since we teach by practical implementations, practice is a must."
      ],
      "description": "Computer vision (CV), a subfield of computer science, focuses on replicating the complex functionalities of the human visual system. In the CV process, real-world images and videos are captured, processed, and analyzed to allow machines to extract contextual, useful information from the physical world.\nUntil recently, computer vision functioned in a limited capacity. But due to the recent innovations in artificial intelligence and deep learning, this field has made great leaps. Today, CV surpasses humans in most routine tasks connected with detecting and labeling objects.\nThe high-quality content of the Mastering Computer Vision from the Absolute Beginning Using Python course presents you with a great opportunity to learn and become an expert. You will learn the core concepts of the CV field. This course will also help you to understand the digital imaging process and identify the key application areas of CV. The course is:\n· Easy to understand.\n· Descriptive.\n· Comprehensive.\n· Practical with live coding.\n· Rich with state of the art and updated knowledge of this field.\nAlthough this course is a compilation of all the basic concepts of CV, you are encouraged to step up and experience more than what you learn. Your understanding of every concept is tested at the end of each section. The Homework assignments/tasks/activities/quizzes along with solutions will assess your learning. Several of these activities are focused on coding so that you are ready to run with implementations.\nThe two hands-on projects in the last section—Change Detection in CCTV Cameras (Real-time) and Smart DVRs (Real-time)—make up the most important learning element of this course. They will help you sharpen your practical skills. Successful completion of these two projects will help you enrich your portfolio and kick-start your career in the CV field.\nThe course tutorials are divided into 320+ videos along with detailed code notebooks. The videos are available in HD, and the total runtime of the videos is 27 hours+.\nNow is the perfect time to learn computer vision. Get started with this best-in-class course without any further delay!\nTeaching is our passion:\nIn this course, we apply the proven learning by doing methodology. We build the interest of learners first. We start from the basics and focus on helping you understand each concept clearly. The explanation of each theoretical concept is followed by practical implementation. We then encourage you to create something new out of your learning.\nOur aim is to help you master the basic concepts of CV before moving onward to advanced concepts. The course material includes online videos, course notes, hands-on exercises, project work, quizzes, and handouts. We also offer you learning support. You can approach our team in case of any queries, and we respond in quick time.\n\n\nCourse Content:\nThe comprehensive course consists of the following topics:\n1. Introduction\na. Intro\ni. What is computer vision?\n2. Image Transformations\na. Introduction to images\ni. Image data structure\nii. Color images\niii. Grayscale images\niv. Color spaces\nv. Color space transformations in OpenCV\nvi. Image segmentation using Color space transformations\nb. 2D geometric transformations\ni. Scaling\nii. Rotation\niii. Shear\niv. Reflection\nv. Translation\nvi. Affine transformation\nvii. Projective geometry\nviii. Affine transformation as a matrix\nix. Application of SVD (Optional)\nx. Projective transformation (Homography)\nc. Geometric transformation estimation\ni. Estimating affine transformation\nii. Estimating Homography\niii. Direct linear transform (DLT)\niv. Building panoramas with manual key-point selection\n3. Image Filtering and Morphology\na. Image Filtering\ni. Low pass filter\nii. High pass filter\niii. Band pass filter\niv. Image smoothing\nv. Image sharpening\nvi. Image gradients\nvii. Gaussian filter\nviii. Derivative of Gaussians\nb. Morphology\ni. Image Binarization\nii. Image Dilation\niii. Image Erosion\niv. Image Thinning and skeletonization\nv. Image Opening and closing\n4. Shape Detection\na. Edge Detection\ni. Definition of edge\nii. Naïve edge detector\niii. Canny edge detector\n1. Efficient gradient computations\n2. Non-maxima suppression using gradient directions\n3. Multilevel thresholding- hysteresis thresholding\nb. Geometric Shape detection\ni. RANSAC\nii. Line detection through RANSAC\niii. Multiple lines detection through RANSAC\niv. Circle detection through RANSAC\nv. Parametric shape detection through RANSAC\nvi. Hough transformation (HT)\nvii. Line detection through HT\nviii. Multiple lines detection through HT\nix. Circle detection through HT\nx. Parametric shape detection through HT\nxi. Estimating affine transformation through RANSAC\nxii. Non-parametric shapes and generalized Hough transformation\n5. Key Point Detection and Matching\na. Corner detection (Key point detection)\ni. Defining Corner\nii. Naïve corner detector\niii. Harris corner detector\n1. Continuous directions\n2. Tayler approximation\n3. Structure tensor\n4. Variance approximation\n5. Multi-scale detection\nb. Project: Building automatic panoramas\ni. Automatic key point detection\nii. Scale assignment\niii. Rotation assignment\niv. Feature extraction (SIFT)\nv. Feature matching\nvi. Image stitching\n6. Motion\na. Optical Flow, Global Flow\ni. Brightness constancy assumption\nii. Linear approximation\niii. Lucas–Kanade method\niv. Global flow\nv. Motion segmentation\nb. Object Tracking\ni. Histogram based tracking\nii. KLT tracker\niii. Multiple object tracking\niv. Trackers comparisons\n7. Object detection\na. Classical approaches\ni. Sliding window\nii. Scale space\niii. Rotation space\niv. Limitations\nb. Deep learning approaches\ni. YOLO a case study\n8. 3D computer vision\na. 3D reconstruction\ni. Two camera setups\nii. Key point matching\niii. Triangulation and structure computation\nb. Applications\ni. Mocap\nii. 3D Animations\n9. Projects\na. Change detection in CCTV cameras (Real-time)\nb. Smart DVRs (Real-time)\n\n\n\n\nAfter completing this course successfully, you will be able to:\n· Relate the concepts and theories in computer vision with real-world problems.\n· Implement any project from scratch that requires computer vision knowledge.\n· Know the theoretical and practical aspects of computer vision concepts.\nWho this course is for:\n· Learners who are absolute beginners and know nothing about Computer Vision.\n· People who want to make smart solutions.\n· People who want to learn computer vision with real data.\n· People who love to learn theory and then implement it using Python.\n· People who want to learn computer vision along with its implementation in realistic projects.\n· Data Scientists.\n· Machine learning experts.\n\n\n\n\nUnlock the fascinating world of Computer Vision and take your first step towards becoming an expert in this field.\nEnroll now and embark on a learning journey that combines theory and hands-on projects. Start mastering Computer Vision today!\n\n\nList of Keywords:\nImage Processing\nDeep Learning for Computer Vision\nArtificial Intelligence in Computer Vision\nMachine Learning Models for Image Analysis\nObject Detection and Recognition\nImage Filtering and Enhancement\nShape Detection Algorithms\nKey Point Detection and Matching Techniques\nOptical Flow and Motion Analysis\n3D Computer Vision and Reconstruction\nReal-time Computer Vision Applications\nChange Detection in CCTV\nSmart DVR Systems\nComputer Vision Projects\nImage Segmentation\nFeature Extraction in CV\nHarris Corner Detector\nScale-Invariant Feature Transform (SIFT)\nRANSAC Algorithm\nYOLO (You Only Look Once)\n3D Reconstruction from Images\nStructure from Motion (SfM)\nMocap (Motion Capture)\nComputer Vision for 3D Animation\nComputer Vision for Data Scientists\nComputer Vision for Machine Learning Practitioners",
      "target_audience": [
        "• Learners who are absolute beginners and know nothing about Computer Vision.",
        "• People who want to make smart solutions.",
        "• People who want to learn computer vision with real data.",
        "• People who love to learn theory and then implement it using Python.",
        "• People who want to learn computer vision along with its implementation in realistic projects.",
        "• Data Scientists.",
        "• Machine learning experts."
      ]
    },
    {
      "title": "Learn Data Science & Machine Learning with R from A-Z",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-with-r-from-a-z/",
      "bio": "Become a professional Data Scientist with R and learn Machine Learning, Data Analysis + Visualization, Web Apps + more!",
      "objectives": [
        "Become a professional Data Scientist, Data Engineer, Data Analyst or Consultant",
        "How to write complex R programs for practical industry scenarios",
        "Learn data cleaning, processing, wrangling and manipulation",
        "Learn Plotting in R (graphs, charts, plots, histograms etc)",
        "How to create resume and land your first job as a Data Scientist",
        "Step by step practical knowledge of R programming language",
        "Learn Machine Learning and it's various practical applications",
        "Building web apps and online, interactive dashboards with R Shiny",
        "Learn Data and File Management in R",
        "Use R to clean, analyze, and visualize data",
        "Learn the Tidyverse",
        "Learn Operators, Vectors, Lists and their application",
        "Data visualization (ggplot2)",
        "Data extraction and web scraping",
        "Full-stack data science development",
        "Building custom data solutions",
        "Automating dynamic report generation",
        "Data science for business"
      ],
      "course_content": {
        "Data Science and Machine Learning Course Intro": [
          "Data Science and Machine Learning Intro Section Overview",
          "What is Data Science?",
          "Machine Learning Overview",
          "Data Science + Machine Learning Marketplace",
          "Who is This Course For?",
          "Data Science and Machine Learning Job Opportunities"
        ],
        "Getting Started with R": [
          "Getting Started with R",
          "R Basics",
          "Working with Files",
          "R Studio",
          "Tidyverse Overview",
          "Additional Resources"
        ],
        "Data Types and Structures in R": [
          "Data Types and Structures in R Section Overview",
          "Basic Types",
          "Vectors Part One",
          "Vectors Part Two",
          "Vectors: Missing Values",
          "Vectors: Coercion",
          "Vectors: Naming",
          "Vectors: Misc.",
          "Working with Matrices",
          "Working with Lists",
          "Introduction to Data Frames",
          "Creating Data Frames",
          "Data Frames: Helper Functions",
          "Data Frames: Tibbles"
        ],
        "Intermediate R": [
          "Intermedia R Section Introduction",
          "Relational Operators",
          "Logical Operators",
          "Conditional Statements",
          "Working with Loops",
          "Working with Functions",
          "Working with Packages",
          "Working with Factors",
          "Dates & Times",
          "Functional Programming",
          "Data Import/Export",
          "Working with Databases"
        ],
        "Data Manipulation in R": [
          "Data Manipulation Section Intro",
          "Tidy Data",
          "The Pipe Operator",
          "{dplyr}: The Filter Verb",
          "{dplyr}: The Select Verb",
          "{dplyr}: The Mutate Verb",
          "{dplyr}: The Arrange Verb",
          "{dplyr}: The Summarize Verb",
          "Data Pivoting: {tidyr}",
          "String Manipulation: {stringr}",
          "Web Scraping: {rvest}",
          "JSON Parsing: {jsonlite}"
        ],
        "Data Visualization in R": [
          "Data Visualization in R Section Intro",
          "Getting Started with Data Visualization in R",
          "Aesthetics Mappings",
          "Single Variable Plots",
          "Two Variable Plots",
          "Facets, Layering, and Coordinate Systems",
          "Styling and Saving"
        ],
        "Creating Reports with R Markdown": [
          "Introduction to R Markdown"
        ],
        "Building Webapps with R Shiny": [
          "Introduction to R Shiny",
          "Creating A Basic R Shiny App",
          "Other Examples with R Shiny"
        ],
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning Part One",
          "Introduction to Machine Learning Part Two"
        ],
        "Data Preprocessing": [
          "Data Preprocessing Intro",
          "Data Preprocessing"
        ]
      },
      "requirements": [
        "Basic computer skills"
      ],
      "description": "Welcome to the Learn Data Science and Machine Learning with R from A-Z Course!\nIn this practical, hands-on course you’ll learn how to program in R and how to use R for effective data analysis, visualization and how to make use of that data in a practical manner. You will learn how to install and configure software necessary for a statistical programming environment and describe generic programming language concepts as they are implemented in a high-level statistical language.\nOur main objective is to give you the education not just to understand the ins and outs of the R programming language, but also to learn exactly how to become a professional Data Scientist with R and land your first job.\nThe course covers practical issues in statistical computing which include programming in R, reading data into R, accessing R packages, writing R functions, debugging, profiling R code, and organizing and commenting on R code. Blending practical work with solid theoretical training, we take you from the basics of R Programming to mastery.\nWe understand that theory is important to build a solid foundation, we understand that theory alone isn’t going to get the job done so that’s why this course is packed with practical hands-on examples that you can follow step by step. Even if you already have some coding experience, or want to learn about the advanced features of the R programming language, this course is for you!\nR coding experience is either required or recommended in job postings for data scientists, machine learning engineers, big data engineers, IT specialists, database developers and much more. Adding R coding language skills to your resume will help you in any one of these data specializations requiring mastery of statistical techniques.\nTogether we’re going to give you the foundational education that you need to know not just on how to write code in R, analyze and visualize data but also how to get paid for your newly developed programming skills.\nThe course covers 6 main areas:\n\n\n1: DS + ML COURSE + R INTRO\nThis intro section gives you a full introduction to the R programming language, data science industry and marketplace, job opportunities and salaries, and the various data science job roles.\nIntro to Data Science + Machine Learning\nData Science Industry and Marketplace\nData Science Job Opportunities\nR Introduction\nGetting Started with R\n\n\n2: DATA TYPES/STRUCTURES IN R\nThis section gives you a full introduction to the data types and structures in R with hands-on step by step training.\nVectors\nMatrices\nLists\nData Frames\nOperators\nLoops\nFunctions\nDatabases + more!\n\n\n3: DATA MANIPULATION IN R\nThis section gives you a full introduction to the Data Manipulation in R with hands-on step by step training.\nTidy Data\nPipe Operator\ndplyr verbs: Filter, Select, Mutate, Arrange + more!\nString Manipulation\nWeb Scraping\n\n\n4: DATA VISUALIZATION IN R\nThis section gives you a full introduction to the Data Visualization in R with hands-on step by step training.\nAesthetics Mappings\nSingle Variable Plots\nTwo-Variable Plots\nFacets, Layering, and Coordinate System\n\n\n5: MACHINE LEARNING\nThis section gives you a full introduction to Machine Learning with hands-on step by step training.\nIntro to Machine Learning\nData Preprocessing\nLinear Regression\nLogistic Regression\nSupport Vector Machines\nK-Means Clustering\nEnsemble Learning\nNatural Language Processing\nNeural Nets\n\n\n6: STARTING A DATA SCIENCE CAREER\nThis section gives you a full introduction to starting a career as a Data Scientist with hands-on step by step training.\nCreating a Resume\nPersonal Branding\nFreelancing + Freelance websites\nImportance of Having a Website\nNetworking\nBy the end of the course you’ll be a professional Data Scientist with R and confidently apply for jobs and feel good knowing that you have the skills and knowledge to back it up.",
      "target_audience": [
        "Students who want to learn about Data Science and Machine Learning"
      ]
    },
    {
      "title": "Python and Data Science for beginners",
      "url": "https://www.udemy.com/course/introduction-to-data-science-with-python-for-beginners/",
      "bio": "Learn hands-on data science and Python from scratch",
      "objectives": [
        "How to set up environment to explore using Jupyter Notebook",
        "How to import Python Libraries into your environment",
        "How to work with Tabular data",
        "How to explore a Pandas DataFrame",
        "How to explore a Pandas Series",
        "How to Manipulate a Pandas DataFrame",
        "How to clean data",
        "How to visualize data"
      ],
      "course_content": {
        "Setting Up The Environment": [
          "Introduction",
          "What is Jupyter Notebook",
          "Installing Jupyter Notebook Server",
          "Running Jupyter Notebook Server",
          "Common Jupyter Notebook Commands",
          "Jupyter Notebook Components",
          "Jupyter Notebook Dashboard",
          "Jupyter Notebook User Interface",
          "Creating a new notebook"
        ],
        "Introduction to Python: Crash course for beginners": [
          "What is Python",
          "High and low level programming languages",
          "Compilers and interpreters",
          "Python Expressions",
          "Python Statements",
          "Python Comments",
          "Python Data Types",
          "Casting Data Types",
          "Python Variables",
          "Python List",
          "Python Tuple",
          "Python Dictionaries",
          "Python Operators",
          "Python Conditional Statements",
          "Python Loops",
          "Python Functions"
        ],
        "Introduction to Data Science": [
          "What is data science",
          "Impact of data science",
          "Data science life cycle",
          "Terminology",
          "Kaggle Data Sets",
          "Probability Sampling",
          "Tabular Data",
          "Exploring Pandas DataFrame",
          "Manipulating a Pandas DataFrame",
          "What is data cleaning",
          "Basic data cleaning process",
          "What is data visualization",
          "Visualizing Qualitative Data",
          "Visualizing Quantitative Data",
          "Thank You"
        ]
      },
      "requirements": [
        "FamiliarIty with data formats eg excel,csv",
        "Comfortable using a computer and internet"
      ],
      "description": "Data science is the study of data. It involves developing methods of recording, storing, and analyzing data to effectively extract useful information\nData is a fundamental part of our everyday work, whether it be in the form of valuable insights about our customers, or information to guide product,policy or systems development.   Big business, social media, finance and the public sector all rely on data scientists to analyse their data and draw out business-boosting insights.\nPython is a dynamic modern object -oriented programming language that is easy to learn and can be used to do a lot of things both big and small. Python is what is referred to as a high level language. That means it is a language that is closer to humans than computer.It is also known as a general purpose programming language due to it's flexibility. Python is used a lot in data science.\nThis course is a beginners course that will introduce you to some basics of data science  using Python.\n\n\nWhat You Will Learn\nHow to set up environment to explore using Jupyter Notebook\nHow to import Python Libraries into your environment\nHow to work with Tabular data\nHow to explore a Pandas DataFrame\nHow to explore a Pandas Series\nHow to Manipulate a Pandas  DataFrame\nHow to clean data\nHow to visualize data",
      "target_audience": [
        "Beginners to Data Science",
        "Beginners to Python"
      ]
    },
    {
      "title": "Applied Statistical Modeling for Data Analysis in R",
      "url": "https://www.udemy.com/course/applied-statistical-modeling-for-data-analysis-in-r/",
      "bio": "Your Complete Guide to Statistical Data Analysis and Visualization For Practical Applications in R",
      "objectives": [
        "Analyze their own data by applying appropriate statistical techniques",
        "Interpret the results of their statistical analysis",
        "Identify which statistical techniques are best suited to their data and questions",
        "Have a strong foundation in fundamental statistical concepts",
        "Implement different statistical analysis in R and interpret the results",
        "Build intuitive data visualizations",
        "Carry out formalized hypothesis testing",
        "Implement linear modelling techniques such multiple regressions and GLMs",
        "Implement advanced regression analysis and multivariate analysis"
      ],
      "course_content": {
        "Introduction to the Basics of Applied Statistical Modelling": [
          "Introduction to the Instructor and Course",
          "Data & Code Used in the Course",
          "Statistics in the Real World",
          "Designing Studies & Collecting Good Quality Data",
          "Different Types of Data",
          "Conclusion to Section 1"
        ],
        "Section 2: The Essentials of the R Programming Language": [
          "Rationale for this section",
          "Introduction to the R Statistical Software & R Studio",
          "Different Data Structures in R",
          "Reading in Data from Different Sources",
          "Indexing and Subsetting of Data",
          "Data Cleaning: Removing Missing Values",
          "Exploratory Data Analysis in R",
          "Conclusion to Section 2",
          "Section 2 Quiz"
        ],
        "Statistical Tools to Learn More About Your Data": [
          "Summarize Quantitative Data",
          "Measures of Center",
          "Measures of Variation",
          "Charting & Graphing Continuous Data",
          "Charting & Graphing Discrete Data",
          "Deriving Insights from Qualitative/Nominal Data",
          "Conclusions to Section 3",
          "Section 3 Quiz"
        ],
        "Probability Distributions": [
          "Background",
          "Data Distribution: Normal Distribution",
          "Checking For Normal Distribution",
          "Standard Normal Distribution and Z-scores",
          "Confidence Interval-Theory",
          "Confidence Interval-Computation in R",
          "Conclusions to Section 4",
          "Section 4 Quiz"
        ],
        "Statistical Inference": [
          "What is Hypothesis Testing?",
          "T-tests: Application in R",
          "Non-Parametric Alternatives to T-Tests",
          "One-way ANOVA",
          "Non-parametric version of One-way ANOVA",
          "Two-way ANOVA",
          "Power Test for Detecting Effect",
          "Conclusions to Section 5",
          "Section 5 Quiz"
        ],
        "Relationship Between Two Different Quantitative Variables": [
          "Explore the Relationship Between Two Quantitative Variables?",
          "Correlation",
          "Linear Regression-Theory",
          "Linear Regression-Implementation in R",
          "The Conditions of Linear Regression",
          "Dealing with Multi-collinearity",
          "What More Does the Regression Model Tell Us?",
          "Linear Regression and ANOVA",
          "Linear Regression With Categorical Variables and Interaction Terms",
          "Analysis of Covariance (ANCOVA)",
          "Selecting the Most Suitable Regression Model",
          "Conclusions to Section 6",
          "Section 6 Quiz"
        ],
        "Other Types of Regression": [
          "Violation of Linear Regression Conditions: Transform Variables",
          "Other Regression Techniques When Conditions of OLS Are Not Met",
          "Model 2 Regression: Standardized Major Axis (SMA) Regression",
          "Polynomial and Non-linear regression",
          "Linear Mixed Effect Models",
          "Generalized Regression Model (GLM)",
          "Logistic Regression in R",
          "Poisson Regression in R",
          "Goodness of fit testing",
          "Conclusions to Section 7",
          "Section 7 Quiz"
        ],
        "Multivariate Analysis": [
          "Why Do Multivariate Analysis?",
          "Cluster Analysis/Unsupervised Learning",
          "Principal Component Analysis (PCA)",
          "Linear Discriminant Analysis (LDA)",
          "Correspondence Analysis",
          "Similarity & Dissimilarity Across Sites",
          "Non-metric multi dimensional scaling (NMDS)",
          "Multivariate Analysis of Variance (MANOVA)",
          "Conclusions to Section 8",
          "Section 8 Quiz"
        ],
        "Miscellaneous Lectures & Information": [
          "Exploratory Data Analysis With xda",
          "Read in Data from Online HTML Tables-Part 1",
          "Read in Data from Online HTML Tables-Part 2",
          "Use R in Colab",
          "Summarise By Time",
          "POSIT",
          "Distributed Computing"
        ]
      },
      "requirements": [
        "Prior Familiarity With the Interface of R and R Studio",
        "Interest in Learning Statistical Modelling",
        "Interest in Applying Statistical Analysis to Real Life Data",
        "Interest in Gleaning Insights About Data (From Any Discipline)",
        "This Course Will be Demonstrated on a Windows OS. You Will Have to Adapt the Code Pertaining to the Changing Working Directories For your OS"
      ],
      "description": "APPLIED STATISTICAL MODELING FOR DATA ANALYSIS IN R\nCOMPLETE GUIDE TO STATISTICAL DATA ANALYSIS & VISUALIZATION FOR PRACTICAL APPLICATIONS IN R\nConfounded by Confidence Intervals? Pondering Over p-values? Hankering Over Hypothesis Testing?\nHello, My name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation).\n\n\nI have several years of experience in analyzing real-life data from different sources using statistical modelling and producing publications for international peer-reviewed journals. If you find statistics books & manuals too vague, expensive & not practical, then you’re going to love this course!\nI created this course to take you by hand teach you all the concepts, and take your statistical modelling from basic to an advanced level for practical data analysis.\nWith this course, I want to help you save time and learn what the arcane statistical concepts have to do with the actual analysis of data and the interpretation of the bespoke results. Frankly, this is the only course you need to complete in order to get a head start in practical statistical modelling for data analysis using R.\n\nMy course has 9.5 hours of lectures and provides a robust foundation to carry out PRACTICAL, real-life statistical data analysis tasks in R, one of the most popular and FREE data analysis frameworks.\nGET ACCESS TO A COURSE THAT IS JAM-PACKED WITH TONS OF APPLICABLE INFORMATION! AND GET A FREE VIDEO COURSE IN MACHINE LEARNING AS WELL!\nThis course is your sure-fire way of acquiring the knowledge and statistical data analysis skills that I acquired from the rigorous training I received at 2 of the best universities in the world, the perusal of numerous books and publishing statistically rich papers in renowned international journals like PLOS One.\nTo be more specific, here’s what the course will do for you:\n\n(a) It will take you (even if you have no prior statistical modelling/analysis background) from a basic level to performing some of the most common advanced statistical data analysis tasks in R.\n\n(b) It will equip you to use R for performing the different statistical data analysis and visualization tasks for data modelling.\n\n(c) It will Introduce some of the most important statistical concepts to you in a practical manner such that you can apply these concepts for practical data analysis and interpretation.\n(d) You will learn some of the most important statistical modelling concepts from probability distributions to hypothesis testing to regression modelling and multivariate analysis.\n(e) You will also be able to decide which statistical modelling techniques are best suited to answer your research questions and applicable to your data and interpret the results.\nThe course will mostly focus on helping you implement different statistical analysis techniques on your data and interpret the results.\nAfter each video, you will learn a new concept or technique which you may apply to your own projects immediately!\nTAKE ACTION NOW :) You’ll also have my continuous support when you take this course just to make sure you’re successful with it.  If my GUARANTEE is not enough for you, you can ask for a refund within 30 days of your purchase in case you’re not completely satisfied with the course.\nTAKE ACTION TODAY! I will personally support you and ensure your experience with this course is a success.",
      "target_audience": [
        "People working in any numerate field which requires data analysis",
        "Students of Environmental Science, Ecology, Biology,Conservation and Other Natural Sciences",
        "People with some prior knowledge of the R interface- (a) installing packages (b) reading in csv files",
        "People carrying out observational or experimental studies"
      ]
    },
    {
      "title": "Data Engineering with Spark Databricks Delta Lake Lakehouse",
      "url": "https://www.udemy.com/course/data-engineering-with-spark-databricks-delta-lake-lakehouse/",
      "bio": "Apache Spark Databricks Lakehouse Delta Lake Delta Tables Delta Caching Scala Python Data Engineering for beginners",
      "objectives": [
        "Acquiring the necessary skills to qualify for an entry-level Data Engineering position",
        "Developing a practical comprehension of Data Lakehouse concepts through hands-on experience",
        "Learning to operate a Delta table by accessing its version history, recovering data, and utilizing time travel functionality",
        "Optimizing a delta table with various techniques like caching, partitioning, and z-ordering for faster analytics",
        "Obtaining practical knowledge in constructing a data pipeline through the usage of Apache Spark on the Databricks platform",
        "Doin analytics within a Databricks AWS Account"
      ],
      "course_content": {},
      "requirements": [
        "Some understanding of Database and SQL queries"
      ],
      "description": "Data Engineering is a vital component of modern data-driven businesses. The ability to process, manage, and analyze large-scale data sets is a core requirement for organizations that want to stay competitive. In this course, you will learn how to build a data pipeline using Apache Spark on Databricks' Lakehouse architecture. This will give you practical experience in working with Spark and Lakehouse concepts, as well as the skills needed to excel as a Data Engineer in a real-world environment.\n\n\nThroughout the Course, You Will Learn:\nConducting analytics using Python and Scala with Spark.\nApplying Spark SQL and Databricks SQL for analytics.\nDeveloping a data pipeline with Apache Spark.\nBecoming proficient in Databricks' community edition.\nManaging a Delta table by accessing version history, restoring data, and utilizing time travel features.\nOptimizing query performance using Delta Cache.\nWorking with Delta Tables and Databricks File System.\nGaining insights into real-world scenarios from experienced instructors.\nCourse Structure:\nBeginning with familiarizing yourself with Databricks' community edition and creating a basic pipeline using Spark.\nProgressing to more complex topics after gaining comfort with the platform.\nLearning analytics with Spark using Python and Scala, including Spark transformations, actions, joins, Spark SQL, and DataFrame APIs.\nAcquiring the knowledge and skills to operate a Delta table, including accessing its version history, restoring data, and utilizing time travel functionality using Spark and Databricks SQL.\nUnderstanding how to use Delta Cache to optimize query performance.\nOptional Lectures on AWS Integration:\n'Setting up Databricks Account on AWS' and 'Running Notebooks Within a Databricks AWS Account.'\nBuilding an ETL pipeline with Delta Live Tables\nProviding additional opportunities to explore Databricks within the AWS ecosystem.\nThis course is designed for Data Engineering beginners with no prior knowledge of Python and Scala required. However, some familiarity with databases and SQL is necessary to succeed in this course. Upon completion, you will have the skills and knowledge required to succeed in a real-world Data Engineer role.\n\n\nThroughout the course, you will work with hands-on examples and real-world scenarios to apply the concepts you learn. By the end of the course, you will have the practical experience and skills required to understand Spark and Lakehouse concepts, and to build a scalable and reliable data pipeline using  Spark on Databricks' Lakehouse architecture.",
      "target_audience": [
        "Data Engineering beginners"
      ]
    },
    {
      "title": "Microsoft Power BI for Advanced Data Transformations",
      "url": "https://www.udemy.com/course/power-bi-advanced-data-transformations-and-modeling/",
      "bio": "Learn how to clean, optimize, and model data tables to develop insightful, actionable insights in Microsoft Power BI",
      "objectives": [
        "Master advanced techniques in the Power BI query editor for optimized data handling.",
        "Gain expertise in 'M' code and the advanced editor for efficient data transformation.",
        "Learn to implement advanced data cleaning and data modeling strategies in Power BI.",
        "Understand and execute effective data management for comprehensive data analytics.",
        "Develop an analytical mindset to create correct relationships within your data model.",
        "Implement organized and intuitive data model layouts for diverse data scenarios."
      ],
      "course_content": {
        "Introduction": [
          "Learning Tips From Enterprise DNA",
          "Microsoft Power BI user-interface updates to be aware of",
          "Course Overview - Let's get started!",
          "Resource - Data Transformations and Modeling in Power BI",
          "Intriguing Power Query Facts",
          "Mastering Best Practices: Query Editor & Data Model",
          "Why It Is Essential To Use The Query Editor in Every Project",
          "A Practical Guide: Importing Data into the Query Editor",
          "Unleashing Query Editor: Tips & Techniques",
          "Demystifying M Code: A Comprehensive Understanding",
          "Troubleshooting in Power BI: A How-to Guide",
          "Crafting Order: Effective Organization within the Query Editor",
          "Introduction - Review",
          "Introduction - Quiz",
          "Your Feedback Matters!"
        ],
        "Data transformation examples": [
          "Intriguing Power Query Facts",
          "Column Transformations: Practical Examples",
          "Row Transformations: Hands-On Techniques",
          "Data Cleansing: How to Filter Unrequired Data from Tables",
          "Date Table Creation & Modification: An Interactive Approach",
          "Data transformation examples - Review",
          "Data transformation examples - Quiz",
          "Active Recall - Data Preparation and Cleaning"
        ],
        "Advanced transformation and querying techniques": [
          "Power Query: Did You Know?",
          "Staging Queries: A Walkthrough",
          "Amplify Your Data: Appending Queries Explained",
          "Comprehensive Guide to Merging or Joining Queries",
          "Turning Data Around: Unpivoting Tables for Efficiency",
          "Extracting Insights: Creating Columns from Examples",
          "Custom & Conditional Columns: Building for Specific Needs",
          "Parameterizing Your Data: Creation & Utilization",
          "Boosting Functionality: Custom Functions in Power BI",
          "Synthesis: Merging Advanced Transformation Techniques",
          "Advanced transformation and querying techniques - Review",
          "Power Query: Beyond Basics",
          "Advanced transformation and querying techniques - Quiz"
        ],
        "Designing advanced data models": [
          "Architecting Insights: Power BI Data Modeling Facts",
          "Data Modeling Goals: What We Aim to Achieve",
          "Crafting Efficient Data Models: Best Practices",
          "Tackling Complexity: Setting Up Intricate Models",
          "Handling Absent Lookup Tables: Alternate Approaches",
          "Constructing Measure Tables/Groups for Better Insights",
          "Leveraging Intermediary Tables in Your Model",
          "Supporting Tables: A Vital Component of Data Modeling",
          "Scenario Analysis with 'What If' Parameters",
          "Enhancing Lookup Tables: Adding Dimensions",
          "Architecting Multi-Layered Models: An Advanced Approach",
          "Designing advanced data models - Review",
          "Designing advanced data models - Quiz"
        ],
        "Optimising your models": [
          "Blueprints of Insight - Exploring Power BI Data Modeling",
          "Active vs Inactive Relationships: Deciphering the Differences",
          "Elevate Your Power BI Experience: Model Optimization Techniques",
          "Shrink Your File, Not Your Data: Reducing Model File Size",
          "Optimising your models - Review",
          "Optimising your models - Quiz"
        ],
        "Course Assignments - Advanced Data Transformations": [
          "Assignment 1: Handling Missing Data & Duplicates"
        ],
        "Additional Resources - Complimentary guides for your data career": [
          "2024 Data Career Guide - Enterprise DNA",
          "Congratulations and Next Steps"
        ]
      },
      "requirements": [
        "Participants need a basic understanding of Power BI and DAX, including familiarity with creating simple reports and basic data manipulation.",
        "An active Power BI account is a prerequisite, as it will serve as the platform for all practical exercises and hands-on training throughout the course.",
        "A fundamental understanding of data concepts and some experience in data analysis is required to effectively grasp the advanced concepts taught in the course.",
        "Lastly, an enthusiasm for learning advanced data transformation and modeling techniques will greatly enhance the learning experience and outcomes."
      ],
      "description": "Deepen your understanding of data transformation and optimization techniques with our advanced Microsoft Power BI course.\n\n\nOver 4 hours of high-quality video content, we break down the complexities of the Power BI query editor, illustrating its importance and impact on effective reporting solutions. From beginners to advanced users, this course broadens your comprehension of intermediate to advanced techniques, empowering you to adeptly clean, optimize, and connect your raw data tables into an efficient analytical model. We demystify the use of DAX formulas to extract accurate results that align with your analytical queries, thereby ensuring a rich, data-driven decision-making process.\n\n\nWe kickstart the course with best practice techniques for the query editor and proceed to detailed instructions on manipulating 'M' code and the advanced editor. Comprehensive lessons on row and column query transformation options lay a solid foundation for understanding complex data structures. The course then navigates through advanced data cleaning and transformation techniques, with hands-on examples demonstrating how to query multiple tables.\n\n\nAs we progress, you'll learn strategic ways to think about and manage your data model, mastering techniques applicable to any data scenario. By course end, you will not only be proficient in advanced data modeling techniques, but also understand how to navigate complex modeling scenarios and situations, and effectively organize your models.\n\n\nThe course comes bundled with a demo dataset and model, offering a practical playground for you to experiment with advanced querying and data modeling techniques. As you embark on this course, expect an enriching journey into the advanced realms of Microsoft Power BI, transforming the way you perceive and use your data.",
      "target_audience": [
        "Power BI users looking to enhance their skills in advanced data transformations.",
        "Data Analysts aiming to improve their data modeling and transformation techniques.",
        "Reporting Managers seeking to create more effective and data-driven reports.",
        "Business Intelligence enthusiasts who want to deepen their understanding of Power BI.",
        "Anyone interested in mastering advanced concepts in Power BI for career advancement."
      ]
    },
    {
      "title": "ChatGPT, Midjourney, ChatGPT 4 & 5, & DallE - The AI Bible",
      "url": "https://www.udemy.com/course/ai-bible-chatgpt-openai-api-dalle-bard-midjourney-gpt4/",
      "bio": "Master ChatGPT, CoPilot, AI Agents & Cutting-Edge AI Tools from an AI Expert to Automate tasks & 10x Your Productivity!",
      "objectives": [
        "Master new cutting-edge AI tools such as ChatGPT, OpenAI API, Google Gemini, and GPT4 and 5",
        "Learn how to leverage ChatGPT, GPT-4, & GPT-5 to automate your daily tasks and 10x your productivity",
        "Understand the fundamentals of prompt engineering and how it applies to generative AI",
        "Leverage Midjourney and use it to generate creative art",
        "Build Custom GPTs and customize them with your own datasets",
        "Gain an understanding of AI Agents and their use cases",
        "Apply your knowledge to build AI-driven projects and applications",
        "Build, train and test your own Artificial Intelligence model using Google Teachable Machines",
        "Understand key AI terminologies",
        "Explore Dall-E and its ability to generate images from textual input",
        "Describe the capabilities of GPT-4, GPT-5 and ChatGPT"
      ],
      "course_content": {
        "Introduction to ChatGPT, GPT-4 and OpenAI": [
          "What's ChatGPT and OpenAI?",
          "ChatGPT Signup and Demo",
          "Join our Free Community & Connect with Learners worldwide:",
          "ChatGPT Superpowers!"
        ],
        "ChatGPT & GPT-4 for Brainstorming Ideas, Office Productivity, and Research!": [
          "Use ChatGPT to Conduct General Research (Better than Google!)",
          "Using ChatGPT for Brainstorming Ideas",
          "ChatGPT for Office Productivity",
          "ChatGPT Pretending to be Your Teacher, Travel Agent, and Cook!"
        ],
        "What’s New in ChatGPT 5 & AI Agents in Copilot": [
          "ChatGPT 5 has been released & GPT-5 is now available in CoPilot!",
          "ChatGPT 5 for Brainstorming, Research, & Creativity",
          "Image and Video Generation Using ChatGPT 5 & Sora",
          "ChatGPT 5 for Vibe Coding & App Development",
          "ChatGPT 5 for Data Extraction, Analysis, & Visualization",
          "Prompt Engineering Fundamentals",
          "AI Agents - Part 1",
          "AI Agents - Part 2",
          "Custom GPTs"
        ],
        "Generating Incredible AI Art with Midjourney": [
          "What is Midjourney and How to Sign Up?",
          "Develop Stunning Jewelry Designs With Midjourney AI",
          "Leverage Midjourney to Develop a Logo for Your Brand",
          "Develop Brand and Product Names"
        ],
        "Generating AI Art with Dall-E": [
          "Download the Materials Here",
          "What is Dall-E, Signup, and Quick Test",
          "Photo Editing, Out Painting and Frame Generation with Dall-E",
          "Interior Design, Selling Products and AI Art with Dall-E"
        ],
        "Advanced Data Analysis Tool (Code Interpreter Before!)": [
          "Download the Materials here",
          "Project Overview and Daily Return Calculations",
          "Data Visualization Libraries in Python",
          "Candlestick Chart",
          "Advanced Data Analysis Demo 1",
          "Advanced Data Analysis Demo 1 - PO Solution",
          "Advanced Data Analysis Demo 2",
          "Advanced Data Analysis Demo 3",
          "Advanced Data Analysis Demo 3 - PO Solution"
        ],
        "ChatGPT Plugins": [
          "Download the Materials Here",
          "Plugin Example 1",
          "Plugin Example 2"
        ],
        "ChatGPT for Programmers": [
          "Find the Proper Programming Language Syntax Using ChatGPT and GPT-4",
          "Prepare a Study Plan and Find Best Resources (Courses/Books) with ChatGPT",
          "Perform Code Generation and Design Using ChatGPT",
          "Perform Code Debugging Using ChatGPT",
          "Optimize Your Code With ChatGPT",
          "Conduct Code Review With ChatGPT",
          "Leverage ChatGPT to Add New Features to Your Code",
          "Use ChatGPT to Test and Validate Your Code",
          "Perform Code Documentation With ChatGPT",
          "Convert from One Programming Language to Another Using ChatGPT"
        ],
        "ChatGPT for Content Creators": [
          "ChatGPT for Video Creators – YouTube, Instagram, & TikTok [Ideas Generation]",
          "ChatGPT for video creators – Scripting for YouTube Long-form Content",
          "ChatGPT for video creators – Scripting YouTube Shorts, Instagram Reels, & TikTok",
          "ChatGPT for Course Creators",
          "ChatGPT for Podcasters",
          "ChatGPT for Copywriters: New Ideas Generation & SEO Optimization – Part 1",
          "ChatGPT for Copywriters: New Ideas Generation & SEO Optimization – Part 2"
        ],
        "ChatGPT for Lawyers and Legal work": [
          "Leveraging ChatGPT for answering common legal Questions",
          "Leveraging ChatGPT for Answering Common Legal Questions",
          "ChatGPT Answering Legal Questions for Business Owners"
        ]
      },
      "requirements": [
        "No prerequisites, you just need access to the internet!"
      ],
      "description": "**Course Update [29 Aug 2025]: A New Module covering ChatGPT 5 has been added**\n**Course Update [28 Aug 2025]: A New Module covering Latest CoPilot AI Agents has been added**\n\n\nWelcome to \"ChatGPT, Midjourney, ChatGPT 4, & DallE - The AI Bible\"!\nThis is a comprehensive course designed to provide a deep understanding of the latest advancements in artificial intelligence and their real-world applications.\nThrough this course, you will explore the groundbreaking technologies behind ChatGPT, Dall-E, GPT-4, GPT-5, and Midjourney, and learn how these AI models are transforming the way we communicate, create, and solve complex problems.\nWith a focus on practical applications and hands-on experience, this course will teach students the fundamentals of prompt engineering, generative AI, and more. By the end of the course, students will have a solid understanding of how to use these tools to solve real-world problems and automate several tasks\n\n\nBy the end of this course, you will be able to:\nLearn how to use ChatGPT and OpenAI API to generate high-quality text.\nExplore DallE and its ability to generate images from textual input.\nLeverage Midjourney and use it to generate creative art.\nUnderstand the fundamentals of prompt engineering and how it applies to generative AI.\nGain an understanding of Google Bard and its use cases.\nGet an early preview of GPT5 and its new features.\nDescribe the capabilities of GPT-4, GPT-5, and ChatGPT.\nUnderstand the principles and applications of Dall-E, an AI-based image synthesis model.\nAnalyze the natural language understanding and generation capabilities of Google Bard.\nApply your knowledge to build AI-driven projects and applications.\n\n\nDon't miss this opportunity to embark on a journey through the cutting-edge world of artificial intelligence!\n\n\nEnroll in the course today to gain a competitive edge in the rapidly-evolving AI landscape.\n\n\nWhether you're an AI enthusiast, a developer, or a professional looking to leverage AI in your industry, this course is the perfect gateway to unlocking the true potential of these transformative technologies.\n\n\nSign up now and take the first step toward mastering the AI revolution!",
      "target_audience": [
        "Anyone who wants to learn about AI, including students, researchers, developers, and professionals.",
        "Seasoned consultants wanting to transform businesses by leveraging AI",
        "Tech enthusiasts who are passionate and new to AI and want to gain practical experience"
      ]
    },
    {
      "title": "Power BI Mastery: Zero to Hero Data Skills",
      "url": "https://www.udemy.com/course/power-bi-mastery-zero-to-hero-data-visualization-skills/",
      "bio": "Learn Data Visualization, Advanced Charts, Power Query, and HR Analytics with Power BI",
      "objectives": [
        "Master the basics of Power BI, including its components and workflow.",
        "Efficiently download and install Power BI on your system.",
        "Create and analyze basic to advanced charts and visualizations.",
        "Develop geographical insights using various map features in Power BI.",
        "Utilize tables and matrices for detailed data representation.",
        "Implement and manage dynamic cards and filters for interactive reports.",
        "Customize slicers for effective data segmentation and exploration.",
        "Harness advanced charting techniques like bar chart races and sunbursts.",
        "Link objects to actions for interactive Power BI reports and dashboards.",
        "Publish, share, and manage Power BI reports and dashboards effectively."
      ],
      "course_content": {
        "Introduction to Power BI": [
          "Introduction to Power BI"
        ],
        "Download and installation": [
          "Download and installation",
          "Source Codes"
        ],
        "Basic Charts in Power BI": [
          "Column chart",
          "Stacked column chart",
          "Pie chart",
          "Donut chart",
          "Funnel chart",
          "Ribbon chart"
        ],
        "How to Create a Map in Power BI": [
          "Simple Map",
          "Filled map",
          "Map with pie chart",
          "Formatting in map",
          "Background changes in map"
        ],
        "Table & Matrix in Power BI": [
          "Creating a simple table",
          "Formatting in table",
          "Conditional formatting in table",
          "Changing aggregation in table",
          "Creating a matrix in power bi",
          "Conditional formatting in matrix",
          "Automatic hierarchy in matrix",
          "Subtotal and grand total",
          "Number formatting in table and matrix"
        ],
        "Cards & Filters in Power BI": [
          "Number card",
          "Text card",
          "Date card",
          "Multi-row card",
          "Filter on visual",
          "Filter on page",
          "Filter on all pages",
          "Drill through"
        ],
        "Slicers in Power BI": [
          "Slicer for text",
          "Format text slicer",
          "Slicer for Date",
          "Format date slicer",
          "Number slicer"
        ],
        "Advanced Charts in Power BI": [
          "Clear-RecycleBin -Confirm false",
          "Drill down donut chart",
          "Word cloud",
          "Sankey chart",
          "Infographic",
          "Play axis",
          "Scroller",
          "Sunburst",
          "Histogram"
        ],
        "Objects & Actions (Hyperlinks)": [
          "Insert object",
          "Insert text",
          "Insert shapes",
          "Insert buttons",
          "Action-web URL",
          "Page navigation",
          "Bookmark"
        ],
        "ower BI Service Introduction": [
          "Creating a superstore report",
          "Publish report to power bi service account",
          "Export(PPT,PDF,PBIX)report and share",
          "Create a dashboard in power bi service",
          "Problem in power bi dashboard and its solution"
        ]
      },
      "requirements": [
        "No prior experience with Power BI required.",
        "Basic understanding of data handling is helpful.",
        "Access to a computer with internet connection.",
        "Power BI Desktop installed (free download from Microsoft)."
      ],
      "description": "Dive deep into the world of data with our comprehensive course, \"Power BI Mastery: Zero to Hero Data Skills\". This meticulously crafted course is tailored for beginners and intermediate users looking to harness the full potential of Power BI for business intelligence and data analysis. Over the span of our modules, you will start with the fundamentals, learning what Power BI is and how it functions. Quickly move from basic concepts to mastering sophisticated data visualization techniques including column charts, maps, and advanced charts like Sankey diagrams and animated bar races.\nBeyond visuals, the course covers essential Power BI services, teaching you to create and publish comprehensive reports, manage dashboards, and share insights across teams with ease. We also delve deep into Power Query, where you will master text, date, and number functions essential for data manipulation.\nPractical, real-world applications are central to the learning experience. Engage in projects such as an HR Data Analytics Project to apply what you've learned in a meaningful way. By the end of this course, you will not only understand Power BI inside out but also be able to make informed decisions using your data, present findings effectively, and advance your career in data science or any data-driven field. Join us to transform raw data into compelling stories with Power BI!",
      "target_audience": [
        "Beginners in Data Science: Ideal for those starting their journey in data analytics.",
        "Business Analysts: Professionals seeking to enhance their data visualization skills.",
        "Project Managers: Managers needing to visualize and report on project data effectively.",
        "Marketing Professionals: Marketers who want to leverage data for strategic decisions.",
        "Small Business Owners: Owners needing to analyze business data without heavy IT support",
        "Students in Business or IT: Academics aiming to bolster their data handling capabilities."
      ]
    },
    {
      "title": "Data Science & Machine Learning(Theory+Projects)A-Z 90 HOURS",
      "url": "https://www.udemy.com/course/data-science-machine-learningtheoryprojectsa-z-90-hours/",
      "bio": "Data Science Python-Learn Statistics for Data Science, Machine Learning for Data Science, Deep Learning for Data Science",
      "objectives": [
        "Key data science and machine learning concepts right from the beginning with a complete unfolding with examples in Python.",
        "Essential Concepts and Algorithms in Machine Learning",
        "Python for Data Science and Data Analysis",
        "Data Understanding and Data Visualization with Python",
        "Probability and Statistics in Python",
        "Feature Engineering and Dimensionality Reduction with Python",
        "Artificial Neural Networks with Python",
        "Convolutional Neural Networks with Python",
        "Recurrent Neural Networks with Python",
        "Detailed Explanation and Live Coding with Python",
        "Building your own AI applications."
      ],
      "course_content": {},
      "requirements": [
        "• No prior knowledge is needed. You will start with the basic concepts and gradually build your knowledge of the subject.",
        "• Enthusiasm and willingness to learn and practice."
      ],
      "description": "Comprehensive Course Description:\nElectrification was undeniably one of the greatest engineering feats of the 20th century. The invention of the electric motor dates back to 1821, with mathematical analysis of electrical circuits following in 1827. However, it took several decades for the full electrification of factories, households, and railways to begin. Fast forward to today, and we are witnessing a similar trajectory with Artificial Intelligence (AI). Despite being formally founded in 1956, AI has only recently begun to revolutionize the way humanity lives and works.\nSimilarly, Data Science is a vast and expanding field that encompasses data systems and processes aimed at organizing and deriving insights from data. One of the most important branches of AI, Machine Learning (ML), involves developing systems that can autonomously learn and improve from experience without human intervention. ML is at the forefront of AI, as it aims to endow machines with independent learning capabilities.\nOur \"Data Science & Machine Learning Full Course in 90 Hours\" offers an exhaustive exploration of both data science and machine learning, providing in-depth coverage of essential concepts in these fields. In today's world, organizations generate staggering amounts of data, and the ability to store, analyze, and derive meaningful insights from this data is invaluable. Data science plays a critical role here, focusing on data modeling, warehousing, and deriving practical outcomes from raw data.\nFor data scientists, AI and ML are indispensable, as they not only help tackle large data sets but also enhance decision-making processes. The ability to transition between roles and apply these methodologies across different stages of a data science project makes them invaluable to any organization.\nWhat Makes This Course Unique?\nThis course is designed to provide both theoretical foundations and practical, hands-on experience. By the end of the course, you will be equipped with the knowledge to excel as a data science professional, fully prepared to apply AI and ML concepts to real-world challenges.\nThe course is structured into several interrelated sections, each of which builds upon the previous one. While you may initially view each section as an independent unit, they are carefully arranged to offer a cohesive and sequential learning experience. This allows you to master foundational skills and gradually tackle more complex topics as you progress.\nThe \"Data Science & Machine Learning Full Course in 90 HOURS\" is crafted to equip you with the most in-demand skills in today’s fast-paced world. The course focuses on helping you gain a deep understanding of the principles, tools, and techniques of data science and machine learning, with a particular emphasis on the Python programming language.\nKey Features:\nComprehensive and methodical pacing that ensures all learners—beginners and advanced—can follow along and absorb the material.\nHands-on learning with live coding, practical exercises, and real-world projects to solidify understanding.\nExposure to the latest advancements in AI and ML, as well as the most cutting-edge models and algorithms.\nA balanced mix of theoretical learning and practical application, allowing you to immediately implement what you learn.\nThe course includes over 700 HD video tutorials, detailed code notebooks, and assessment tasks that challenge you to apply your knowledge after every section. Our instructors, passionate about teaching, are available to provide support and clarify any doubts you may have along your learning journey.\nCourse Content Overview:\nPython for Data Science and Data Analysis:\nIntroduction to problem-solving, leading up to complex indexing and data visualization with Matplotlib.\nNo prior knowledge of programming is required.\nMaster data science packages such as NumPy, Pandas, and Matplotlib.\nAfter completing this section, you will have the skills necessary to work with Python and data science packages, providing a solid foundation for transitioning to other programming languages.\nData Understanding and Visualization with Python:\nDelve into advanced data manipulation and visualization techniques.\nExplore widely used packages, including Seaborn, Plotly, and Folium, for creating 2D/3D visualizations and interactive maps.\nGain the ability to handle complex datasets, reducing your dependency on core Python language and enhancing your proficiency with data science tools.\nMastering Probability and Statistics in Python:\nLearn the theoretical foundation of data science by mastering Probability and Statistics.\nUnderstand critical concepts like conditional probability, statistical inference, and estimations—key pillars for ML techniques.\nExplore practical applications and derive important relationships through Python code.\nMachine Learning Crash Course:\nA thorough walkthrough of the theoretical and practical aspects of machine learning.\nBuild machine learning pipelines using Sklearn.\nDive into more advanced ML concepts and applications, preparing you for deeper exploration in subsequent sections.\nFeature Engineering and Dimensionality Reduction:\nUnderstand the importance of data preparation for improving model performance.\nLearn techniques for selecting and transforming features, handling missing data, and enhancing model accuracy and efficiency.\nThe section includes real-world case studies and coding examples in Python.\nArtificial Neural Networks (ANNs) with Python:\nANNs have revolutionized machine learning with their ability to process large amounts of data and identify intricate patterns.\nLearn the workings of TensorFlow, Google’s deep learning framework, and apply ANN models to real-world problems.\nConvolutional Neural Networks (CNNs) with Python:\nGain a deep understanding of CNNs, which have revolutionized computer vision and many other fields, including audio processing and reinforcement learning.\nBuild and train CNNs using TensorFlow for various applications, from facial recognition to neural style transfer.\nBy the End of This Course, You Will Be Able To:\nUnderstand key principles and theories in Data Science and Machine Learning.\nImplement Python-based machine learning models using real-world datasets.\nApply advanced data science techniques to solve complex problems.\nTake on challenging roles in data science and machine learning with confidence.\nWho Should Enroll:\nIndividuals from non-engineering backgrounds eager to transition into Data Science.\nAspiring data scientists who want to work with real-world datasets.\nBusiness analysts looking to gain expertise in Data Science & ML.\nAnyone passionate about programming, numbers, and data-driven decision-making.\nEnroll now and start your exciting journey in the fields of Data Science and Machine Learning. This course simplifies even the most complex concepts and makes learning a rewarding experience.",
      "target_audience": [
        "• People who want to enter the Machine Learning field.",
        "• People who want to become perfect in their data speak.",
        "• People who want to learn Data Science & Machine Learning along with its implementation in realistic projects.",
        "• People who want to learn Data Science & Machine Learning with real datasets in Data Science.",
        "• People from a non-engineering background who want to enter the Data Science field.",
        "• Individuals who are passionate about numbers and programming.",
        "• Beginners in Data Science field.",
        "• Business Analysts."
      ]
    },
    {
      "title": "Beginner to Advanced MLOps on GCP-CI/CD, Kubernetes Jenkins",
      "url": "https://www.udemy.com/course/mastering-advanced-mlops-on-gcp-cicd-kubernetes-kubeflow/",
      "bio": "Simply streamline ML pipelines with Kubernetes, GitLab CI, Jenkins, Prometheus, Grafana, Kubeflow & Minikube on GCP.",
      "objectives": [
        "Build and manage robust continuous integration and deployment pipelines using tools like GitHub Action and Jenkins tailored for machine learning s, GitLab CI/CD",
        "Utilize containerization and orchestration tools such as Docker, Kubeflow, and Minikube to create scalable, production-ready ML systems on GCP.",
        "Efficiently manage and secure ML data with PostgreSQL while implementing real-time monitoring and visualization dashboards using Grafana.",
        "Apply best practices in scaling, resource management, and security compliance to ensure efficient and secure ML operations in cloud environments."
      ],
      "course_content": {
        "COURSE INTRODUCTION": [
          "INTRODUCTION"
        ],
        "Hotel Reservation Prediction with MLFlow, Jenkins and GCP Deployment": [
          "Introduction to the Project",
          "Database Setup with GCP Buckets",
          "Project Setup",
          "Data Ingestion with GCP",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Model Training and Experiment Tracking using MLFLOW",
          "Training Pipeline and Data & Code Versioning",
          "User App Building using Flask and CHATGPT",
          "CI-CD Deployment using Jenkins and Google Cloud Run"
        ],
        "Hybrid Anime Recommender System with Comet-ML , DVC , Jenkins and Kubernetes": [
          "Introduction to the Project",
          "Database Setup using GCP Buckets",
          "Project Setup",
          "Data Ingestion with GCP",
          "Jupyter Notebook Testing - Part 1",
          "Jupyter Notebook Testing - Part 2",
          "Data Processing",
          "Model Architecture and Model Training",
          "Experiment Tracking using COMET-ML",
          "Building Training Pipeline",
          "Data Versioning using DVC & Code Versioning using GitHub",
          "Building Prediction Helper Functions",
          "User App Building with Flask and CHATGPT",
          "CI-CD Deployment using Jenkins and Google Kubernetes"
        ],
        "User Survival Prediction with Astro Airflow , SQL , Redis , Grafana & Prometheus": [
          "Introduction to the Project",
          "Database Setup using GCP Buckets",
          "Project Setup",
          "Data Engineering ETL Pipeline using Airflow and PostgreSQL",
          "Data Ingestion using PSYCOPG2",
          "Jupyter Notebook Testing",
          "Building Feature Store using REDIS",
          "Data Processing with Feature Storing",
          "Model Training with Feature Extraction",
          "Training Pipeline and Data & Code Versioning",
          "User App Building using Flask and CHATGPT",
          "Data Drift Detection using ALIBI-DETECT",
          "ML Monitoring using Grafana and Prometheus with Setup"
        ],
        "Custom Guns Object Detection with Tensorboard, DVC, FastAPI and Postman": [
          "Introduction to the Project",
          "Project Setup",
          "Data Ingestion using KAGGLE",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Building Model Architecture",
          "Model Training",
          "Experiment Tracking using TensorBoard",
          "Training Pipeline using DVC",
          "Data and Code Versioning",
          "API Building and Testing using FastAPI , SwaggerUI and Postman"
        ],
        "Colorectal Cancer Prediction with Mlflow+DagsHUB ,Minikube Kubernetes & Kubeflow": [
          "Introduction to the Project",
          "Project Setup",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Model Training",
          "Experiment Tracking using MLFLOW and DAGSHUB",
          "User App Building Using Flask and ChatGPT",
          "KUBEFLOW and MINIKUBE Installation and Setup",
          "Building Kubeflow Pipelines"
        ],
        "MINOR MLOPS PROJECT - 1 using CIRCLE CI": [
          "IMPORTANT NOTE",
          "Introduction to the Project",
          "Project Setup",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Model Training",
          "User App Building using Flask and CHATGPT",
          "Training Pipeline and Data & Code Versioning",
          "CI-CD Deployment using CIRCLE-CI and Google Kubernetes"
        ],
        "MINOR MLOPS PROJECT - 2 using GITLAB CI/CD": [
          "IMPORTANT NOTE",
          "Introduction to the Project",
          "Project Setup",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Model Training",
          "User App Building using Flask and ChatGPT",
          "Training Pipeline and Data & Code Versioning using GITLAB",
          "Google Cloud Setup",
          "CI-CD Deployment using GITLAB CI/CD"
        ],
        "MINOR MLOPS PROJECT - 3 using GITHUB ACTIONS": [
          "IMPORTANT NOTE",
          "Introduction to the Project",
          "Project Setup",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Model Training",
          "User App Building using Flask and ChatGPT",
          "Google Cloud Setup",
          "Training Pipeline and Data & Code Versioning",
          "CI-CD Deployment using GITHUB ACTIONS"
        ],
        "Australia Weather Rain Prediction using Github Actions, Circle CI and GITLAB": [
          "Introduction to the Project",
          "Project Setup",
          "Jupyter Notebook Testing",
          "Data Processing",
          "Model Training and Training Pipeline",
          "User App Building using Flask and ChatGPT",
          "Google Cloud Setup",
          "Dockerfile , Kubernetes Deployment file and Data & Code Versioning using GitHub",
          "CI-CD Deployment using GITHUB ACTIONS",
          "CI-CD Deployment using Circle CI",
          "CI-CD Deployment using GITLAB CI/CD"
        ]
      },
      "requirements": [
        "Programming Proficiency: Basic to intermediate experience with programming, particularly in Python, which is widely used in machine learning and scripting for automation.",
        "A basic understanding of machine learning principles"
      ],
      "description": "This Beginner to Advanced MLOps Course covers a wide range of technologies and tools essential for building, deploying, and automating ML models in production.\n\nTechnologies & Tools Used Throughout the Course\nExperiment Tracking & Model Management: MLFlow, Comet-ML, TensorBoard\nData & Code Versioning: DVC, Git, GitHub, GitLab\nCI/CD Pipelines & Automation: Jenkins, ArgoCD, GitHub Actions, GitLab CI/CD, CircleCI\nCloud & Infrastructure: GCP (Google Cloud Platform), Minikube, Google Cloud Run, Kubernetes\nDeployment & Containerization: Docker, Kubernetes, FastAPI, Flask\nData Engineering & Feature Storage: PostgreSQL, Redis, Astro Airflow, PSYCOPG2\nML Monitoring & Drift Detection: Prometheus, Grafana, Alibi-Detect\nAPI & Web App Development: FastAPI, Flask, ChatGPT, Postman, SwaggerUI\nHow These Tools & Techniques Help\nExperiment Tracking & Model Management\nHelps in logging, comparing, and tracking different ML model experiments.\nMLFlow & Comet-ML provide centralized tracking of hyperparameters and performance metrics.\nData & Code Versioning\nEnsures reproducibility by tracking data changes over time.\nDVC manages large datasets, and GitHub/GitLab maintains version control for code and pipelines.\nCI/CD Pipelines & Automation\nAutomates ML workflows from model training to deployment.\nJenkins, GitHub Actions, GitLab CI/CD, and ArgoCD handle continuous integration & deployment.\nCloud & Infrastructure\nGCP provides scalable infrastructure for data storage, model training, and deployment.\nMinikube enables Kubernetes testing on local machines before deploying to cloud environments.\nDeployment & Containerization\nDocker containerizes applications, making them portable and scalable.\nKubernetes manages ML deployments for high availability and scalability.\nData Engineering & Feature Storage\nPostgreSQL & Redis store structured and real-time ML features.\nAirflow automates ETL pipelines for seamless data processing.\nML Monitoring & Drift Detection\nPrometheus & Grafana visualize ML model performance in real-time.\nAlibi-Detect helps in identifying data drift and model degradation.\nAPI & Web App Development\nFastAPI & Flask create APIs for real-time model inference.\nChatGPT integration enhances chatbot-based ML applications.\nSwaggerUI & Postman assist in API documentation & testing.\nThis course ensures a complete hands-on approach to MLOps, covering everything from data ingestion, model training, versioning, deployment, monitoring, and CI/CD automation to make ML projects production-ready and scalable.",
      "target_audience": [
        "Machine Learning Engineers & Data Scientists: Those who want to bridge the gap between model development and scalable deployment.",
        "DevOps & MLOps Practitioners: Individuals aiming to integrate CI/CD pipelines and container orchestration into ML workflows.",
        "Cloud & Infrastructure Specialists: Professionals seeking to deepen their expertise in GCP and related cloud-native tools.",
        "Technical Leaders & Architects: Decision-makers responsible for designing and maintaining robust, scalable ML systems in production."
      ]
    },
    {
      "title": "Computer Vision: Python OCR & Object Detection Quick Starter",
      "url": "https://www.udemy.com/course/computer-vision-python-ocr-object-detection-quick-starter/",
      "bio": "Quick Starter for Optical Character Recognition, Image Recognition Object Detection and Object Recognition using Python",
      "objectives": [
        "Optical Character Recognition with Tesseract Library, Image Recognition using Keras, Object Recognition using MobileNet SSD, Mask R-CNN, YOLO, Tiny YOLO",
        "optical Character Recognition",
        "Image Recognition",
        "Object Recognition"
      ],
      "course_content": {
        "Course Introduction and Table of Contents": [
          "Course Introduction and Table of Contents"
        ],
        "Introduction to OCR Concepts and Libraries": [
          "Introduction to OCR Concepts and Libraries"
        ],
        "Setting up Environment - Anaconda": [
          "Setting up Environment - Anaconda"
        ],
        "Python Basics (Optional)": [
          "Python Basics - Part 1 - Assignment",
          "Python Basics - Part 2 - Flow Control",
          "Python Basics - Part 3 - Data Structures",
          "Python Basics - Part 4 - Functions"
        ],
        "Tesseract OCR Setup": [
          "Tesseract OCR Setup - Part 1",
          "Tesseract Download Link",
          "Tesseract OCR Setup - Part 2"
        ],
        "OpenCV Setup": [
          "OpenCV Setup"
        ],
        "Tesseract Image OCR Implementation": [
          "Tesseract Image OCR Implementation - Part 1",
          "Tesseract Image OCR Implementation - Part 2"
        ],
        "Optional: cv2.imshow() Not Responding Issue Fix": [
          "Optional: cv2.imshow() Not Responding Issue Fix",
          "Optional Fix 2: cv2.imshow() Not Responding"
        ],
        "Optional: OCR Text not printing in console": [
          "Fix text not printing in console"
        ],
        "Introduction to CNN - Convolutional Neural Networks - Theory Session": [
          "Introduction to CNN - Convolutional Neural Networks - Theory Session"
        ]
      },
      "requirements": [
        "A decent configuration computer (preferably Windows) and an enthusiasm to dive into the world of OCR, Image and Object Recognition using Python"
      ],
      "description": "Hi There!\n\n\nwelcome to my new course 'Optical Character Recognition and Object Recognition Quick Start with Python'. This is the third course from my Computer Vision series.\n\n\nImage Recognition, Object Detection, Object Recognition and also Optical Character Recognition are among the most used applications of Computer Vision.\n\n\nUsing these techniques, the computer will be able to recognize and classify either the whole image, or multiple objects inside a single image predicting the class of the objects with the percentage accuracy score. Using OCR, it can also recognize and convert text in the images to machine readable format like text or a document.\n\n\nObject Detection and Object Recognition is widely used in many simple applications and also complex ones like self driving cars.\n\n\nThis course will be a quick starter for people who wants to dive into Optical Character Recognition, Image Recognition and Object Detection using Python without having to deal with all the complexities and mathematics associated with typical Deep Learning process.\n\n\nLet's now see the list of interesting topics that are included in this course.\n\n\nAt first we will have an introductory theory session about Optical Character Recognition technology.\n\n\nAfter that, we are ready to proceed with preparing our computer for python coding by downloading and installing the anaconda package and will check and see if everything is installed fine.\n\n\nMost of you may not be coming from a python based programming background. The next few sessions and examples will help you get the basic python programming skill to proceed with the sessions included in this course. The topics include Python assignment, flow-control, functions and data structures.\n\n\nThen we will install the dependencies and libraries that we require to do the Optical Character Recognition. We are using Tesseract Library to do the OCR. At first we will install the Library and then its python bindings. We will also install OpenCV, which is the Open Source Computer Vision library in Python.\n\n\nWe also will install the Pillow library, which is the Python Image Library. Then we will have an introduction to the steps involved in the Optical Character Recognition and later will proceed with coding and implementing the OCR program. We will use few example images to do a Character Recognition testing and will verify the results.\n\n\nThen we will have an introduction to Convolutional Neural Networks , which we will be using to do the Image Recognition. Here we will be classifying a full image based on the single primary object in it.\n\n\nWe will then proceed with installing the Keras Library which we will be using to do the Image recognition. We will be using the built in , pre-trained Models that are included in Keras. The base code in python is also provided in the Keras documentation.\n\n\nAt first We will be using the popular pre-trained model architecture called the VGGNet. we will have an introductory session about the architecture of VGGNet. Then we will proceed with using the pre-trained VGGNet 16 Model included in keras to do Image Recognition and classification. We will try with few sample images to check the predictions. Then will move on to a deeper VGGNet 19 Model included in keras to do Image Recognition and classification.\n\n\nThen we will try the ResNet pre-trained model included with the Keras library. We will include the model in the code and then we will try with few sample images to check the predictions.\n\n\nAnd after that we will try the Inception pre-trained model. We will also include the model in the code and then we will try with few sample images to check the predictions. Then will go ahead with the Xception pre-trained model. Here also, we will  include the model in the code and then we will try with few sample images.\n\n\nAnd those were Image Recognition pre-trained models, which can only label and classify a complete image based on the primary object in it. Now we will proceed with Object Recognition in which we can detect and label multiple objects in a single image.\n\n\nAt first we will have an introduction to MobileNet-SSD Pre-trained Model, which is single shot detector that is capable of detecting multiple objects in a scene. We will be also be having a quick discussion about the dataset that is used to train this model.\n\n\nLater we will be implementing the MobileNet-SSD Pre-trained Model in our code and will get the predictions and bounding box coordinates for every object detected. We will draw the bounding box around the objects in the image and write the label along with the confidence value.\n\n\nThen we will go ahead with object detection from a live video. We will be streaming the real-time live video from the computer's webcam and will try to detect objects from it. We will draw rectangle around each object detected in the live video along with the label and confidence.\n\n\nIn the next session, we will go ahead with object detection from a pre-saved video. We will be streaming the saved video from our folder and will try to detect objects from it. We will draw rectangle around each object detected along with the label and confidence.\n\n\nLater we will be going ahead with the Mask-RCNN Pre-trained Model. In the previous model, we were only able to get a bounding box around the object, but in Mask-RCNN, we can get both the box co-ordinates as well the mask over the exact shape of object detected. We will have an introduction about this model and its details.\n\n\nLater we will be implementing the Mask-RCNN Pre-trained Model in our code and as the first step we will get the predictions and bounding box coordinates for every object detected. We will draw the bounding box around the objects in the image and write the label along with the confidence value.\n\n\nLater we will be getting the mask returned for each object predicted. We will process that data and use it to draw translucent multi coloured masks over each and every object detected and write the label along with the confidence value.\n\n\nThen we will go ahead with object detection from a live video using Mask-RCNN. We will be streaming the real-time live video from the computer's webcam and will try to detect objects from it. We will draw the mask over the perimeter of each object detected in the live video along with the label and confidence.\n\n\nAnd like we did for our previous model, we will go ahead with object detection from a pre-saved video using Mask-RCNN. We will be streaming the saved video from our folder and will try to detect objects from it. We will draw coloured masks for object detected along with the label and confidence.\n\n\nThe Mask-RCNN is very accurate with vast class list but will be very slow in processing images using low power CPU based computers. MobileNet-SSD is fast but less accurate and low in number of classes. We need a perfect blend of speed and accuracy which will take us to Object Detection and Recognition using YOLO pre-trained model. we will have an overview about the yolo model in the next session and then we will implement yolo object detection from a single image.\n\n\nAnd using that as the base, we will try the yolo model for object detection from a real time webcam video and we will check the performance. Later we will use it for object recognition from the pre-saved video file.\n\n\nTo further improve the speed of frames processed, we will use the model called Tiny YOLO which is a light weight version of the actual yolo model. We will use tiny yolo at first for the pre-saved video and will analyse the accuracy as well as speed and then we will try the same for a real-time video from webcam and see the difference in performance compared to actual yolo.\n\n\nThat's all about the topics which are currently included in this quick course. The code, images and libraries used in this course has been uploaded and shared in a folder. I will include the link to download them in the last session or the resource section of this course. You are free to use the code in your projects with no questions asked.\n\n\nAlso after completing this course, you will be provided with a course completion certificate which will add value to your portfolio.\n\n\nSo that's all for now, see you soon in the class room. Happy learning and have a great time.",
      "target_audience": [
        "Beginners or who wants to start with Python based OCR, Image Recognition and Object Recognition"
      ]
    },
    {
      "title": "Python for Data Science & Machine Learning: Zero to Hero",
      "url": "https://www.udemy.com/course/python-for-data-science-machine-learning-zero-to-hero/",
      "bio": "Master Data Science & Machine Learning in Python: Numpy, Pandas, Matplotlib, Scikit-Learn, Machine Learning, and more!",
      "objectives": [
        "Gain familiarity with Pandas, a data analysis tool",
        "Get a grasp on the theory behind basic and multiple linear regression",
        "Tackle regression problems easily",
        "Discover the logic behind decision trees",
        "Acquaint yourself with the various clustering algorithms"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Python for Data Science & ML bootcamp!",
          "Python: A Brief Overview",
          "The Python Installation Procedure",
          "What Jupyter is?",
          "Set up Anaconda on Different Operating Systems",
          "How to integrate Python into Jupyter?",
          "Handling Directories in Jupyter Notebook",
          "Input & Output",
          "Working with different datatypes",
          "Variables",
          "Arithmetic Operators",
          "Comparison Operators",
          "Logical Operators",
          "Conditional statements",
          "Loops",
          "Sequences Part 1: Lists",
          "Sequences Part 2: Dictionaries",
          "Sequences Part 3: Tuples",
          "Functions Part 1: Built-in Functions",
          "Functions Part 2: User-defined Functions",
          "Course Materials"
        ],
        "The Must-Have Python Data Science Libraries": [
          "Completing Library Setup",
          "Library Importing",
          "Pandas: A Data Science Library",
          "NumPy: A Data Science Library",
          "NumPy vs. Pandas",
          "Matplotlib Library for Data Science",
          "Seaborn Library for Data Science"
        ],
        "NumPy Mastery: Everything you need to know about NumPy": [
          "Intro to NumPy arrays",
          "Creating NumPy arrays",
          "Indexing NumPy arrays",
          "Array shape",
          "Iterating Over NumPy Arrays",
          "Basic NumPy arrays: zeros()",
          "Basic NumPy arrays: ones()",
          "Basic NumPy arrays: full()",
          "Adding a scalar",
          "Subtracting a scalar",
          "Multiplying by a scalar",
          "Dividing by a scalar",
          "Raise to a power",
          "Transpose",
          "Element-wise addition",
          "Element-wise subtraction",
          "Element-wise multiplication",
          "Element-wise division",
          "Matrix multiplication",
          "Statistics"
        ],
        "DataFrames and Series in Python's Pandas": [
          "What is a Python Pandas DataFrame?",
          "What is a Python Pandas Series?",
          "DataFrame vs Series",
          "Creating a DataFrame using lists",
          "Creating a DataFrame using a dictionary",
          "Loading CSV data into python",
          "Changing the Index Column",
          "Inplace",
          "Examining the DataFrame: Head & Tail",
          "Statistical summary of the DataFrame",
          "Slicing rows using bracket operators",
          "Indexing columns using bracket operators",
          "Boolean list",
          "Filtering Rows",
          "Filtering rows using & and | operators",
          "Filtering data using loc()",
          "Filtering data using iloc()",
          "Adding and deleting rows and columns",
          "Sorting Values",
          "Exporting and saving pandas DataFrames",
          "Concatenating DataFrames",
          "groupby()"
        ],
        "Data Cleaning Techniques for Better Data": [
          "Introduction to Data Cleaning",
          "Quality of Data",
          "Examples of Anomalies",
          "Median-based Anomaly Detection",
          "Mean-based anomaly detection",
          "Z-score-based Anomaly Detection",
          "Interquartile Range for Anomaly Detection",
          "Dealing with missing values",
          "Regular Expressions",
          "Feature Scaling"
        ],
        "Exploratory Data Analysis in Python": [
          "Introduction",
          "What is Exploratory Data Analysis?",
          "Univariate Analysis",
          "Univariate Analysis: Continuous Data",
          "Univariate Analysis: Categorical Data",
          "Bivariate analysis: Continuous & Continuous",
          "Bivariate analysis: Categorical & Categorical",
          "Bivariate analysis: Continuous & Categorical",
          "Detecting Outliers",
          "Categorical Variable Transformation"
        ],
        "Python for Time-Series Analysis: A Primer": [
          "Introduction to Time Series",
          "Getting stock data using yfinance",
          "Converting a Dataset into Time Series",
          "Working with Time Series",
          "Time Series Data Visualization with Python"
        ],
        "Python for Data Visualization: Library Resources, and Sample Graphs": [
          "Introduction",
          "Setting Up Matplotlib",
          "Plotting Line Plots using Matplotlib",
          "Title, Labels & Legend",
          "Plotting Histograms",
          "Plotting Bar Charts",
          "Plotting Pie Charts",
          "Plotting Scatter Plots",
          "Plotting Log Plots",
          "Plotting Polar Plots",
          "Handling Dates",
          "Creating multiple subplots in one figure"
        ],
        "The Basics of Machine Learning": [
          "Why do we need machine learning?",
          "Machine Learning Use Cases",
          "Approaches to Machine Learning",
          "What is Supervised learning?",
          "What is Unsupervised learning?",
          "Supervised learning vs Unsupervised learning"
        ],
        "Simple Linear Regression with Python": [
          "Introduction to regression",
          "How Does Linear Regression Work?",
          "Line representation",
          "Implementation in python: Importing libraries & datasets",
          "Implementation in python: Distribution of the data",
          "Implementation in python: Creating a linear regression object"
        ]
      },
      "requirements": [
        "The ability to do simple math",
        "No programming experience needed",
        "No prior data science knowledge required",
        "Readiness, flexibility, and passion for learning"
      ],
      "description": "This machine learning course will provide you the fundamentals of how companies like Google, Amazon, and even Udemy utilize machine learning and artificial intelligence (AI) to glean meaning and insights from massive data sets. Glassdoor and Indeed both report that the average salary for a data scientist is $120,000. This is the standard, not the exception.\nData scientists are already quite desirable. It's difficult to keep them on staff in today's tight labor market. There is a severe shortage of people who possess the rare combination of scientific training, computer expertise, and analytical talents.\nToday's data scientists are held to the same standards as the Wall Street \"quants\" of the '80s and '90s. When the need arose for innovative algorithms and data approaches, physicists and mathematicians flocked to investment banks and hedge funds.\nSo, it's no surprise that data science is rising to prominence as a promising career path in the modern day. It is analytic in focus, driven by code, and performed on a computer. As a result, it shouldn't be a shock that the demand for data scientists has been growing steadily in the workplace for the past few years.\nOn the other hand, availability has been low. Obtaining the education and experience necessary to be hired as a data scientist is tough. And that's why we made this course in the first place!\nEach topic is described in plain English, and the course does its best to avoid mathematical notations and jargon. Once you have access to the source code, you can experiment with it and improve upon it. Learning and applying these algorithms in the real world, rather than in a theoretical or academic setting, is the focus of this course.\nEach video will leave you with a new perspective that you can implement right away!\nIf you have no background in statistics, don't let that stop you from enrolling in this course; we welcome students of all levels.",
      "target_audience": [
        "Aspiring Machine Learning Professionals",
        "Anyone interested in expanding their skill set with machine learning and Python",
        "Inquisitive technologists interested in seeing Machine Learning in action",
        "Those who are already proficient in programming and want to expand their capabilities by learning about machine learning"
      ]
    },
    {
      "title": "Text Mining, Scraping and Sentiment Analysis with R",
      "url": "https://www.udemy.com/course/r-social-media-mining-scraping-with-twitter/",
      "bio": "Learn how to use Twitter social media data for your R text mining work.",
      "objectives": [
        "use R for social media mining",
        "get data from Twitter to do text analysis",
        "perform web scraping tasks using the twitteR package",
        "know which packages to use for web scraping",
        "get R and Twitter connected",
        "know how to perform a sentiment analysis in R",
        "plot text data visualizations"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the R-Tutorials Social Media Mining Course",
          "Package Overview: NLP",
          "Package Overview: Web Technologies and Services",
          "Course Links",
          "Course Script",
          "Worksheet - Exercises"
        ],
        "Scraping and Text Mining": [
          "Twitter Developer Account",
          "Important: Connection Information",
          "Code Section: Social Media Mining",
          "Connection of R Studio with Twitter",
          "Alternative Authentication",
          "Alternative Authentication Code",
          "Scraping with TwitteR",
          "Text Mining with \"tm\" - Text Cleaning and Transformations",
          "Wordcloud",
          "Document Term Matrix and Frequent Terms",
          "Dendrogram and Term Groups",
          "Exercise - Quiz",
          "Exercise: Text Mining",
          "Solution: Text Mining Part 1",
          "Solution: Text Mining Part 2",
          "Exercise Code: Text Mining",
          "Further R Exercises"
        ],
        "Working with Strings - gsub and the Regular Expression syntax": [
          "Regular Expressions and gsub for sentiment analysis - handling of scraped data",
          "Code Section: Strings",
          "Working with Strings - Introduction",
          "Working with Strings - gsub",
          "Working with Strings - gsub advanced",
          "Regular Expression Overview",
          "Working with Strings - Library Stringr",
          "Exercise and Solution: Strings in R"
        ],
        "Sentiment Analysis": [
          "Section Code: Sentiment Analysis",
          "Sentiment Analysis Basics",
          "Score Sentiment Function - J. Breen Approach",
          "Tweets for Sentiment Analysis",
          "Visualizing the Sentiments",
          "Exercise: Sentiment Analysis",
          "Solution: Sentiment Analysis",
          "Exercise Code: Sentiment Analysis"
        ]
      },
      "requirements": [
        "intermediate R knowledge is required (R Level 1 course)",
        "R program ready on your computer",
        "basic understanding of social media and web technologies"
      ],
      "description": "Are you an advanced R user, looking to expand your R toolbox?\nAre you interested in social media sentiment analysis?\nDo you want to learn how you can get and use Twitter data for your R analysis?\nDo you want to learn how you can systematically find related words (keywords) to a search term using Twitter and R?\nAre you interested in creating visualizations like wordclouds out of text data?\nDo you want to learn which R packages you can use for web scraping and text analysis purposes?\nIf YES came to your mind to some of those points – this course might be tailored towards your needs!\nThis course will teach you anything you need to know about how to handle social media data in R. We will use Twitter data as our example dataset.\nDuring this course we will take a walk through the whole text analysis process of Twitter data.\nAt first you will learn which packages are available for social media analysis.\nYou will learn how to scrape social media (Twitter) data and get it into your R session.\nAfter that we will filter, clean and structure our text corpus.\nThe next step is the visualization of the text data via wordclouds and dendrograms.\nAnd in the last section we will do a whole sentiment analysis by using a common word lexicon.\nAll of those steps are accompanied by exercise sessions so that you can check if you can put the information to work.\nAccording to the teaching principles of R Tutorials every section is enforced with exercises for a better learning experience. You can download the code pdf of every section to try the presented code on your own.\nDisclaimer required by Twitter: 'TWITTER, TWEET, RETWEET and the Twitter logo are trademarks of Twitter, Inc or its affiliates.'",
      "target_audience": [
        "everybody interested in social media analysis",
        "everybody interested in using R for web scraping",
        "everybody interested in sentiment analysis",
        "everybody interested in text analysis",
        "everybody interested in enlarging their R toolbox"
      ]
    },
    {
      "title": "Monitoring and Evaluation (M&E) in Development Projects",
      "url": "https://www.udemy.com/course/monitoring_evaluation/",
      "bio": "Become a Monitoring and Evaluation Specialist in 3 Hours. Project Management, M&E, PMP, LogFrame, Data Analysis",
      "objectives": [
        "Introduce to the fundamental of Monitoring and Evaluation",
        "Practice How to develop Performance Monitoring Plan (PMP)",
        "Develop Project Logical Framework(LogFrame) and Performance Indicator reference sheet (PIRS)",
        "Receive free practical tools and resources of M&E for using in your development projects",
        "Receive free guideline to use tools effectively",
        "Understand key elements, process, and technology of data collection methods",
        "Understand the basic Data Analysis and Data Visualization",
        "Practice Data Analysis and Visualization (Report Dashboard) on Ms. Excel and Power BI",
        "Understand how to set appropriate times to Evaluate projects",
        "Free Bonus of Practice Course: Advanced Pivot Table for Data Analysis"
      ],
      "course_content": {
        "Introduction": [
          "Introduction to the course"
        ],
        "Module 1: Introduction to Monitoring and Evaluation": [
          "Introduction to Monitoring and Evaluation",
          "Monitoring and Evaluation in Project Cycles"
        ],
        "Module 2: Performance Monitoring Plan": [
          "Module 2_Performance Monitoring Plan",
          "Key Element of PMP",
          "Logic Model and Examples",
          "Logical Framework and Exercise",
          "Logical Framework Template Example in one USAID Project",
          "Performance Indicator Reference Sheet (PIRS) and Template",
          "Work Plan Example and Resource for download"
        ],
        "Modul 3: Data Collection": [
          "Data Collection introduction",
          "Introduction to Probability Sampling",
          "Introduction of Non-Probability Sampling",
          "Practice on Sample Size Calculation in the Excel Template",
          "Data Collection Technology and Software Recommendation",
          "Developing Data Collection Form on Google Form"
        ],
        "Module 4: Introduction to Data Analysis": [
          "Introduction to Data Analysis",
          "Data Analysis in Microsoft Excel (Basic and Simple way)",
          "Data Analysis in Microsoft Excel (Intelligent and Personalized Suggestion)",
          "Descriptive Analysis on Microsoft Excel (Statistic Formula)"
        ],
        "Module 5: Introduction to Data Visualization": [
          "Introduction to Data Visualization",
          "Data Visualization and Report Dashboard on Power BI (Part 1)",
          "Data Visualization and Report Dashboard on Power BI (Part 2)"
        ],
        "Module 6: Evaluation": [
          "Introduction to Evaluation",
          "Introduction to Evaluation (con.)"
        ],
        "Final Test": [
          "Final Quiz"
        ],
        "Free Bonus of Course : \"Advanced Pivot Table for Data Analysis\"": [
          "Creating a PivotTable from worksheet data",
          "Refreshing PivotTables",
          "Working with PivotTable Values",
          "Working with calculating values in a PivotTable",
          "Create Calculated Fields",
          "Creating a PivotChart",
          "Use slicers to filter PivotTable data",
          "Create a PivotTable timeline to filter dates"
        ],
        "Recommended Related Courses": [
          "Related Courses"
        ]
      },
      "requirements": [
        "No experience needed. You will learn everything you want to know."
      ],
      "description": "In this course you will go through 6 modules that are very important basic key elements in Monitoring and Evaluation in development project.\nModule 1: You will learn “Introduction to Monitoring and Evaluation(M&E)”. This means you will understand what is monitoring?; what is evaluation? What is the difference between them? You will understand the importance of M&E and when do we need to conduct monitoring and evaluation through the project cycle?\nModule 2: You will understand “Performance Monitoring Plan (PMP)” and Why PMP is important?\nHow to develop PMP? You will know all the key elements of PMP. In this module, you will understand the logic model of projects and the example and exercise for you to take it easy to understand for all practitioners. That is important to under what outputs, outcomes, and impact of the project.\nModule 3: You will understand what does the data collection make a sense to you? The process of data collection and softwares and tools used for data collection.\nModule 4: You are going to learn what types are there used for data analysis? You will understand the Descriptive Analysis in detailed steps. However, I will develop the data analysis course separately for technical persons or professional person at this field.\nModule 5: “ data visualization”, this module, you will understand about how to visualize your data to the audience and donors; understand the tools that you can use for data visualization such as table, chart or infographics and that kind of things. You will know the type of charts that we can use in difference purposes.\nModule 6: “Evaluation”, this module, you will learn more about the evaluation and the key elements of the evaluations as well as when we conduct evaluations in the stages of project cycle.\nWho need to learn the course?\nCourse is created for All university students who want to strengthen more capacity on project management, something related to M&E and  find a job as M&E specialist with high salary,\nNGO Staff who want to increase more skills in Monitoring and Evaluation in development project,\nPrivate Sector Staffs who increase more skill to strengthen the effectiveness of implementation plan,\nBusiness owner who are willing to effectively manage the business and the management change as well as good decision making based on data\nEveryone who want to understand about monitoring and evaluation",
      "target_audience": [
        "The course is for all learners who are willing to gain the skill in Monitoring and Evaluation."
      ]
    },
    {
      "title": "Machine Learning and Deep Learning Bootcamp in Python",
      "url": "https://www.udemy.com/course/introduction-to-machine-learning-in-python/",
      "bio": "Machine Learning, Neural Networks, Deep Learning and Reinforcement Learning, GAN in Keras and TensorFlow",
      "objectives": [
        "Solving regression problems (linear regression and logistic regression)",
        "Solving classification problems (naive Bayes classifier, Support Vector Machines - SVMs)",
        "Using neural networks (feedforward neural networks, deep neural networks, convolutional neural networks and recurrent neural networks",
        "The most up to date machine learning techniques used by firms such as Google or Facebook",
        "Face detection with OpenCV",
        "TensorFlow and Keras",
        "Deep learning - deep neural networks, convolutional neural networks (CNNS), recurrent neural networks (RNNs)",
        "Reinforcement learning - Q learning and deep Q learning approaches",
        "Transformers (ChatGPT)"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Environment Setup": [
          "Installing Python",
          "Installing PyCharm",
          "Installing TensorFlow and Keras"
        ],
        "Artificial Intelligence Basics": [
          "Why to learn artificial intelligence and machine learning?",
          "Types of artificial intelligence learning",
          "Fundamentals of statistics"
        ],
        "### MACHINE LEARNING ###": [
          "Machine learning section"
        ],
        "Linear Regression": [
          "What is linear regression?",
          "Linear regression theory - optimization",
          "Linear regression theory - gradient descent",
          "Linear regression implementation I",
          "Linear regression implementation II",
          "Mathematical formulation of linear regression",
          "Linear Regression Quiz"
        ],
        "Logistic Regression": [
          "What is logistic regression?",
          "Logistic regression and maximum likelihood estimation",
          "Logistic regression example I - sigmoid function",
          "Logistic regression example II- credit scoring",
          "Logistic regression example III - credit scoring",
          "Why is logistic regression linear?",
          "Mathematical formulation of logistic regression",
          "Logistic Regression Quiz"
        ],
        "Cross Validation": [
          "What is cross validation?",
          "Cross validation example",
          "Cross Validation Quiz"
        ],
        "K-Nearest Neighbor Classifier": [
          "What is the k-nearest neighbor classifier?",
          "Concept of lazy learning",
          "Distance metrics - Euclidean-distance",
          "Bias and variance trade-off",
          "K-nearest neighbor implementation I",
          "K-nearest neighbor implementation II",
          "K-nearest neighbor implementation III",
          "Mathematical formulation of k-nearest neighbor classifier",
          "K-Nearest Neighbor Quiz"
        ],
        "Naive Bayes Classifier": [
          "What is the naive Bayes classifier?",
          "Naive Bayes classifier illustration",
          "Naive Bayes classifier implementation",
          "What is text clustering?",
          "Text clustering - inverse document frequency (TF-IDF)",
          "Naive Bayes example - clustering news",
          "Mathematical formulation of naive Bayes classifier",
          "Naive Bayes Classifier Quiz"
        ],
        "Support Vector Machines (SVMs)": [
          "What are Support Vector Machines (SVMs)?",
          "Linearly separable problems",
          "Non-linearly separable problems",
          "Kernel functions",
          "Support vector machine example I - simple",
          "Support vector machine example II - iris dataset",
          "Support vector machines example III - parameter tuning",
          "Support vector machine example IV - digit recognition",
          "Support vector machine example V - digit recognition",
          "Advantages and disadvantages",
          "Mathematical formulation of Support Vector Machines (SVMs)",
          "Support Vector Machines Quiz"
        ]
      },
      "requirements": [
        "Basic Python - we will use Panda and Numpy as well (we will cover the basics during implementations)"
      ],
      "description": "Interested in Machine Learning and Deep Learning ? Then this course is for you!\nThis course is about the fundamental concepts of machine learning, deep learning, reinforcement learning and machine learning. These topics are getting very hot nowadays because these learning algorithms can be used in several fields from software engineering to investment banking.\nIn each section we will talk about the theoretical background for all of these algorithms then we are going to implement these problems together. We will use Python with SkLearn, Keras and TensorFlow.\n### MACHINE LEARNING ###\nLinear Regression\nunderstanding linear regression model\ncorrelation and covariance matrix\nlinear relationships between random variables\ngradient descent and design matrix approaches\nLogistic Regression\nunderstanding logistic regression\nclassification algorithms basics\nmaximum likelihood function and estimation\nK-Nearest Neighbors Classifier\nwhat is k-nearest neighbour classifier?\nnon-parametric machine learning algorithms\nNaive Bayes Algorithm\nwhat is the naive Bayes algorithm?\nclassification based on probability\ncross-validation\noverfitting and underfitting\nSupport Vector Machines (SVMs)\nsupport vector machines (SVMs) and support vector classifiers (SVCs)\nmaximum margin classifier\nkernel trick\nDecision Trees and Random Forests\ndecision tree classifier\nrandom forest classifier\ncombining weak learners\nBagging and Boosting\nwhat is bagging and boosting?\nAdaBoost algorithm\ncombining weak learners (wisdom of crowds)\nClustering Algorithms\nwhat are clustering algorithms?\nk-means clustering and the elbow method\nDBSCAN algorithm\nhierarchical clustering\nmarket segmentation analysis\n### NEURAL NETWORKS AND DEEP LEARNING ###\nFeed-Forward Neural Networks\nsingle layer perceptron model\nfeed.forward neural networks\nactivation functions\nbackpropagation algorithm\nDeep Neural Networks\nwhat are deep neural networks?\nReLU activation functions and the vanishing gradient problem\ntraining deep neural networks\nloss functions (cost functions)\nConvolutional Neural Networks (CNNs)\nwhat are convolutional neural networks?\nfeature selection with kernels\nfeature detectors\npooling and flattening\nRecurrent Neural Networks (RNNs)\nwhat are recurrent neural networks?\ntraining recurrent neural networks\nexploding gradients problem\nLSTM and GRUs\ntime series analysis with LSTM networks\nTransformers\nword embeddings\nquery, key and value matrices\nattention and attention scores\ntraining a transformer\nChatGPT and transformers\nGenerative Adversarial Networks (GANs)\nwhat are GANs\ngenerator and discriminator\nhow to train a GAN\nimplementation of a simple GAN architecture\nNumerical Optimization (in Machine Learning)\ngradient descent algorithm\nstochastic gradient descent theory and implementation\nADAGrad and RMSProp algorithms\nADAM optimizer explained\nADAM algorithm implementation\nReinforcement Learning\nMarkov Decision Processes (MDPs)\nvalue iteration and policy iteration\nexploration vs exploitation problem\nmulti-armed bandits problem\nQ learning and deep Q learning\nlearning tic tac toe with Q learning and deep Q learning\nYou will get lifetime access to 150+ lectures plus slides and source codes for the lectures!\nThis course comes with a 30 day money back guarantee! If you are not satisfied in any way, you'll get your money back.\nSo what are you waiting for? Learn Machine Learning, Deep Learning in a way that will advance your career and increase your knowledge, all in a fun and practical way!\nThanks for joining the course, let's get started!",
      "target_audience": [
        "This course is meant for newbies who are not familiar with machine learning, deep learning, computer vision and reinforcement learning or students looking for a quick refresher"
      ]
    },
    {
      "title": "R Programming Complete Certification Training",
      "url": "https://www.udemy.com/course/r-programming-training/",
      "bio": "R concepts, coding examples. Data structure, loops, functions, packages, plots/charts, data/files, decision-making in R.",
      "objectives": [
        "Deep practical knowledge of R programming language",
        "Become a Data Scientist, Data Engineer, Data Analyst or Consultant",
        "Fundamentals and setup of R Language",
        "Get familiar with RStudio",
        "Variables and Data Types",
        "Input-Output Features in R",
        "Operators in R",
        "Data Structure in R",
        "Vectors, Lists and their application",
        "R Programs for Lists and Vectors in RStudio",
        "Matrix and application of Matrices in R with R Programs",
        "Arrays with R Programs for Arrays in RStudio",
        "Data Frames and R Programs for Data Frame in RStudio",
        "Factors, application of Factors, R Programs for Factors in RStudio",
        "Decision-making in R, types of decision-making statements with R Programs",
        "Loops in R, flowcharts and programs for loops in R",
        "Functions in R",
        "Strings in R",
        "Packages in R",
        "Data and File Management in R",
        "Plotting in R (graphs, charts, plots, histograms)",
        "Write complex R programs for practical industry scenarios"
      ],
      "course_content": {
        "Introduction to R Programming": [
          "Introduction to R Programming"
        ],
        "Setup of R Language": [
          "Setup of R Language"
        ],
        "Variables and Data Types": [
          "Variables and Data Types - part 1",
          "Variables and Data Types - part 2"
        ],
        "Input-Output Features": [
          "Input-Output Features - part 1",
          "Input-Output Features - part 2"
        ],
        "Operators in R": [
          "Operators in R - part 1",
          "Operators in R - part 2"
        ],
        "Vectors - Data Structure": [
          "Vectors - Data Structure - part 1",
          "Vectors - Data Structure - part 2"
        ],
        "List - Data Structure": [
          "List - Data Structure - part 1",
          "List - Data Structure - part 2"
        ],
        "Matrix - Data Structure": [
          "Matrix - Data Structure - part 1",
          "Matrix - Data Structure - part 2"
        ],
        "Arrays - Data Structure": [
          "Arrays - Data Structure - part 1",
          "Arrays - Data Structure - part 2"
        ],
        "Data Frame - Data Structure": [
          "Data Frame - Data Structure - part 1",
          "Data Frame - Data Structure - part 2",
          "Data Frame - Data Structure - part 3"
        ]
      },
      "requirements": [
        "Enthusiasm and determination to make your mark on the world!"
      ],
      "description": "A warm welcome to the R Programming course by Uplatz.\n\n\nR is a programming language that provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering) and graphical techniques, and is highly extensible.\nWhile there is something called the S language which is often the vehicle of choice for research in statistical methodology, on the other hand R provides an Open Source route to participation in that activity. R is nothing but an integrated suite of software facilities for data manipulation, calculation and graphical display. It includes an effective data handling and storage facility, a suite of operators for calculations on arrays, in particular matrices, a large, coherent, integrated collection of intermediate tools for data analysis, graphical facilities for data analysis and display either on-screen or on hardcopy, and a well-developed, simple and effective programming language which includes conditionals, loops, user-defined recursive functions and input and output facilities.\nR can be considered as an integrated version of a programming language and software environment for statistical analysis, graphics representation and reporting. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and is currently developed by the R Development Core Team. R is freely available under the GNU General Public License, and pre-compiled binary versions are provided for various operating systems like Linux, Windows and Mac. This programming language was named R, based on the first letter of first name of the two R authors (Robert Gentleman and Ross Ihaka), and partly a play on the name of the Bell Labs Language S.\nThis R Programming course by Uplatz is designed for software programmers, statisticians and data miners who are looking forward for developing statistical software using R programming. Even if you are a beginner, this R course is the perfect place to start. If you are trying to understand the R programming language as a beginner, this R Programming course will provide you enough understanding on almost all the concepts of the language from where you can take yourself to higher levels of expertise. This R tutorial will provide you an opportunity to take a deep-dive into R programming and build your R skills from scratch. To get the most out of the R programming training, you would need to practice as you proceed with the tutorials.\n\n\nAfter successful completion of the R Programming training course you will be able to:\nMaster the use of the R and RStudio interactive environment\nExpand R by installing R packages\nExplore and understand how to use the R documentation\nRead Structured Data into R from various sources\nUnderstand the different data types in R\nUnderstand the different data structures in R\nR programming constructs - variables, functions, string manipulation, loops, etc.\nConduct decision-making using R\nAble to do Data and file management in R\nPackages in R\nPlotting and Visualization in R\nand more...\n\n\nR Programming - Course Syllabus\n\n\n1. Fundamentals of R Language\nIntroduction to R\nHistory of R\nWhy R programming Language\nComparison between R and Python\nApplication of R\n\n\n2. Setup of R Language\nLocal Environment setup\nInstalling R on Windows\nInstalling R on Linux\nRStudio\nWhat is RStudio?\nInstallation of RStudio\nFirst Program - Hello World\n\n\n3. Variables and Data Types\nVariables in R\nDeclaration of variable\nVariable assignment\nFinding variable\nData types in R\nData type conversion\nR programs for Variables and Data types in RStudio\n\n\n4. Input-Output Features in R\nscan() function\nreadline() function\npaste() function\npaste0() function\ncat() function\nR Programs for implementing these functions in RStudio\n\n\n5. Operators in R\nArithmetic Operators\nRelational Operators\nLogical Operators\nAssignment Operators\nMiscellaneous Operators\nR Programs to perform various operations using operators in RStudio\n\n\n6. Data Structure in R (part-I)\nWhat is data structure?\nTypes of data structure\nVector\n- What is a vector in R?\n- Creating a vector\n- Accessing element of vector\n- Some more operations on vectors\n- R Programs for vectors in RStudio\nApplication of Vector in R\nList\n- What is a list in R?\n- Creating a list\n- Accessing element of list\n- Modifying element of list\n- Some more operations on list\nR Programs for list in RStudio\n\n\n7. Data Structure in R (part-II)\nMatrix or Matrices\n- What is matrix in R?\n- Creating a matrix\n- Accessing element of matrix\n- Modifying element of matrix\n- Matrix Operations\nR Programs for matrices in RStudio\nApplication of Matrices in R\nArrays\n- What are arrays in R?\n- Creating an array\n- Naming rows and columns\n- Accessing element of an array\n- Some more operations on arrays\nR Programs for arrays in RStudio\n\n\n8. Data Structure in R (part-III)\nData frame\n- What is a data frame in R?\n- Creating a data frame\n- Accessing element of data frame\n- Modifying element of data frame\n- Add the new element or component in data frame\n- Deleting element of data frame\n- Some more operations on data frame\nR Programs for data frame in RStudio\nFactors\n- Factors in R\n- Creating a factor\n- Accessing element of factor\n- Modifying element of factor\nR Programs for Factors in RStudio\nApplication of Factors in R\n\n\n9. Decision Making in R\nIntroduction to Decision making\nTypes of decision-making statements\nIntroduction, syntax, flowchart and programs for\n- if statement\n- if…else statement\n- if…else if…else statement\n- switch statement\n\n\n10. Loop control in R\nIntroduction to loops in R\nTypes of loops in R\n- for loop\n- while loop\n- repeat loop\n- nested loop\nbreak and next statement in R\nIntroduction, syntax, flowchart and programs for\n- for loop\n- while loop\n- repeat loop\n- nested loop\n\n\n11. Functions in R\nIntroduction to function in R\nBuilt-in Function\nUser-defined Function\nCreating a Function\nFunction Components\nCalling a Function\nRecursive Function\nVarious programs for functions in RStudio\n\n\n12. Strings in R\nIntroduction to string in R\n- Rules to write R Strings\n- Concatenate two or more strings in R\n- Find length of String in R\n- Extract Substring from a String in R\n- Changing the case i.e. Upper to lower case and lower to upper case\nVarious programs for String in RStudio\n\n\n13. Packages in R\nIntroduction to Packages in R\nGet the list of all the packages installed in RStudio\nInstallation of the packages\nHow to use the packages in R\nUseful R Packages for Data Science\nR program for package in RStudio\n\n\n14. Data and File Management in R\nGetting and Setting the Working Directory\nInput as CSV File\nAnalysing the CSV File\nWriting into a CSV File\nR programs to implement CSV file\n\n\n15. Plotting in R (Part-I)\nLine graph\nScatterplots\nPie Charts\n3D Pie Chart\n\n\n16. Plotting in R (Part-II)\nBar / line chart\nHistogram\nBox plot",
      "target_audience": [
        "R Developers & Data Developers",
        "Data Scientists - R, Python",
        "Newbies and beginners aspiring for a career in programming & statistical analysis",
        "Data Engineers and Statistical Analysts",
        "R & Python Programmers",
        "Technical & Analytics Consultants",
        "Anyone wishing to learn data science and machine learning",
        "Lead R Developers",
        "R Modelling Analysts",
        "Data Software Developers",
        "Financial and Marketing Analysts",
        "Software Engineers",
        "Web Application Developers",
        "Business Analysts and Consultants",
        "Data Science and Machine Learning enthusiasts"
      ]
    },
    {
      "title": "Machine Learning & Data Science: The Complete Visual Guide",
      "url": "https://www.udemy.com/course/visual-guide-to-machine-learning/",
      "bio": "Learn data science & machine learning topics with simple, step-by-step demos and user-friendly Excel models (NO code!)",
      "objectives": [
        "Build foundational machine learning & data science skills WITHOUT writing complex code",
        "Play with interactive, user-friendly Excel models to learn how machine learning techniques actually work",
        "Enrich datasets using feature engineering techniques like one-hot encoding, scaling and discretization",
        "Predict categorical outcomes using classification models like K-nearest neighbors, naïve bayes, and decision trees",
        "Build accurate forecasts and projections using linear and non-linear regression models",
        "Apply powerful techniques for clustering, association mining, outlier detection, and dimensionality reduction",
        "Learn how to select and tune models to optimize performance, reduce bias, and minimize drift",
        "Explore unique, hands-on case studies to simulate how machine learning can be applied to real-world cases"
      ],
      "course_content": {
        "Getting Started": [
          "Course Structure & Outline",
          "READ ME: Important Notes for New Students",
          "DOWNLOAD: Course Resources",
          "Setting Expectations"
        ],
        "Intro to the ML Landscape": [
          "Intro to Machine Learning",
          "When is ML the right fit?",
          "The Machine Learning Process",
          "The Machine Learning Landscape"
        ],
        "PART 1: QA & Data Profiling": [
          "Part 1: QA & Data Profiling"
        ],
        "Preliminary Data QA": [
          "Introduction",
          "Why QA?",
          "Variable Types",
          "Empty Values",
          "Range Calculations",
          "Count Calculations",
          "Left & Right Censored Data",
          "Table Structure",
          "CASE STUDY: Preliminary QA",
          "BEST PRACTICES: Preliminary QA",
          "QUIZ: Preliminary Data QA"
        ],
        "Univariate Profiling": [
          "Introduction",
          "Categorical Variables",
          "Discretization",
          "Nominal vs. Ordinal",
          "Categorical Distributions",
          "Numerical Variables",
          "Histograms & Kernel Densities",
          "CASE STUDY: Histograms",
          "Normal Distribution",
          "CASE STUDY: Normal Distribution",
          "Univariate Data Profiling",
          "Mode",
          "Mean",
          "Median",
          "Percentile",
          "Variance",
          "Standard Deviation",
          "Skewness",
          "BEST PRACTICES: Univariate Profiling",
          "QUIZ: Univariate Profiling"
        ],
        "Multivariate Profiling": [
          "Introduction",
          "Categorical-Categorical",
          "CASE STUDY: Heat Maps",
          "Categorical-Numerical",
          "Multivariate Kernel Densities",
          "Violin Plots",
          "Box Plots",
          "Limitations of Categorical Distributions",
          "Numerical-Numerical",
          "Correlation",
          "Correlation vs. Causation",
          "Visualizing Third Dimension",
          "CASE STUDY: Correlation",
          "BEST PRACTICES: Multivariate Profiling",
          "QUIZ: Multivariate Profiling",
          "Looking Ahead to Part 2"
        ],
        "PART 2: Classification Modeling": [
          "Part 2: Classification Modeling"
        ],
        "Intro to Classification": [
          "Supervised vs. Unsupervised Learning",
          "Classification vs. Regression",
          "RECAP: Key Concepts",
          "Classification 101",
          "Classification Workflow",
          "Feature Engineering",
          "Data Splitting",
          "Overfitting",
          "Intro to Classification"
        ],
        "Classification Models": [
          "Common Classification Models",
          "Intro to K-Nearest Neighbors (KNN)",
          "KNN Examples",
          "CASE STUDY: KNN",
          "Intro to Naïve Bayes",
          "Naïve Bayes | Frequency Tables",
          "Naïve Bayes | Conditional Probability",
          "CASE STUDY: Naïve Bayes",
          "Intro to Decision Trees",
          "Decision Trees | Entropy 101",
          "Entropy & Information Gain",
          "Decision Tree Examples",
          "Random Forests",
          "CASE STUDY: Decision Trees",
          "Intro to Logistic Regression",
          "Logistic Regression Example",
          "False Positives vs. False Negatives",
          "Logistic Regression Equation",
          "The Likelihood Function",
          "Multivariate Logistic Regression",
          "CASE STUDY: Logistic Regression",
          "Intro to Sentiment Analysis",
          "Cleaning Text Data",
          "\"Bag of Words\" Analysis",
          "CASE STUDY: Sentiment Analysis",
          "Classification Models"
        ],
        "Model Selection & Tuning": [
          "Intro to Selection & Tuning",
          "Hyperparameters",
          "Imbalanced Classes",
          "Confusion Matrix",
          "Accuracy, Precision & Recall",
          "Multi-class Confusion Matrix",
          "Multi-class Scoring",
          "Model Selection",
          "Model Drift",
          "Model Selection & Tuning",
          "Looking ahead to Part 3"
        ]
      },
      "requirements": [
        "This is a beginner-friendly course (no prior knowledge or math/stats background required)",
        "We'll use Microsoft Excel (Office 365) for some course demos, but participation is optional"
      ],
      "description": "This course is for everyday people looking for an intuitive, beginner-friendly introduction to the world of machine learning and data science.\n\n\nBuild confidence with guided, step-by-step demos, and learn foundational skills from the ground up. Instead of memorizing complex math or learning a new coding language, we'll break down and explore machine learning techniques to help you understand exactly how and why they work.\n\n\nFollow along with simple, visual examples and interact with user-friendly, Excel-based models to learn topics like linear and logistic regression, decision trees, KNN, naïve bayes, hierarchical clustering, sentiment analysis, and more – without writing a SINGLE LINE of code.\n\n\nThis course combines 4 best-selling courses from Maven Analytics into a single masterclass:\n\n\nPART 1: Univariate & Multivariate Profiling\nPART 2: Classification Modeling\nPART 3: Regression & Forecasting\nPART 4: Unsupervised Learning\n\n\nPART 1: Univariate & Multivariate Profiling\nIn Part 1 we’ll introduce the machine learning workflow and common techniques for cleaning and preparing raw data for analysis. We’ll explore univariate analysis with frequency tables, histograms, kernel densities, and profiling metrics, then dive into multivariate profiling tools like heat maps, violin & box plots, scatter plots, and correlation:\n\n\nSection 1: Machine Learning Intro & Landscape\nMachine learning process, definition, and landscape\n\n\nSection 2: Preliminary Data QA\nVariable types, empty values, range & count calculations, left/right censoring, etc.\n\n\nSection 3: Univariate Profiling\nHistograms, frequency tables, mean, median, mode, variance, skewness, etc.\n\n\nSection 4: Multivariate Profiling\nViolin & box plots, kernel densities, heat maps, correlation, etc.\n\n\nThroughout the course, we’ll introduce real-world scenarios to solidify key concepts and simulate actual data science and business intelligence cases. You’ll use profiling metrics to clean up product inventory data for a local grocery, explore Olympic athlete demographics with histograms and kernel densities, visualize traffic accident frequency with heat maps, and more.\n\n\nPART 2: Classification Modeling\nIn Part 2 we’ll introduce the supervised learning landscape, review the classification workflow, and address key topics like dependent vs. independent variables, feature engineering, data splitting and overfitting. From there we'll review common classification models like K-Nearest Neighbors (KNN), Naïve Bayes, Decision Trees, Random Forests, Logistic Regression and Sentiment Analysis, and share tips for model scoring, selection, and optimization:\n\n\nSection 1: Intro to Classification\nSupervised learning & classification workflow, feature engineering, splitting, overfitting & underfitting\n\n\nSection 2: Classification Models\nK-nearest neighbors, naïve bayes, decision trees, random forests, logistic regression, sentiment analysis\n\n\nSection 3: Model Selection & Tuning\nHyperparameter tuning, imbalanced classes, confusion matrices, accuracy, precision & recall, model drift\n\n\nYou’ll help build a simple recommendation engine for Spotify, analyze customer purchase behavior for a retail shop, predict subscriptions for an online travel company, extract sentiment from a sample of book reviews, and more.\n\n\nPART 3: Regression & Forecasting\nIn Part 3 we’ll introduce core building blocks like linear relationships and least squared error, and practice applying them to univariate, multivariate, and non-linear regression models. We'll review diagnostic metrics like R-squared, mean error, F-significance, and P-Values, then use time-series forecasting techniques to identify seasonality, predict nonlinear trends, and measure the impact of key business decisions using intervention analysis:\n\n\nSection 1: Intro to Regression\nSupervised learning landscape, regression vs. classification, prediction vs. root-cause analysis\n\n\nSection 2: Regression Modeling 101\nLinear relationships, least squared error, univariate & multivariate regression, nonlinear transformation\n\n\nSection 3: Model Diagnostics\nR-squared, mean error, null hypothesis, F-significance, T & P-values, homoskedasticity, multicollinearity\n\n\nSection 4: Time-Series Forecasting\nSeasonality, auto correlation, linear trending, non-linear models, intervention analysis\n\n\nYou’ll see how regression analysis can be used to estimate property prices, forecast seasonal trends, predict sales for a new product launch, and even measure the business impact of a new website design.\n\n\nPART 4: Unsupervised Learning\nIn Part 4 we’ll explore the differences between supervised and unsupervised machine learning and introduce several common unsupervised techniques, including cluster analysis, association mining, outlier detection and dimensionality reduction. We'll break down each model in simple terms and help you build an intuition for how they work, from K-means and apriori to outlier detection, principal component analysis, and more:\n\n\nSection 1: Intro to Unsupervised Machine Learning\nUnsupervised learning landscape & workflow, common unsupervised techniques, feature engineering\n\n\nSection 2: Clustering & Segmentation\nClustering basics, K-means, elbow plots, hierarchical clustering, dendograms\n\n\nSection 3: Association Mining\nAssociation mining basics, apriori, basket analysis, minimum support thresholds, markov chains\n\n\nSection 4: Outlier Detection\nOutlier detection basics, cross-sectional outliers, nearest neighbors, time-series outliers, residual distribution\n\n\nSection 5: Dimensionality Reduction\nDimensionality reduction basics, principle component analysis (PCA), scree plots, advanced techniques\n\n\nYou'll see how K-means can help identify customer segments, how apriori can be used for basket analysis and recommendation engines, and how outlier detection can spot anomalies in cross-sectional or time-series datasets.\n\n\n__________\n\n\nReady to dive in? Join today and get immediate, LIFETIME access to the following:\n\n\n9+ hours of on-demand video\nML Foundations ebook (350+ pages)\nDownloadable Excel project files\nExpert Q&A forum\n30-day money-back guarantee\n\n\nIf you're an analyst or aspiring data professional looking to build the foundation for a successful career in machine learning or data science, you've come to the right place.\n\n\nHappy learning!\n-Josh & Chris\n\n\n__________\nLooking for our full business intelligence stack? Search for \"Maven Analytics\" to browse our full course library, including Excel, Power BI, MySQL, Tableau and Machine Learning courses!\n\n\nSee why our courses are among the TOP-RATED on Udemy:\n\n\n\"Some of the BEST courses I've ever taken. I've studied several programming languages, Excel, VBA and web dev, and Maven is among the very best I've seen!\" Russ C.\n\n\n\"This is my fourth course from Maven Analytics and my fourth 5-star review, so I'm running out of things to say. I wish Maven was in my life earlier!\" Tatsiana M.\n\n\n\"Maven Analytics should become the new standard for all courses taught on Udemy!\" Jonah M.",
      "target_audience": [
        "Anyone looking to learn the foundations of machine learning through interactive, beginner-friendly demos",
        "Data Analysts or BI experts looking to transition into data science or build a fundamental understanding of machine learning",
        "R or Python users seeking a deeper understanding of the models and algorithms behind their code",
        "Excel users who want to learn and apply powerful tools for predictive analytics"
      ]
    },
    {
      "title": "Deep Learning Mastery",
      "url": "https://www.udemy.com/course/deep-learning-masterclass/",
      "bio": "Learn about Complete Life Cycle of a Deep Learning Project. Implement different Neural networks using Tensorflow.",
      "objectives": [
        "You will learn the complete life cycle of a Data Science Project with Machine Learning and Deep Learning.",
        "Learn about different Neural Networks like ANN, CNN and RNN.",
        "Learn about pandas, numpy, matplotlib, sklearn, tensorflow that are some of the most important python libraries used in Data Science, ML and DL.",
        "You will build practical projects like Gold Price Prediction, Image Class Prediction and Stock Price Prediction using different Neural networks."
      ],
      "course_content": {},
      "requirements": [
        "Basic understanding of Python Programming Language."
      ],
      "description": "Deep learning is a subfield of machine learning that is focused on building neural networks with many layers, known as deep neural networks. These networks are typically composed of multiple layers of interconnected \"neurons\" or \"units\", which are simple mathematical functions that process information. The layers in a deep neural network are organized in a hierarchical manner, with lower layers processing basic features and higher layers combining these features to represent more abstract concepts.\nDeep learning models are trained using large amounts of data and powerful computational resources, such as graphics processing units (GPUs). Training deep learning models can be computationally intensive, but the models can achieve state-of-the-art performance on a wide range of tasks, including image classification, natural language processing, speech recognition, and many others.\nThere are different types of deep learning models, such as feedforward neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), and many more. Each type of model is suited for a different type of problem, and the choice of model will depend on the specific task and the type of data that is available.\n\n\nIN THIS COURSE YOU WILL LEARN :\n\n\nComplete Life Cycle of Data Science Project.\nImportant Data Science Libraries like Pandas, Numpy, Matplotlib, Seaborn, sklearn etc...\nHow to choose appropriate Machine Learning or Deep Learning Model for your project\nMachine Learning Fundamentals\nRegression and Classification in Machine Learning\nArtificial Neural Networks (ANN)\nConvolutional Neural Networks (CNN)\nRecurrent Neural Networks (RNN)\nTensorflow and Keras\nDifferent projects like Gold Price Prediction, Stock Price Prediction, Image Classification etc...\n\n\nALL THE BEST !!!",
      "target_audience": [
        "Anyone who wants to get started with Deep Learning.",
        "Data Science and ML folks who want to learn about Neural Networks and Deep Learning."
      ]
    },
    {
      "title": "DeepFakes & Voice Cloning: Machine Learning The Easy Way",
      "url": "https://www.udemy.com/course/deepfakes-voice-cloning/",
      "bio": "Stable Diffusion, Midjourney, Generative AI, Fake News, +More for SEO, Internet Marketing, & Social Engineering!",
      "objectives": [
        "Learn how to change lips / mouth movements for foreign film dubs",
        "Voice cloning",
        "TTS (text-to-speech)",
        "Generate talking head videos with AI (artificial intelligence)",
        "Make anyone say whatever you want",
        "Change what someone says in a video",
        "Convert an image of a person into a moving video (motion copying)"
      ],
      "course_content": {},
      "requirements": [
        "Be comfortable with using a computer and Google Colab",
        "To run the code on other platforms (such as your own computer), you should know enough to setup your Python environment, install libraries, and modify filepaths as appropriate"
      ],
      "description": "*** Please note: Only the VIP version of the course covers Face Swap / Face Swapping. Contact the instructor if you are unsure of what that means!\n\n\nUnleash Your Creative Potential with DeepFakes Mastery\nUnlock the captivating power of DeepFakes and embark on a groundbreaking journey where reality and imagination intertwine. Dive into the world of voice cloning, seamless video manipulation, and awe-inspiring motion transfer. Welcome to our comprehensive DeepFakes Mastery course, where you'll acquire the skills to bring your wildest visions to life.\n\n\nWhy DeepFakes Mastery?\nImagine having the ability to create jaw-dropping videos that defy the limits of reality. With DeepFakes Mastery, you'll gain an arsenal of cutting-edge techniques that enable you to do:\n\n\n1. Voice Cloning: Give Life to Your Thoughts Experience the extraordinary power of voice cloning as you seamlessly mimic any voice you desire. Whether it's cloning your own voice or bringing iconic voices to life, our course will guide you through the intricacies of this mesmerizing technology. Take control of narratives, breathe life into animations, and craft unforgettable stories.\n2. Video Voiceover Manipulation: Shape Reality to Your Will Unleash your creativity by combining voices with videos, making it appear as though the person on screen is saying something entirely different. With our expert guidance, you'll master the art of manipulating video voiceovers, ensuring the perfect synchronization between audio and visuals. Craft compelling narratives, deliver impactful messages, or create entertaining content that captivates your audience.\n3. Motion Transfer: Redefine Possibilities Reimagine the limits of motion as you blend the movements from one video with an image of someone else. Our course empowers you with the skills to perform awe-inspiring motion transfers, forging seamless connections between various visual elements. Showcase your ingenuity in film production, breathe life into virtual characters, or elevate your video storytelling to new heights.\n\n\nWhat Sets Our Course Apart?\n1. Comprehensive Learning Structure Our course is meticulously designed to cater to both beginners and seasoned professionals. Starting with the fundamentals, we'll guide you through each step, ensuring a solid understanding of DeepFakes concepts. As you progress, you'll explore advanced techniques, sharpening your expertise and gaining mastery over this groundbreaking technology.\n2. Hands-On Practical Exercises Theory alone cannot unleash your creative potential. That's why our course focuses on practical, hands-on exercises, allowing you to apply your knowledge in real-world scenarios. Engage in immersive projects, tackle challenges, and transform theoretical concepts into tangible results. You'll witness your skills grow exponentially as you tackle each exercise head-on.\n3. Expert Instruction from Industry Veterans Learn from the best in the field. Our course is crafted and delivered by industry veterans, armed with years of experience in DeepFakes technology. Benefit from their wisdom, insider tips, and tried-and-true methods. They'll guide you through the complexities of DeepFakes, ensuring you develop a strong foundation and unleash your creative potential.\n4. Supportive Community Join a vibrant community of like-minded individuals who share your passion for DeepFakes. Collaborate, exchange ideas, and seek inspiration from fellow learners and professionals. Our inclusive community will be there to support you every step of the way, providing feedback, encouragement, and a platform to showcase your remarkable creations.\n5. Lifetime Access With our course, your learning journey never ends. Gain lifetime access to the course materials and stay updated with the latest advancements in DeepFakes, ensuring you remain at the cutting edge of this revolutionary field.\n\n\nUnleash Your Creativity Today!\nDon't miss this extraordinary opportunity to harness the captivating power of DeepFakes. Join our DeepFakes Mastery course today and embark on a transformative journey where reality meets imagination. Discover the secrets of DeepFakes Mastery and revolutionize your storytelling, entertainment, and creative endeavors.\n\n\nWhat You'll Gain:\nIn-depth knowledge of voice cloning techniques to bring any voice to life, opening up a world of possibilities for character creation, dubbing, and animated storytelling.\nThe ability to seamlessly manipulate video voiceovers, enabling you to alter the dialogue, create parodies, or convey messages with precision and impact.\nProficiency in motion transfer, allowing you to blend the movements of one video with an image of someone else, empowering you to create mesmerizing visual effects and captivating narratives.\n\n\nWho Can Benefit:\nFilmmakers and Video Content Creators: Enhance your storytelling capabilities, create unforgettable characters, and produce stunning visual effects that captivate your audience.\nVoiceover Artists: Expand your repertoire by mastering voice cloning techniques, opening up new opportunities for dubbing, character voiceovers, and narration.\nSocial Media Influencers: Stand out from the crowd with engaging and unique content, leveraging DeepFakes to create entertaining videos that go viral.\nAdvertising and Marketing Professionals: Push the boundaries of creativity, crafting compelling and memorable campaigns that leave a lasting impact.\nAnimation and Gaming Enthusiasts: Bring virtual characters to life with realistic voiceovers and seamlessly blended motions, taking your creations to the next level.\n\n\nYour Journey Starts Now:\n\n\nEnroll in DeepFakes Mastery: Sign up today and gain instant access to our comprehensive course materials, including step-by-step tutorials, practical exercises, and expert guidance.\nMaster the Fundamentals: Build a strong foundation by learning the core principles of DeepFakes technology, ensuring you have a solid understanding of the concepts and techniques involved.\nHands-On Projects: Dive into immersive exercises designed to challenge and inspire you. Apply your knowledge to real-world scenarios and witness your skills evolve with each project.\nAdvanced Techniques: Explore cutting-edge techniques and advanced applications of DeepFakes, elevating your creations to new heights and pushing the boundaries of what's possible.\nJoin the Community: Connect with fellow learners, share your progress, and collaborate on exciting projects. Our supportive community will be there to inspire, encourage, and provide valuable feedback.\nLifetime Access: Enjoy lifetime access to the course materials, ensuring you can revisit and reinforce your knowledge whenever you need.\n\n\nUnleash Your Creative Potential Today:\nDon't let the limitations of reality confine your imagination. Embrace the power of DeepFakes and embark on a transformative journey where you become the master of your digital domain. Enroll in DeepFakes Mastery now and unlock the secrets to creating awe-inspiring videos that leave a lasting impression. Start shaping reality with your creativity and redefine the art of visual storytelling. The possibilities are limitless!\n\n\nSuggested Prerequisites:\nProficiency with using a computer and Google Colab.\nTo run the code on other platforms (such as your own computer), you should know enough to setup your Python environment, install libraries, and modify filepaths as appropriate.",
      "target_audience": [
        "Filmmakers and Video Content Creators",
        "Voiceover Artists",
        "Social Media Influencers",
        "Advertising and Marketing Professionals",
        "Animation and Gaming Enthusiasts",
        "Anyone interested in Machine Learning, Data Science, Deep Learning, and Artificial Intelligence"
      ]
    },
    {
      "title": "Natural Language Processing with Deep Learning in Python",
      "url": "https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/",
      "bio": "Complete guide on deriving and implementing word2vec, GloVe, word embeddings, and sentiment analysis with recursive nets",
      "objectives": [
        "Understand and implement word2vec",
        "Understand the CBOW method in word2vec",
        "Understand the skip-gram method in word2vec",
        "Understand the negative sampling optimization in word2vec",
        "Understand and implement GloVe using gradient descent and alternating least squares",
        "Use recurrent neural networks for parts-of-speech tagging",
        "Use recurrent neural networks for named entity recognition",
        "Understand and implement recursive neural networks for sentiment analysis",
        "Understand and implement recursive neural tensor networks for sentiment analysis",
        "Use Gensim to obtain pretrained word vectors and compute similarities and analogies",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Outline, Review, and Logistical Things": [
          "Introduction, Outline, and Review",
          "How to Succeed in this Course",
          "Where to get the code / data for this course",
          "Preprocessed Wikipedia Data",
          "How to Open Files for Windows Users"
        ],
        "Beginner's Corner: Working with Word Vectors": [
          "What are vectors?",
          "What is a word analogy?",
          "Trying to find and assess word vectors using TF-IDF and t-SNE",
          "Pretrained word vectors from GloVe",
          "Pretrained word vectors from word2vec",
          "Text Classification with word vectors",
          "Text Classification in Code",
          "Using pretrained vectors later in the course",
          "Suggestion Box"
        ],
        "Review of Language Modeling and Neural Networks": [
          "Review Section Intro",
          "Bigrams and Language Models",
          "Bigrams in Code",
          "Neural Bigram Model",
          "Neural Bigram Model in Code",
          "Neural Network Bigram Model",
          "Neural Network Bigram Model in Code",
          "Improving Efficiency",
          "Improving Efficiency in Code",
          "Review Section Summary"
        ],
        "Word Embeddings and Word2Vec": [
          "Return of the Bigram",
          "CBOW",
          "Skip-Gram",
          "Hierarchical Softmax",
          "Negative Sampling",
          "Negative Sampling - Important Details",
          "Why do I have 2 word embedding matrices and what do I do with them?",
          "Word2Vec implementation tricks",
          "Word2Vec implementation outline",
          "Word2Vec in Code with Numpy",
          "Tensorflow or Theano - Your Choice!",
          "Word2Vec Tensorflow Implementation Details",
          "Word2Vec Tensorflow in Code",
          "Alternative to Wikipedia Data: Brown Corpus"
        ],
        "Word Embeddings using GloVe": [
          "GloVe Section Introduction",
          "Matrix Factorization for Recommender Systems - Basic Concepts",
          "Matrix Factorization Training",
          "Expanding the Matrix Factorization Model",
          "Regularization for Matrix Factorization",
          "GloVe - Global Vectors for Word Representation",
          "Recap of ways to train GloVe",
          "GloVe in Code - Numpy Gradient Descent",
          "GloVe in Code - Alternating Least Squares",
          "GloVe in Tensorflow with Gradient Descent",
          "Visualizing country analogies with t-SNE",
          "Hyperparameter Challenge",
          "Training GloVe with SVD (Singular Value Decomposition)"
        ],
        "Unifying Word2Vec and GloVe": [
          "Pointwise Mutual Information - Word2Vec as Matrix Factorization",
          "PMI in Code"
        ],
        "Using Neural Networks to Solve NLP Problems": [
          "Parts-of-Speech (POS) Tagging",
          "How can neural networks be used to solve POS tagging?",
          "Parts-of-Speech Tagging Baseline",
          "Parts-of-Speech Tagging Recurrent Neural Network in Theano",
          "Parts-of-Speech Tagging Recurrent Neural Network in Tensorflow",
          "How does an HMM solve POS tagging?",
          "Parts-of-Speech Tagging Hidden Markov Model (HMM)",
          "Named Entity Recognition (NER)",
          "Comparing NER and POS tagging",
          "Named Entity Recognition Baseline",
          "Named Entity Recognition RNN in Theano",
          "Named Entity Recognition RNN in Tensorflow",
          "Hyperparameter Challenge II"
        ],
        "Recursive Neural Networks (Tree Neural Networks)": [
          "Recursive Neural Networks Section Introduction",
          "Sentences as Trees",
          "Data Description for Recursive Neural Networks",
          "What are Recursive Neural Networks / Tree Neural Networks (TNNs)?",
          "Building a TNN with Recursion",
          "Trees to Sequences",
          "Recursive Neural Tensor Networks",
          "RNTN in Tensorflow (Tips)",
          "RNTN in Tensorflow (Code)",
          "Recursive Neural Network in TensorFlow with Recursion"
        ],
        "Theano and Tensorflow Basics Review": [
          "(Review) Theano Basics",
          "(Review) Theano Neural Network in Code",
          "(Review) Tensorflow Basics",
          "(Review) Tensorflow Neural Network in Code"
        ],
        "Appendix / FAQ Finale": [
          "What is the Appendix?"
        ]
      },
      "requirements": [
        "Install Numpy, Matplotlib, Sci-Kit Learn, and Theano or TensorFlow (should be extremely easy by now)",
        "Understand backpropagation and gradient descent, be able to derive and code the equations on your own",
        "Code a recurrent neural network from basic primitives in Theano (or Tensorflow), especially the scan function",
        "Code a feedforward neural network in Theano (or Tensorflow)",
        "Helpful to have experience with tree algorithms"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nIn this course we are going to look at NLP (natural language processing) with deep learning.\nPreviously, you learned about some of the basics, like how many NLP problems are just regular machine learning and data science problems in disguise, and simple, practical methods like bag-of-words and term-document matrices.\nThese allowed us to do some pretty cool things, like detect spam emails, write poetry, spin articles, and group together similar words.\nIn this course I’m going to show you how to do even more awesome things. We’ll learn not just 1, but 4 new architectures in this course.\nFirst up is word2vec.\nIn this course, I’m going to show you exactly how word2vec works, from theory to implementation, and you’ll see that it’s merely the application of skills you already know.\nWord2vec is interesting because it magically maps words to a vector space where you can find analogies, like:\nking - man = queen - woman\nFrance - Paris = England - London\nDecember - Novemeber = July - June\nFor those beginners who find algorithms tough and just want to use a library, we will demonstrate the use of the Gensim library to obtain pre-trained word vectors, compute similarities and analogies, and apply those word vectors to build text classifiers.\n\n\nWe are also going to look at the GloVe method, which also finds word vectors, but uses a technique called matrix factorization, which is a popular algorithm for recommender systems.\nAmazingly, the word vectors produced by GLoVe are just as good as the ones produced by word2vec, and it’s way easier to train.\nWe will also look at some classical NLP problems, like parts-of-speech tagging and named entity recognition, and use recurrent neural networks to solve them. You’ll see that just about any problem can be solved using neural networks, but you’ll also learn the dangers of having too much complexity.\nLastly, you’ll learn about recursive neural networks, which finally help us solve the problem of negation in sentiment analysis. Recursive neural networks exploit the fact that sentences have a tree structure, and we can finally get away from naively using bag-of-words.\nAll of the materials required for this course can be downloaded and installed for FREE. We will do most of our work in Numpy, Matplotlib, and Theano. I am always available to answer your questions and help you along your data science journey.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\nSee you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus (taking derivatives)\nmatrix addition, multiplication\nprobability (conditional and joint distributions)\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\nneural networks and backpropagation, be able to derive and code gradient descent algorithms on your own\nCan write a feedforward neural network in Theano or TensorFlow\nCan write a recurrent neural network / LSTM / GRU in Theano or TensorFlow from basic primitives, especially the scan function\nHelpful to have experience with tree algorithms\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students and professionals who want to create word vector representations for various NLP tasks",
        "Students and professionals who are interested in state-of-the-art neural network architectures like recursive neural networks",
        "SHOULD NOT: Anyone who is not comfortable with the prerequisites."
      ]
    },
    {
      "title": "Generative AI Unleashed: Exploring Possibilities and Future",
      "url": "https://www.udemy.com/course/generative-ai-techniques-applications-and-ethics/",
      "bio": "Understanding the Power and Impact of artificial Intelligence in Generating Content",
      "objectives": [
        "Gain a solid understanding of Generative AI principles and techniques to create intelligent, data-driven generative models.",
        "Learn the principles and techniques of Generative AI to create intelligent, data-driven generative models.",
        "Demonstrate proficiency in evaluating and selecting appropriate Generative AI techniques based on specific project requirements and constraints.",
        "Explore how Generative AI can be applied to diverse fields, such as art, healthcare, gaming, and business.",
        "Develop a critical understanding of the ethical considerations, privacy concerns, and societal impacts of Generative AI technology.",
        "Understand the key techniques in Generative AI, such as Bayesian models, autoregressive models, VAEs, GANs, and transformers, to solve real-world problems.",
        "Stay up-to-date on the latest advancements and future trends in Generative AI to enable continuous learning and adaptation in this dynamic field."
      ],
      "course_content": {
        "Introduction": [
          "The Concept of Generative AI",
          "Importance and Potential of Generative AI",
          "A Brief History of Generative AI",
          "Fundamental Concepts in Generative AI",
          "Knowledge check"
        ],
        "Techniques in Generative AI": [
          "Bayesian Models",
          "Autoregressive Models",
          "Variational Autoencoders (VAEs)",
          "Generative Adversarial Networks (GANs)",
          "Transformers in Generative AI",
          "Role of Reinforcement Learning in Generative AI",
          "Knowledge check"
        ],
        "Advances in Generative AI Techniques": [
          "Evolution and Progress of GANs, VAEs, and Transformers",
          "New Techniques and Approaches in Generative AI",
          "Knowledge check"
        ],
        "Applications of Generative AI": [
          "Applications of Generative AI in Art, healthcare, gaming, and Business World",
          "Emerging Applications of Generative AI",
          "Knowledge check"
        ],
        "Ethical and Societal Considerations": [
          "Ethical Implications of AI-Generated Content",
          "The Deepfake Phenomenon",
          "Knowledge check"
        ],
        "Future Trends in Generative AI": [
          "Latest Developments and Emerging Trends in Generative AI",
          "The Intersection of Generative AI and Other AI Disciplines",
          "Conclusion",
          "Knowledge check"
        ]
      },
      "requirements": [
        "A willingness to engage in self-directed learning and explore complex topics in Generative AI.",
        "Basic understanding of machine learning principles and concepts.",
        "Comfortable with mathematical concepts like probability and statistics."
      ],
      "description": "Unleash your creativity and explore the world of Generative AI in this comprehensive course. From fundamental principles to cutting-edge techniques, you'll gain hands-on experience with data-driven models that open up new realms of innovation.\nIn the first part of the course, dive into the fundamental principles of Generative AI, including Bayesian models and autoregressive models. Discover the power of Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to generate realistic and imaginative content. Gain insights into the role of reinforcement learning and transformers in driving advancements in Generative AI.\nNext, explore a range of real-world applications where Generative AI is making its mark. From art and healthcare to gaming and the business world, witness how Generative AI is transforming industries with its potential for creative content generation, drug discovery, procedural content generation, advertising, and marketing.\nFinally, delve into the ethical and societal considerations surrounding Generative AI. Understand the implications of AI-generated content, privacy concerns, and the rise of deepfakes. Get a glimpse into the future trends and emerging techniques that will shape the landscape of Generative AI.\nJoin us on this captivating journey to become a leader in the world of Generative AI, and unlock your creative potential through intelligent algorithms.",
      "target_audience": [
        "Data Scientists and Machine Learning Engineers who want to expand their skill set and delve into the realm of generative models.",
        "AI Researchers and Practitioners seeking to understand the latest advancements and applications of Generative AI.",
        "Computer Science and Engineering students who want to specialize in the field of AI and gain hands-on experience with generative models.",
        "Professionals in the fields of art, healthcare, gaming, advertising, and marketing, who wish to leverage Generative AI for innovative and creative solutions.",
        "Decision-makers, managers, and entrepreneurs who want to gain a comprehensive understanding of Generative AI to make informed strategic decisions.",
        "AI enthusiasts and lifelong learners who are passionate about staying updated on cutting-edge AI technologies and exploring new frontiers."
      ]
    },
    {
      "title": "Power BI - Mastering DAX for Power BI",
      "url": "https://www.udemy.com/course/mastering-dax-for-advanced-powerbi-reporting/",
      "bio": "Demystifying DAX for Advanced Power BI Reporting",
      "objectives": [
        "Understand how the DAX engine works",
        "Understand Calculated Tables, Columns, Measures and when to use what",
        "Simplified understanding of DAX Evaluation Context, Row, Filter and Context Transition",
        "Understand the most used DAX functions",
        "Write Time Intelligence formulas",
        "Understand when to use Iterator functions"
      ],
      "course_content": {
        "Introduction & Overview of DAX": [
          "Introduction",
          "Pre-requisites and Outline",
          "Quick Overview of DAX",
          "Importance of DAX in Power BI",
          "Understanding the Case Study Data",
          "Understanding the Case Study Data Model",
          "Quiz 1",
          "Introduction to Calculated Columns",
          "Introduction to Calculated Tables",
          "Introduction to Measures",
          "Introduction to Row Level Security (RLS)",
          "Quiz 2",
          "Test your understanding of Intro to DAX"
        ],
        "Understanding DAX Pillars and Functions": [
          "Pillars of DAX",
          "Understanding Model Cardinality",
          "Understanding Cross Filtering Direction",
          "Introduction to DAX Functions",
          "Using the IF() function",
          "Using the SWITCH() Function",
          "Fetching columns from another table with RELATED() Function",
          "Introduction to ALL() and ALLEXCEPT() Functions",
          "Introduction to VALUES() and DISTINCT() Functions",
          "Quiz 3",
          "Test your understanding of IF and SWITCH"
        ],
        "Understanding how the DAX Engine works": [
          "Introduction to Evaluation Context",
          "Understanding Filter Context",
          "How DAX work behind the scenes",
          "Introduction to Aggregators and Iterators",
          "Understanding Iterators with SUMX",
          "Aggregating on a Filtered Table",
          "How DAX computes Filtered aggregates (part 1)",
          "How DAX computes Filtered aggregates (part 2)",
          "How to expand filters with ALL() Function",
          "Quiz 4",
          "Test your understanding of SUMX, FILTER and RELATED"
        ],
        "Manipulating Filters Context with CALCULATE() Function": [
          "Introduction to CALCULATE() function",
          "Introduction to Context Transition",
          "Understanding Row Context within a Measure",
          "Understanding Context Transition within a Measure",
          "Introduction to CALCULATE() function in Measures",
          "Using CALCULATE() to filter multiple columns",
          "Using OR Operator to filter multiple values from the same column",
          "Using the IN Operator as an alternative to OR to filter values matching lists",
          "Understanding the four types of Filtering",
          "Using ALL() as a filter argument in CALCULATE() expression",
          "Understanding how ALL() filters columns in a DAX Measure (part 1)",
          "Understanding how ALL() filters columns in a DAX Measure (part 2)",
          "Understanding Variables",
          "Activating Filters between tables using CALCULATE() and CROSSFILTER Functions",
          "Test your understanding of CALCULATE"
        ],
        "Time Intelligence Calculations in DAX": [
          "Overview of Time Intelligence in DAX",
          "Creating a Date Table with CALENDARAUTO() Function",
          "Extracting Year column from a date with FORMAT() or YEAR() Functions",
          "Creating and Sorting a Month Column using FORMAT() and MONTH() Functions",
          "Setting up table to test Time Intelligence Calculations",
          "Calculating Previous Month Values with CALCULATE() and PREVIOUSMONTH() Functions",
          "Expanding PREVIOUSMONTH() Function with VALUES() and DATEADD() Functions",
          "Expanding PREVIOUSMONTH() Function with VALUES() and DATEADD() Functions (part2)",
          "Calculating Year to Date Values with CALCULATE() and DATESYTD()",
          "Expanding DATESYTD Function with FILTER, DATEADD and DATESBETWEEN",
          "Using TOTALYTD() to calculate Year to Date Values",
          "Calculating values for the same period in a previous year",
          "Expanding the SAMEPERIODLASTYEAR() Function with VALUES() and DATEADD()",
          "Test your understanding of Time Intelligence Functions"
        ],
        "Other DAX Calculations": [
          "Understanding RANKX() Function",
          "Creating Top N Visuals using the Filters Pane",
          "How to TOPN() function works",
          "Calculating Top N Measures with CALCULATE() and TOPN() Functions",
          "Adding a Parameter Table to make Top N Measure dynamic",
          "Test your understanding of TOPN and RANKX functions",
          "Bonus Lectures"
        ]
      },
      "requirements": [
        "Students should be familiar with Power BI reporting already.",
        "Some Excel formulas knowledge is an advantage but not necessary.",
        "Students should have the latest version of Power BI for practice"
      ],
      "description": "Background\n\n\nDAX is the formula language of Power BI, Excel Power Pivot and SSAS Tabular. The formula language simply unlocks advanced capabilities for reporting in Power BI.\nHowever, DAX is not an easy language to learn. Mastering it requires a proper understanding of the rudiments and a lot of constant practice.\nIn this course, I have used a very explanatory approach to ensure that you will understand, learn and master DAX faster than most courses where formulas ae being copied from a text editor without proper explanation of what is happening behind it.\nAs a Microsoft Most Valuable Professional for the Data Platform and a Microsoft Certified Trainer, I will be able to answer your DAX questions as you continue to develop your skills after this course.\n\n\nWhat you will learn.\n\n\nThis course is suitable for all levels of Power BI users. You will learn from the introductory concepts to build a solid background. By the time you complete the course, you should have a better understanding of:\n1. DAX usage in Power BI (Calculated Columns, Calculated Tables, Measures and Row Level Security)\n2. The most important and used DAX functions\n3. How the DAX Engine works behind the scenes\n4. The CALCULATE function in DAX\n5. Row Context, Filter Context and Context Transition\n6. Time Intelligence functions and calculations\n7. Other important DAX functions like RANKX and TOPN\n\n\nAssignments.\nBecause DAX is about doing , and beyond looking, I am sorry to say that I included assignments for you. Don't worry if you stumble here and there while completing the assignments, it's completely normal. You will just have to watch the parts of the videos again till you are able to complete them. It's all for good.\nThe more DAX you write, the better you get over time. So this is a good opportunity to write a lot of them.",
      "target_audience": [
        "Intermediate to Advanced Power BI Users"
      ]
    },
    {
      "title": "R Programming for Simulation and Monte Carlo Methods",
      "url": "https://www.udemy.com/course/r-programming-for-simulation-and-monte-carlo-methods/",
      "bio": "Learn to program statistical applications and Monte Carlo simulations with numerous \"real-life\" cases and R software.",
      "objectives": [
        "Use R software to program probabilistic simulations, often called Monte Carlo simulations.",
        "Use R software to program mathematical simulations and to create novel mathematical simulation functions.",
        "Use existing R functions and understand how to write their own R functions to perform simulated inference estimates, including likelihoods and confidence intervals, and to model other cases of stochastic simulation.",
        "Be able to generate different different families (and moments) of both discrete and continuous random variables.",
        "Be able to simulate parameter estimation, Monte-Carlo Integration of both continuous and discrete functions, and variance reduction techniques."
      ],
      "course_content": {
        "Review of Vectors, Matrices, Lists and Functions": [
          "Course Introduction",
          "Install R and RStudio",
          "Review: Vectors, Matrices, Lists (part 1)",
          "Review: Vectors, Matrices, Lists (part 2)",
          "Sequences and Replications (part 1)",
          "Sort and Order",
          "Using Matrices (part 2)",
          "Sequences and Replications (part 2)",
          "Creating a Matrix (part 1)",
          "List Structures and Horsekicks (part 1)",
          "Dpois() Function and Horsekicks (part 2)",
          "Sampling from a Dataframe",
          "Section 1 Exercises"
        ],
        "Simulation Examples: Tossing a Coin": [
          "R Expressions Exercises Answers (part 1)",
          "R Expressions Exercises Answers (part 2)",
          "Introduction to Simulation: A Game of Tossing a Coin (part 1)",
          "Introduction to Simulation: A Game of Tossing a Coin (part 2)",
          "Write a Simulation Function (part 1)",
          "Write a Simulation Function (part 2)",
          "Continue Coin Tossing Simulation (part 3)",
          "Continue Coin Tossing Simulation (part 4)"
        ],
        "Simulation Examples: Returning Checked Hats": [
          "Random Permutations: Hat Problem (part 1)",
          "Random Permutations: Hat Problem (part 2 )",
          "Random Permutations: Hat Problem (part 3)",
          "Random Permutations: Hat Problem (part 4)",
          "Random Permutations: Hat Problem (part 5)",
          "Random Permutations: Hat Problem (part 6)",
          "Checking Hats Exercise"
        ],
        "Simulation Examples: Collecting Baseball Cards and \"Streaky\" Behavior": [
          "Solution to Checking Hats Exercise",
          "Collecting Baseball Cards Simulation (part 1)",
          "Collecting Baseball Cards Simulation (part 2)",
          "Collecting Baseball Cards Simulation (part 3)",
          "Collecting Baseball Cards Simulation (part 4)",
          "Collecting Quarters Exercise",
          "Collecting State Quarters Exercise Solution",
          "\"Streaky\" Baseball Batting Behavior (part 1)",
          "\"Streaky\" Baseball Batting Behavior (part 2)",
          "\"Streaky\" Baseball Batting Behavior (part 3)",
          "\"Streaky\" Behavior Exercise"
        ],
        "Monte Carlo Methods for Inference": [
          "Solution to \"Streaky\" Behavior Exercise",
          "Using Monte Carlo Simulation to Estimate Inference",
          "Sleepless in Seattle (part 1)",
          "Sleepless in Seattle (part 2)",
          "Applying Monte Carlo Methods to Inference (part 1)",
          "Applying Monte Carlo Methods to Inference (part 2)",
          "Applying Monte Carlo Methods to Inference (part 3)",
          "Applying Monte Carlo Methods to Inference (part 4)",
          "Applying Monte Carlo Methods to Inference (part 5)",
          "Comparing Estimators: The Taxi Problem (part 1)",
          "Comparing Estimators: The Taxi Problem (part 2)",
          "Late to Class Again ? Exercise"
        ],
        "Stochastic Simulation and Random Variable Generation": [
          "Late to Class Again Exercise Solution",
          "What is Stochastic Simulation ?",
          "Simulation and Random Variable Generation (part 1)",
          "Simulation and Random Variable Generation (part 2)",
          "Simulation and Random Variable Generation (part 3)",
          "Simulating Discrete Random Variables (part 1)",
          "Simulating Discrete Random Variables (part 2)",
          "Simulating Discrete Random Variables (part 3)",
          "Root Finding: Newton-Raphson Technique (part 1)",
          "Root Finding: Newton-Raphson Technique (part 2)",
          "Create Random Variables Exercise"
        ],
        "Inverse and General Transforms": [
          "Create Random Variables Exercise Solution (part 1)",
          "Create Random Variables Exercise Solution (part 2)",
          "Inverse Transforms (part 1)",
          "Inverse Transforms (part 2)",
          "General Transformations (part 1)",
          "General Transformations (part 2)",
          "Accept-Reject Method (part 1)",
          "Accept-Reject Method (part 2)",
          "Accept-Reject Methods (part 3)",
          "Random Variable (Poisson) Exercise 2"
        ],
        "Simulating Numerical Integration": [
          "Random Variable Exercise Solution (part 1)",
          "Random Variable Exercise Solution (part 2)",
          "Introduction to Simulating Numerical Integration (part 1)",
          "Introduction to Simulating Numerical Integration (part 2)",
          "Simpson's Rule for Trapezoidal Approximation",
          "Simulating Numerical Integration (part 1)",
          "Simulating Numerical Integration (part 2)",
          "More on Simpson's Rule",
          "Simpson's Rule with phi Functions",
          "Phi Functions Exercises",
          "Hit and Miss (part 1)",
          "Hit and Miss (part 2)"
        ],
        "Permutation Tests": [
          "Phi Functions (Numerical Integration) Exercise Solution",
          "Permutation Tests on a Distribution: Chckwts Example (part 1)",
          "Permutation Tests on a Distribution: Chckwts Example (part 2)",
          "Permutation Tests on a Distribution: Chckwts Example (part 3)",
          "Permutation Tests on a Distribution: Chckwts Example (part 4)",
          "Finish Permutation Tests and an Exercise"
        ],
        "Simulation Case Studies: Seed Dispersal": [
          "Solution to Permutation Tests Exercises",
          "Seed Dispersal Case Study: Object Orientation",
          "Seed Dispersal Case: Creating Classes and Functions (part 1)",
          "Seed Dispersal Case: Creating Classes and Functions (part 2)",
          "Seed Dispersal Case (part 1)",
          "Seed Dispersal Case (part 2)",
          "Seed Dispersal Case (part 3)",
          "Seed Dispersal Case (part 4)",
          "Finish Seed Dispersal Case"
        ]
      },
      "requirements": [
        "Students will need to install the popular no-cost R Console and RStudio software (instructions provided)."
      ],
      "description": "R Programming for Simulation and Monte Carlo Methods focuses on using R software to program probabilistic simulations, often called Monte Carlo Simulations. Typical simplified \"real-world\" examples include simulating the probabilities of a baseball player having a 'streak' of twenty sequential season games with 'hits-at-bat' or estimating the likely total number of taxicabs in a strange city when one observes a certain sequence of numbered cabs pass a particular street corner over a 60 minute period. In addition to detailing half a dozen (sometimes amusing) 'real-world' extended example applications, the course also explains in detail how to use existing R functions, and how to write your own R functions, to perform simulated inference estimates, including likelihoods and confidence intervals, and other cases of stochastic simulation. Techniques to use R to generate different characteristics of various families of random variables are explained in detail. The course teaches skills to implement various approaches to simulate continuous and discrete random variable probability distribution functions, parameter estimation, Monte-Carlo Integration, and variance reduction techniques. The course partially utilizes the Comprehensive R Archive Network (CRAN) spuRs package to demonstrate how to structure and write programs to accomplish mathematical and probabilistic simulations using R statistical software.",
      "target_audience": [
        "You do NOT need to be experienced with R software and you do NOT need to be an experienced programmer.",
        "Course is good for practicing quantitative analysis professionals.",
        "Course is good for graduate students seeking research data and scenario analysis skills.",
        "Anyone interested in learning more about programming statistical applications with R software would benefit from this course."
      ]
    },
    {
      "title": "SQL & Database Design A-Z™: Learn MS SQL Server + PostgreSQL",
      "url": "https://www.udemy.com/course/sqldatabases/",
      "bio": "Learn Both SQL Server & PostgreSQL By Doing. Enhance Your Data Analytics Career With Real World Data Science Exercises",
      "objectives": [
        "Create basic SQL Queries",
        "Create advanced SQL Queries",
        "Create Left, Right, Inner and Full Outer joins",
        "Create new tables, alter existing tables in Databases",
        "Normalize Databases",
        "Understand database design",
        "Understand first, second and third normal form schemas"
      ],
      "course_content": {},
      "requirements": [
        "A basic knowledge of computers"
      ],
      "description": "Are you interested in a career in Data Science or Data Analytics?\nIn that case, inevitably you are going to encounter databases in your work.\nBut how do you interact with databases?\nThe answer is simple: SQL\nSQL stands for Structured Query Language and this is one of the main tools used to organize databases, input data into them and extract it on request.\nIn this course you will learn how to create queries in a popular variation of SQL called PostgreSQL.\nAnd even if at your workplace you are using a different variation (e.g. Oracle, SQL Server or MySQL), you will find that the skills you learn in this course are easily transferable.\nBut there are many SQL courses out there, so the question is:\nWhat makes this course stand out?\nThe unique advantage of this course is that in addition to learning SQL you will also master the concepts of\nDatabase Design\n.\nWe will cover off topics such as:\n\n- OLAP vs OLTP databases (Online Analytics Processing & Online Transaction Processing): you will understand exactly how and why the designs of these two types of Databases differ\n\n- Normalization of Databases: we will show you the theory behind normalization AND together we will practice how to normalize a Database step-by-step\nWhy is that important?\nKnowing how databases are designed is not a compulsory skill to have for a Data Scientist / Analyst. However, it's a HUGE added benefit.\nThese skills will allow you to better interact with databases and derive results and extract insights from your data faster.\nThis course is designed with the Data Scientists and Analysts in mind, so if you want to propel your Data Science career, then this course is for you!\nWe look forward to seeing you inside,\nKirill & Ilya",
      "target_audience": [
        "Anybody who wants to learn PostgreSQL",
        "Anybody who wants to better understand how databases work",
        "Anybody who wants to enhance their Data Science career"
      ]
    },
    {
      "title": "Deep Learning: Advanced Computer Vision (GANs, SSD, +More!)",
      "url": "https://www.udemy.com/course/advanced-computer-vision/",
      "bio": "VGG, ResNet, Inception, SSD, RetinaNet, Neural Style Transfer, GANs +More in Tensorflow, Keras, and Python",
      "objectives": [
        "Understand and apply transfer learning",
        "Understand and use state-of-the-art convolutional neural nets such as VGG, ResNet and Inception",
        "Understand and use object detection algorithms like SSD",
        "Understand and apply neural style transfer",
        "Understand state-of-the-art computer vision topics",
        "Class Activation Maps",
        "GANs (Generative Adversarial Networks)",
        "Object Localization Implementation Project",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {},
      "requirements": [
        "Know how to build, train, and use a CNN using some library (preferably in Python)",
        "Understand basic theoretical concepts behind convolution and neural networks",
        "Decent Python coding skills, preferably in data science and the Numpy Stack"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis is one of the most exciting courses I’ve done and it really shows how fast and how far deep learning has come over the years.\nWhen I first started my deep learning series, I didn’t ever consider that I’d make two courses on convolutional neural networks.\nI think what you’ll find is that, this course is so entirely different from the previous one, you will be impressed at just how much material we have to cover.\nLet me give you a quick rundown of what this course is all about:\nWe’re going to bridge the gap between the basic CNN architecture you already know and love, to modern, novel architectures such as VGG, ResNet, and Inception (named after the movie which by the way, is also great!)\nWe’re going to apply these to images of blood cells, and create a system that is a better medical expert than either you or I. This brings up a fascinating idea: that the doctors of the future are not humans, but robots.\nIn this course, you’ll see how we can turn a CNN into an object detection system, that not only classifies images but can locate each object in an image and predict its label.\nYou can imagine that such a task is a basic prerequisite for self-driving vehicles. (It must be able to detect cars, pedestrians, bicycles, traffic lights, etc. in real-time)\nWe’ll be looking at a state-of-the-art algorithm called SSD which is both faster and more accurate than its predecessors.\nAnother very popular computer vision task that makes use of CNNs is called neural style transfer.\nThis is where you take one image called the content image, and another image called the style image, and you combine these to make an entirely new image, that is as if you hired a painter to paint the content of the first image with the style of the other. Unlike a human painter, this can be done in a matter of seconds.\nI will also introduce you to the now-famous GAN architecture (Generative Adversarial Networks), where you will learn some of the technology behind how neural networks are used to generate state-of-the-art, photo-realistic images.\nCurrently, we also implement object localization, which is an essential first step toward implementing a full object detection system.\nI hope you’re excited to learn about these advanced applications of CNNs, I’ll see you in class!\n\n\n\nAWESOME FACTS:\nOne of the major themes of this course is that we’re moving away from the CNN itself, to systems involving CNNs.\nInstead of focusing on the detailed inner workings of CNNs (which we've already done), we'll focus on high-level building blocks. The result? Almost zero math.\nAnother result? No complicated low-level code such as that written in Tensorflow, Theano, or PyTorch (although some optional exercises may contain them for the very advanced students). Most of the course will be in Keras which means a lot of the tedious, repetitive stuff is written for you.\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nKnow how to build, train, and use a CNN using some library (preferably in Python)\nUnderstand basic theoretical concepts behind convolution and neural networks\nDecent Python coding skills, preferably in data science and the Numpy Stack\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students and professionals who want to take their knowledge of computer vision and deep learning to the next level",
        "Anyone who wants to learn about object detection algorithms like SSD and YOLO",
        "Anyone who wants to learn how to write code for neural style transfer",
        "Anyone who wants to use transfer learning",
        "Anyone who wants to shorten training time and build state-of-the-art computer vision nets fast"
      ]
    },
    {
      "title": "AWS Essentials: A Complete Beginner's Guide",
      "url": "https://www.udemy.com/course/aws-essentials-a-complete-beginners-guide/",
      "bio": "Build a Strong Foundation in AWS Services",
      "objectives": [
        "Benefits of using AWS",
        "AWS services overview",
        "Navigating the AWS console",
        "Creating instances, storage volumes, and security groups",
        "Cloud Service Models SaaS, PaaS, IaaS, FaaS",
        "AWS compute services",
        "Amazon e2c",
        "AWS Security Services",
        "DevOps Services"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Introduction to AWS (Amazon Web Services)",
          "AWS Cloud Computing Services",
          "How Cloud Computing Helps in AWS",
          "What is Cloud Computing",
          "Cloud Service Models SaaS, PaaS, IaaS, FaaS",
          "Cloud Deployment Models and Their Uses with Examples",
          "Common Myths about Cloud Computing",
          "Major Cloud Service Providers with Examples",
          "Careers in AWS",
          "AWS Role-Based Certifications",
          "Aws Certification",
          "AWS Global Infrastructure",
          "AWS Core Service domains",
          "AWS free tier and AWS console Global Infrastructure",
          "AWS compute services",
          "AWS Elastic Load Balancer",
          "Types of elastic load balnce",
          "AWS Autoscaling",
          "Amazon e2c",
          "Amazon ec2 Instances",
          "Amazon ec2 Instance Types",
          "Amazon Elastic Block Store",
          "Amazon S3 Glacier",
          "AWS Security Services",
          "AWS Database Services",
          "AWS Networking Services-",
          "AWS DevOps Services",
          "AWS CodePipeline",
          "Class Project 1"
        ]
      },
      "requirements": [
        "No experience required"
      ],
      "description": "Are you new to Amazon Web Services (AWS) and feeling overwhelmed by the vast array of services and concepts? This course is designed to provide you with a solid foundation in AWS, making it easy to understand and start using the cloud.\nKey Topics Covered:\nAWS Management Console: Master the AWS Management Console, the primary interface for interacting with AWS services. Learn how to navigate the console, use search functions, and access documentation.\nCreating and Managing AWS Resources: Discover how to create, configure, and manage various AWS resources, including instances, storage volumes, security groups, and load balancers. Understand the importance of best practices for resource management to ensure optimal performance and cost-efficiency.\nUnderstanding AWS Billing: Learn about AWS pricing models, including on-demand, reserved instances, and spot instances. Understand how to monitor your AWS usage and optimize your costs.\nIntroduction to AWS: Learn about the basics of cloud computing and why AWS is a leading provider.\nEssential AWS Services: Explore core services like EC2, S3, VPC, and IAM.\nAWS Management Console: Get familiar with the AWS console and its navigation.\nCreating and Managing AWS Resources: Learn how to create and manage various AWS resources.\nUnderstanding AWS Billing: Understand AWS pricing models and how to manage your costs.\nWhat You'll Learn:\nThe fundamental concepts of cloud computing\nHow to navigate the AWS Management Console\nThe key AWS services and their use cases\nBest practices for creating and managing AWS resources\nHow to optimize your AWS costs\nWho This Course Is For:\nBeginners who want to learn about AWS\nIT professionals looking to expand their skills\nStudents interested in cloud computing",
      "target_audience": [
        "Beginners who want to learn about AWS"
      ]
    },
    {
      "title": "Data Science and Machine Learning Bootcamp with R",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-bootcamp-with-r/",
      "bio": "Learn how to use the R programming language for data science and machine learning and data visualization!",
      "objectives": [
        "Program in R",
        "Use R for Data Analysis",
        "Create Data Visualizations",
        "Use R to handle csv,excel,SQL files or web scraping",
        "Use R to manipulate data easily",
        "Use R for Machine Learning Algorithms",
        "Use R for Data Science"
      ],
      "course_content": {
        "Course Introduction": [
          "Introduction to Course",
          "Course Curriculum",
          "What is Data Science?",
          "Course FAQ"
        ],
        "Course Best Practices": [
          "How to Get Help in the Course!",
          "Welcome to the Course.",
          "Installation and Set-Up"
        ],
        "Windows Installation Set-Up": [
          "Windows Installation Procedure"
        ],
        "Mac OS Installation Set-Up": [
          "Mac OS Installation Procedure"
        ],
        "Linux Installation": [
          "Linux/Unbuntu Installation Procedure"
        ],
        "Development Environment Overview": [
          "Development Environment Overview",
          "Course Notes",
          "Guide to RStudio"
        ],
        "Introduction to R Basics": [
          "Introduction to R Basics",
          "Arithmetic in R",
          "Variables",
          "R Basic Data Types",
          "Vector Basics",
          "Vector Operations",
          "Comparison Operators",
          "Vector Indexing and Slicing",
          "Getting Help with R and RStudio",
          "R Basics Training Exercise",
          "R Basics Training Exercise - Solutions Walkthrough"
        ],
        "R Matrices": [
          "Introduction to R Matrices",
          "Creating a Matrix",
          "Matrix Arithmetic",
          "Matrix Operations",
          "Matrix Selection and Indexing",
          "Factor and Categorical Matrices",
          "Matrix Training Exercise",
          "Matrix Training Exercises - Solutions Walkthrough"
        ],
        "R Data Frames": [
          "Introduction to R Data Frames",
          "Data Frame Basics",
          "Data Frame Indexing and Selection",
          "Overview of Data Frame Operations - Part 1",
          "Overview of Data Frame Operations - Part 2",
          "Data Frame Training Exercise",
          "Data Frame Training Exercises - Solutions Walkthrough"
        ],
        "R Lists": [
          "List Basics"
        ]
      },
      "requirements": [
        "Computer Access with download privileges.",
        "Basic Math Skills"
      ],
      "description": "Data Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems!\nThis course is designed for both complete beginners with no programming experience or experienced developers looking to make the jump to Data Science!\nThis comprehensive course is comparable to other Data Science bootcamps that usually cost thousands of dollars, but now you can learn all that information at a fraction of the cost! With over 100 HD video lectures and detailed code notebooks for every lecture this is one of the most comprehensive course for data science and machine learning on Udemy!\nWe'll teach you how to program with R, how to create amazing data visualizations, and how to use Machine Learning with R! Here a just a few of the topics we will be learning:\nProgramming with R\nAdvanced R Features\nUsing R Data Frames to solve complex tasks\nUse R to handle Excel Files\nWeb scraping with R\nConnect R to SQL\nUse ggplot2 for data visualizations\nUse plotly for interactive visualizations\nMachine Learning with R, including:\nLinear Regression\nK Nearest Neighbors\nK Means Clustering\nDecision Trees\nRandom Forests\nData Mining Twitter\nNeural Nets and Deep Learning\nSupport Vectore Machines\nand much, much more!\nEnroll in the course and become a data scientist today!",
      "target_audience": [
        "Anyone interested in becoming a Data Scientist"
      ]
    },
    {
      "title": "Practical Deep Learning: Master PyTorch in 15 Days",
      "url": "https://www.udemy.com/course/deep-learning-practical/",
      "bio": "From Zero to your own models: Learn PyTorch with hands-on projects. No previous experience required.",
      "objectives": [
        "15-Day Roadmap to PyTorch Mastery: Build, train, and deploy deep learning models in a structured timeline - from beginner to pro",
        "Hands-On Project Focus: Create real-world applications like spam filters, image classifiers, and price predictors",
        "Practical Deployment Expertise: Transform models into interactive apps with Gradio for immediate, hands-on results",
        "From Basics to Advanced: Dive into neurons, CNNs, transfer learning, and more. Master the PyTorch easily on the way",
        "Effective Data Handling: Learn how to preprocess, optimize, and evaluate diverse data types (images, text, ...)",
        "Have fun while learning: Many interactive quizzes and practical exams included",
        "Harness the Power of Transfer Learning with ResNet for Advanced Classification",
        "Master Essential Tools: Python, PyTorch, Jupyter, and Visual Studio Code"
      ],
      "course_content": {
        "Introduction": [
          "Overview: Practical Deep Learning"
        ],
        "[Day 1]: Foundations of Neural Networks: From Models and Neurons to Tensors": [
          "Course Materials",
          "Installing the necessary tools (Windows)",
          "Installing the necessary tools (Linux)",
          "Installing the necessary tools (macOS)",
          "Running a first file",
          "What is a model?",
          "Test your knowledge about the Foundations of Machine Learning and Models",
          "Developing a First Neuron",
          "Test Your Knowledge of the Structure and Mathematical Model of a Neuron",
          "A First Tensor",
          "Test Your Knowledge of Tensors",
          "Unpacking Tensors, Accessing Vectors",
          "Matrix in a Tensor",
          "Creating a Model",
          "The dtype of a Tensor"
        ],
        "[Day 2]: Neuron Training: From Adjusting Parameters to Batch Learning": [
          "Introduction to Neuron Training",
          "What is learning?",
          "Test your knowledge on Training Neurons and Learning Parameters",
          "How Neuron Learns: A Scalable Approach",
          "Test your knowledge on Loss Functions, Learning Rates, Parameter Initialization",
          "Understanding Gradient Descent for Neuron Optimization",
          "Test your knowledge on Gradient Descent",
          "Training a Neuron 1: Preparing and Optimizing",
          "Optimizing Training for Our Neuron Model",
          "Training a Neuron 2: Iterative Learning and Adjustments",
          "Test your knowledge on Data Handling and Iterative Training",
          "The Importance of Mean Squared Error in Model Training",
          "Batch Learning and Making Predictions with PyTorch",
          "Test your knowledge on Batch Learning, Loss Functions and Training Process",
          "Making the Most of Role Plays [Beta]",
          "[S2 & S3]: Checkpoint Call"
        ],
        "[Day 3 & 4]: Single Neuron Regression: Predicting Used Car Prices with PyTorch": [
          "[Day 3]: Introduction to Predicting Used Car Prices",
          "Overview of the Used Car Price Dataset",
          "Getting Started with Jupyter: Interactive Python Programming",
          "Test your knowledge on Used Car Dataset and Jupyter",
          "Exploring the Used Car Dataset with Pandas",
          "Investigating Key Data Relationships for Model Training",
          "Test your knowledge on Data Exploration and Preparation with Pandas",
          "Finalizing Input and Target Columns for Model Training",
          "Structuring Data for Model Input and Running an Initial Prediction",
          "Training the Model: Initial Setup and Challenges",
          "Test your knowledge on Data Preparation and Initial Neuron Training Steps",
          "[Day 4]: Understanding Output Normalization for Stable Learning",
          "Implementing Output Normalization in PyTorch for Consistent Predictions",
          "Test your knowledge on Output Data Normalization",
          "Understanding Input Normalization for Consistent Training",
          "Implementing Input Normalization in PyTorch for Improved Predictions",
          "Test your knowledge on Input Data Normalization",
          "Experimenting with Training Parameters Through Loss Visualization",
          "Saving and Loading Model in PyTorch",
          "Test your knowledge on Data Preparation, Model Training and Evaluation",
          "Exercise: Adding an Additional Column to the Model",
          "Solution: Adding an Additional Column to the Model"
        ],
        "[Day 5 & 6]: Neuron Classifier: Spam Detection in SMS": [
          "[Day 5]: Introduction to Spam Detection",
          "Exploring and Preprocessing the SMS Spam Dataset",
          "Using Count Vectorizer to Transform Text into Numerical Data",
          "Test your knowledge on Spam Detection and Text Preprocessing",
          "Optional / Extra: Exploring TF-IDF Vectorizer for Improved Text Preprocessing",
          "Training the Model for Spam Classification",
          "Optimizing Training for Our Neuron Classifier",
          "Understanding the Sigmoid Activation Function for Probability Output",
          "Test your knowledge on Model Training and Sigmoid Function for Spam Detection",
          "Switching to Binary Cross Entropy Loss for Effective Training",
          "Using BCE with Sigmoid for Loss Calculation and Prediction",
          "Evaluating Model with Key Performance Metrics",
          "Test your knowledge on Loss Functions and Evaluation Metrics in Spam Detection",
          "[Day 6]: Understanding Training, Validation and Test Data in Model Development",
          "Implementing Training and Validation Data Splits in Python",
          "Applying and Evaluating the Model on Fresh Data",
          "Test your knowledge on Data Segmentation in Model Development",
          "Optional / extra: Improving Spam Detection with Large Language Model Embeddings",
          "Optional / extra: Generating Embeddings with BART for Spam Detection",
          "Optional / extra: Building a Function to Generate Embeddings for Spam Detection",
          "Optional / extra: Integrating Embeddings into the Spam Filter",
          "Optional / extra: Test your knowledge on Enhancing Detection with LLM Embeddings",
          "Test your knowledge on Spam Detection Techniques",
          "[S4 & S5]: Model Results Review"
        ],
        "[Day 6]: Exam": [
          "PRACTICE EXAM: Test your knowledge so far (1/2)"
        ],
        "[Day 7 & 8]: Neural Network Classifier: Student Exam Results Prediction": [
          "[Day 7]: From Single Neuron to Neural Networks",
          "Optional: Understanding Activation Functions in Neural Networks",
          "Optional / extra: Exploring Nonlinearity and Its Impact on Neural Networks",
          "Understanding Backpropagation in Neural Networks",
          "Optional: Decoding the Mathematics of Backpropagation",
          "Test your knowledge on Neural Network Fundamentals",
          "Analyzing Student Performance Data for Exam Predictions",
          "Optional: Applying a Single Neuron to Student Exam Data",
          "Building and Training Our First Neural Network",
          "Optimizing Training for Our Neural Network Classifier",
          "Test your knowledge on Data Analysis and Neural Network Training",
          "Evaluating Neural Network Performance",
          "Simplifying the Code with nn.Sequential",
          "Test your knowledge on Neural Network Application Techniques",
          "[Day 8]: Introducing ReLU Activation Function",
          "Optimizing Training with Adam",
          "Test your knowledge on Optimizing Neural Networks with ReLU and Adam",
          "Implementing Mini-Batch Learning for Efficient Training",
          "Optimizing Loss Tracking in Mini-Batch Training",
          "Test your knowledge on Essential Neural Network Concepts"
        ],
        "[Day 8]: Exercise: Loan Approval Classification": [
          "Introduction to Loan Approval Prediction",
          "Exploring the Loan Approval Dataset",
          "Solution Part 1: Preparing Data for the Loan Approval Model",
          "Solution Part 2: Building and Training the Loan Approval Model"
        ],
        "[Day 9 & 10]: Neural Network for Multi-Class Classification: Handwritten Digits": [
          "[Day 9]: Introduction to Handwritten Digit Classification",
          "Exploring MNIST Data with TorchVision",
          "From Dataset to DataLoader: Preparing Data for Neural Network",
          "Test your knowledge on Data Preparation for Neural Network Training",
          "Building a Binary Classifier for 0 Detection",
          "Evaluating the Binary Classifier for 0 Detection",
          "Test your knowledge on Binary Classifier Essentials",
          "Multi-Class Classification in Neural Networks",
          "Understanding One-Hot Encoding",
          "Test your knowledge on Preparing Data for Multi-Class Classification",
          "Training a Neural Network for Multi-Class Classification",
          "Optimizing Training for Our Neural Network Multi-Class Classifier",
          "Evaluating a Neural Network for Multi-Class Classification",
          "Test your knowledge on Neural Network Adjustments for Multi-Class Classification",
          "[Day 10]: Understanding Softmax for Class Probability Normalization",
          "Applying Softmax in Neural Network",
          "Experimenting with Different Neural Network Architectures",
          "Test your knowledge on Softmax and Network Architecture",
          "Understanding Overfitting in Neural Networks",
          "Demonstrating Overfitting in Neural Network Training",
          "Strategies to Counter Overfitting",
          "Test your knowledge on Overfitting in Neural Network",
          "Optional / extra: Applying a Neural Network to Custom Images",
          "Optional / extra: Overcoming Preprocessing Challenges in Model Application",
          "Test your knowledge on Multi-Class Classifier Fundamentals",
          "[S7 & S9]: Model Scaling Check-In"
        ],
        "[Day 11 & 12]: Convolutional Networks: Fashion Item Classification (multi-class)": [
          "[Day 11]: Introduction to Convolutional Neural Networks",
          "Exploring Fashion MNIST Data",
          "Optional: Assessing Previous Model Performance on Fashion MNIST Data",
          "Exploring Edge Detection with the Sobel Operator",
          "Test your knowledge on CNN Basics and Image Processing",
          "Understanding the Structure of Convolutional Neural Networks for Edge Detection",
          "Part 1: Implementing a CNN",
          "Part 2: Advancing CNN Implementation",
          "Test your knowledge on CNN Architecture and Functionality",
          "Optimizing Training for Our CNN",
          "Reducing CNN Complexity with Max Pooling",
          "Test your knowledge on Max Pooling in CNNs",
          "Utilizing GPU Acceleration with PyTorch",
          "Optional: Enabling CUDA on NVIDIA GPUs",
          "Leveraging Google Colab's Free GPU",
          "Optimizing Tensor Computations on GPU",
          "Running Simple Model on GPU",
          "Accelerating CNN Execution Speed with GPU",
          "Test your knowledge on Utilizing GPUs with PyTorch",
          "[Day 12]: Advancing CNN Complexity",
          "Enhancing CNN Performance with Increased Filter Complexity",
          "Test your knowledge on CNN Layer Configurations",
          "Introducing Dropout for Improved Generalization",
          "Optimizing CNN with Dropout Layers",
          "Refining CNN with Batch Normalization",
          "Test your knowledge on Dropout and Batch Normalization",
          "Optional: Understanding the Mathematics of Batch Normalization",
          "Optional / extra: Application of Overfitting Detection and Model Finalization",
          "Test your knowledge on Key CNN Techniques"
        ]
      },
      "requirements": [
        "Basic Python Knowledge: You should be comfortable writing simple Python scripts and understanding common data types, loops, and functions.",
        "No Prior AI/Deep Learning Experience Needed: We’ll start from the ground up, so beginners are welcome.",
        "A Reliable Internet Connection & A Computer: You’ll need to download Python, PyTorch, and related tools, and possibly explore cloud-based resources.",
        "A Willingness to Learn & Experiment: Your curiosity and motivation are the most essential prerequisites. Everything else, we’ll guide you through step-by-step."
      ],
      "description": "Have you ever watched AI automatically classify images or detect spam and thought, “I wish I could do that”? Have you ever wondered how a spam filter works? Or do you want to master Deep Learning in a hands-on way?\nWith this course, you’ll learn how to build and deploy your own deep learning models in just 15 days - gaining practical, hands-on experience every step of the way.\nWhy This Course?\nFrom day one, you’ll get comfortable with the essential concepts that power modern AI. No fluff, no endless theory - you'll learn by building real-world projects like Spam filters, or image detections. By the end, you won’t just know what neurons and neural networks are - you’ll be able to train, refine, and apply them to projects that truly matter.\nWho Is This Course For?\nAbsolute beginners eager to break into the world of AI and deep learning.\nData enthusiasts who want to strengthen their portfolios with hands-on projects.\nDevelopers and data scientists looking to deepen their PyTorch and model deployment skills.\nAnyone who craves a clear roadmap to mastering deep learning, one day at a time.\nWhat Makes This Course Unique?\nDay-by-Day Progression: Follow a structured, 15-day plan that ensures you never feel lost or overwhelmed.\nReal-World Projects: Predict used car prices, detect spam in SMS, classify handwritten digits, recognize fashion items—all using deep learning techniques.\nModern Tools & Frameworks: Master industry-standard tools like PyTorch and dive into CNNs, transfer learning with ResNet, and more.\nPractical Deployment: Learn how to turn your trained models into interactive apps with Gradio, making your projects truly come alive.\nBy the End of This Course, You Will:\nConfidently implement, train, and evaluate deep learning models.\nUnderstand how to prepare and process various types of data, from text to images.\nKnow how to improve and optimize your models to achieve better performance.\nBe ready to deploy your AI solutions, making them accessible and interactive for real users.\nNo Prior Experience Needed\nWhether you’re a coding novice or a data analyst stepping into AI, this course starts from the very basics. You’ll be guided through installing Python, PyTorch, and setting up your coding environment, all the way to training full-fledged neural networks on your GPU.\nGet Ready to Dive In\nIf you’ve always wanted to get into deep learning, now is your chance. Enroll today and join me on a practical, hands-on journey that will transform the way you see and build AI solutions. In 15 days, you’ll have gone from curious beginner to proud deep learning practitioner—with real projects to show for it.",
      "target_audience": [
        "Absolute beginners eager to break into the world of AI and deep learning.",
        "Data enthusiasts who want to strengthen their portfolios with hands-on projects.",
        "Developers and data scientists looking to deepen their PyTorch and model deployment skills.",
        "Anyone who craves a clear roadmap to mastering deep learning, one day at a time."
      ]
    },
    {
      "title": "Python & Introduction to Data Science",
      "url": "https://www.udemy.com/course/python-introduction-to-data-science/",
      "bio": "Learn the basics of Python and the most important Data Science libraries with this step by step guide!",
      "objectives": [
        "Basic Notebook commands",
        "Variables and conversions in Python",
        "Variables, lists, dictionaries, sets, classes in Python",
        "Definition of a function",
        "Date management",
        "Reading and writing files",
        "Mathematical functions in Numpy",
        "Functions to create random data",
        "Indexing methods",
        "Pivot tables in Pandas",
        "Display options",
        "RAM memory optimization for large amounts of data"
      ],
      "course_content": {
        "Introduction": [
          "01 Python & Introduction to Data Science"
        ],
        "Python": [
          "2.01 Configuration of the development environment",
          "2.02 How to install Python libraries",
          "2.03 Basic Notebook Controls",
          "2.04 Introduction to Python",
          "2.05 Operations in Python",
          "2.06 Variables and conversions in Python",
          "2.07 Strings and functions of modifications",
          "2.08 Python’s Lists",
          "2.09 Functions with lists",
          "2.10 Dictionaries in Python",
          "2.11 Functions with dictionaries",
          "2.12 Set in Python",
          "2.13 Assignment mechanism in Python",
          "2.14 Conditional instructions in Python",
          "2.15 Python iteration instructions",
          "2.16 Creating functions in Python",
          "2.17 Scripts and modules in Python",
          "2.18 Error handling in Python",
          "2.19 Reading and writing files in Python",
          "2.20 Classes in Python",
          "2.21 Inheritance of classes in Python",
          "2.22 Time management functions",
          "2.23 Practical exercises with Python (1)",
          "2.24 Practical exercises with Python (2)",
          "2.25 Practical exercises with Python (3)",
          "2.26 Practical exercises with Python (4)",
          "2.27 Practical exercises with Python (5)",
          "2.28 Practical exercises with Python (6)"
        ],
        "Numpy": [
          "3.01 Introduction to Numpy",
          "3.02 Arrays in Numpy",
          "3.03 Indexing of matrices in Numpy",
          "3.04 Copy, arange and random in Numpy",
          "3.05 Data type and conversion to Numpy",
          "3.06 Mathematical Functions in Numpy",
          "3.07 Order functions in Numpy",
          "3.08 Data management functions in Numpy",
          "3.09 Functions to create arrays in Numpy (1)",
          "3.10 Functions to create arrays in Numpy (2)",
          "3.11 Logical operations in Numpy",
          "3.12 Random in Numpy",
          "3.13 Reading files in Numpy",
          "3.14 Writing files in Numpy",
          "3.15 Practical exercises with Numpy (1)",
          "3.16 Practical exercises with Numpy (2)",
          "3.17 Practical exercises with Numpy (3)",
          "3.18 Practical exercises with Numpy (4)",
          "3.19 Practical exercises with Numpy (5)",
          "3.20 Practical exercises with Numpy (6)",
          "3.21 Practical exercises with Numpy (7)"
        ],
        "Pandas": [
          "4.01 Introduction to Pandas",
          "4.02 DataFrame and Series in Pandas",
          "4.03 Indexing methods in Pandas",
          "4.04 Groupby in Pandas",
          "4.05 Mathematical Operations in Pandas",
          "4.06 Indexing and editing of a Series data",
          "4.07 Indexing, editing and deletion of a DataFrame",
          "4.08 Merge in DataFrame",
          "4.09 Display options in Pandas",
          "4.10 Pivot chart in Pandas",
          "4.11 Managing dates in Pandas (1)",
          "4.12 Managing dates in Pandas (2)",
          "4.13 Processing data in Pandas (1)",
          "4.14 Processing of data in Pandas (2)",
          "4.15 Methods for editing strings in Pandas",
          "4.16 Advanced indexing methods in Pandas",
          "4.17 Create graphs in Pandas",
          "4.18 Memory management for large data"
        ]
      },
      "requirements": [
        "No programming experience is required for the course",
        "A computer with internet connection",
        "Compatible with all languages, the course is in English",
        "The course is on Windows 10 but is also available with other operative systems"
      ],
      "description": "Python is the most important language in the field of data, and its libraries for analysis and modeling are the most relevant tools to use.\nIn this course we will start building the basics of Python and then going to deepen the fundamental libraries like Numpy, Pandas, and Matplotlib.\nThe four main features of this course are:\n1. Clear and simplified language, suitable for everyone\n2. Practical and efficient\n3. Examples, illustrations and demonstrations with relative explanations\n4. Continuous updating of contents and exercises",
      "target_audience": [
        "Researchers in the field of data analysis, machine learning and data mining, who want to consolidate the basics",
        "Beginners who want to start learning the Python programming language",
        "Programmers who already have experience with other languages and want to learn the Python language",
        "Any student wishing to pursue a career in the field of Data Science",
        "Anyone who wants to approach this new field for work or personal growth"
      ]
    },
    {
      "title": "R Data Pre-Processing & Data Management - Shape your Data!",
      "url": "https://www.udemy.com/course/r-data-management-shape-your-data/",
      "bio": "Learn how to prepare your data for great analytics in R.",
      "objectives": [
        "import data into R in several ways while also beeing able to identify a suitable import tool",
        "select and implement a proper object class (data.frame, data.table, data_frame)",
        "convert your data into (and understand) a tidy data format",
        "filter and query your data based on a wide range of parameters",
        "join 2 data tables together with dplyr 2 table verb syntax",
        "use SQL code within R",
        "translate basic R into SQL",
        "work with dates and time",
        "work with strings using regular expressions",
        "detecting outliers in datasets"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Managing Expectations and Course Orientation",
          "Data Pre-Processing as Integral Part of Data Science",
          "Let's See an R Example of Data Pre-Processing",
          "Lures Example Script"
        ],
        "Data Import and Data Structuring": [
          "Script: Data import",
          "Importing Data and Snippets",
          "Using fread to handle big data fast",
          "Choosing the right class for your data",
          "Further R Exercises"
        ],
        "Cleaning Your Data": [
          "Script: Data cleaning",
          "tidyr - How tidy data looks like",
          "Wide to long data format",
          "Splitting columns",
          "Long to wide data format"
        ],
        "Querying and Filtering Data with data.table": [
          "Script: Querying with data.table",
          "What is data.table?",
          "Basic queries",
          "Queries at column level",
          "The by paramater for queries",
          "Update on recycle queries",
          "Keys",
          "Data.table exercises",
          "Data.table solutions"
        ],
        "Queries and Filtering Exercises": [
          "Query exercises INTRO",
          "10 Exercises on 'data.frame'",
          "Data.frame Exercise Script",
          "Data.frame Solutions 1-4",
          "Data.frame Solutions 5-10",
          "10 Exercises on 'data.table'",
          "Data.table Exercise Script",
          "Data.table Solutions 1-4",
          "Data.table Solutions 5 - 10"
        ],
        "Using dplyr on one and multiple Datasets": [
          "Script: dplyr",
          "Single Table Verbs in 'dplyr'",
          "Two Table Verbs - Mutating Joins",
          "Two Table Verbs - Filtering Joins and handling of ID mismatches",
          "Two Table Verbs - Set Operations"
        ],
        "Integrate SQL into R": [
          "Script: Integrate SQL",
          "Get package dbplyr",
          "R to SQL Translator",
          "Using SQL within R",
          "Set Up a SQLite Database in R"
        ],
        "Detecting Outliers": [
          "Outlier Script",
          "Introduction to Outlier Detection",
          "Detecting Outliers in Univariate Datasets",
          "Detecting Outliers in Multivariate Datasets"
        ],
        "Working with Strings - Regular Expressions": [
          "Script: Working with Strings",
          "Regular Expressions and Gsub",
          "What You Should Know about Strings in R",
          "The Gsub Family of Functions and Regular Expressions",
          "Regular Expressions Syntax",
          "A Great Add On Package",
          "Working with Strings in R: Exercise with Solution"
        ],
        "Working with Dates and Time": [
          "Data management and time series INTRO",
          "Importing a Time Series From Excel",
          "Section Script",
          "Classes POSIXt, Date and Chron",
          "Lubridate: Input and Time Zones",
          "Lubridate: Weekdays and Intervals",
          "Lubridate: Exercise Data Frame",
          "Lubridate: Calculations and Leap Years",
          "Lubridate: Data Handling Exercise",
          "Further R Exercises"
        ]
      },
      "requirements": [
        "Computer with R and RStudio ready to use",
        "You should have basic R / RStudio knowledge",
        "Required add on packages will be listed in the course orientation video"
      ],
      "description": "Let’s get your data in shape!\nData Pre-Processing is the very first step in data analytics. You cannot escape it, it is too important. Unfortunately this topic is widely overlooked and information is hard to find.\nWith this course I will change this!\nData Pre-Processing as taught in this course has the following steps:\n1.       Data Import: this might sound trivial but if you consider all the different data formats out there you can imagine that this can be confusing. In the course we will take a look at a standard way of importing csv files, we will learn about the very fast fread method and I will show you what you can do if you have more exotic file formats to handle.\n2.       Selecting the object class: a standard data.frame might be fine for easy standard tasks, but there are more advanced classes out there like the data.table. Especially with those huge datasets nowadays, a data.frame might not do it anymore. Alternatives will be demonstrated in this course.\n3.       Getting your data in a tidy form: a tidy dataset has 1 row for each observation and 1 column for each variable. This might sound trivial, but in your daily work you will find instances where this simple rule is not followed. Often times you will not even notice that the dataset is not tidy in its layout. We will learn how tidyr can help you in getting your data into a clean and tidy format.\n4.       Querying and filtering: when you have a huge dataset you need to filter for the desired parameters. We will learn about the combination of parameters and implementation of advanced filtering methods. Especially data.table has proven effective for that sort of querying on huge datasets, therefore we will focus on this package in the querying section.\n5.       Data joins: when your data is spread over 2 different tables but you want to join them together based on given criteria, you will need joins for that. There are several methods of data joins in R, but here we will take a look at dplyr and the 2 table verbs which are such a great tool to work with 2 tables at the same time.\n6.       Integrating and interacting with SQL: R is great at interacting with SQL. And SQL is of course the leading database language, which you will have to learn sooner or later as a data scientist. I will show you how to use SQL code within R and there is even a R to SQL translator for standard R code. And we will set up a SQLite database from within R.\n7.  Outlier detection: Datasets often contain values outside a plausible range. Faulty data generation or entry happens regularly. Statistical methods of outlier detection help to identify these values. We will take a look at the implemention of these.\n8. Character strings as well as dates and time have their own rules when it comes to pre-processing. In this course we will also take a look at these types of data and how to effectively handle it in R.\nHow do you best prepare yourself for this course?\nYou only need a basic knowledge of R to fully benefit from this course. Once you know the basics of RStudio and R you are ready to follow along with the course material. Of course you will also get the R scripts which makes it even easier.\nThe screencasts are made in RStudio so you should get this program on top of R. Add on packages required are listed in the course.\nAgain, if you want to make sure that you have proper data with a tidy format, take a look at this course. It will make your analytics with R much easier!",
      "target_audience": [
        "Data pre-processing is a crucial step of data related work - therefore this course is intended for all R users"
      ]
    },
    {
      "title": "Complete Data Wrangling & Data Visualisation With Python",
      "url": "https://www.udemy.com/course/complete-data-wrangling-data-visualisation-with-python/",
      "bio": "Learn to Preprocess, Wrangle and Visualise Data For Practical Data Science Applications in Python",
      "objectives": [
        "Install and Get Started With the Python Data Science Environment- Jupyter/iPython",
        "Read In Data Into The Jupiter/iPython Environment From Different Sources",
        "Carry Out Basic Data Pre-processing & Wrangling In the Jupyter Environment",
        "Learn to IDENTIFY Which Visualisations Should be Used in ANY given Situation",
        "Go From A Basic Level To Performing Some Of The MOST COMMON Data Preprocessing, Data Wrangling & Data Visualization Tasks In Jupyter",
        "How To Use Some Of The MOST IMPORTANT Data Wrangling & Visualisation Packages Such As Matplotlib",
        "Build POWERFUL Visualisations and Graphs from REAL DATA",
        "Apply Data Visualization Concepts For PRACTICAL Data Analysis & Interpretation",
        "Gain PROFICIENCY In Data Preprocessing, Data Wrangling & Data Visualisation In Jupyter By Putting Your Soon-To-Be-Acquired Knowledge Into IMMEDIATE Application"
      ],
      "course_content": {
        "INTRODUCTION TO THE COURSE: The Key Concepts and Software Tools": [
          "Welcome to the Course",
          "Data & Script For the Course",
          "Python Data Science Environment",
          "For Mac Users",
          "Introduction to IPython/Jupyter",
          "ipython in Browser"
        ],
        "Read in Data From Different Sources With Pandas": [
          "What are Pandas?",
          "Read CSV Data",
          "Read Excel Data",
          "Read in HTML Data"
        ],
        "Data Cleaning": [
          "Remove NA Values",
          "Missing Values in a Real Dataset",
          "Data Imputation",
          "Imputing Qualitative Values",
          "Use k-NN for Data Imputation"
        ],
        "Basic Data Wrangling": [
          "Basic Principles",
          "Preliminary Data Explorations",
          "Basic Data Handling With Conditional Statements",
          "Drop Column/Row",
          "Change Column Name",
          "Change the Column Type",
          "Explore Date Related Data",
          "Simple Date Related Computations"
        ],
        "More Data Wrangling": [
          "Data Grouping",
          "Data Subsetting and Indexing",
          "More Data Subsetting",
          "Extract Information From Strings",
          "(Fuzzy) String Matching",
          "Ranking & Sorting",
          "Concatenate",
          "Merging and Joining"
        ],
        "Feature Selection and Transformation": [
          "Correlation Analysis",
          "Using Correlation to Decide Which Features to Retain",
          "Univariate Feature Selection",
          "Recursive Feature Elimination (RFE)",
          "Theory Behind PCA",
          "Implement PCA",
          "Data Standardisation",
          "Create a New Feature"
        ],
        "Theory Behind Data Visualisation": [
          "What is Data Visualisation?",
          "Some Theoretical Principles Behind Data Visualisation"
        ],
        "Most Common Data Visualizations": [
          "Histograms-Visualize the Distribution of Continuous Numerical Variables",
          "Boxplots-Visualize the Distribution of Continuous Numerical Variables",
          "Scatter plot-Relationship Between Two Numerical Variables",
          "Barplot",
          "Pie Chart",
          "Line Charts",
          "More Line Charts",
          "Some More Plot Types",
          "And Some More"
        ],
        "Miscallaneous Information": [
          "Using Colabs as an Online Jupyter Notebook",
          "Github",
          "What is data science?",
          "Different Data Types",
          "Posit On POSIT",
          "Introduction to Distributed Computing"
        ]
      },
      "requirements": [
        "The Ability To Install the Anaconda Environment On Your Computer/Laptop",
        "Know how to install and load packages in Anaconda",
        "Interest in Learning to Process and Visualise Real Data"
      ],
      "description": "Hello, My name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation).\n\n\nI have several years of experience in analyzing real life data from different sources using statistical modeling and producing publications for international peer reviewed journals. If you find statistics books & manuals too vague, expensive & not practical, then you’re going to love this course!\nI created this course to take you by hand and teach you all the concepts, and tackle the most fundamental building block on practical data science- data wrangling and visualisation.\nGET ACCESS TO A COURSE THAT IS JAM PACKED WITH TONS OF APPLICABLE INFORMATION!\n\n\nThis course is your sure-fire way of acquiring the knowledge and statistical data analysis wrangling and visualisation skills that I acquired from the rigorous training I received at 2 of the best universities in the world, perusal of numerous books and publishing statistically rich papers in renowned international journal like PLOS One.\nTo be more specific, here’s what the course will do for you:\n\n(a) It will take you (even if you have no prior statistical modelling/analysis background) from a basic level to performing some of the most common data wrangling tasks in Python.\n\n(b) It will equip you to use some of the most important Python data wrangling and visualisation packages such as seaborn.\n\n(c) It will Introduce some of the most important data visualisation concepts to you in a practical manner such that you can apply these concepts for practical data analysis and interpretation.\n(d) You will also be able to decide which wrangling and visualisation techniques are best suited to answer your research questions and applicable to your data and interpret the results.\nThe course will mostly focus on helping you implement different techniques on real-life data such as Olympic and Nobel Prize winners\n\n\nAfter each video you will learn a new concept or technique which you may apply to your own projects immediately! Reinforce your knowledge through practical quizzes and assignments.\nTAKE ACTION NOW :) You’ll also have my continuous support when you take this course just to make sure you’re successful with it.  If my GUARANTEE is not enough for you, you can ask for a refund within 30 days of your purchase in case you’re not completely satisfied with the course.\nTAKE ACTION TODAY! I will personally support you and ensure your experience with this course is a success.",
      "target_audience": [
        "Students Interested In Getting Started With Data Science Applications In The Jupyter Environment",
        "Students Interested in Learning About the Common Pre-processing Data Tasks",
        "Students Interested in Gaining Exposure to Common Python Packages Such As pandas",
        "Those Interested in Learning About Different Kinds of Data Visualisations",
        "Those Interested in Learning to Create Publication Quality Visualisations"
      ]
    },
    {
      "title": "Databricks - Master Azure Databricks for Data Engineers",
      "url": "https://www.udemy.com/course/master-azure-databricks-for-data-engineers/",
      "bio": "Learn Azure Databricks for professional data engineers using PySpark and Spark SQL with an end-to-end capstone project",
      "objectives": [
        "Databricks in Azure Cloud",
        "Working with DBFS and Mounting Storage",
        "Unity Catalog - Configuring and Working",
        "Unity Catalog User Provisioning and Security",
        "Working with Delta Lake and Delta Tables",
        "Manual and Automatic Schema Evolution",
        "Incremental Ingestion into Lakehouse",
        "Databricks Autoloader",
        "Delta Live Tables and DLT Pipelines",
        "Databricks Repos and Databricks Workflow",
        "Databricks Rest API and CLI",
        "Capstone Project"
      ],
      "course_content": {
        "Before you start": [
          "Course Prerequisites",
          "About the Course",
          "How to access Course Material and Resources",
          "Note for Students - Before Start"
        ],
        "Introduction": [
          "Introduction to Data Engineering",
          "Apache Spark to Data Engineering Platform",
          "Introduction to Databricks Platform"
        ],
        "Getting Started": [
          "What will you learn in this section",
          "Creating Azure Cloud Account",
          "Azure Portal Overview",
          "Creating Databricks Workspace Service",
          "Introduction to Databricks Workspace",
          "Azure Databricks Platform Architecture"
        ],
        "Working in Databricks Workspace": [
          "What will you learn in this section",
          "How to create Spark Cluster",
          "Working with Databricks Notebook",
          "Notebook Magic Commands",
          "Databricks Utilities Package"
        ],
        "Working with Databricks File System - DBFS": [
          "What will you learn in this section",
          "Introduction to DBFS",
          "Working with DBFS Root",
          "Mounting ADLS to DBFS"
        ],
        "Working with Unity Catalog": [
          "What will you learn in this section",
          "Introduction to Unity Catalog",
          "Setup Unity Catalog",
          "Unity Catalog User Provisioning",
          "Working with Securable Objects"
        ],
        "Working with Delta Lake and Delta Tables": [
          "What will you learn in this section",
          "Introduction to Delta Lake",
          "Creating Delta Table",
          "Sharing data for External Delta Table",
          "Reading Delta Table",
          "Delta Table Operations",
          "Delta Table Time Travel",
          "Convert Parquet to Delta",
          "Delta Table Schema Validation",
          "Delta Table Schema Evolution",
          "Look Inside Delta Table",
          "Delta Table Utilities and Optimization"
        ],
        "Working with Databricks Incremental Ingestion Tools": [
          "What will you learn in this section",
          "Architecture and Need for Incremental Ingestion",
          "Using Copy Into with Manual Schema Evolution",
          "Using Copy Into with Automatic Schema Evolution",
          "Streaming Ingestion with Manual Schema Evolution",
          "Streaming Ingestion with Automatic Schema Evolution",
          "Introduction to Databricks Autoloader",
          "Autoloader with Automatic Schema Evolution"
        ],
        "Working with Databricks Delta Live Tables (DLT)": [
          "What will you learn in this section",
          "Introduction to Databricks DLT",
          "Understand DLT Use Case Scenario",
          "Setup DLT Scenario Dataset",
          "Creating DLT Workload in SQL",
          "Creating DLT Pipeline for your Workload",
          "Creating DLT Workload in Python"
        ],
        "Databricks Project and Automation Features": [
          "What will you learn in this section",
          "Working with Databricks Repos",
          "Working with Databricks Workflows",
          "Working with Databricks Rest API",
          "Working with Databricks CLI"
        ]
      },
      "requirements": [
        "Python Programming Language",
        "Apache Spark and Dataframe APIs using Python",
        "Spark Structured Streaming APIs using Python"
      ],
      "description": "About the Course\nI am creating Databricks - Master Azure Databricks for Data Engineers using the Azure cloud platform. This course will help you learn the following things.\n\n\nDatabricks in Azure Cloud\nWorking with DBFS and Mounting Storage\nUnity Catalog - Configuring and Working\nUnity Catalog User Provisioning and Security\nWorking with Delta Lake and Delta Tables\nManual and Automatic Schema Evolution\nIncremental Ingestion into Lakehouse\nDatabricks Autoloader\nDelta Live Tables and DLT Pipelines\nDatabricks Repos and Databricks Workflow\nDatabricks Rest API and CLI\nCapstone Project\nThis course also includes an End-To-End Capstone project. The project will help you understand the real-life project design, coding, implementation, testing, and CI/CD approach.\nWho should take this Course?\nI designed this course for data engineers who are willing to develop Lakehouse projects following the Medallion architecture approach using the Databrick cloud platform. I am also creating this course for data and solution architects responsible for designing and building the organization’s Lakehouse platform infrastructure. Another group of people is the managers and architects who do not directly work with Lakehouse implementation. Still, they work with those implementing Lakehouse at the ground level.\nSpark Version used in the Course.\nThis course uses Databricks in Azure Cloud and Apache Spark 3.5. I have tested all the source codes and examples used in this course on Azure Databricks Cloud using Databricks Runtime 13.3.",
      "target_audience": [
        "Data Engineers",
        "Data Engineering Solution Architects"
      ]
    },
    {
      "title": "Intro to Natural Language Processing (NLP) in Python for AI",
      "url": "https://www.udemy.com/course/intro-to-natural-language-processing-in-python-for-ai/",
      "bio": "Learn the NLP Technology Behind AI Tools Like ChatGPT: Understanding, Generating, and Classifying Human Language",
      "objectives": [
        "Natural Language Processing for AI",
        "Text preprocessing techniques",
        "Text tagging and entity extraction",
        "Sentiment analysis",
        "Uncovering topics in the text",
        "Text classification",
        "Vectorizing text for machine learning"
      ],
      "course_content": {
        "Introduction": [
          "Introduction to the course",
          "Download course materials",
          "Introduction to NLP",
          "NLP in everyday life",
          "Supervised vs Unsupervised NLP"
        ],
        "Text Preprocessing": [
          "The importance of data preparation",
          "Lowercase",
          "Removing stop words",
          "Regular expressions",
          "Tokenization",
          "Stemming",
          "Lemmatization",
          "N-grams",
          "Practical task"
        ],
        "Identifying Parts of Speech and Named Entities": [
          "Text tagging",
          "Parts of speech (POS) tagging",
          "Named entity recognition (NER)",
          "Practical task"
        ],
        "Sentiment Analysis": [
          "What is sentiment analysis?",
          "Rule-based sentiment analysis",
          "Pre-trained transformer models",
          "Practical task"
        ],
        "Vectorizing Text": [
          "Numerical representation of text",
          "Bag of Words model",
          "TF-IDF"
        ],
        "Topic Modelling": [
          "What is topic modelling?",
          "When to use topic modelling?",
          "Latent Dirichlet Allocation",
          "LDA in Python",
          "Latent Semantic Analysis",
          "LSA in Python"
        ],
        "Building Your Own Text Classifier": [
          "Building a custom text classifier",
          "Logistic regression",
          "Naive Bayes",
          "Linear Support Vector Machine"
        ],
        "Case Study: Categorizing Fake News": [
          "Introducing the project",
          "Exploring our data through POS tags",
          "Extracting named entities",
          "Processing the text",
          "Does sentiment differ between news types?",
          "What topics appear in fake news? (Part 1)",
          "What topics appear in fake news? (Part 2)",
          "Categorizing fake news with a custom classifier"
        ],
        "The Future of NLP": [
          "What is deep learning?",
          "Deep learning for NLP",
          "Non-English NLP",
          "What's next for NLP?"
        ]
      },
      "requirements": [
        "Basic Python programming skills"
      ],
      "description": "Are you passionate about Artificial Intelligence and Natural Language Processing?\nDo you want to pursue a career as a data scientist or as an AI engineer?\nIf that’s the case, then this is the perfect course for you!\nIn this Intro to Natural Language Processing in Python course you will explore essential topics for working with text data. Whether you want to create custom text classifiers, analyze sentiment, or explore concealed topics, you’ll learn how NLP works and obtain the tools and concepts necessary to tackle these challenges.\nNatural language processing is an exciting and rapidly evolving field that fundamentally impacts how we interact with technology. In this course, you’ll learn to unlock the power of natural language processing and will be equipped with the knowledge and skills to start working on your own NLP projects.\nThe training offers you access to high quality Full HD videos and practical coding exercises. This is a format that facilitates easy comprehension and interactive learning. One of the biggest advantages of all trainings produced by 365 Data Science is their structure. This course makes no exception. The well-organized curriculum ensures you will have an amazing experience.\nYou won’t need prior natural language processing training to get started—just basic Python skills and familiarity with machine learning.\nThis introduction to NLP guides you step-by-step through the entire process of completing a project. We’ll cover models and analysis and the fundamentals, such as processing and cleaning text data and how to get data in the correct format for NLP with machine learning.\nWe'll utilize algorithms like Latent Dirichlet Allocation, Transformer models, Logistic Regression, Naive Bayes, and Linear SVM, along with such techniques as part-of-speech (POS) tagging and Named Entity Recognition (NER).\nYou'll get the opportunity to apply your newly acquired skills through a comprehensive case study, where we'll guide you through the entire project, covering the following stages:\nText cleansing\nIn-depth content analysis\nSentiment analysis\nUncovering hidden themes\nUltimately crafting a customized text classification model\nBy completing the course, you’ll receive а verifiable NLP certificate and will add an excellent project to your portfolio to show off your ability to analyze text like a pro.\nSo, what are you waiting for?\nClick Buy Now and start your AI journey today!",
      "target_audience": [
        "Aspiring data scientists and AI engineers",
        "AI and LLM students",
        "Data science students",
        "Data scientists",
        "Anyone interested to learn how to work with Natural Language Processing"
      ]
    },
    {
      "title": "Artificial Neural Networks for Business Managers in R Studio",
      "url": "https://www.udemy.com/course/neural-network-understanding-and-building-an-ann-in-r/",
      "bio": "You do not need coding or advanced mathematics background for this course. Understand how predictive ANN models work",
      "objectives": [
        "Get a solid understanding of Artificial Neural Networks (ANN) and Deep Learning",
        "Understand the business scenarios where Artificial Neural Networks (ANN) is applicable",
        "Building a Artificial Neural Networks (ANN) in R",
        "Use Artificial Neural Networks (ANN) to make predictions",
        "Use R programming language to manipulate data and make statistical computations",
        "Learn usage of Keras and Tensorflow libraries"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course",
          "Introduction to Neural Networks and Course flow",
          "Course Resources",
          "This is a Milestone!"
        ],
        "Setting Up R Studio and R crash course": [
          "Installing R and R studio",
          "Basics of R and R studio",
          "Packages in R",
          "Inputting data part 1: Inbuilt datasets of R",
          "Inputting data part 2: Manual data entry",
          "Inputting data part 3: Importing from CSV or Text files",
          "Creating Barplots in R",
          "Creating Histograms in R"
        ],
        "Single Cells - Perceptron and Sigmoid Neuron": [
          "Perceptron",
          "Activation Functions",
          "Quiz"
        ],
        "Neural Networks - Stacking cells to create network": [
          "Basic Terminologies",
          "Gradient Descent",
          "Back Propagation",
          "Quiz"
        ],
        "Important concepts: Common Interview questions": [
          "Some Important Concepts",
          "Quiz"
        ],
        "Standard Model Parameters": [
          "Hyperparameters",
          "Quiz"
        ],
        "Practice Test": [
          "Test your conceptual understanding"
        ],
        "Tensorflow and Keras": [
          "Keras and Tensorflow",
          "Installing Keras and Tensorflow",
          "Quiz"
        ],
        "R - Dataset for classification problem": [
          "Data Normalization and Test-Train Split",
          "More about test-train split"
        ],
        "R - Building and training the Model": [
          "Building,Compiling and Training",
          "Evaluating and Predicting"
        ]
      },
      "requirements": [
        "Students will need to install R Studio software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Artificial Neural Network (ANN) course that teaches you everything you need to create a Neural Network model in R, right?\nYou've found the right Neural Networks course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using Neural network Models.\nHave a clear understanding of Advanced Neural network concepts such as Gradient Descent, forward and Backward Propagation etc.\nCreate Neural network models in R using Keras and Tensorflow libraries and analyze their results.\nConfidently practice, discuss and understand Deep Learning concepts\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Neural networks course.\nIf you are a business Analyst or an executive, or a student who wants to learn and apply Deep learning in Real world problems of business, this course will give you a solid base for that by teaching you some of the most advanced concepts of Neural networks and their implementation in R Studio without getting too Mathematical.\nWhy should you choose this course?\nThis course covers all the steps that one should take to create a predictive model using Neural Networks.\nMost courses only focus on teaching how to run the analysis but we believe that having a strong theoretical understanding of the concepts enables us to create a good model . And after running the analysis, one should be able to judge how good the model is and interpret the results to actually be able to help the business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using Deep learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 250,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Practice test, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take practice test to check your understanding of concepts. There is a final practical assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Neural network based model i.e. a Deep Learning model, to solve business problems.\nBelow are the course contents of this course on ANN:\nPart 1 - Setting up R studio and R Crash course\nThis part gets you started with R.\nThis section will help you set up the R and R studio on your system and it'll teach you how to perform some basic operations in R.\nPart 2 - Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nPart 3 - Creating Regression and Classification ANN model in R\nIn this part you will learn how to create ANN models in R Studio.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. We also solve a regression problem in which we try to predict house prices in a location. We will also cover how to create complex ANN architectures using functional API. Lastly we learn how to save and restore models.\nWe also understand the importance of libraries such as Keras and TensorFlow in this part.\nPart 4 - Data Preprocessing\nIn this part you will learn what actions you need to take to prepare Data for the analysis, these steps are very important for creating a meaningful.\nIn this section, we will start with the basic theory of decision tree then we cover data pre-processing topics like  missing value imputation, variable transformation and Test-Train split.\nPart 5 - Classic ML technique - Linear Regression\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you\nunderstand where the concept is coming from and how it is important. But even if you don't understand\nit,  it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a Neural Network model in R will soar. You'll have a thorough understanding of how to use ANN to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow are some popular FAQs of students who want to start their Deep learning journey-\n\n\nWhy use R for Deep Learning?\nUnderstanding R is one of the valuable skills needed for a career in Machine Learning. Below are some reasons why you should learn Deep learning in R\n1. It’s a popular language for Machine Learning at top tech firms. Almost all of them hire data scientists who use R. Facebook, for example, uses R to do behavioral analysis with user post data. Google uses R to assess ad effectiveness and make economic forecasts. And by the way, it’s not just tech firms: R is in use at analysis and consulting firms, banks and other financial institutions, academic institutions and research labs, and pretty much everywhere else data needs analyzing and visualizing.\n2. Learning the data science basics is arguably easier in R. R has a big advantage: it was designed specifically with data manipulation and analysis in mind.\n3. Amazing packages that make your life easier. Because R was designed with statistical analysis in mind, it has a fantastic ecosystem of packages and other resources that are great for data science.\n4. Robust, growing community of data scientists and statisticians. As the field of data science has exploded, R has exploded with it, becoming one of the fastest-growing languages in the world (as measured by StackOverflow). That means it’s easy to find answers to questions and community guidance as you work your way through projects in R.\n5. Put another tool in your toolkit. No one language is going to be the right tool for every job. Adding R to your repertoire will make some projects easier – and of course, it’ll also make you a more flexible and marketable employee when you’re looking for jobs in data science.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Neural Network journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master ANN from Beginner level in short span of time"
      ]
    },
    {
      "title": "Machine Learning and AI: Support Vector Machines in Python",
      "url": "https://www.udemy.com/course/support-vector-machines-in-python/",
      "bio": "Artificial Intelligence and Data Science Algorithms in Python for Classification and Regression",
      "objectives": [
        "Apply SVMs to practical applications: image recognition, spam detection, medical diagnosis, and regression analysis",
        "Understand the theory behind SVMs from scratch (basic geometry)",
        "Use Lagrangian Duality to derive the Kernel SVM",
        "Understand how Quadratic Programming is applied to SVM",
        "Support Vector Regression",
        "Polynomial Kernel, Gaussian Kernel, and Sigmoid Kernel",
        "Build your own RBF Network and other Neural Networks based on SVM"
      ],
      "course_content": {
        "Welcome": [
          "Introduction",
          "Course Objectives",
          "Course Outline",
          "Where to get the code and data"
        ],
        "Beginner's Corner": [
          "Beginner's Corner: Section Introduction",
          "Image Classification with SVMs",
          "Spam Detection with SVMs",
          "Medical Diagnosis with SVMs",
          "Regression with SVMs",
          "Cross-Validation",
          "How do you get the data? How do you process the data?",
          "Suggestion Box"
        ],
        "Review of Linear Classifiers": [
          "Basic Geometry",
          "Normal Vectors",
          "Logistic Regression Review",
          "Loss Function and Regularization",
          "Prediction Confidence",
          "Nonlinear Problems",
          "Linear Classifiers Section Conclusion"
        ],
        "Linear SVM": [
          "Linear SVM Section Introduction and Outline",
          "Linear SVM Problem Setup and Definitions",
          "Margins",
          "Linear SVM Objective",
          "Linear and Quadratic Programming",
          "Slack Variables",
          "Hinge Loss (and its Relationship to Logistic Regression)",
          "Linear SVM with Gradient Descent",
          "Linear SVM with Gradient Descent (Code)",
          "Linear SVM Section Summary"
        ],
        "Duality": [
          "Duality Section Introduction",
          "Duality and Lagrangians (part 1)",
          "Lagrangian Duality (part 2)",
          "Relationship to Linear Programming",
          "Predictions and Support Vectors",
          "Why Transform Primal to Dual?",
          "Duality Section Conclusion"
        ],
        "Kernel Methods": [
          "Kernel Methods Section Introduction",
          "The Kernel Trick",
          "Polynomial Kernel",
          "Gaussian Kernel",
          "Using the Gaussian Kernel",
          "Why does the Gaussian Kernel correspond to infinite-dimensional features?",
          "Other Kernels",
          "Mercer's Condition",
          "Kernel Methods Section Summary"
        ],
        "Implementations and Extensions": [
          "Dual with Slack Variables",
          "Simple Approaches to Implementation",
          "SVM with Projected Gradient Descent Code",
          "Kernel SVM Gradient Descent with Primal (Theory)",
          "Kernel SVM Gradient Descent with Primal (Code)",
          "SMO (Sequential Minimal Optimization)",
          "Support Vector Regression",
          "Multiclass Classification"
        ],
        "Neural Networks (Beginner's Corner 2)": [
          "Neural Networks Section Introduction",
          "RBF Networks",
          "RBF Approximations",
          "What Happened to Infinite Dimensionality?",
          "Build Your Own RBF Network",
          "Relationship to Deep Learning Neural Networks",
          "Neural Network-SVM Mashup",
          "Neural Networks Section Conclusion"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ]
      },
      "requirements": [
        "Calculus, Matrix Arithmetic / Geometry, Basic Probability",
        "Python and Numpy coding",
        "Logistic Regression"
      ],
      "description": "Support Vector Machines (SVM) are one of the most powerful machine learning models around, and this topic has been one that students have requested ever since I started making courses.\nThese days, everyone seems to be talking about deep learning, but in fact there was a time when support vector machines were seen as superior to neural networks. One of the things you’ll learn about in this course is that a support vector machine actually is a neural network, and they essentially look identical if you were to draw a diagram.\nThe toughest obstacle to overcome when you’re learning about support vector machines is that they are very theoretical. This theory very easily scares a lot of people away, and it might feel like learning about support vector machines is beyond your ability. Not so!\nIn this course, we take a very methodical, step-by-step approach to build up all the theory you need to understand how the SVM really works. We are going to use Logistic Regression as our starting point, which is one of the very first things you learn about as a student of machine learning. So if you want to understand this course, just have a good intuition about Logistic Regression, and by extension have a good understanding of the geometry of lines, planes, and hyperplanes.\nThis course will cover the critical theory behind SVMs:\nLinear SVM derivation\nHinge loss (and its relation to the Cross-Entropy loss)\nQuadratic programming (and Linear programming review)\nSlack variables\nLagrangian Duality\nKernel SVM (nonlinear SVM)\nPolynomial Kernels, Gaussian Kernels, Sigmoid Kernels, and String Kernels\nLearn how to achieve an infinite-dimensional feature expansion\nProjected Gradient Descent\nSMO (Sequential Minimal Optimization)\nRBF Networks (Radial Basis Function Neural Networks)\nSupport Vector Regression (SVR)\nMulticlass Classification\n\n\nFor those of you who are thinking, \"theory is not for me\", there’s lots of material in this course for you too!\nIn this course, there will be not just one, but two full sections devoted to just the practical aspects of how to make effective use of the SVM.\nWe’ll do end-to-end examples of real, practical machine learning applications, such as:\nImage recognition\nSpam detection\nMedical diagnosis\nRegression analysis\nFor more advanced students, there are also plenty of coding exercises where you will get to try different approaches to implementing SVMs.\nThese are implementations that you won't find anywhere else in any other course.\n\n\nThanks for reading, and I’ll see you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nCalculus\nMatrix Arithmetic / Geometry\nBasic Probability\nLogistic Regression\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Beginners who want to know how to use the SVM for practical problems",
        "Experts who want to know all the theory behind the SVM",
        "Professionals who want to know how to effectively tune the SVM for their application"
      ]
    },
    {
      "title": "Build 75 Powerful Data Science & Machine Learning Projects",
      "url": "https://www.udemy.com/course/real-world-data-science-projects-practically/",
      "bio": "Build & Deploy Data Science, Machine Learning, Deep Learning (Python, Flask, Django, AWS, Azure, GCP, Heruko Cloud)",
      "objectives": [
        "Understand the full product workflow for the machine learning lifecycle",
        "Implement Machine Learning Algorithms, Learn how to improve your Machine Learning Models",
        "Real life case studies and projects to understand how things are done in the real world",
        "Make robust Machine Learning models, Master Machine Learning on Python",
        "Explore how to deploy your machine learning models.",
        "Clean your input data to remove outliers",
        "Know which Machine Learning model to choose for each type of problem",
        "Build a portfolio of work to have on your resume"
      ],
      "course_content": {
        "Introduction To The Course": [
          "Introduction To The Course",
          "Udemy Course Outline",
          "Course Bonuses: Cheat Sheets, Downloads, Mind maps, Guides.",
          "Udemy Course Feedback"
        ],
        "Project-1: Build Pan Card Detector": [
          "Introduction",
          "Loading Libraries and Dataset",
          "Creating the Pan Card Detector",
          "Creating the Flask App",
          "Creating Important Functions",
          "Deploy the App in Heroku",
          "Testing the Deployed Pan Card Detector",
          "Download the project files"
        ],
        "Project-2: Build Dog Breed Prediction": [
          "Introduction",
          "Importing the Data and Libraries",
          "Data Preprocessing",
          "Build and Train Model",
          "Testing the Model",
          "Creating the Flask App",
          "Running the App in System",
          "Download the project files"
        ],
        "Project-3: Image Watermarking App -Deploy On Heroku": [
          "Introduction",
          "Importing Libraries",
          "Create Text and Image Watermark",
          "Creating the App",
          "Deploying the App",
          "Download The Project Files"
        ],
        "Project-4: Traffic sign classification": [
          "Introduction",
          "Importing the Data",
          "Image processing",
          "Creating and Testing the Model",
          "Creating Model for Test Set",
          "Download The Project Files"
        ],
        "Project-5: Text Extraction From Images Application": [
          "Introduction",
          "Importing Libraries and Data",
          "Extracting the Text from Image",
          "Modifying the Extractor",
          "Creating the Extractor App",
          "Running the Extractor App",
          "Download The Project Files"
        ],
        "Project-6: Plant Disease Prediction Streamlit App": [
          "Introduction to Plant Disease Prediction Streamlit App",
          "Importing Libraries and Data",
          "Understanding the Data",
          "Model Building",
          "Creating an App Using Streamlit",
          "Download The Project Files"
        ],
        "Project-7: Vehicle Detection And Counting Flask App": [
          "Introduction to Vehicle Detection",
          "Importing Libraries and Data",
          "Transforming Images and Creating Output",
          "Creating a Flask App",
          "Download The Project Files"
        ],
        "Project-8: Create A Face Swapping Flask App": [
          "Introduction",
          "Importing Libraries and Data",
          "Data Preprocessing and Creating Output",
          "Creating a Flask App",
          "Download The Project Files"
        ],
        "Project-9: Bird Species Prediction Flask App": [
          "Introduction to Bird Species Prediction",
          "Importing Libraries and Data",
          "Data Processing",
          "Creating ML Model",
          "Creating a Flask App",
          "Download The Project Files"
        ]
      },
      "requirements": [
        "Basic knowledge of machine learning"
      ],
      "description": "In This Course, Solve Business Problems Using Data Science Practically. Learn To Build & Deploy Machine Learning, Data Science, Artificial Intelligence, Auto Ml, Deep Learning, Natural Language Processing (Nlp) Web Applications Projects With Python (Flask, Django, Heroku, AWS, Azure, GCP, IBM Watson, Streamlit Cloud).\n\n\nAccording to Glassdoor, the average salary for a Data Scientist is $117,345/yr. This is above the national average of $44,564. Therefore, a Data Scientist makes 163% more than the national average salary.\nThis makes Data Science a highly lucrative career choice. It is mainly due to the dearth of Data Scientists resulting in a huge income bubble.\nSince Data Science requires a person to be proficient and knowledgeable in several fields like Statistics, Mathematics, and Computer Science, the learning curve is quite steep. Therefore, the value of a Data Scientist is very high in the market.\nA Data Scientist enjoys a position of prestige in the company. The company relies on its expertise to make data-driven decisions and enable them to navigate in the right direction.\nFurthermore, the role of a Data Scientist depends on the specialization of his employer company. For example – A commercial industry will require a data scientist to analyze their sales.\nA healthcare company will require data scientists to help them analyze genomic sequences. The salary of a Data Scientist depends on his role and type of work he has to perform. It also depends on the size of the company which is based on the amount of data they utilize.\nStill, the pay scale of Data scientists is way above other IT and management sectors. However, the salary observed by Data Scientists is proportional to the amount of work that they must put in. Data Science needs hard work and requires a person to be thorough with his/her skills.\nDue to several lucrative perks, Data Science is an attractive field. This, combined with the number of vacancies in Data Science makes it an untouched gold mine. Therefore, you should learn Data Science in order to enjoy a fruitful career.\n\n\nIn This Course, We Are Going To Work On 75 Real World Data Science, Machine Learning Projects Listed Below:\nProject-1: Pan Card Tempering Detector App -Deploy On Heroku\nProject-2: Dog breed prediction Flask App\nProject-3: Image Watermarking App -Deploy On Heroku\nProject-4: Traffic sign classification\nProject-5: Text Extraction From Images Application\nProject-6: Plant Disease Prediction Streamlit App\nProject-7: Vehicle Detection And Counting Flask App\nProject-8: Create A Face Swapping Flask App\nProject-9: Bird Species Prediction Flask App\nProject-10: Intel Image Classification Flask App\n\n\nProject-11: Language Translator App Using IBM Cloud Service -Deploy On Heroku\nProject-12: Predict Views On Advertisement Using IBM Watson -Deploy On Heroku\nProject-13: Laptop Price Predictor -Deploy On Heroku\nProject-14: WhatsApp Text Analyzer -Deploy On Heroku\nProject-15: Course Recommendation System -Deploy On Heroku\nProject-16: IPL Match Win Predictor -Deploy On Heroku\nProject-17: Body Fat Estimator App -Deploy On Microsoft Azure\nProject-18: Campus Placement Predictor App -Deploy On Microsoft Azure\nProject-19: Car Acceptability Predictor -Deploy On Google Cloud\nProject-20: Book Genre Classification App -Deploy On Amazon Web Services\n\n\nProject 21 : DNA classification Deep Learning for finding E.Coli -AWS - Deploy On AWS\nProject 22 : Predict the next word in a sentence. - AWS - Deploy On AWS\nProject 23 : Predict Next Sequence of numbers using LSTM - AWS - Deploy On AWS\nProject 24 : Keyword Extraction from text using NLP - Deploy On Azure\nProject 25 : Correcting wrong spellings (correct spelling prediction) - Deploy On Azure\nProject 26 : Music popularity classififcation - Deploy On Google App Engine\nProject 27 : Advertisement Classification - Deploy On Google App Engine\nProject 28 : Image Digit Classification - Deploy On AWS\nProject 29 : Emotion Recognition using Neural Network - Deploy On AWS\nProject 30 : Breast cancer Classification - Deploy On AWS\n\n\nProject-31: Sentiment Analysis Django App -Deploy On Heroku\nProject-32: Attrition Rate Django Application\nProject-33: Find Legendary Pokemon Django App -Deploy On Heroku\nProject-34: Face Detection Streamlit App\nProject-35: Cats Vs Dogs Classification Flask App\nProject-36: Customer Revenue Prediction App -Deploy On Heroku\nProject-37: Gender From Voice Prediction App -Deploy On Heroku\nProject-38: Restaurant Recommendation System\nProject-39: Happiness Ranking Django App -Deploy On Heroku\nProject-40: Forest Fire Prediction Django App -Deploy On Heroku\n\n\nProject-41: Build Car Prices Prediction App -Deploy On Heroku\nProject-42: Build Affair Count Django App -Deploy On Heroku\nProject-43: Build Shrooming Predictions App -Deploy On Heroku\nProject-44: Google Play App Rating prediction With Deployment On Heroku\nProject-45: Build Bank Customers Predictions Django App -Deploy On Heroku\nProject-46: Build Artist Sculpture Cost Prediction Django App -Deploy On Heroku\nProject-47: Build Medical Cost Predictions Django App -Deploy On Heroku\nProject-48: Phishing Webpages Classification Django App -Deploy On Heroku\nProject-49: Clothing Fit-Size predictions Django App -Deploy On Heroku\nProject-50: Build Similarity In-Text Django App -Deploy On Heroku\n\n\nProject-51 : Sonic wave velocity prediction using Signal Processing Techniques\nProject-52 : Estimation of Pore Pressure using Machine Learning\nProject-53 : Audio processing using ML\nProject-54 : Text characterisation using Speech recognition\nProject-55 : Audio classification using Neural networks\nProject-56 : Developing a voice assistant\nProject-57 : Customer segmentation\nProject-58 : FIFA 2019 Analysis\nProject-59 : Sentiment analysis of web scrapped data\nProject-60 : Determing Red Vine Quality\n\n\nProject-61: Heart Attack Risk Prediction Using Eval ML (Auto ML)\nProject-62: Credit Card Fraud Detection Using Pycaret (Auto ML)\nProject-63: Flight Fare Prediction Using Auto SK Learn (Auto ML)\nProject-64: Petrol Price Forecasting Using Auto Keras\nProject-65: Bank Customer Churn Prediction Using H2O Auto ML\nProject-66: Air Quality Index Predictor Using TPOT With End-To-End Deployment (Auto ML)\nProject-67: Rain Prediction Using ML models & PyCaret With Deployment (Auto ML)\nProject-68: Pizza Price Prediction Using ML And EVALML(Auto ML)\nProject-69: IPL Cricket Score Prediction Using TPOT (Auto ML)\nProject-70: Predicting Bike Rentals Count Using ML And H2O Auto ML\n\n\nProject-71: Concrete Compressive Strength Prediction Using Auto Keras (Auto ML)\nProject-72: Bangalore House Price Prediction Using Auto SK Learn (Auto ML)\nProject-73: Hospital Mortality Prediction Using PyCaret (Auto ML)\nProject-74: Employee Evaluation For Promotion Using ML And Eval Auto ML\nProject-75: Drinking Water Potability Prediction Using ML And H2O Auto ML\n\n\nThe Only Course You Need To Become A Data Scientist, Get Hired And Start A New Career\n\n\nNote (Read This): This Course Is Worth Of Your Time And Money, Enroll Now Before Offer Expires.",
      "target_audience": [
        "Beginners in data science"
      ]
    },
    {
      "title": "Docker Course from Basics to Advanced [Full course]",
      "url": "https://www.udemy.com/course/docker-container-course-for-beginners/",
      "bio": "Dive into the world of Docker and learn about Dockerfiles and Container Management",
      "objectives": [
        "Container concepts in docker",
        "Docker container management",
        "Docker image management",
        "Basics of Dockerfile"
      ],
      "course_content": {},
      "requirements": [
        "Working knowledge of Unix/Linux"
      ],
      "description": "Welcome to the Docker for Microservices Course!\nIn the ever-evolving landscape of software development, embracing Microservices Architecture has become imperative for organizations seeking agility, scalability, and efficiency. This course is your gateway to mastering Docker, a pivotal technology in the Microservices ecosystem. Join us as we explore the why, what, and how of leveraging Docker to build, deploy, and scale Microservices effectively.\nWhy Docker for Microservices? Docker has revolutionized the way we package, distribute, and run applications. When combined with Microservices, Docker offers a powerful solution to the challenges of deploying and managing complex systems. By encapsulating each microservice in a lightweight, portable container, Docker ensures consistency across various environments, eliminates dependency issues, and facilitates seamless collaboration between development and operations teams. This course will unveil the synergy between Docker and Microservices, enabling you to create a robust and efficient deployment pipeline.\nWhat is Docker and Microservices? Docker is a containerization platform that allows developers to encapsulate applications and their dependencies in isolated containers. Microservices, on the other hand, break down monolithic applications into smaller, independently deployable services. Together, Docker and Microservices empower organizations to achieve continuous integration and delivery, optimize resource utilization, and enhance scalability. In this course, you'll gain hands-on experience with Docker, learning how to containerize microservices and orchestrate them for seamless deployment.\nHow to Harness Docker for Microservices Success? This course is designed to demystify the implementation of Docker in Microservices Architecture. We'll guide you through the fundamentals of Docker containerization, exploring concepts such as Docker images, containers, and orchestration tools like Docker Compose and Kubernetes. You'll discover best practices for designing containerized microservices, ensuring efficient communication between services, and managing the complete lifecycle of your applications.\nThrough a combination of practical demonstrations, real-world examples, and interactive exercises, you'll not only understand the intricacies of Docker for Microservices but also gain the skills necessary to navigate the complexities of modern software development.\nJoin us on this transformative journey and unlock the potential of Docker for Microservices. Whether you're a developer aiming to streamline your workflow or an IT professional looking to optimize your infrastructure, this course is your passport to mastering the art of containerized Microservices with Docker. Let's dive in and shape the future of scalable and resilient applications together!\n\nContainerization of the applications is going on in the full swing across the IT industry. Docker's course covers the fundamental concepts of Docker containers. Along with the concepts it also covers the most useful commands related to container management, image management, and Dockerfile. After studying this course one would be ready to dive deeper into the world of container orchestration.\nDocker's course becomes the necessary prerequisite for learning Docker Swarm and Kubernetes.\n\nEnroll now!! see you in class.\n\nHappy Learning!\nTeam Edyoda",
      "target_audience": [
        "Freshers who are new to Docker"
      ]
    },
    {
      "title": "Data Science: Python for Data Analysis Full Bootcamp",
      "url": "https://www.udemy.com/course/mastering-python-data-handling-analysis-and-visualization/",
      "bio": "Learn and build your Python Programming skills from the ground up in addition to Python Data Science libraries and tools",
      "objectives": [
        "Code with Python Programming Language",
        "Python Functional Programming",
        "Structure Data using collection containers",
        "Object-Oriented Design",
        "Advanced Python Foundations",
        "Handling Data with Python Libraries",
        "Numerical Python",
        "Extracting and Analyzing data from different resources",
        "Data Analysis with Pandas",
        "Data Visualization using matplotlib",
        "Advanced Visualization with Seaborn",
        "Build Python solutions for data science",
        "Get Instructor QA Support and help"
      ],
      "course_content": {},
      "requirements": [
        "No Python prior experience is required to take this Training",
        "Computer and Internet access"
      ],
      "description": "Hello and welcome to Data Science: Python for Data Analysis Full Bootcamp.\nData science is a huge field, and one of the promising fields that is spreading in a fast way. Also, it is one of the very rewarding, and it is increasing in expansion day by day, due to its great importance and benefits, as it is the future.\nData science enables companies to measure, track, and record performance metrics for facilitating and enhancing decision making. Companies can analyze trends to make critical decisions to engage customers better, enhance company performance, and increase profitability.\nAnd the employment of data science and its tools depends on the purpose you want from them.\nFor example, using data science in health care is very different from using data science in finance and accounting, and so on. And I’ll show you the core libraries for data handling, analysis and visualization which you can use in different areas.\nOne of the most powerful programming languages that are used for Data science is Python, which is an easy, simple and very powerful language with many libraries and packages that facilitate working on complex and different types of data.\n\n\nThis course will cover:\nPython tools for Data Analysis\nPython Basics\nPython Fundamentals\nPython Object-Oriented\nAdvanced Python Foundations\nData Handling with Python\nNumerical Python(NumPy)\nData Analysis with Pandas\nData Visualization with Matplotlib\nAdvanced Graphs with Seaborn\nInstructor QA Support and Help\nHD Video Training + Working Files + Resources + QA Support.\nIn this course, you will learn how to code in Python from the beginning and then you will master how to deal with the most famous libraries and tools of the Python language related to data science, starting from data collection, acquiring and analysis to visualize data with advanced techniques, and based on that, the necessary decisions are taken by companies.\nI am Ahmed Ibrahim, a software engineer and Instructor and I have taught more than 500,000 engineers and developers around the world in topics related to programming languages and their applications, and in this course, we will dive deeply into the core Python fundamentals, Advanced Foundations, Data handling libraries, Numerical Python, Pandas, Matplotlib and finally Seaborn.\nI hope that you will join us in this course to master the Python language for data analysis and Visualization like professionals in this field.\nWe have a lot to cover in this course.\nLet’s get started!",
      "target_audience": [
        "Python beginners and newbies",
        "Data Scientist who knows other language tools",
        "New Python Data Analysts",
        "Data Science Beginners",
        "New developers and Programmers",
        "Programmers and developers who know other programming language but are new to python",
        "Anyone who wants to use Python for data analysis and visualization in a short time!"
      ]
    },
    {
      "title": "Python for Data Science",
      "url": "https://www.udemy.com/course/python-data-science-master-course/",
      "bio": "Level up in Data Science using Python, master Numpy, Pandas, Data Visualisation, Web Scraping, Automation, SQL and more.",
      "objectives": [
        "Python fundamentals for beginners!",
        "Learn to use Python for Data Science",
        "Data Acquistion using Beautiful Soup, Scrapy",
        "Automation using Selenium",
        "Data Analysis using Numpy, Pandas, SQL",
        "Data Visualisation using Seaborn, Matplotlib",
        "Introduction to Machine Learning",
        "Building 5 projects using data science concepts"
      ],
      "course_content": {
        "Introduction": [
          "Course Orientation",
          "Code Repository!!!",
          "Doubt Support Guidelines",
          "Course Update Logs"
        ],
        "Setting Up the Environment. !!!": [
          "Installing Anaconda",
          "Installing Anaconda [for Windows]",
          "Jupyter Notebook Overview"
        ],
        "Python Fundamentals": [
          "Print, Variable and Datatypes",
          "Input in Python",
          "Operators in Python",
          "Conditional Statements",
          "Loops and Iterations",
          "Break and Continue",
          "Odd Vs. Even",
          "Largest Number",
          "Factorial of a Number",
          "Prime Numbers",
          "Python Fundamental Quiz"
        ],
        "Python In-built Data Structures": [
          "Strings",
          "String Functions",
          "Palindrome Strings",
          "Count Vowels",
          "Lists",
          "List Comprehension",
          "Second Largest Element",
          "No Duplicacy",
          "Tuples",
          "Sets",
          "Dictionaries",
          "Reverse a Dictionary",
          "Most Frequent",
          "Pythion In-Built Data Structures Quiz"
        ],
        "Python Functions and Modules": [
          "Function Introduction",
          "Default Arguments in Functions",
          "Args and kwargs",
          "In-built Functions",
          "Print Pattern",
          "Prime Numbes - II",
          "What are Python Modules?",
          "Custom Modules",
          "What is __name__ == '__main__' ?",
          "Python Functions and Modules Quiz"
        ],
        "Object Oriented Programming system (OOPs)": [
          "OOPS : Classes and Objects",
          "Constructors",
          "Instance Methods",
          "Class Variables",
          "Add die() method",
          "Magic Functions",
          "Inheritance",
          "Add kill() method",
          "Polymorphism",
          "OOPs Test",
          "Python OOPs Quiz"
        ],
        "Statistics and Probability": [
          "What is Data ?",
          "What is Statistics & Probability",
          "Population Vs Sample",
          "Types of Statistics",
          "Mean, Mode, Median",
          "Range, Variance, Deviation",
          "Histograms",
          "Normal Distribution",
          "Standardization",
          "Marginal, Joint and Conditional Probability",
          "Questions on Probability",
          "Bayes Theorem"
        ],
        "Numpy": [
          "Why Numpy?",
          "Numpy Arrays",
          "Special Arrays in Numpy",
          "Indexing and Masking",
          "Basic Operations in Numpy Arrays",
          "More Operations in Arrays",
          "Shape Manipulation",
          "Broadcasting",
          "Vectorisation",
          "Numpy Assignment",
          "Quiz Time : NumPy",
          "Padded Matrix"
        ],
        "Data Analysis- Pandas": [
          "Pandas Introduction and Series",
          "Introduction to DataFrames",
          "Indexing in DataFrames",
          "Masking and Boolean Indexing",
          "Iris Dataset",
          "Grouping Data",
          "Handling Missing Data",
          "Concatenate DataFrames",
          "Merging DataFrames",
          "Output Files",
          "Pandas Assignment",
          "Quiz Time : Pandas"
        ],
        "Data Visualisation - Matplotlib": [
          "Introduction to Matplotlib - Line Plots",
          "Scatter Plot",
          "Bar Graph",
          "Pie Chart",
          "Histograms",
          "Creating Subplots",
          "3D Plots",
          "Working with Images",
          "Border Image Challenge Solution",
          "Visualising Iris Dataset",
          "Movie Visualisation Challenge",
          "Movie Visualisation Challenge - Solution"
        ]
      },
      "requirements": [
        "Be able to use a computer with internet access",
        "Passion to learn, discuss and explore",
        "Basic programming experience (recommended)"
      ],
      "description": "Are you ready to take the next leap in your journey to become a Data Scientist?\n\nThis hands-on course is designed for absolute beginners as well as for proficient programmers who want to use the Python for solving real life problems. You will learn how analyse data, make interesting data visualisations, drive insights, scrape web, automate boring tasks and working with databases using SQL.\nData Science has one of the most rewarding jobs of the 21st century and fortune-500 tech companies are spending heavily on data scientists! Data Science as a career is very rewarding and offers one of the highest salaries in the world. This course is designed for both beginners with some programming experience or experienced developers looking to enter the world of Data Science!\nThis comprehensive course is taught by Mohit Uniyal, who is a popular Data Science Bootcamp instructor in India and has taught thousands of students in several online and in-person courses over last 3+ years. This course is worth thousands of dollars, but Coding Minutes is providing you this course to you at a fraction of its original cost!  This is action oriented course, we not just delve into theory but focus on the practical aspects by building 5 projects. With over 150+ High Quality video lectures, easy to understand explanations and complete code repository this is one of the most detailed and robust course for learning data science.\nThe course starts with basics of Python and then diving deeper into data science topics! Here are some of the topics that you will learn in this course.\nProgramming with Python\nNumeric Computation using NumPy\nData Analysis using Pandas\nData Visualisation using Matplotlib\nData Visualisation using Seaborn\nFetching data from Web API's\nData Acquisition\nWeb Scraping using Beautiful Soup\nBuilding a Web Crawler using Scrapy\nAutomating boring stuff using Selenium\nLanguage of Databases - SQL!\nIntroduction to Machine Learning\nand much, much more!\nSign up for the course and take your first step towards becoming a data science engineer! See you in the course!",
      "target_audience": [
        "Beginner Python developers curious about data science",
        "Working professionals with no prior experience in data science",
        "College graduates pursuing Master's or Ph.d in Machine Learning /Data Science",
        "Developers looking forward to build a career in data science"
      ]
    },
    {
      "title": "Learn Python for Data Science & Machine Learning from A-Z",
      "url": "https://www.udemy.com/course/python-for-data-science-machine-learning/",
      "bio": "Become a professional Data Scientist and learn how to use NumPy, Pandas, Machine Learning and more!",
      "objectives": [
        "Become a professional Data Scientist, Data Engineer, Data Analyst or Consultant",
        "Learn data cleaning, processing, wrangling and manipulation",
        "How to create resume and land your first job as a Data Scientist",
        "How to use Python for Data Science",
        "How to write complex Python programs for practical industry scenarios",
        "Learn Plotting in Python (graphs, charts, plots, histograms etc)",
        "Learn to use NumPy for Numerical Data",
        "Machine Learning and it's various practical applications",
        "Supervised vs Unsupervised Machine Learning",
        "Learn Regression, Classification, Clustering and Sci-kit learn",
        "Machine Learning Concepts and Algorithms",
        "K-Means Clustering",
        "Use Python to clean, analyze, and visualize data",
        "Building Custom Data Solutions",
        "Statistics for Data Science",
        "Probability and Hypothesis Testing"
      ],
      "course_content": {
        "Introduction": [
          "Who is This Course For?",
          "Data Science + Machine Learning Marketplace",
          "Data Science Job Opportunities",
          "Data Science Job Roles",
          "What is a Data Scientist?",
          "How To Get a Data Science Job",
          "Data Science Projects Overview"
        ],
        "Data Science & Machine Learning Concepts": [
          "Why We Use Python?",
          "What is Data Science?",
          "What is Machine Learning?",
          "Machine Learning Concepts & Algorithms",
          "What is Deep Learning?",
          "Machine Learning vs Deep Learning"
        ],
        "Python For Data Science": [
          "What is Programming?",
          "Why Python for Data Science?",
          "What is Jupyter?",
          "What is Google Colab?",
          "Python Variables, Booleans and None",
          "Getting Started with Google Colab",
          "Python Operators",
          "Python Numbers & Booleans",
          "Python Strings",
          "Python Conditional Statements",
          "Python For Loops and While Loops",
          "Python Lists",
          "More about Lists",
          "Python Tuples",
          "Python Dictionaries",
          "Python Sets",
          "Compound Data Types & When to use each one?",
          "Python Functions",
          "Object Oriented Programming in Python"
        ],
        "Statistics for Data Science": [
          "Intro To Statistics",
          "Descriptive Statistics",
          "Measure of Variability",
          "Measure of Variability Continued",
          "Measures of Variable Relationship",
          "Inferential Statistics",
          "Measure of Asymmetry",
          "Sampling Distribution"
        ],
        "Probability & Hypothesis Testing": [
          "What Exactly is Probability?",
          "Expected Values",
          "Relative Frequency",
          "Hypothesis Testing Overview"
        ],
        "NumPy Data Analysis": [
          "Intro NumPy Array Data Types",
          "NumPy Arrays",
          "NumPy Arrays Basics",
          "NumPy Array Indexing",
          "NumPy Array Computations",
          "Broadcasting"
        ],
        "Pandas Data Analysis": [
          "Introduction to Pandas",
          "Introduction to Pandas Continued"
        ],
        "Python Data Visualization": [
          "Data Visualization Overview",
          "Different Data Visualization Libraries in Python",
          "Python Data Visualization Implementation"
        ],
        "Machine Learning": [
          "Introduction To Machine Learning"
        ],
        "Data Loading & Exploration": [
          "Exploratory Data Analysis"
        ]
      },
      "requirements": [
        "Students should have basic computer skills",
        "Students would benefit from having prior Python Experience but not necessary"
      ],
      "description": "Learn Python for Data Science & Machine Learning from A-Z\nIn this practical, hands-on course you’ll learn how to program using Python for Data Science and Machine Learning. This includes data analysis, visualization, and how to make use of that data in a practical manner.\nOur main objective is to give you the education not just to understand the ins and outs of the Python programming language for Data Science and Machine Learning, but also to learn exactly how to become a professional Data Scientist with Python and land your first job.\nWe'll go over some of the best and most important Python libraries for data science such as NumPy, Pandas, and Matplotlib +\nNumPy —  A library that makes a variety of mathematical and statistical operations easier; it is also the basis for many features of the pandas library.\nPandas — A Python library created specifically to facilitate working with data, this is the bread and butter of a lot of Python data science work.\nNumPy and Pandas are great for exploring and playing with data. Matplotlib is a data visualization library that makes graphs as you’d find in Excel or Google Sheets. Blending practical work with solid theoretical training, we take you from the basics of Python Programming for Data Science to mastery.\nThis Machine Learning with Python course dives into the basics of machine learning using Python. You'll learn about supervised vs. unsupervised learning, look into how statistical modeling relates to machine learning, and do a comparison of each.\nWe understand that theory is important to build a solid foundation, we understand that theory alone isn’t going to get the job done so that’s why this course is packed with practical hands-on examples that you can follow step by step. Even if you already have some coding experience, or want to learn about the advanced features of the Python programming language, this course is for you!\nPython coding experience is either required or recommended in job postings for data scientists, machine learning engineers, big data engineers, IT specialists, database developers, and much more. Adding Python coding language skills to your resume will help you in any one of these data specializations requiring mastery of statistical techniques.\nTogether we’re going to give you the foundational education that you need to know not just on how to write code in Python, analyze and visualize data and utilize machine learning algorithms but also how to get paid for your newly developed programming skills.\nThe course covers 5 main areas:\n\n1: PYTHON FOR DS+ML COURSE INTRO\nThis intro section gives you a full introduction to the Python for Data Science and Machine Learning course, data science industry, and marketplace, job opportunities and salaries, and the various data science job roles.\nIntro to Data Science + Machine Learning with Python\nData Science Industry and Marketplace\nData Science Job Opportunities\nHow To Get a Data Science Job\nMachine Learning Concepts & Algorithms\n2: PYTHON DATA ANALYSIS/VISUALIZATION\nThis section gives you a full introduction to the Data Analysis and Data Visualization with Python with hands-on step by step training.\nPython Crash Course\nNumPy Data Analysis\nPandas Data Analysis\n3: MATHEMATICS FOR DATA SCIENCE\nThis section gives you a full introduction to the mathematics for data science such as statistics and probability.\nDescriptive Statistics\nMeasure of Variability\nInferential Statistics\nProbability\nHypothesis Testing\n4:  MACHINE LEARNING\nThis section gives you a full introduction to Machine Learning including Supervised & Unsupervised ML with hands-on step-by-step training.\nIntro to Machine Learning\nData Preprocessing\nLinear Regression\nLogistic Regression\nK-Nearest Neighbors\nDecision Trees\nEnsemble Learning\nSupport Vector Machines\nK-Means Clustering\nPCA\n5: STARTING A DATA SCIENCE CAREER\nThis section gives you a full introduction to starting a career as a Data Scientist with hands-on step by step training.\nCreating a Resume\nCreating a Cover Letter\nPersonal Branding\nFreelancing + Freelance websites\nImportance of Having a Website\nNetworking\nBy the end of the course you’ll be a professional Data Scientist with Python and confidently apply for jobs and feel good knowing that you have the skills and knowledge to back it up.",
      "target_audience": [
        "Students who want to learn about Python for Data Science & Machine Learning"
      ]
    },
    {
      "title": "Machine Learning Optimization Using Genetic Algorithm",
      "url": "https://www.udemy.com/course/machine-learning-optimization-using-genetic-algorithm/",
      "bio": "Learn how to optimize Machine Learning algorithms' performances and apply feature selection using Genetic Algorithm",
      "objectives": [
        "Apply Genetic Algorithm to optimize Machine Learning algorithms",
        "Apply Genetic Algorithm on Support Vector Machines and Multilayer Perceptron Neural Networks",
        "Apply Genetic Algorithm for Feature Selection",
        "Learn how to code Genetic Algorithm in Python from scratch"
      ],
      "course_content": {
        "Introduction": [
          "Promo Video",
          "Course Outline",
          "Support Vector Machine",
          "Neural Networks",
          "Quiz #1",
          "Optimization",
          "P vs. NP Problems Resource (IMPORTANT!!!)",
          "Metaheuristics",
          "Quiz #2"
        ],
        "Genetic Algorithm": [
          "PLEASE READ",
          "Genetic Algorithm #1",
          "Genetic Algorithm #2",
          "Genetic Algorithm #3",
          "Genetic Algorithm - Pseudocode and Flowchart",
          "Genetic Algorithm - Methodology",
          "The Purpose of Genetic Algorithm"
        ],
        "Dataset": [
          "Dataset",
          "Dataset"
        ],
        "Support Vector Machine Optimization for a Regression Problem": [
          "Updated Course (IMPORTANT)",
          "SVM Optimization #1 - Objective Function Value #1",
          "SVM Optimization #2 - Objective Function Value #2",
          "SVM Optimization #3 - Objective Function Value #3",
          "SVM Optimization #4 - Objective Function Value #4",
          "SVM Optimization #5 - Objective Function Value #5 (and the Dataset)",
          "The Dataset",
          "SVM Optimization #6 - Objective Function Value #6",
          "SVM Optimization #7 - Objective Function Value #7",
          "SVM Optimization #8 - Selecting Parents #1",
          "SVM Optimization #9 - Selecting Parents #2",
          "SVM Optimization #10 - Selecting Parents #3",
          "SVM Optimization #11 - Selecting Parents #4",
          "SVM Optimization #12 - Crossover Operator #1",
          "SVM Optimization #13 - Crossover Operator #2",
          "SVM Optimization #14 - Crossover Operator #3",
          "SVM Optimization #15 - Crossover Operator #4",
          "SVM Optimization #16 - Mutation Operator #1",
          "SVM Optimization #17 - Mutation Operator #2",
          "SVM Optimization #18 - Mutation Operator #3",
          "SVM Optimization #19 - Functions and Packages",
          "SVM Optimization #20 - Optimizing SVM on the Dataset #1",
          "SVM Optimization #21 - Optimizing SVM on the Dataset #2",
          "SVM Optimization #22 - Optimizing SVM on the Dataset #3",
          "SVM Optimization #23 - Optimizing SVM on the Dataset #4",
          "SVM Optimization #24 - Optimizing SVM on the Dataset #5",
          "SVM Optimization #25 - Optimizing SVM on the Dataset #6",
          "SVM Optimization #26 - Optimizing SVM on the Dataset #7"
        ],
        "Multilayer Perceptron Neural Network Optimization for a Regression Problem": [
          "Updated Course Reminder (IMPORTANT)",
          "MLP Optimization #1",
          "MLP Optimization #2",
          "MLP Optimization #3",
          "MLP Optimization #4",
          "MLP Optimization #5",
          "MLP Optimization #6"
        ],
        "Support Vector Machine Optimization for a Classification Problem": [
          "SVM Optimization"
        ],
        "Feature Selection Using Genetic Algorithm": [
          "Feature Selection #1",
          "Feature Selection #2",
          "Feature Selection #3"
        ],
        "BONUS OFFER!!": [
          "Bonus Lecture: Discounted Coupons"
        ],
        "Appendix": [
          "Machine Learning",
          "Quiz #3"
        ]
      },
      "requirements": [
        "Basic knowledge in Machine Learning",
        "Basic knowledge in Operations Research and Optimization - (not a must, but helpful)",
        "Basic programming skills in Python - (not a must, but helpful)"
      ],
      "description": "In this course, you will learn what hyperparameters are, what Genetic Algorithm is, and what hyperparameter optimization is. In this course, you will apply Genetic Algorithm to optimize the performance of Support Vector Machines (SVMs) and Multilayer Perceptron Neural Networks (MLP NNs). It is referred to as hyperparameter tuning or parameter tuning. You will also learn how to do feature selection using Genetic Algorithm.\nHyperparameter optimization will be done on two datasets:\nA regression dataset for the prediction of cooling and heating loads of buildings\nA classification dataset regarding the classification of emails into spam and non-spam\nThe SVM and MLP will be applied on the datasets without optimization and compare their results to after their optimization\nFeature Selection will be done on one dataset:\nClassification of benign tumors from malignant tumors in a breast cancer dataset\nBy the end of this course, you will have learnt how to code Genetic Algorithm in Python and how to optimize your machine learning algorithms for maximum performance. You would have also learnt how to apply Genetic Algorithm for feature selection.\nTo sum up:\nYou will learn what hyperparameters are (sometimes referred to as parameters, though different)\nYou will learn Genetic Algorithm\nYou will use Genetic Algorithm to optimize the performance of your machine learning algorithms\nMaximize your model's accuracy and predictive abilities\nOptimize the performance of SVMs and MLP Neural Networks\nApply feature selection to extract the features that are relevant to the predicted output\nGet the best out of your machine learning model\nRemove redundant features, which in return will reduce the time and complexity of your model\nUnderstand what are the features that have a relationship to the output and which do not\nYou do not need to have a lot of knowledge and experience in optimization or Python programming - it helps, but not a must to succeed in this course.\nThis course will teach you how to optimize the functionality of your machine learning algorithms\nWhere every single line of code is explained thoroughly\nThe code is written in a simple manner that you will understand how things work and how to code Genetic Algorithm even with zero knowledge in Python\nBasically, you can think of this as not only a course that teaches you how to optimize your machine learning model, but also Python programming!\nPlease feel free to ask me any question! Don't like the course? Ask for a 30-day refund!!\n\n\nReal Testaments -->\n1) \"This is my second course with Dana. This course is a combination of Metaheuristic and machine learning. It gives a wide picture of machine learning hyperparameter optimization. I recommend taking this course if you know basics of machine learning and you want to solve some problems using ML. By applying the techniques of GA optimization, you will have better performance of ML. The codes provided in this course are very straightforward and easy to understand. The course deserves five stars because of the lecture contents and examples. The instructor knowledgeable about the topic and talented in programming.\" -- Abdulaziz, 5 star rating\n2) \"An excellent course! Great for anyone interested in fine-tuning their machine-learning models. I really enjoyed the from scratch implementations and how well they are explained. These implementations from scratch help one understand the theory very well. An interesting thing to point out is that this course uses Metaheustistics to optimise machine-learning. However, you can use machine-learning classifiers to help your Metaheuristic predict good or bad regions.\" -- Dylan, 5 star rating\n3) \"Very helpful, for application of optimization algorithm to optimize ML algorithm parameters and got to do this using python, wonderful.\" -- Erigits, 5 star rating\n4) \"well explained course. The topic is not an easy one but to date the explanations have been clear. The course has an interesting spreadsheet project.\" -- Martin, 5 star rating\n5) \"Thank you very much for this awesome course. Lots of new things learn from this course.\" -- Md. Mahmudul, 5 star rating",
      "target_audience": [
        "Anyone who wants to learn Genetic Algorithm",
        "Anyone who would like to optimize the functionality of their Machine Learning algorithms",
        "Anyone who would like to learn feature selection",
        "Anyone who wants to code Genetic Algorithm in Python"
      ]
    },
    {
      "title": "LLMs with Google Cloud and Python",
      "url": "https://www.udemy.com/course/llms-with-google-cloud-and-python/",
      "bio": "Learn to use Google Cloud's latest LLM models in Google Cloud Platform and with Python!",
      "objectives": [
        "Gain a comprehensive understanding of how large language models function, along with their core components and mechanisms.",
        "Master the setup and utilization of Large Language Models on the Google Cloud platform, including understanding the pricing structure and model options.",
        "Develop the ability to effectively use Vertex AI's Python LLM API, understanding and implementing various parameters like token limit, temperature, and more.",
        "Acquire practical skills in designing and developing a customer service chatbot using generative AI, ensuring it meets industry standards.",
        "Delve into the nuances of coding models, from generating prompts to ensuring accurate code completions, enhancing your skill set as a developer.",
        "Understand the art and science of prompt engineering, learning to structure, ideate, and transform prompts for tasks such as summarization, classification, extr",
        "Learn advanced techniques in text-embedding and context injection, gaining proficiency in similarity searches and context-based prompting.",
        "Enhance your ability to fine-tune models using the Google Cloud Console, ensuring optimal performance and outcomes for specific tasks and applications."
      ],
      "course_content": {},
      "requirements": [
        "Python programming experience",
        "Permissions to set-up a Google Cloud Account (Credit Card may be required)"
      ],
      "description": "Unlock the Hidden Potential of Large Language Models with this Google Cloud Course!\n\n\nStep into the transformative realm of language models and learn how to harness their expansive potential with Google Cloud and Python. This in-depth Udemy course offers a perfect fusion of theoretical insights and practical skills.\n\n\nAbout the Course:\n\n\nThe course kicks off with a solid foundation in Large Language Models, helping students understand their complexity and functioning. As you delve into the main modules, you will become proficient in using the Google Cloud platform, effortlessly navigating the Generative AI Studio, comprehending its pricing, and selecting the best model for your needs. The course also delves into effective methods for Zero, One, and Few Shot Prompting, offering a comprehensive learning journey.\n\n\nWe then explore the nuances of the Vertex AI Python LLM API, shedding light on parameters essential for fine-tuning models to peak performance. You'll learn about everything from token limits to temperature settings and sophisticated stop sequences in great detail.\n\n\nA highlight of this course is the practical labs, where students can create various tools, such as an advanced Customer Service Chatbot and a state-of-the-art Translation and Summarization AI Bot. These labs go beyond coding, applying theoretical knowledge to tangible, real-world applications.\n\n\nThe course also ventures into the captivating area of prompt engineering. Students will learn to craft effective prompts for tasks like summarization and extraction. In addition, you'll explore text embeddings, learning about context injection in prompts and boosting model accuracy with similarity searches!\n\n\nBy the end of the course, students will have the skills to customize their models in the Google Cloud Console, tailoring them to their specific objectives.\n\n\nWho Should Enroll?\n\n\nThis course is ideal for AI enthusiasts looking to broaden their knowledge, developers who want to integrate advanced language models into their projects seamlessly, and anyone fascinated by the wonders of Google Cloud and Python in AI.\n\n\nYour Future Is Here!\n\n\nBegin a journey that combines deep theoretical knowledge with practical expertise. With its expert-led instruction and structured modules, this course is a guiding light for those passionate about the wonders of language models.\n\n\nDecide today and shape your future.",
      "target_audience": [
        "Python developers looking to leverage Large Language Models from Google Cloud"
      ]
    },
    {
      "title": "Data Lake, Firehose, Glue, Athena, S3 and AWS SDK for .NET",
      "url": "https://www.udemy.com/course/data-lake-firehose-glue-athena-s3-and-aws-sdk-for-net/",
      "bio": "Leverage AWS Kinesis Data Firehose, AWS Glue, S3, Athena and the AWS SDK to build a Data Lake.",
      "objectives": [
        "How to build a Data Lake using Firehose API for .NET, S3, AWS Glue and Athena"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Overview of AWS Web Services in a Data Lake": [
          "AWS Serverless pieces to our Data Lake jigsaw puzzle"
        ],
        "AWS RDS and S3": [
          "S3 Bucket creation",
          "RDS creation"
        ],
        "AWS Kinesis Firehose, C# and the AWS SDK": [
          "Creating Kinesis Data Firehose delivery streams",
          "Console .NET core C# entity-framework project",
          "Kinesis Firehose C# API class",
          "Kinesis Firehose C# API send data example"
        ],
        "AWS Glue, Athena, ETL jobs, Triggers, and using Hangfire": [
          "Creating Glue Crawlers",
          "Query Data in Athena using SQL",
          "Creating AWS ETL jobs",
          "Hangfire - Automation"
        ],
        "AWS Lake Formation, Cognito and Parquet .NET": [
          "AWS Lake Formation",
          "AWS Cognito",
          "Parquet .NET - Alternative to using Firehose"
        ],
        "Final thoughts and advice": [
          "Conclusion"
        ]
      },
      "requirements": [
        "You must have an active AWS Account to utilized the included code.",
        "Prior knowledge of AWS Web Services would be helpful, but not needed."
      ],
      "description": "The purpose of this class is to demonstrate a proof of concept using a series of lab exercise's (in the AWS Console using AWS Kinesis Data Firehose, AWS Glue, S3, Athena and the AWS SDK, with C# code using the AWS SDK) of building a Data Lake in the AWS ecosystem.\nIn this class, we will be sending data from a local SQL Server database to AWS RDS securely, and automatically.\nI am utilizing the .NET SDK for AWS, however, this could easily be migrated to your language of choice once you understand the concepts that I am teaching.\nI will be providing the full working source code of this proof of concept.\nI will also provide a working example of creating Parquet files and sending to S3 (without using Firehose).",
      "target_audience": [
        "AWS developers interested in learning about building a data lake using AWS services and the AWS SDK."
      ]
    },
    {
      "title": "Data Analysis Real world use-cases- Hands on Python",
      "url": "https://www.udemy.com/course/data-analysis-real-world-use-cases-hands-on-python/",
      "bio": "Build a Portfolio of 5 Data Analysis Projects with Python, Seaborn,Pandas,Plotly, numpy etc & get a job of Data Analyst",
      "objectives": [
        "Get a job as a data Analyst on an average $156,000 after showcase these Projects on your Resume",
        "By the end of this course you will understand the inner workings of the data analytics pipeline -joining,manipulating,filtering, extracting data ,Analysing Data",
        "Solve any problem in your business, job or in real-time with powerful data analysis libraries",
        "you will expertise in Pandas , Numpy , Seaborn, Matplotlib , Plotly ,Folium, Geopy , Wordcloud , re and many other..",
        "Learn how to work with various data within python, including: Excel Data,Geographical data,Text Data and Time Series Data Data",
        "Solve any problem in your business, job or in real-time with powerful data analysis libraries"
      ],
      "course_content": {
        "Intro to this course & course Benefits": [
          "Intro to this course",
          "Utilize QnA of the course ( Golden Oppurtunity ) !",
          "How to follow this course-Must Watch",
          "Installation of Anaconda Navigator",
          "Quick Summary of Jupyter Notebook"
        ],
        "Introduction to Data Analysis !": [
          "What is Data Analysis ?",
          "Data Analysis vs Data Science"
        ],
        "Introduction to Life-Cycle of Data Analytics Project": [
          "First Stage : Business Understanding in Real World",
          "Second Stage : What is ETL (Extract ,Transform,Load) Pipeline ?",
          "Third Stage : What is EDA ( Exploratory Data Analysis ) ?",
          "Fourth Stage : Conclusions and Dashboarding !"
        ],
        "Project 1-->> Uber New York Data Analysis": [
          "Datasets & Resources",
          "How to Read data for Analysis !",
          "Performing Data pre-processing/Data cleaning !",
          "Analysing which month have max Uber Pickups ?",
          "Analysing Hourly Rush in New york !",
          "Analysing most active Uber Base-number !",
          "Lets Collect entire data : Data Collection !",
          "Perform Spatial Analysis to find rush of Uber Pickups",
          "Perform Pair wise Analysis to figure out Rush !",
          "How to Automate Your Analysis"
        ],
        "Project 2-->> Bitcoin Data Analysis": [
          "Datasets & Resources",
          "Lets Read data & perform basic Descriptive analysis !",
          "Doing Data Pre-processing !",
          "Analysing change in price of the Bitcoin overtime !",
          "Analysing Bitcoin prices using Candle-stick chart",
          "Analysing closing price in-depth !",
          "Perform Analysis on closing Price on Yearly , Quarterly & monthly basis !",
          "Analysing Daily change in Closing price of stocks",
          "Dashboarding Part 1 : Dashboarding with Streamlit: Concepts & Setup",
          "Dashboarding Part 2 : How to create Drop-Down menu on Dashboards !",
          "Dashboarding Part 3 : how to add subplots on Dashbaords !",
          "Dashboarding Part 4 : Launching your Dashboard !"
        ],
        "Project 3-->> Amazon Customers Data Analysis": [
          "Datasets & Resources",
          "How to Read Data from SQLite Database.",
          "Data Preparation !",
          "How Amazon recommend product",
          "Analysing which product has good number of Reviews ?",
          "Understanding Behaviours of Amazon Users !",
          "Analysing your frequent Users !",
          "Perform Sentiment Analysis on Data."
        ],
        "Project 4-->> Hotel Booking Data Analysis": [
          "Datasets & Resources",
          "Lets read Data",
          "Doing Data cleaning !",
          "Performing descriptive analysis !",
          "Perform Spatial Analysis on Guests Home-Town",
          "Analysing Difference between assigned and reserved room types !",
          "which market segment has highest bookings ?",
          "Understanding Pattern in guests arrival !",
          "Analysing distribution of \"guests arrival\""
        ],
        "Project 5-->> Covid-19 Data Analysis": [
          "Datasets & Resources",
          "Prepare your Data for the Analysis",
          "Analysing Total cases, Deaths, Recovered & active cases",
          "Perform EDA on Data",
          "Analysing those countries that gets badly affected by Corona",
          "Perform In-depth Analysis on Data",
          "Automate Your Analysis"
        ],
        "Bonus Session": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "You will need to install Anaconda. We will show you how to do it in one of the first lectures of the course"
      ],
      "description": "This is the first course that gives hands-on Data Analysis Projects using Python..\n\n\nStudent Testimonials:\n\n\nShan Singh is absolutely amazing! Step-by-step projects with clear explanations. Easy to understand. Real-world  Data Analysis projects. Simply the best course on Data Analysis that I could find on Udemy! After the course you can easily start your career as a Data Analyst.- Nicholas Nita\n\n\nThis is the best course for people who have just learnt python basics(prerequisite for this course) and want to become Data Analyst/Data Scientist. This will act as bridge between fundamental theoretical python syntax to its application by using most important data analysis packages(Pandas, Matplotlib, Plotly etc). - Mirza Hyder Baig\n\n\nVery good course, on one side the instructor elaborates on technic general knowledge like what is integer (signed/un-signed and what it contains) on the other side he is very short and to the chase with the python commands and the requirements execution flow - Tal Ioffe\n\n\nsuperb. .. what a good soul he is ...his voice is filled with love and humbleness and understanding....he knows the pains of a begineer ...when he explains it fells like he is explaining to a 5 year old kid.... -\n\n\n\n\n\n\n\n\nCan you start right now?\nA frequently asked question of Python Beginners is: \"Do I need to become an expert in Python coding before I can start working on Data Analysis Projects?\"\nThe clear answer is: \"No!\nYou just require some Python Basics like data types, simple operations/operators, lists and numpy arrays that you can learn from my Free Python course 'Basics Of Python'\nAs a Summary, if you primarily want to use Python for Data Science/Data Analytics or as a replacement for Excel, then this course is a perfect match!\n\n\n\n\n\n\nWhy should you take this Course?\nIt explains Real-world Data Analysis Projects on  real Data . No toy data! This is the simplest & best way to become a Data Analyst/Data Scientist\nIt shows and explains the full real-world Data. Starting with importing messy data, cleaning data, merging and concatenating data, grouping and aggregating data, Exploratory Data Analysis through to preparing and processing data for Statistics, Data Analysis , Machine Learning and Data Presentation.\n\n\nIt gives you plenty of opportunities to practice and code on your own. Learning by doing.\nIn real-world Data Analysis projects, coding and the business side of things are equally important. This is probably the only course that teaches both: in-depth Python Coding and Big-Picture Thinking like How you can come up with a conclusion by doing Data Analysis ..\nGuaranteed Satisfaction: Otherwise, get your money back with 30-Days-Money-Back-Guarantee.",
      "target_audience": [
        "Everyone who want to step into Data Science/Data Analytics.",
        "Anyone interested about the rapidly expanding world of data Analytics/Data Science",
        "Data Scientists/Data Analyst who want to improve their Data Handling/Manipulation/Analysis skills.",
        "Anyone who want to switch Data Projects from Excel to Python (e.g. in Research/Science)",
        "Excel users looking to learn a more powerful software for data analysis"
      ]
    },
    {
      "title": "Artificial Intelligence with Machine Learning, Deep Learning",
      "url": "https://www.udemy.com/course/artificial-intelligence-with-machine-learning-deep-learning/",
      "bio": "Artificial Intelligence (AI) with Python Machine Learning and Python Deep Learning, Transfer Learning, Tensorflow",
      "objectives": [
        "Machine learning isn’t just useful for predictive texting or smartphone voice recognition.",
        "Learn Artificial intelligence with Machine Learning and deep learning with Hands-On Examples",
        "Machine Learning Terminology, machine learning a-z",
        "What is Machine Learning?",
        "Evaluation Metrics for Python machine learning, Python Deep learning",
        "Supervised Learning and unsupervised learning, transfer learning, ai, artificial intelligence programming",
        "Machine Learning with SciKit Learn",
        "Python, python machine learning and deep learning",
        "Machine Learning, machine learning A-Z",
        "Deep Learning, Deep learning a-z",
        "Machine learning is constantly being applied to new industries and new problems. Whether you’re a marketer, video game designer, or programmer",
        "Machine learning describes systems that make predictions using a model trained on real-world data.",
        "Machine learning is being applied to virtually every field today. That includes medical diagnoses, facial recognition, weather forecasts, image processing",
        "It's possible to use machine learning without coding, but building new systems generally requires code.",
        "What is the best language for machine learning? Python is the most used language in machine learning.",
        "Engineers writing machine learning systems often use Jupyter Notebooks and Python together.",
        "Machine learning is generally divided between supervised machine learning and unsupervised machine learning.",
        "Python instructors on Udemy specialize in everything from software development to data analysis, and are known for their effective, friendly instruction",
        "What are the limitations of Python? Python is a widely used, general-purpose programming language, but it has some limitations.",
        "How is Python used? Python is a general programming language used widely across many industries and platforms.",
        "How is Python used? Python is a general programming language used widely across many industries and platforms.",
        "How do I learn Python on my own? Python has a simple syntax that makes it an excellent programming language for a beginner to learn."
      ],
      "course_content": {
        "Numpy": [
          "Installing Anaconda Distribution for Windows",
          "Notebook Project Files Link regarding NumPy Python Programming Language Library",
          "Installing Anaconda Distribution for MacOs",
          "6 Article Advice And Links about Numpy, Numpy Pyhon",
          "Installing Anaconda Distribution for Linux",
          "Introduction to NumPy Library",
          "The Power of NumPy",
          "Creating NumPy Array with The Array() Function",
          "Creating NumPy Array with Zeros() Function",
          "Creating NumPy Array with Ones() Function",
          "Creating NumPy Array with Full() Function",
          "Creating NumPy Array with Arange() Function",
          "Creating NumPy Array with Eye() Function",
          "Creating NumPy Array with Linspace() Function",
          "Creating NumPy Array with Random() Function",
          "Properties of NumPy Array",
          "Identifying the Largest Element of a Numpy Array",
          "Detecting Least Element of Numpy Array: Min(), Ar",
          "Reshaping a NumPy Array: Reshape() Function",
          "Concatenating Numpy Arrays: Concatenate() Functio",
          "Splitting One-Dimensional Numpy Arrays: The Split",
          "Splitting Two-Dimensional Numpy Arrays: Split(),",
          "Sorting Numpy Arrays: Sort() Function",
          "Indexing Numpy Arrays",
          "Slicing One-Dimensional Numpy Arrays",
          "Slicing Two-Dimensional Numpy Arrays",
          "Assigning Value to One-Dimensional Arrays",
          "Assigning Value to Two-Dimensional Array",
          "Fancy Indexing of One-Dimensional Arrrays",
          "Fancy Indexing of Two-Dimensional Arrrays",
          "Combining Fancy Index with Normal Indexing",
          "Combining Fancy Index with Normal Slicing",
          "Operations with Comparison Operators",
          "Arithmetic Operations in Numpy",
          "Statistical Operations in Numpy",
          "Solving Second-Degree Equations with NumPy"
        ],
        "Pandas": [
          "Pandas Project Files Link",
          "Introduction to Pandas Library",
          "Creating a Pandas Series with a List",
          "Creating a Pandas Series with a Dictionary",
          "Creating Pandas Series with NumPy Array",
          "Object Types in Series",
          "Examining the Primary Features of the Pandas Seri",
          "Most Applied Methods on Pandas Series",
          "Indexing and Slicing Pandas Series",
          "Creating Pandas DataFrame with List",
          "Creating Pandas DataFrame with NumPy Array",
          "Creating Pandas DataFrame with Dictionary",
          "Examining the Properties of Pandas DataFrames",
          "Element Selection Operations in Pandas DataFrames: Lesson 1",
          "Element Selection Operations in Pandas DataFrames: Lesson 2",
          "Top Level Element Selection in Pandas DataFrames:Lesson 1",
          "Top Level Element Selection in Pandas DataFrames:Lesson 2",
          "Top Level Element Selection in Pandas DataFrames:Lesson 3",
          "Element Selection with Conditional Operations in Pandas Data Frames",
          "Adding Columns to Pandas Data Frames",
          "Removing Rows and Columns from Pandas Data frames",
          "Null Values in Pandas Dataframes",
          "Dropping Null Values: Dropna() Function",
          "Filling Null Values: Fillna() Function",
          "Setting Index in Pandas DataFrames",
          "Multi-Index and Index Hierarchy in Pandas DataFrames",
          "Element Selection in Multi-Indexed DataFrames",
          "Selecting Elements Using the xs() Function in Multi-Indexed DataFrames",
          "Concatenating Pandas Dataframes: Concat Function",
          "Merge Pandas Dataframes: Merge() Function: Lesson 1",
          "Merge Pandas Dataframes: Merge() Function: Lesson 2",
          "Merge Pandas Dataframes: Merge() Function: Lesson 3",
          "Merge Pandas Dataframes: Merge() Function: Lesson 4",
          "Joining Pandas Dataframes: Join() Function",
          "Loading a Dataset from the Seaborn Library",
          "Examining the Data Set 1",
          "Aggregation Functions in Pandas DataFrames",
          "Examining the Data Set 2",
          "Coordinated Use of Grouping and Aggregation Functions in Pandas Dataframes",
          "Advanced Aggregation Functions: Aggregate() Function",
          "Advanced Aggregation Functions: Filter() Function",
          "Advanced Aggregation Functions: Transform() Function",
          "Advanced Aggregation Functions: Apply() Function",
          "Examining the Data Set 3",
          "Pivot Tables in Pandas Library",
          "Accessing and Making Files Available",
          "Data Entry with Csv and Txt Files",
          "Data Entry with Excel Files",
          "Outputting as an CSV Extension",
          "Outputting as an Excel File"
        ],
        "First Contact with Machine Learning": [
          "What is Machine Learning?",
          "Machine Learning Terminology",
          "Project Files Link",
          "Quiz"
        ],
        "Evalution Metrics in Machine Learning": [
          "Classification vs Regression in Machine Learning",
          "Machine Learning Model Performance Evaluation: Classification Error Metrics",
          "Evaluating Performance: Regression Error Metrics in Python",
          "Machine Learning With Python",
          "Quiz"
        ],
        "Supervised Learning with Machine Learning": [
          "What is Supervised Learning in Machine Learning?",
          "Quiz"
        ],
        "Linear Regression Algorithm in Machine Learning A-Z": [
          "Linear Regression Algorithm Theory in Machine Learning A-Z",
          "Linear Regression Algorithm With Python Part 1",
          "Linear Regression Algorithm With Python Part 2",
          "Linear Regression Algorithm With Python Part 3",
          "Linear Regression Algorithm With Python Part 4"
        ],
        "Bias Variance Trade-Off in Machine Learning": [
          "What is Bias Variance Trade-Off?",
          "Quiz"
        ],
        "Logistic Regression Algorithm in Machine Learning A-Z": [
          "What is Logistic Regression Algorithm in Machine Learning?,",
          "Logistic Regression Algorithm with Python Part 1",
          "Logistic Regression Algorithm with Python Part 2",
          "Logistic Regression Algorithm with Python Part 3",
          "Logistic Regression Algorithm with Python Part 4",
          "Logistic Regression Algorithm with Python Part 5"
        ],
        "K-fold Cross-Validation in Machine Learning A-Z": [
          "K-Fold Cross-Validation Theory",
          "K-Fold Cross-Validation with Python"
        ],
        "K Nearest Neighbors Algorithm in Machine Learning A-Z": [
          "K Nearest Neighbors Algorithm Theory",
          "K Nearest Neighbors Algorithm with Python Part 1",
          "K Nearest Neighbors Algorithm with Python Part 2",
          "K Nearest Neighbors Algorithm with Python Part 3",
          "Quiz"
        ]
      },
      "requirements": [
        "Determination to learn artificial intelligence and patience",
        "Desire to master on python, machine learning a-z, deep learning a-z",
        "Motivation to learn the the second largest number of job postings relative program language among all others",
        "Learn to create Machine Learning and Deep Algorithms in Python Code templates included.",
        "Desire to learn artificial intelligence, deep learning, machine learning methods, supervised learning",
        "Desire to learn history of machine learning, ai, artificial learning",
        "Desire to learn fundamentals of machine learning, deep learning, artificial intelligence, ai, tensorflow"
      ],
      "description": "Hello there,\nWelcome to the “Artificial Intelligence with Machine Learning, Deep Learning ” course\nArtificial intelligence, Machine learning python, python, machine learning, Django, ethical hacking, python Bootcamp, data analysis, machine learning python, python for beginners, data science, machine learning, Django\nArtificial Intelligence (AI) with Python Machine Learning and Python Deep Learning, Transfer Learning, Tensorflow\nIt’s hard to imagine our lives without machine learning Predictive texting, email filtering, and virtual personal assistants like Amazon’s Alexa and the iPhone’s Siri, are all technologies that function based on machine learning algorithms and mathematical models\nAi, TensorFlow, PyTorch, scikit learn, reinforcement learning, supervised learning, teachable machine, python machine learning, TensorFlow python, ai technology, azure machine learning, semi-supervised learning, deep neural network, artificial general intelligence\nMachine learning isn’t just useful for predictive texting or smartphone voice recognition Machine learning is constantly being applied to new industries and new problems Whether you’re a marketer, video game designer, or programmer, my course on Udemy is here to help you apply machine learning to your work\n\nData Science Careers Are Shaping The Future\nData science experts are needed in almost every field, from government security to dating apps Millions of businesses and government departments rely on big data to succeed and better serve their customers So data science careers are in high demand\nUdemy offers highly-rated data science courses that will help you learn how to visualize and respond to new data, as well as develop innovative new technologies Whether you’re interested in machine learning, data mining, or data analysis, Udemy has a course for you\nIf you want to learn one of the employer’s most requested skills?\nIf you are curious about Data Science and looking to start your self-learning journey into the world of data with Python?\nIf you are an experienced developer and looking for a landing in Data Science!\nIn all cases, you are at the right place!\nWe've designed for you “Artificial Intelligence with Machine Learning, Deep Learning” a straightforward course for Python Programming Language and Machine Learning\nIn the course, you will have down-to-earth way explanations with projects With this course, you will learn machine learning step-by-step I made it simple and easy with exercises, challenges, and lots of real-life examples\nWe will open the door of the Data Science and Machine Learning a-z world and will move deeper You will learn the fundamentals of Machine Learning A-Z and its beautiful libraries such as Scikit Learn\nThroughout the course, we will teach you how to artificial intelligence and fundamentals of machine learning and use powerful machine learning python algorithms\nWhether you work in machine learning or finance or are pursuing a career in web development or data science, Python is one of the most important skills you can learn Python's simple syntax is especially suited for desktop, web, and business applications Python's design philosophy emphasizes readability and usability Python was developed upon the premise that there should be only one way (and preferably one obvious way) to do things, a philosophy that has resulted in a strict level of code standardization The core programming language is quite small and the standard library is also large In fact, Python's large library is one of its greatest benefits, providing a variety of different tools for programmers suited for many different tasks\nInterested in the field of Machine Learning? Then this course is for you! It was designed by two professional Data Scientists who will share their knowledge and help you learn complex theories, algorithms, and coding libraries in a simple way They will walk you step-by-step into the World of Machine Learning With every tutorial, you will develop new skills and improve your understanding of this challenging yet lucrative sub-field of Data Science\nLearn python and how to use it to python data analysis and visualization, present data Includes tons of code data visualization\nIn this course, you will learn artificial intelligence, machine learning, deep learning\n\nAlso during the course, you will learn:\n\nThis Machine Learning course is for everyone!\nMy \"Machine Learning with Hands-On Examples in Data Science\" is for everyone! If you don’t have any previous experience, not a problem! This course is expertly designed to teach everyone from complete beginners, right through to professionals ( as a refresher)\nWhy do we use a Python programming language in Machine learning?\nPython is a general-purpose, high-level, and multi-purpose programming language The best thing about Python is, it supports a lot of today’s technology including vast libraries for Twitter, data mining, scientific calculations, designing, back-end server for websites, engineering simulations, artificial learning, augmented reality and what not! Also, it supports all kinds of App development\nWhat you will learn?\nIn this course, we will start from the very beginning and go all the way to the end of \"Artificial intelligence with Machine Learning\" with examples\nBefore each lesson, there will be a theory part After learning the theory parts, we will reinforce the subject with practical examples\nDuring the course you will learn the following topics:\nWhat is Machine Learning?\nWhat is AI (artificial intelligence)?\nMore About Machine Learning\nMachine Learning Terminology\nEvaluation Metrics\nWhat is Classification vs Regression?\nEvaluating Performance-Classification Error Metrics\nEvaluating Performance-Regression Error Metrics\nMachine Learning with Python\nSupervised Learning\nartificial intelligence\nMachine learning\nMachine learning python\nEthical hacking, python Bootcamp\nFundamentals of Data analysis\nPython machine learning\nPython programming\nPython examples\nPython hands-on\nDeep learning a-z\nMachine learning a-z\nMachine learning & data science a-z\nmachine learning algorithms\nunsupervised learning\ntransfer learning\nwhat is numpy?\nwhat is data science?\nWith my up-to-date course, you will have a chance to keep yourself up-to-date and equip yourself with a range of Python programming skills I am also happy to tell you that I will be constantly available to support your learning and answer questions\nArtificial intelligence is growing exponentially There is no doubt about that But the further AI advances, the more complex the problems it needs to solve become The only way to solve such complex problems is with Deep Learning — which is why it's at the heart of Artificial Intelligence This course will help understand the broad and complex concept of Deep Learning in a robust, organized structure You will be working on real-world data sets to help you be confident in all the techniques on an instinctual level\n\nThis course has suitable for everybody who is interested in Machine Learning and Deep Learning concepts\nFirst of all, in this course, we will learn some fundamental stuff of Python and the Numpy library These are our first steps in our Deep Learning journey After then we take a little trip to Machine Learning history Then we will arrive at our next stop Machine Learning Here we learn the machine learning concepts, machine learning workflow, models and algorithms, and what is neural network concept After then we arrive at our next stop Artificial Neural network And now our journey becomes an adventure In this adventure we'll enter the Keras world then we exit the Tensorflow world Then we'll try to understand the Convolutional Neural Network concept But our journey won't be over Then we will arrive at Recurrent Neural Network and LTSM We'll take a look at them After a while, we'll trip to the Transfer Learning concept And then we arrive at our final destination Projects Our play garden Here we'll make some interesting machine learning models with the information we've learned along our journey\n\nDuring the course you will learn:\n\nWhat is the AI, Machine Learning, and Deep Learning\nHistory of Machine Learning\nTuring Machine and Turing Test\nThe Logic of Machine Learning such as\nUnderstanding the machine learning models\nMachine Learning models and algorithms\nGathering data\nData pre-processing\nChoosing the right algorithm and model\nTraining and testing the model\nEvaluation\nArtificial Neural Network with these topics\nWhat is ANN\nAnatomy of NN\nThe Engine of NN\nTensorflow\nConvolutional Neural Network\nRecurrent Neural Network and LTSM\nTransfer Learning\n\nIn this course, we will start from the very beginning and go all the way to the end of \"Deep Learning\" with examples\nBefore we start this course, we will learn which environments we can be used for developing deep learning projects\nWhy would you want to take this course?\nOur answer is simple: The quality of teaching\nOAK Academy based in London is an online education company OAK Academy gives education in the field of IT, Software, Design, development in English, Portuguese, Spanish, Turkish, and a lot of different languages on the Udemy platform where it has over 1000 hours of video education lessons OAK Academy both increases its education series number by publishing new courses, and it makes students aware of all the innovations of already published courses by upgrading\nWhen you enroll, you will feel the OAK Academy`s seasoned developers' expertise Questions sent by students to our instructors are answered by our instructors within 48 hours at the latest\nWhat is machine learning?\nMachine learning describes systems that make predictions using a model trained on real-world data For example, let's say we want to build a system that can identify if a cat is in a picture We first assemble many pictures to train our machine learning model During this training phase, we feed pictures into the model, along with information around whether they contain a cat While training, the model learns patterns in the images that are the most closely associated with cats This model can then use the patterns learned during training to predict whether the new images that it's fed contain a cat In this particular example, we might use a neural network to learn these patterns, but machine learning can be much simpler than that Even fitting a line to a set of observed data points, and using that line to make new predictions, counts as a machine learning model\nWhat is machine learning used for?\nMachine learning is being applied to virtually every field today That includes medical diagnoses, facial recognition, weather forecasts, image processing, and more In any situation in which pattern recognition, prediction, and analysis are critical, machine learning can be of use Machine learning is often a disruptive technology when applied to new industries and niches Machine learning engineers can find new ways to apply machine learning technology to optimize and automate existing processes With the right data, you can use machine learning technology to identify extremely complex patterns and yield highly accurate predictions\nDoes machine learning require coding?\nIt's possible to use machine learning without coding, but building new systems generally requires code For example, Amazon’s Rekognition service allows you to upload an image via a web browser, which then identifies objects in the image This uses a pre-trained model, with no coding required However, developing machine learning systems involves writing some Python code to train, tune, and deploy your models It's hard to avoid writing code to pre-process the data feeding into your model Most of the work done by a machine learning practitioner involves cleaning the data used to train the machine They also perform “feature engineering” to find what data to use and how to prepare it for use in a machine learning model Tools like AutoML and SageMaker automate the tuning of models Often only a few lines of code can train a model and make predictions from it An introductory understanding of Python will make you more effective in using machine learning systems\nWhat are the limitations of Python?\nPython is a widely used, general-purpose programming language, but it has some limitations Because Python is an interpreted, dynamically typed language, it is slow compared to a compiled, statically typed language like C Therefore, Python is useful when speed is not that important Python's dynamic type system also makes it use more memory than some other programming languages, so it is not suited to memory-intensive applications The Python virtual engine that runs Python code runs single-threaded, making concurrency another limitation of the programming language Though Python is popular for some types of game development, its higher memory and CPU usage limits its usage for high-quality 3D game development That being said, computer hardware is getting better and better, and the speed and memory limitations of Python are getting less and less relevant making Python even more popular\nHow is Python used?\nPython is a general programming language used widely across many industries and platforms One common use of Python is scripting, which means automating tasks in the background Many of the scripts that ship with Linux operating systems are Python scripts Python is also a popular language for machine learning, data analytics, data visualization, and data science because its simple syntax makes it easy to quickly build real applications You can use Python to create desktop applications Many developers use it to write Linux desktop applications, and it is also an excellent choice for web and game development Python web frameworks like Flask and Django are a popular choices for developing web applications Recently, Python is also being used as a language for mobile development via the Kivy third-party library, although there are currently some drawbacks Python needs to overcome when it comes to mobile development\nWhat jobs use Python?\nPython is a popular language that is used across many industries and in many programming disciplines DevOps engineers use Python to script website and server deployments Web developers use Python to build web applications, usually with one of Python's popular web frameworks like Flask or Django Data scientists and data analysts use Python to build machine learning models, generate data visualizations, and analyze big data Financial advisors and quants (quantitative analysts) use Python to predict the market and manage money Data journalists use Python to sort through information and create stories Machine learning engineers use Python to develop neural networks and artificial intelligent systems\nHow do I learn Python on my own?\nPython has a simple syntax that makes it an excellent programming language for a beginner to learn To learn Python on your own, you first must become familiar with the syntax But you only need to know a little bit about Python syntax to get started writing real code; you will pick up the rest as you go Depending on the purpose of using it, you can then find a good Python tutorial, book, or course that will teach you the programming language by building a complete application that fits your goals If you want to develop games, then learn Python game development If you're going to build web applications, you can find many courses that can teach you that, too Udemy’s online courses are a great place to start if you want to learn Python on your own\nWhat is data science?\nWe have more data than ever before But data alone cannot tell us much about the world around us We need to interpret the information and discover hidden patterns This is where data science comes in Data science uses algorithms to understand raw data The main difference between data science and traditional data analysis is its focus on prediction Data science seeks to find patterns in data and use those patterns to predict future data It draws on machine learning to process large amounts of data, discover patterns, and predict trends Data science includes preparing, analyzing, and processing data It draws from many scientific fields, and as a science, it progresses by creating new algorithms to analyze data and validate current methods\nWhat does a data scientist do?\nData Scientists use machine learning to discover hidden patterns in large amounts of raw data to shed light on real problems This requires several steps First, they must identify a suitable problem Next, they determine what data are needed to solve such a situation and figure out how to get the data Once they obtain the data, they need to clean the data The data may not be formatted correctly, it might have additional unnecessary data, it might be missing entries, or some data might be incorrect Data Scientists must, therefore, make sure the data is clean before they analyze the data To analyze the data, they use machine learning techniques to build models Once they create a model, they test, refine, and finally put it into production\nWhat are the most popular coding languages for data science?\nPython is the most popular programming language for data science It is a universal language that has a lot of libraries available It is also a good beginner language R is also popular; however, it is more complex and designed for statistical analysis It might be a good choice if you want to specialize in statistical analysis You will want to know either Python or R and SQL SQL is a query language designed for relational databases Data scientists deal with large amounts of data, and they store a lot of that data in relational databases Those are the three most-used programming languages Other languages such as Java, C++, JavaScript, and Scala are also used, albeit less so If you already have a background in those languages, you can explore the tools available in those languages However, if you already know another programming language, you will likely be able to pick up Python very quickly\nHow long does it take to become a data scientist?\nThis answer, of course, varies The more time you devote to learning new skills, the faster you will learn It will also depend on your starting place If you already have a strong base in mathematics and statistics, you will have less to learn If you have no background in statistics or advanced mathematics, you can still become a data scientist; it will just take a bit longer Data science requires lifelong learning, so you will never really finish learning A better question might be, \"How can I gauge whether I know enough to become a data scientist?\" Challenge yourself to complete data science projects using open data The more you practice, the more you will learn, and the more confident you will become Once you have several projects that you can point to as good examples of your skillset as a data scientist, you are ready to enter the field\nHow can I learn data science on my own?\nIt is possible to learn data science on your own, as long as you stay focused and motivated Luckily, there are a lot of online courses and boot camps available Start by determining what interests you about data science If you gravitate to visualizations, begin learning about them Starting with something that excites you will motivate you to take that first step If you are not sure where you want to start, try starting with learning Python It is an excellent introduction to programming languages and will be useful as a data scientist Begin by working through tutorials or Udemy courses on the topic of your choice Once you have developed a base in the skills that interest you, it can help to talk with someone in the field Find out what skills employers are looking for and continue to learn those skills When learning on your own, setting practical learning goals can keep you motivated\nDoes data science require coding?\nThe jury is still out on this one Some people believe that it is possible to become a data scientist without knowing how to code, but others disagree A lot of algorithms have been developed and optimized in the field You could argue that it is more important to understand how to use the algorithms than how to code them yourself As the field grows, more platforms are available that automate much of the process However, as it stands now, employers are primarily looking for people who can code, and you need basic programming skills The data scientist role is continuing to evolve, so that might not be true in the future The best advice would be to find the path that fits your skill set\nWhat skills should a data scientist know?\nA data scientist requires many skills They need a strong understanding of statistical analysis and mathematics, which are essential pillars of data science A good understanding of these concepts will help you understand the basic premises of data science Familiarity with machine learning is also important Machine learning is a valuable tool to find patterns in large data sets To manage large data sets, data scientists must be familiar with databases Structured query language (SQL) is a must-have skill for data scientists However, nonrelational databases (NoSQL) are growing in popularity, so a greater understanding of database structures is beneficial The dominant programming language in Data Science is Python — although R is also popular A basis in at least one of these languages is a good starting point Finally, to communicate findings, data scientists require knowledge of visualizations Data visualizations allow them to share complex data in an accessible manner\nIs data science a good career?\nThe demand for data scientists is growing We do not just have data scientists; we have data engineers, data administrators, and analytics managers The jobs also generally pay well This might make you wonder if it would be a promising career for you A better understanding of the type of work a data scientist does can help you understand if it might be the path for you First and foremost, you must think analytically Data science is about gaining a more in-depth understanding of info through data Do you fact-check information and enjoy diving into the statistics? Although the actual work may be quite technical, the findings still need to be communicated Can you explain complex findings to someone who does not have a technical background? Many data scientists work in cross-functional teams and must share their results with people with very different backgrounds If this sounds like a great work environment, then it might be a promising career for you\nVideo and Audio Production Quality\nAll our videos are created/produced as high-quality video and audio to provide you with the best learning experience\nYou will be,\nSeeing clearly\nHearing clearly\nMoving through the course without distractions\n\n\nYou'll also get:\nLifetime Access to The Course\nFast & Friendly Support in the Q&A section\nUdemy Certificate of Completion Ready for Download\nWe offer full support, answering any questions\nIf you are ready to learn the “Artificial intelligence with machine learning, deep learning” course\nDive in now! See you in the course!",
      "target_audience": [
        "Anyone who wants to start learning \"Machine Learning\"",
        "Anyone who needs a complete guide on how to start and continue their career with machine learning",
        "Anyone who needs a complete guide on how to start and continue their career with machine learning",
        "Students Interested in Beginning Data Science Applications in Python Environment",
        "People Wanting to Specialize in Anaconda Python Environment for Data Science and Scientific Computing",
        "Students Wanting to Learn the Application of Supervised Learning (Classification) on Real Data Using Python",
        "People who want to learn machine learning, deep learning, python",
        "People who want to learn artificial intelligence",
        "People who want to learn artificial intelligence with machine learning",
        "People who want to learn artificial intelligence with deep learning",
        "People who want to learn artificial intelligence with transfer learning, supervised learning",
        "People who want to learn artificial intelligence with machine learning, deep learning, transfer learning, supervised learning, unsupervised machine learning methods, ai"
      ]
    },
    {
      "title": "Modern Artificial Intelligence Masterclass: Build 6 Projects",
      "url": "https://www.udemy.com/course/modern-artificial-intelligence-applications/",
      "bio": "Harness the power of AI to solve practical, real-world problems in Finance, Tech, Art and Healthcare",
      "objectives": [
        "Deploy Emotion AI-based model using Tensorflow 2.0 Serving and use the model to make inference.",
        "Understand the concept of Explainable AI and uncover the blackbox nature of Artificial Neural Networks and visualize their hidden layers using GradCam technique",
        "Develop Deep Learning model to automate and optimize the brain tumor detection processes at a hospital.",
        "Build and train AI model to detect and localize brain tumors using ResNets and ResUnet networks (Healthcare applications).",
        "Understand the theory and intuition behind Segmentation models and state of the art ResUnet networks.",
        "Build, train, deploy AI models in business to predict customer default on credit card using AWS SageMaker XGBoost algorithm.",
        "Optimize XGBoost model parameters using hyperparameters optimization search.",
        "Apply AI in business applications by performing customer market segmentation to optimize marketing strategy.",
        "Understand the underlying theory and mathematics behind DeepDream algorithm for Art generation.",
        "Develop, train, and test State-of-the art DeepDream algorithm to create AI-based art masterpieces using Keras API in TF 2.0.",
        "Develop ANNs models and train them in Google Colab while leveraging the power of GPUs and TPUs."
      ],
      "course_content": {
        "Introduction": [
          "Introduction and Welcome Message",
          "Introduction, Key Tips and Best Practices",
          "Course Outline and Key Learning Outcomes",
          "Get the Materials"
        ],
        "Emotion AI": [
          "Project Introduction and Welcome Message",
          "Task #1 - Understand the Problem Statement & Business Case",
          "Task #2 - Import Libraries and Datasets",
          "Task #3 - Perform Image Visualizations",
          "Task #4 - Perform Images Augmentation",
          "Task #5 - Perform Data Normalization and Scaling",
          "Task #6 - Understand Artificial Neural Networks (ANNs) Theory & Intuition",
          "Task #7 - Understand ANNs Training & Gradient Descent Algorithm",
          "Task #8 - Understand Convolutional Neural Networks and ResNets",
          "Task #9 - Build ResNet to Detect Key Facial Points",
          "Task #10 - Compile and Train Facial Key Points Detector Model",
          "Task #11 - Assess Trained ResNet Model Performance",
          "Task #12 - Import and Explore Facial Expressions (Emotions) Datasets",
          "Task #13 - Visualize Images for Facial Expression Detection",
          "Task #14 - Perform Image Augmentation",
          "Task #15 - Build & Train a Facial Expression Classifier Model",
          "Task #16 - Understand Classifiers Key Performance Indicators (KPIs)",
          "Task #17 - Assess Facial Expression Classifier Model",
          "Task #18 - Make Predictions from Both Models: 1. Key Facial Points & 2. Emotion",
          "Task #19 - Save Trained Model for Deployment",
          "Task #20 - Serve Trained Model in TensorFlow 2.0 Serving",
          "Task #21 - Deploy Both Models and Make Inference"
        ],
        "AI in Healthcare": [
          "Project Introduction and Welcome Message",
          "Task #1 - Understand the Problem Statement and Business Case",
          "Task #2 - Import Libraries and Datasets",
          "Task #3 - Visualize and Explore Datasets",
          "Task #4 - Understand the Intuition behind ResNet and CNNs",
          "Task #5 - Understand Theory and Intuition Behind Transfer Learning",
          "Task #6 - Train a Classifier Model To Detect Brain Tumors",
          "Task #7 - Assess Trained Classifier Model Performance",
          "Task #8 - Understand ResUnet Segmentation Models Intuition",
          "Task #9 - Build a Segmentation Model to Localize Brain Tumors",
          "Task #10 - Train ResUnet Segmentation Model",
          "Task #11 - Assess Trained ResUNet Segmentation Model Performance"
        ],
        "AI in Business (Marketing)": [
          "Project Introduction and Welcome Message",
          "Task #1 - Understand AI Applications in Marketing",
          "Task #2 - Import Libraries and Datasets",
          "Task #3 - Perform Exploratory Data Analysis (Part #1)",
          "Task #4 - Perform Exploratory Data Analysis (Part #2)",
          "Task #5 - Understand Theory and Intuition Behind K-Means Clustering Algorithm",
          "Task #6 - Apply Elbow Method to Find the Optimal Number of Clusters",
          "Task #7 - Apply K-Means Clustering Algorithm",
          "Task #8 - Understand Intuition Behind Principal Component Analysis (PCA)",
          "Task #9 - Understand the Theory and Intuition Behind Auto-encoders",
          "Task #10 - Apply Auto-encoders and Perform Clustering"
        ],
        "AI In Business (Finance) & AutoML": [
          "Project Introduction and Welcome Message",
          "Notes on Amazon Web Services (AWS)",
          "Task #1 - Understand the Problem Statement & Business Case",
          "Task #2 - Import Libraries and Datasets",
          "Task #3 - Visualize and Explore Dataset",
          "Task #4 - Clean Up the Data",
          "Task #5 - Understand the Theory & Intuition Behind XG-Boost Algorithm",
          "Task #6 - Understand XG-Boost Algorithm Key Steps",
          "Task #7 - Train XG-Boost Algorithm Using Scikit-Learn",
          "Task #8 - Perform Grid Search and Hyper-parameters Optimization",
          "Task #9 - Understand XG-Boost in AWS SageMaker",
          "Task #10 - Train XG-Boost in AWS SageMaker",
          "Task #11 - Deploy Model and Make Inference",
          "Task #12 - Train and Deploy Model Using AWS AutoPilot (Minimal Coding Required!)"
        ],
        "Creative AI": [
          "Project Introduction and Welcome Message",
          "Task #1 - Understand the Problem Statement & Business Case",
          "Task #2 - Import Model with Pre-trained Weights",
          "Task #3 - Import and Merge Images",
          "Task #4 - Run the Pre-trained Model and Explore Activations",
          "Task #5 - Understand the Theory & Intuition Behind Deep Dream Algorithm",
          "Task #6 - Understand The Gradient Operations in TF 2.0",
          "Task #7 - Implement Deep Dream Algorithm Part #1",
          "Task #8 - Implement Deep Dream Algorithm Part #2",
          "Task #9 - Apply DeepDream Algorithm to Generate Images",
          "Task #10 - Generate DeepDream Video"
        ],
        "Explainable AI with Zero Coding": [
          "Explainable AI Dataset Download & Link to DataRobot",
          "Project Overview on Food Recognition with AI",
          "DataRobot Demo 1 - Upload and Explore Dataset",
          "DataRobot Demo 2 - Train AI/ML Model",
          "DataRobot Demo 3 - Explainable AI"
        ],
        "Crash Course on AWS, S3, and SageMaker": [
          "What is AWS and Cloud Computing?",
          "Key Machine Learning Components and AWS Tour",
          "Regions and Availability Zones",
          "Amazon S3",
          "EC2 and Identity and Access Management (IAM)",
          "AWS Free Tier Account Setup and Overview",
          "AWS SageMaker Overview",
          "AWS SageMaker Walk-through",
          "AWS SageMaker Studio Overview",
          "AWS SageMaker Studio Walk-through",
          "AWS SageMaker Model Deployment"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic knowledge of programming is recommended but not required."
      ],
      "description": "# Course Update June 2021: Added a study on Explainable AI with Zero Coding\nArtificial Intelligence (AI) revolution is here!\n“Artificial Intelligence market worldwide is projected to grow by US$284.6 Billion driven by a compounded growth of 43. 9%. Deep Learning, one of the segments analyzed and sized in this study, displays the potential to grow at over 42. 5%.” (Source: globenewswire).\nAI is the science that empowers computers to mimic human intelligence such as decision making, reasoning, text processing, and visual perception. AI is a broader general field that entails several sub-fields such as machine learning, robotics, and computer vision.\nFor companies to become competitive and skyrocket their growth, they need to leverage AI power to improve processes, reduce cost and increase revenue. AI is broadly implemented in many sectors nowadays and has been transforming every industry from banking to healthcare, transportation and technology.\nThe demand for AI talent has exponentially increased in recent years and it’s no longer limited to Silicon Valley! According to Forbes, AI Skills are among the most in-demand for 2020.\nThe purpose of this course is to provide you with knowledge of key aspects of modern Artificial Intelligence applications in a practical, easy and fun way. The course provides students with practical hands-on experience using real-world datasets. The course covers many new topics and applications such as Emotion AI, Explainable AI, Creative AI, and applications of AI in Healthcare, Business, and Finance.\nOne key unique feature of this course is that we will be training and deploying models using Tensorflow 2.0 and AWS SageMaker. In addition, we will cover various elements of the AI/ML workflow covering model building, training, hyper-parameters tuning, and deployment. Furthermore, the course has been carefully designed to cover key aspects of AI such as Machine learning, deep learning, and computer vision.\nHere’s a summary of the projects that we will be covering:\n· Project #1 (Emotion AI): Emotion Classification and Key Facial Points Detection Using AI\n· Project #2 (AI in HealthCare): Brain Tumor Detection and Localization Using AI\n· Project #3 (AI in Business/Marketing): Mall Customer Segmentation Using Autoencoders and Unsupervised Machine Learning Algorithms\n· Project #4: (AI in Business/Finance): Credit Card Default Prediction Using AWS SageMaker's XG-Boost Algorithm (AutoPilot)\n· Project #5 (Creative AI): Artwork Generation by AI\n· Project #6 (Explainable AI): Uncover the Blackbox nature of AI\n\n\nWho this course is for:\nThe course is targeted towards AI practitioners, aspiring data scientists, Tech enthusiasts, and consultants wanting to gain a fundamental understanding of data science and solve real world problems. Here’s a list of who is this course for:\n· Seasoned consultants wanting to transform industries by leveraging AI.\n· AI Practitioners wanting to advance their careers and build their portfolio.\n· Visionary business owners who want to harness the power of AI to maximize revenue, reduce costs and optimize their business.\n· Tech enthusiasts who are passionate about AI and want to gain real-world practical experience.\n\n\nCourse Prerequisites:\nBasic knowledge of programming is recommended. However, these topics will be extensively covered during early course lectures; therefore, the course has no prerequisites, and is open to anyone with basic programming knowledge. Students who enroll in this course will master data science fundamentals and directly apply these skills to solve real world challenging business problems.",
      "target_audience": [
        "Seasoned consultants wanting to transform industries by leveraging AI.",
        "AI Practitioners wanting to advance their careers and build their portfolio.",
        "Visionary business owners who want to harness the power of AI to maximize revenue, reduce costs and optimize their business.",
        "Visionary business owners who want to harness the power of AI to maximize revenue, reduce costs and optimize their business."
      ]
    },
    {
      "title": "The Complete Artificial Intelligence (AI) for Professionals",
      "url": "https://www.udemy.com/course/the-complete-artificial-intelligence-ai-for-professionals/",
      "bio": "Learn Google Gemini, ChatGPT SORA along with 100+ AI tools and use them to Master Business, Ethics and Innovation!",
      "objectives": [
        "Comprehensive Understanding of AI",
        "Learn 100+ AI tools",
        "Real-World Application of AI",
        "Ethical Considerations",
        "AI Tools and Resources",
        "AI Myths and Realities",
        "AI's Impact on Business Growth",
        "Selecting and Implementing AI Tools",
        "Measuring AI Performance and ROI",
        "Language Processing Techniques",
        "AI in Decision-Making",
        "AI's Role in Task Automation",
        "Enhancing Customer Experiences",
        "Prompting Engineering and Language Generation",
        "Chained Prompting",
        "AI in Academic Research",
        "AI in Creative Fields",
        "Current AI Tools and Trends",
        "ChatGPT",
        "Google Bard",
        "Gemini",
        "OpenAI SORA"
      ],
      "course_content": {
        "Introduction to AI and its potential along with understanding of AI": [
          "Understanding AI",
          "What are LLMs and How to Prompt for LLMs",
          "How Image Generators Work?",
          "How Audio Generators work ?",
          "How Video Generators work ?",
          "How Voice Assisstants work ?",
          "What is Productivity AI and how it works ?",
          "Which AI tool to use and when ? The Complete Road Map!",
          "Latest Impact of AI in Small Medium and Large Scale Businesses",
          "Business Growth & Real-World AI Applications in Business"
        ],
        "AI Alchemy: Myths, Ethics, and Mastering the Future": [
          "Introduction to machine learning, natural language processing",
          "Understanding AI Techniques: Machine Learning, NLP, and Machine Vision",
          "AI Techniques Unveiled: Test Your Knowledge on ML, NLP, and Beyond!",
          "How AI differs from traditional programming",
          "AI vs Traditional Programming: Understanding the Differences",
          "AI vs. Traditional Programming: Can you identify the Difference?",
          "Common misconceptions and myths about AI",
          "Exploring 6 AI Myths by Google",
          "Key ethical considerations when implementing AI in business",
          "Artificial Intelligence: examples of ethical dilemmas by UNESCO",
          "AI in Action: Debunking Myths & Discovering Realities"
        ],
        "Leveraging AI in Your Business": [
          "Identifying opportunities for AI in your business",
          "AI Ascendancy: Navigating the Future of Business in 7 Transformative Steps",
          "The Ultimate Guide to Harnessing AI Opportunities for Your Business",
          "AI Odyssey: Navigating the Business Frontier Quiz Challenge",
          "Best practices for selecting and implementing AI tools",
          "Decoding AI for Business: Steps, Stories, and Strategies",
          "Decoding AI for Enterprises: Test Your Knowledge!",
          "Building a business case for AI adoption",
          "AI in Action: Crafting a Compelling Business Case for Intelligent Innovation",
          "AI for Business: Navigating the Path to Intelligent Innovation Quiz",
          "Evaluating AI performance and ROI",
          "The Power and Challenge of AI in Business: A Deep Dive into ROI",
          "AI in Business: ROI Revelations Quiz"
        ],
        "AI-Powered Business Transformations": [
          "What is RPA and how RPA and AI Are Linked and Used Today ?",
          "Unraveling the Future of Work: RPA vs. AI - What You Need to Know",
          "10 Real-World Examples of Robotic Process Automation for Everyone",
          "The Great Automation Showdown: RPA vs. AI - Interactive Quiz",
          "Automating repetitive tasks with AI",
          "How AI-Powered Tools Revolutionise Task Automation",
          "Why is RPA transformative? Useful Links..!",
          "Improving decision-making with AI insights and analytics",
          "Using AI to enhance customer experiences",
          "6 Ways AI Elevates Customer Experiences: Unveiling the CX Revolution",
          "Decoding the Influence of Artificial Intelligence on Decision-Making",
          "Understand how IBM is Revolutionising the Way Businesses Connect",
          "Transforming Customer Experiences Quiz",
          "Enhancing cybersecurity with AI",
          "Unleashing AI's Power: Transforming Cybersecurity for Modern Businesses",
          "The power of AI in Cybersecurity: A Game-Changer in the Digital Battleground",
          "AI in Cybersecurity: Are You Cyber Savvy?"
        ],
        "Prompt Engineering": [
          "things to know about prompting",
          "The Art of Prompting: More Than Just Asking",
          "What is the difference between prompting, tokenisation, parsing, and sentiment",
          "How AI will understand each of them ?",
          "Understanding Words and Feelings: A Simple Guide to Language Processing",
          "List of prompts by categorising it for various different needs and usefulness 01",
          "List of prompts by categorising it for various different needs and usefulness 02",
          "Prompted Insights: A Guide to Effective Questioning",
          "What is chained prompting",
          "Insight on \"Chain Prompting\"",
          "Deciphering Language Tech: An NLP Quiz Odyssey",
          "Introducing new information to the LLM [Part 1]",
          "Introducing new information to the LLM [Part 2]"
        ],
        "AI Chronicles: Unveiling the Real-World Wonders of Artificial Intelligence": [
          "AI Unleashed : Weekly Real-World Scenarios Await You!",
          "What Awaits You: Section Guide",
          "How Spotifys AI-Driven Recommendations Work",
          "Case Study: How Spotify Achieved Success with AI",
          "The Algorithmic Revolution in Retail A Deep Dive into Data-Driven Approach",
          "Case Study: Stitch Fix's Success with AI in Personal Styling",
          "The Top 11 AI Tools Revolutionizing Business in 2025",
          "AI Wizards: Unleash Your Business's Magic with These 11 Enchanted Tools"
        ],
        "Learn Google Gemini using Bard: Unlocking the Power of the Next-Gen AI": [
          "Section Intro",
          "What is Google Gemini ?",
          "Google Gemini Resources",
          "What is BARD and Gemini ?",
          "Further Analysis",
          "Bard Account and Extensions",
          "Playing with the Bard YOUTUBE extension to enhance your Data",
          "Youtube Prompts for BARD",
          "Bard's AUDIO feature",
          "Bard's Google Workspace interaction [killing the Industry]",
          "BARD Prompts for Google Workspace",
          "Analyse the webpage using BARD",
          "How to retrace data and website from IMAGE or SCREENSHOTS using BARD",
          "Prompts you can use to analyse a website and images or screenshots using Bard",
          "How to generate Real-time data using BARD ?",
          "Prompts for Google Flights, Maps, Hotels, and Real-Time Data with Bard"
        ],
        "Google Gemini Era 1.5 [Latest]": [
          "Google - Welcome to the Gemini Era",
          "Google I/O 24 Key highlights: Gemini Live, Multimodal Capabilities, Gems & more",
          "Understanding the New Feature called AI Overview",
          "Gemini New Features for Developers",
          "Gemini Pro in Workspace Apps",
          "Ask Photos: A New Way to Search Your Photos with Gemini",
          "ImaGen 3: Google DeepMinds Latest Test-to-Image Model",
          "Veo: Google DeepMind's Advanced Generative Video Model",
          "Project Astra: Google DeepMind's Universal AI Agent"
        ],
        "Learn how to use AI in Marketing [Inspired by the book AI Marketing Canvas]": [
          "Section Intro",
          "Applications of AI in Marketing",
          "Where does the data for data-driven AI come from?",
          "Reading Material from research Paper",
          "The Role of Algorithms in AI [Machine Learning vs. Statistics]",
          "Diving Deep into Deep Learning",
          "Case Study on Automation of Marketing Models",
          "Role of Algorithms [Research paper and Reading Materials]",
          "What We Have Understood So Far ?",
          "Lesson 1 - The Emerging Electric Vehicle Market",
          "An Autonomous Semi-Truck?",
          "Insights on the Tesla Semi",
          "Autonomous Vehicles: Expectations and Reality",
          "The Autonomous Market vs. The Current Market",
          "Exploring Network Effects",
          "Understanding Money and Subsidy Sides in Network Effects",
          "Strategies to Trigger Network Effects",
          "Can Machines Have Personalities?",
          "Lesson 1 and 2 Takeaways",
          "Lesson 3: The Power of Data in AI-Driven Marketing",
          "Case Study AI-Driven Marketing",
          "Understanding Network-based Business Model",
          "Discussion Questions [Part-1]",
          "Detailed Case Study: Netflix's Strategic Success",
          "Keys to Success for Netflix and Spotify",
          "Disney Arrives on the Scene ?",
          "Why is Disney Entering the Streaming Space ?",
          "How Disney+ Enhances the Customer Relationship",
          "The Paradox of Civilization",
          "Applying the Paradox to Modern Businesses",
          "Sources of Advantage",
          "Netflix's Sustainable Competitive Advantage",
          "Lesson 3 Takeaway",
          "Welcome to Lesson 4: How Nodes are Approaching AI",
          "The Success of the Washington Post",
          "Additional Case Study [Important]",
          "Charlie Beckett on Innovation in Journalism – Part 1",
          "Charlie Beckett on Innovation in Journalism – Part 2",
          "Networks and Nodes Exploring AI in Marketing",
          "How Are Nodes Approaching AI?",
          "AI Relationship Moments",
          "Consumer Data and the Coke Freestyle Machine",
          "Lesson 4 Takeaway",
          "AI Integration Strategy: Leading the Team to Implement AI Solutions for Business Innovation"
        ],
        "Remove Plagiarism and AI detection [Create Human Written content using AI]": [
          "Section Outline",
          "What is Plagiarism",
          "Reading Material [University of Oxford]",
          "Choosing the Right Plagiarism Checking Tools",
          "Plagiarism Detectors [Resource]",
          "How to make Online Available blogs free from Plagiarism ?",
          "Interesting ? Whats next ?",
          "AI generated Content by Google BARD and is it 100% free from Plagiarism?",
          "Understanding AI Detectors and how to use these tools?",
          "AI Detectors [Resource]",
          "Lets do the same with ChatGPT",
          "Rewrite AI generated Article to bypass AI Detector",
          "How to Reduce from 100% to less than 5% AI written blog",
          "Further Investigation!",
          "How to make it Human Written?",
          "Understanding Plagiarism and AI Content Detection",
          "Section Recap and personal views"
        ]
      },
      "requirements": [
        "There are no specific prerequisites or requirements for taking this course. We've intentionally designed it to be accessible to learners of all backgrounds, skill levels, and experiences. Whether you're a complete beginner or an intermediate learner, we welcome you to embark on this AI journey with us. To ensure that we lower the barrier for beginners and make this course as inclusive as possible, here are some considerations: No Prior AI Knowledge Needed: You don't need any prior knowledge of Artificial Intelligence (AI) to start this course. We begin from the basics and gradually build your understanding, making it suitable for those who are entirely new to the field. No Coding Experience Required: While AI often involves coding and programming, this course does not assume any prior coding experience. We'll introduce coding concepts in a beginner-friendly manner for those interested in diving into AI programming. Accessible Tools: Throughout the course, we'll introduce and explain various AI tools and technologies. You don't need to have any specific software or equipment beforehand; we'll guide you on how to access and use relevant tools as you progress. A Curious Mind: The most important prerequisite for this course is a curious and open mind. We encourage a willingness to explore, ask questions, and engage with the course materials actively. By eliminating specific prerequisites and ensuring the course is beginner-friendly, we aim to make AI education accessible to a wide range of learners, regardless of their background or previous experience. So, if you're eager to learn about AI, you're already well-prepared to take this course."
      ],
      "description": "The Complete Artificial Intelligence (AI) for Professionals Course\nAre you ready to embark on a transformative journey into the captivating world of Artificial Intelligence (AI)? Welcome to the Complete Artificial Intelligence (AI) for Professionals Course, where the possibilities are endless, and your potential is boundless!\n\n\nWhy AI, and Why Now?\nAI is no longer a futuristic dream—it's our present reality. As recent reports predict AI's disruption of markets worldwide, there's never been a more critical moment to equip yourself with AI knowledge and skills. Whether you're an individual seeking personal growth, a business owner aiming to stay ahead, or a professional eager to navigate the AI landscape, this course is tailor-made for you!\n\n\nMeet Your Course Creator\nSay hello to Debayan, your expert guide on this exhilarating AI journey. With over 3 years of experience in e-learning content creation, Debayan's previous courses in Augmented Reality and Cybersecurity have earned acclaim, with more than 200,000 enrollments and an impressive 4.5+ rating out of 5. Armed with a Bachelor's degree in Computer Science and a Master's in Cybersecurity, Debayan brings a wealth of industry knowledge to this AI course. And, of course, let's not forget Arthur, the voice behind the course.\n\n\nWhat's in Store for You?\nThis course is a treasure trove of AI wisdom, meticulously structured into 10 thoughtfully crafted sections:\n\n\nSection 1: Introduction to AI and its potential along with understanding of AI\nLatest Impact of AI on Small, Medium, and Large Businesses: A Comparative Analysis\nUnlocking Business Growth: Dive Deeper with These Enlightening Articles (Read)\nReal-world examples of AI applications in business\nAI Impact Insights: Fact or Fiction Quiz\n\n\nSection 2: AI Alchemy: Myths, Ethics, and Mastering the Future\nIntroduction to machine learning, natural language processing\nUnderstanding AI Techniques: Machine Learning, NLP, and Machine Vision\nHow AI differs from traditional programming\nAI vs Traditional Programming: Understanding the Differences\nCommon misconceptions and myths about AI\nExploring 6 AI Myths by Google\nKey ethical considerations when implementing AI in business\nArtificial Intelligence: examples of ethical dilemmas by UNESCO\nFull of quizzes and reading materials\n\n\nSection 3: Leveraging AI in Your Business\nIdentifying opportunities for AI in your business\nAI Ascendancy: Navigating the Future of Business in 7 Transformative Steps\nThe Ultimate Guide to Harnessing AI Opportunities for Your Business\nBest practices for selecting and implementing AI tools\nDecoding AI for Business: Steps, Stories, and Strategies\nBuilding a business case for AI adoption\nAI in Action: Crafting a Compelling Business Case for Intelligent Innovation\nEvaluating AI performance and ROI\nThe Power and Challenge of AI in Business: A Deep Dive into ROI\nFull of quizzes and reading materials\n\n\nSection 4: AI-Powered Business Transformations\nUnraveling the Future of Work: RPA vs. AI - What You Need to Know\n10 Real-World Examples of Robotic Process Automation for Everyone\nAutomating repetitive tasks with AI\nHow AI-Powered Tools Revolutionize Task Automation\nImproving decision-making with AI insights and analytics\nUsing AI to enhance customer experiences\n6 Ways AI Elevates Customer Experiences: Unveiling the CX Revolution\nDecoding the Influence of Artificial Intelligence on Decision-Making\nUnderstand how IBM is Revolutionizing the Way Businesses Connect\nEnhancing cybersecurity with AI\nUnleashing AI's Power: Transforming Cybersecurity for Modern Businesses\nThe power of AI in Cybersecurity: A Game-Changer in the Digital Battleground\nFull of quizzes and reading materials\n\n\nSection 5: Prompt Engineering\nThings to know about prompting\nThe Art of Prompting: More Than Just Asking\nWhat is the difference between prompting, tokenization, parsing, and sentiment\nNow with the above example, explain how AI will understand each of them\nUnderstanding Words and Feelings: A Simple Guide to Language Processing\nList of prompts by categorizing it for various different needs and usefulness\nPrompted Insights: A Guide to Effective Questioning\nWhat is chained prompting\nInsight on \"Chain Prompting\"\nFull of quizzes and reading materials\n\n\nSection 6: AI Chronicles: Unveiling the Real-World Wonders of Artificial Intelligence\nWeekly Real-World Scenarios Await You!\nHow Spotify's AI-Driven Recommendations Work\nCase Study: How Spotify Achieved Success with AI\nThe Algorithmic Revolution in Retail: A Deep Dive into Data-Driven Approach\nCase Study: Stitch Fix's Success with AI in Personal Styling\n7 AI Tools To Run Your Business From A to Z\n\n\nSection 7: Learn Google Gemini using Bard: Unlocking the Power of the Next-Gen AI\nWhat is Google Gemini?\nGoogle Gemini Resources\nUnderstanding BARD and Gemini\nFurther Analysis\nBard Account and Extensions\nUtilizing Bard with YouTube Extensions\nEffective YouTube Prompts for BARD\nExploring Bard's AUDIO Feature\nBard's Interaction with Google Workspace [Industry Killer]\nCreating Impactful BARD Prompts for Google Workspace\nAnalyzing Webpages Using BARD\nRetracing Data from Images/Screenshots with BARD\nCrafting Prompts for Website and Image Analysis\nHow to Generate Real-time Data with BARD\nPrompts for Google Flights, Maps, Hotels, and Real-Time Data with Bard\nSection 8: Remove Plagiarism and AI detection [Create Human Written content using AI]\nUnderstanding what is Plagiarism\nA detailed Section Outline\nIn-depth Reading Material from the University of Oxford\nGuidance on Choosing the Right Plagiarism-Checking Tools\nA Resourceful List of Plagiarism Detectors\nTechniques to Make Online Blogs Plagiarism-Free\nAnalysis of AI-generated Content by Google BARD\nHow to use AI Detectors effectively\nApplication of the same principles with ChatGPT\nMethods to Rewrite AI-Generated Articles\nStrategies to Reduce AI Detection in Blogs\nSteps to Humanize AI Written Content\nAn Interactive Quiz on Plagiarism and AI Content Detection\nA Comprehensive Section Recap and Personal Views\n\n\nSection 9: Learn how to use AI in Marketing [Inspired by the book AI Marketing Canvas]\n\n\nSection 10: Decoding ChatGPT\n\n\nSection 11: Learn Custom GPTs: A Professional's Toolkit\n\n\nSection 12: Create mobile App using ChatGPT\n\n\nSection 13: The Power of Visualization in Professional Life using GPT [Pie-Chart, Flow Chart, Mind Map, etc]\n\n\nSection 14: Learn OpenAI SORA\n\n\nSection 15: Complete section on Meta AI Llama\n\n\nSection 16: AI Tools for Graphic Designers\n\n\nSection 17: Ten Insane AI Tools Every Creator Should Be Using in 2024\n\n\nSection 18: Academic AI Tools You Just HAVE to Know About in 2024\n\n\nSection 19: AI Pulse: Monthly Insights and Innovations\n\n\nSection 20 and Beyond: Coming Soon!\n\n\nWhy Should You Take This Course?\nIn this AI-packed adventure, you'll:\nLearn from Scratch: No prior AI knowledge needed. We start from the basics and guide you through every step.\nMaster Real-World AI: Our course is loaded with real-world scenarios and practical insights.\nAccess a Wealth of Resources: Dive into quizzes and reading materials to enhance your understanding.\nStay Ahead of the Curve: AI is the future, and this course ensures you're at the forefront.\nLearn from an Expert: Debayan, with his extensive industry experience, is your trusted mentor.\nRegular Updates: As AI evolves, so does this course. New tools, trends, and insights are regularly added.\nNewsletter: Stay updated with AI news and trends through our newsletter.\nValue for Money: This course offers exceptional value with its breadth of content and regular updates.\nReady to empower yourself with AI and compete with multi-billion-dollar companies? Enroll now and embark on a journey that will reshape your career and future!\n\n\nWith \"The Complete Artificial Intelligence (AI) for Professionals,\" you're not just learning about AI; you're mastering it, applying it, and transforming your professional life. Don't miss this opportunity to stay ahead in the AI-powered world.\n\n\nEnroll today!",
      "target_audience": [
        "Beginners/Intermediate/Experts in AI",
        "Individuals Seeking Personal Growth Through AI",
        "Small Business Owners and Entrepreneurs",
        "Corporate Professionals",
        "Students and Researchers Exploring AI in Academia",
        "Creative Professionals Harnessing AI's Creative Potential",
        "Business Leaders and Decision Makers Embracing AI Strategy",
        "Tech Enthusiasts and Curious Minds",
        "Ethical Consideration Advocates",
        "Lifelong Learners Embracing AI as a Lifelong Skill",
        "Business Owners and Entrepreneurs Eager to Leverage AI",
        "Intermediate Professionals in AI",
        "Professionals Keen on Navigating AI Trends",
        "Aspiring Computer Scientists",
        "IT Professionals Exploring AI"
      ]
    },
    {
      "title": "From 0 to 1: Hive for Processing Big Data",
      "url": "https://www.udemy.com/course/from-0-to-1-hive/",
      "bio": "End-to-End Hive : HQL, Partitioning, Bucketing, UDFs, Windowing, Optimization, Map Joins, Indexes",
      "objectives": [
        "Write complex analytical queries on data in Hive and uncover insights",
        "Leverage ideas of partitioning, bucketing to optimize queries in Hive",
        "Customize hive with user defined functions in Java and Python",
        "Understand what goes on under the hood of Hive with HDFS and MapReduce"
      ],
      "course_content": {
        "You, Us & This Course": [
          "You, Us & This Course"
        ],
        "Introducing Hive": [
          "Hive: An Open-Source Data Warehouse",
          "Hive and Hadoop",
          "Hive vs Traditional Relational DBMS",
          "HiveQL and SQL"
        ],
        "Hadoop and Hive Install": [
          "Hadoop Install Modes",
          "Hadoop Install Step 1 : Standalone Mode",
          "Hadoop Install Step 2 : Pseudo-Distributed Mode",
          "Hive install",
          "Code-Along: Getting started"
        ],
        "Hadoop and HDFS Overview": [
          "What is Hadoop?",
          "HDFS or the Hadoop Distributed File System"
        ],
        "Hive Basics": [
          "Primitive Datatypes",
          "Collections_Arrays_Maps",
          "Structs and Unions",
          "Create Table",
          "Insert Into Table",
          "Insert into Table 2",
          "Alter Table",
          "HDFS",
          "HDFS CLI - Interacting with HDFS",
          "Code-Along: Create Table",
          "Code-Along : Hive CLI"
        ],
        "Built-in Functions": [
          "Three types of Hive functions",
          "The Case-When statement, the Size function, the Cast function",
          "The Explode function",
          "Code-Along : Hive Built - in functions"
        ],
        "Sub-Queries": [
          "Quirky Sub-Queries",
          "More on subqueries: Exists and In",
          "Inserting via subqueries",
          "Code-Along : Use Subqueries to work with Collection Datatypes",
          "Views"
        ],
        "Partitioning": [
          "Indices",
          "Partitioning Introduced",
          "The Rationale for Partitioning",
          "How Tables are Partitioned",
          "Using Partitioned Tables",
          "Dynamic Partitioning: Inserting data into partitioned tables",
          "Code-Along : Partitioning"
        ],
        "Bucketing": [
          "Introducing Bucketing",
          "The Advantages of Bucketing",
          "How Tables are Bucketed",
          "Using Bucketed Tables",
          "Sampling"
        ],
        "Windowing": [
          "Windowing Introduced",
          "Windowing - A Simple Example: Cumulative Sum",
          "Windowing - A More Involved Example: Partitioning",
          "Windowing - Special Aggregation Functions"
        ]
      },
      "requirements": [
        "Hive requires knowledge of SQL. If you don't know SQL, please head to the SQL primer at the end of the course first.",
        "You'll need to know Java if you are interested in the sections on custom user defined functions",
        "No other prerequisites: The course covers everything you need to install Hive and run queries!"
      ],
      "description": "Prerequisites: Hive requires knowledge of SQL. The course includes and SQL primer at the end. Please do that first if you don't know SQL. You'll need to know Java if you want to follow the sections on custom functions.\nTaught by a 4 person team including 2 Stanford-educated, ex-Googlers  and 2 ex-Flipkart Lead Analysts. This team has decades of practical experience in working with large-scale data.\n\nHive is like a new friend with an old face (SQL). This course is an end-to-end, practical guide to using Hive for Big Data processing.\nLet's parse that\nA new friend with an old face: Hive helps you leverage the power of Distributed computing and Hadoop for Analytical processing. It's interface is like an old friend : the very SQL like HiveQL. This course will fill in all the gaps between SQL and what you need to use Hive.\nEnd-to-End: The course is an end-to-end guide for using Hive:  whether you are analyst who wants to process data  or an Engineer who needs to build custom functionality or optimize performance - everything you'll need is right here. New to SQL? No need to look elsewhere. The course  has a primer on all the basic SQL constructs, .\nPractical: Everything is taught using real-life examples, working queries and code .\nWhat's Covered:\nAnalytical Processing: Joins, Subqueries, Views, Table Generating Functions, Explode, Lateral View, Windowing and more\nTuning Hive for better functionality: Partitioning, Bucketing, Join Optimizations, Map Side Joins, Indexes, Writing custom User Defined functions in Java. UDF, UDAF, GenericUDF, GenericUDTF,  Custom functions in Python,  Implementation of MapReduce for Select, Group by and Join\nFor SQL Newbies: SQL In Great Depth",
      "target_audience": [
        "Yep! Analysts who want to write complex analytical queries on large scale data",
        "Yep! Engineers who want to know more about managing Hive as their data warehousing solution"
      ]
    },
    {
      "title": "Applied Time Series Analysis and Forecasting with R Projects",
      "url": "https://www.udemy.com/course/r-applied-time-series-analysis-forecasting-r-projects-r-tutorials/",
      "bio": "Use R to work on real world time series analysis and forecasting examples. Applied data science with R.",
      "objectives": [
        "Perform standard time series analysis tasks",
        "Get ARIMA and exponential smoothing models in R",
        "Do forecasting in R",
        "Work with irregularly spaced time series",
        "Model time series with trend and seasonality",
        "Scrape stock data from yahoo finance",
        "Import different types of time series data",
        "Use automatic model selection in R",
        "Select the best packages for time series analysis in R"
      ],
      "course_content": {
        "Course Introduction": [
          "Introduction",
          "Managing Expectations",
          "Main R Functions in Time Series Analysis",
          "Supporting Resources",
          "Course Link List"
        ],
        "Project I Trending Data: Singapur Labor Force Participation Rate": [
          "Importing The Data",
          "Mission Statement",
          "Project I Script",
          "The Exponential Smoothing Family of Models",
          "The Holt Linear Trend Model",
          "The ARIMA Model",
          "Comparison Plot with 'Ggplot2'",
          "Bonus Exercise: In-Sample Forecasts vs Actual Data"
        ],
        "Project II Seasonal Data: Monthly Inflation Rates of Germany": [
          "Getting Familiar with The Data",
          "Importing The Data",
          "Mission Statement",
          "Project II Script",
          "Seasonal Decomposition",
          "Seasonal ARIMA",
          "Exponential Smoothing with ETS",
          "Time Series Cross Validation"
        ],
        "Project III Irregularly Spaced Data: Analyze A Biotech Stock": [
          "Mission Statement",
          "Scraping the Data From Yahoo Finance",
          "Exploring the Data",
          "Project III Script",
          "Getting a Regular Time Series",
          "Visually Analyzing the Data"
        ],
        "Project IV Neural Networks: Neural Nets and Interactive Graphs": [
          "Mission Statement",
          "Getting Familiar with the Dataset",
          "Project IV Script",
          "Cleaning with tidyr",
          "Fitting the Neural Net Model",
          "Interactive Graph with Dygraphs",
          "Course Summary and Further Options"
        ]
      },
      "requirements": [
        "You need R/RStudio on your computer (add on packages will be outlined)",
        "Basic R skills are required",
        "Basic statistics skills would be helpful"
      ],
      "description": "Welcome to the world of R and Time Series Analysis!\nAt the moment R is the leading open source software for time series analysis and forecasting. No other tool, not even python, comes close to the functions and features available in R. Things like exponential smoothing, ARIMA models, time series cross validation, missing data handling, visualizations and forecasts are easily accessible in R and its add on packages. Therefore, R is the right choice for time series analysis and this course gives you an opportunity to train and practice it.\nSo how is the course structured?\nThis is a hands on course with 3 distinct projects to solve! Each project has a main topic and a secondary topic. Both are discussed on real world data. In the first project you work with trending data, and as a secondary topic you will learn how to create standard and ggplot2 time series visualizations. The dataset for that project will be an employment rate dataset.\nThe second project with the German monthly inflation rates over the last 10 years shows how to model seasonal datasets. And you will also compare the models with time series cross validation.\nIn the third project you will connect R to yahoo finance and scrape stock data. The resulting data requires loads of pre-processing and cleaning including missing data imputation. Once we prepared the data, we will check out which weekday is the best for buying and selling the Novartis stock.\nYou should know some R to be able to follow along. There is for example the introduction to time series analysis and forecasting course. That course is more a step by step guide while this one is an applied and project based one. Both courses can be taken on their own, or you take a look at both and learn the subject from 2 different angles.\nAs always you will get the course script as a text file. Of course you get all the standard Udemy benefits like 30 days money back guarantee, lifetime access, instructor support and a certificate for your CV.",
      "target_audience": [
        "Data scientists, economists and all sorts of professionals working with time series datasets",
        "Entrepreneurs and marketing experts interested in finding patterns in time series data",
        "Students required to perform time series analysis"
      ]
    },
    {
      "title": "Machine Learning & Data Science A-Z: Hands-on Python 2024",
      "url": "https://www.udemy.com/course/data-science-machine-learning-a-z-hands-on-python/",
      "bio": "Learn NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, Scipy and develop Machine Learning Models in Python",
      "objectives": [
        "Understanding the basic concepts",
        "Complete tutorial about basic packages like Numpy and Pandas",
        "Data Visualization",
        "Data Preprocessing",
        "Understanding the concept behind the algorithms",
        "Developing different kinds of Machine Learning models",
        "Knowing how to optimize your models' hyperparameters",
        "Learn how to develop models based on the requirement of your future business"
      ],
      "course_content": {},
      "requirements": [
        "Python's basic syntax"
      ],
      "description": "Are you interested in data science and machine learning, but you don't have any background, and you find the concepts confusing?\nAre you interested in programming in Python, but you always afraid of coding?\nI think this course is for you!\nEven if you are familiar with machine learning, this course can help you to review all the techniques and understand the concept behind each term.\nThis course is completely categorized, and we don't start from the middle! We actually start from the concept of every term, and then we try to implement it in Python step by step. The structure of the course is as follows:\nChapter1: Introduction and all required installations\nChapter2: Useful Machine Learning libraries (NumPy, Pandas & Matplotlib)\nChapter3: Preprocessing\nChapter4: Machine Learning Types\nChapter5: Supervised Learning: Classification\nChapter6: Supervised Learning: Regression\nChapter7: Unsupervised Learning: Clustering\nChapter8: Model Tuning\nFurthermore, you learn how to work with different real datasets and use them for developing your models. All the Python code templates that we write during the course together are available, and you can download them with the resource button of each section.\nRemember! That this course is created for you with any background as all the concepts will be explained from the basics! Also, the programming in Python will be explained from the basic coding, and you just need to know the syntax of Python.",
      "target_audience": [
        "Anyone with any background that interested in Data Science and Machine Learning with at least high school knowledge in mathematic",
        "Beginners, intermediate and even advanced students in the field of artificial intelligence, Data Science and Machine Learning",
        "Students in college that looking for securing their future jobs",
        "Employees that look forward to excel their job level by learning machine learning",
        "Anyone who afraid of coding in Python but interested in Machine Learning Concepts",
        "Any one who wants to create a new business using machine learning",
        "Graduate students and researchers that want to apply machine learning models in their thesis and projects"
      ]
    },
    {
      "title": "Learn DBT from Scratch",
      "url": "https://www.udemy.com/course/learn-dbt-from-scratch/",
      "bio": "Complete guide to Learning DBT including connecting it to a Data Warehouse",
      "objectives": [
        "Connect DBT to Snowflake or another database",
        "Create SQL transformations that use consistent logic",
        "Test SQL transformations and underlying data",
        "Run transformations on a schedule",
        "Add snapshots for slowly changing dimensional tables",
        "Test your code in a dev environment",
        "Learn DBT Best Practices",
        "Advanced DBT Topics"
      ],
      "course_content": {},
      "requirements": [
        "SQL",
        "Github",
        "Ability to work in the command line"
      ],
      "description": "What you'll learn\nWelcome to this course, Learn DBT from Scratch. DBT lets you build a system of transformations on your data, with tests, scheduled runs, multiple environments, flexibility, and more all without needing a team of engineers to set up and manage your workflow. By the end of this course, you will have:\nset up DBT locally and on the cloud\nconnected DBT to Snowflake (or a data warehouse of your choice)\ncreate your own SQL transformations on data\ntest your transformations\nsnapshot your data to keep track of how your data changes over time\nlearn DBT best practices\nIn this course, you'll be presented with the summarized information you need so that you can quickly get DBT implemented in your data pipeline (or in a brand new, data warehouse).\nWhy you should learn DBT\nDBT is not one of the first technical skills most Data Scientists or Analysts think to learn. It’s not as exciting as machine learning algorithms, and it’s not as easy to show off as a fancy data visualization.\nBut DBT is an absolutely fundamental skill for any Data Scientist or Analyst due to all of its capabilities. Because DBT is so flexible, there are almost an endless amount of ways you can integrate DBT into your data architecture. Some features that DBT provides you that all Data Scientists and Analysts should be using in their work include:\nCreating consistent aggregations for your analysis in a single location\nConsistently testing your transformations and underlying data\nRunning your data transformations on a schedule\nTest your code in a DEV environment\nAbout DBT\nDBT is pioneering modern analytics engineering. DBT applies the principles of software engineering to analytics code, an approach that dramatically increases your leverage as a data analyst. They believe that data analysts are the most valuable employees of modern, data-driven businesses and they build tools that empower analysts to own the entire analytics engineering workflow.",
      "target_audience": [
        "Data Analysts and Scientists looking to improve their data pipeline",
        "Prospective Data Scientists and Analysts interested in learning data engineering",
        "Data Analytics Managers looking to understand the capabilities of DBT"
      ]
    },
    {
      "title": "Master Pandas and Python for Data Handling [2025]",
      "url": "https://www.udemy.com/course/master-pandas-and-python-for-data-handling/",
      "bio": "Learn to Master Python, Pandas and Advanced Data Handling for Data Analysis, Data Science, and Machine Learning",
      "objectives": [
        "Master Python programming with Python’s native data structures, data transformers, functions, object orientation, and logic",
        "Master the Pandas library for Advanced Data Handling",
        "Perform Advanced Data Handling",
        "Manipulate data and use advanced multi-dimensional uneven data structures",
        "Use Python’s advanced object-oriented programming and make your own custom objects, functions and how to generalize functions",
        "Use the language and fundamental concepts of the Pandas library and to handle all aspects of creating, changing, modifying, and selecting Data from a Pandas D",
        "Use file handling with Pandas and how to combine Pandas DataFrames with Pandas concat, join, and merge functions/methods",
        "Perform advanced data preparation including advanced model-based imputation of missing data and the scaling and standardizing of data",
        "Make advanced data descriptions and statistics with Pandas. Rank, sort, cross-tabulate, pivot, melt, transpose, and group data",
        "[Bonus] Make advanced Data Visualizations with Pandas, Matplotlib, and Seaborn",
        "Use and design advanced Python constructions and execute detailed Data Handling tasks with Python incl. File Handling",
        "[Cloud computing]: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources.",
        "Option: To use the Anaconda Distribution (for Windows, Mac, Linux)",
        "Option: Use Python environment fundamentals with the Conda package management system"
      ],
      "course_content": {},
      "requirements": [
        "Everyday experience using a computer with either Windows, MacOS, iOS, Android, ChromeOS, or Linux is recommended",
        "Access to a computer with an internet connection",
        "Programming experience is not needed and you will be taught everything you need",
        "The course only uses costless software",
        "Walk-you-through installation and setup videos for Cloud computing and Windows 10/11 is included"
      ],
      "description": "This two-in-one video course will teach you to master Python 3, Pandas 2-3, and Data Handling.\nPython 3 is one of the most popular programming languages in the world, and Pandas 2 and future 3 is the most powerful, efficient, and useful Data Handling library in existence.\nYou will learn to master Python's native building blocks and powerful object-oriented programming. You will design your own advanced constructions of Python’s building blocks and execute detailed Data Handling tasks with Python.\nYou will learn to master the Pandas library and to use its powerful Data Handling techniques for advanced Data Science, Statistics, and Machine Learning Data Handling tasks. The Pandas library is a fast, powerful, flexible, and easy-to-use open-source data analysis and data manipulation tool, which is directly usable with the Python programming language.\n\n\nYou will learn to:\n\n\nMaster Python programming with Python’s data structures, data transformers, functions, object orientation, and logic\nUse and design advanced Python constructions and execute detailed Data Handling tasks with Python incl. File Handling\nUse Python’s advanced object-oriented programming and make your own custom objects, functions and how to generalize functions\nManipulate data and use advanced multi-dimensional uneven data structures\n\n\nMaster the Pandas library for Advanced Data Handling\nUse the language and fundamental concepts of the Pandas library and to handle all aspects of creating, changing, modifying, and selecting Data from a Pandas DataFrame object\nUse file handling with Pandas and how to combine Pandas DataFrames with Pandas concat, join, and merge functions/methods\nPerform advanced data preparation including advanced model-based imputation of missing data and the scaling and standardizing of data\nMake advanced data descriptions and statistics with Pandas. Rank, sort, cross-tabulate, pivot, melt, transpose, and group data\n[Bonus] Make advanced Data Visualizations with Pandas, Matplotlib, and Seaborn\nPerform Advanced Data Handling\n\n\n[Cloud computing]: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources.\nOption: To use the Anaconda Distribution (for Windows, Mac, Linux)\nOption: Use Python environment fundamentals with the Conda package management system and command line installing/updating of libraries and packages – golden nuggets to improve your quality of work life.\nAnd much more…\n\n\nThis course is an excellent way to learn to master Python, Pandas and Data Handling! Data Handling is the process of making data useful and usable for data analysis. Most Data Scientists and Machine Learning Engineers spends about 80% of their working efforts and time on Data Handling tasks.\nBeing good at Python, Pandas, and Data Handling are extremely useful and time-saving skills that functions as a force multiplier for productivity.\n\n\nThis course is designed for anyone who wants to:\nlearn to Master Python 3 from scratch or the beginner level\nlearn to Master Python 3 and knows another programming language\nreach the Master - intermediate Python programmer level as required by many advanced Udemy courses in Python, Data Science, or Machine Learning\nlearn Data Handling with Python\nlearn to Master the Pandas library\nlearn Data Handling skills that work as a force multiplier and that they will have use of in their entire career\nlearn advanced Data Handling and improve their capabilities and productivity\n\n\nRequirements:\nEveryday experience using a computer with either Windows, MacOS, iOS, Android, ChromeOS, or Linux is recommended\nAccess to a computer with an internet connection\nProgramming experience is not needed and you will be taught everything you need\nThe course only uses costless software\nWalk-you-through installation and setup videos for Cloud computing and Windows 10/11 is included\n\n\nThis course is the course we ourselves would want to be able to enroll in if we could time-travel and become new students. In our opinion, this course is the best course to learn to Master Python, Pandas, and Data Handling.\n\n\nEnroll now to receive 25+ hours of video tutorials with manually edited English captions, and a certificate of completion after completing the course!",
      "target_audience": [
        "Anyone who wants to learn to Master Python 3 from scratch or the beginner level",
        "Anyone who wants to learn to Master Python 3 and knows another programming language",
        "Anyone who wants to reach the Master - intermediate Python programmer level as required by many advanced Udemy courses in Python, Data Science, or Machine Learning",
        "Anyone who wants to learn Data Handling with Python",
        "Anyone who wants to learn to Master the Pandas library",
        "Anyone who wants to learn Data Handling skills that work as a force multiplier and that they will have use of in their entire career",
        "Anyone who wants to learn advanced Data Handling and improve their capabilities and productivity"
      ]
    },
    {
      "title": "Python for NLP: The First Step to Text Analysis",
      "url": "https://www.udemy.com/course/introduction-to-natural-language-processing/",
      "bio": "A Practical Guide to Text Processing: Master the First Step of NLP with Python, Regular Expressions, and NLTK.",
      "objectives": [
        "Learn Basics of Natural Language Processing (NLP)",
        "Learn to use Regular Expressions to extract patterns from text",
        "Perform Text pre-processing",
        "Perform Text classification",
        "Reading and working with text data using Python"
      ],
      "course_content": {
        "Module 1 : Introduction to Natural Language Processing": [
          "Welcome to the Course",
          "About the Course",
          "Introduction to Natural Language Processing",
          "Exercise : Introduction to Natural Language Processing",
          "Python for Data Science (Optional)"
        ],
        "Module 2: Learn to use Regular Expressions": [
          "Welcome to Module",
          "Understanding Regular Expression",
          "Implementing Regular Expression in Python",
          "Exercise : Implementing Regular Expression in Python"
        ],
        "Module 3: First Step of NLP - Text Processing": [
          "Welcome to Module",
          "Tokenization and Text Normalization",
          "Exercise : Tokenization and Text Normalization",
          "Exploring Text Data",
          "Part of Speech Tagging and Grammar Parsing",
          "Exercise : Part of Speech Tagging and Grammar Parsing",
          "Implementing Text Pre-processing Using NLTK",
          "Exercise : Implementing Text Pre-processing Using NLTK",
          "Build a Basic ML Model for Text Classification"
        ],
        "Bonus Lecture": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "This course requires you to know Machine Learning.",
        "Familiarity with Python would be an added advantage.",
        "There is no requirement of past experience in NLP."
      ],
      "description": "Do you want to build AI systems that can understand, analyze, and generate human language?\nWelcome to the our course! This course is the culmination of your journey into the foundational world of Natural Language Processing (NLP). We will focus on the most crucial and practical first step of any NLP project: Text Processing.\nYou'll get hands-on experience with the essential tools and techniques for handling raw text data. We'll start by mastering Regular Expressions (Regex), an indispensable skill for pattern matching and data cleaning. Then, we will move on to core text normalization techniques like tokenization and learn how to perform linguistic analysis using Part-of-Speech (POS) tagging and grammar parsing.\nThe final section of this course is all about application. You will learn to use the powerful Natural Language Toolkit (NLTK) to implement everything you've learned. By the end, you'll be able to transform unstructured text into a format suitable for machine learning, culminating in the creation of your very own basic text classification model.\nThis course is your complete guide to the basics of NLP.  It will not only give you a strong theoretical understanding but also provide the practical skills you need to confidently tackle your own text-based data science projects.",
      "target_audience": [
        "Beginner Data Science Professionals interested in Natural Language Processing",
        "Python Developers",
        "Data Scientists"
      ]
    },
    {
      "title": "LangChain with Python Bootcamp",
      "url": "https://www.udemy.com/course/langchain-with-python-bootcamp/",
      "bio": "Build real world applications with Large Language Models and LangChain!",
      "objectives": [
        "Learn to use LangChain for Model Inputs and Outputs to easily switch out LLMs.",
        "Discover how to perform data connections with Vector Databases such as ChromaDB with your LLMs and Langchain.",
        "Understand how to utilize LangChain memory to keep track of User and AI conversations.",
        "Use LangChain to build out custom agents to accomplish tasks with LLMs."
      ],
      "course_content": {
        "Introduction": [
          "Introduction and Course Notebooks Download",
          "Course Introduction and Overview"
        ],
        "Models - Input and Outputs": [
          "Introduction to Models - Inputs and Outputs",
          "OPTIONAL - OPENAI ACCOUNT SETUP",
          "Using LLMs with LangChain",
          "Chat Models with LangChain",
          "Prompt Templates",
          "Prompt and Models - Exercise",
          "Prompt and Models - Exercise Solution",
          "Few Shot Prompt Template",
          "Parsing Output - Part One",
          "Parsing Output - Part Two",
          "Parsing Output - Part Three",
          "Serialization -Saving and Loading Prompts",
          "Models - Inputs and Outputs - Project Exercise",
          "Models - Inputs and Outputs - Project Exercise Solution"
        ],
        "Data Connections": [
          "Introduction to Data Connections",
          "Document Loaders - Part One",
          "Document Loaders - Integrations",
          "Document Loading Exercise",
          "Document Loading Exercise - Solution",
          "Document Transformers",
          "Text Embedding",
          "Vector Store",
          "Vector Store - Retrievers",
          "MultiQuery Retrieval",
          "Context Compression",
          "Data Connections Exercise",
          "Data Connections Exercise - Solutions"
        ],
        "Chains": [
          "Introduction to LangChain Chains",
          "LLMChain Object",
          "SimpleSequentialChain",
          "SequentialChain",
          "LLMRouterChain",
          "TransformChain",
          "OpenAI Function Calling with LangChain",
          "MathChain",
          "Additional Chains - QA Documents",
          "Chains - Exercise",
          "Chains - Exercise Solution"
        ],
        "Memory": [
          "Introduction to Memory",
          "ChatMessageHistory Object",
          "ConversationBufferMemory",
          "ConversationBufferWindowMemory",
          "ConversationSummaryMemory"
        ],
        "Agents": [
          "Introduction to Agents",
          "Agent Basics",
          "Agent Tools",
          "Custom Tools",
          "Conversation Agents"
        ]
      },
      "requirements": [
        "Python Programming Experience Required, recommended you also have familiarity with OpenAI"
      ],
      "description": "Welcome to the LangChain Udemy course: Unlock the Power of Language Models with Python!\nReady to develop cutting-edge applications powered by language models? LangChain is the framework you need. With LangChain, create data-aware and agentic applications that connect language models with other data sources and enable interaction with the environment.\nWhy Choose LangChain?\nLangChain offers numerous benefits for your language model development needs:\nComponents: LangChain provides modular and user-friendly abstractions for working with language models, along with a wide range of implementations.\nOff-the-shelf chains: Start building applications quickly with pre-built chains designed for specific tasks. Modify existing chains or create new ones for more complex or customized use-cases.\nLangChain Course Modules:\nLangChain offers standard interfaces and external integrations for various modules:\nModel I/O: Easily interface with language models.\nData connection: Connect with application-specific data sources.\nChains: Construct sequences of calls to accomplish specific tasks.\nAgents: Enable chains to choose tools based on high-level directives.\nMemory: Persist application state between runs of a chain.\nWith LangChain you'll be able to quickly build real world applications that directly sync up with Large Language Models, such as OpenAI's GPT-4 API!\nRevolutionize your applications with the power of language models. Enroll in our LangChain Udemy course now and unlock limitless possibilities!",
      "target_audience": [
        "Python developers interested in using LangChain to develop LLM Applications"
      ]
    },
    {
      "title": "Data Science Methods and Algorithms [2025]",
      "url": "https://www.udemy.com/course/data-science-methods-and-algorithms-2024/",
      "bio": "Learn Data Science Methods and Algorithms with Pandas and Python [2025]",
      "objectives": [
        "Knowledge about Data Science methods, algorithms, theory, best practices, and tasks",
        "Deep hands-on knowledge of Data Science and know how to handle common Data Science tasks with confidence",
        "Detailed and deep Master knowledge of Regression, Prediction, Classification, Supervised Learning, Cluster Analysis, and Unsupervised Learning",
        "Hands-on knowledge of Scikit-learn, Statsmodels, Matplotlib, Seaborn, and some other Python libraries",
        "Advanced knowledge of A.I. prediction models and automatic model creation",
        "Cloud computing: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources",
        "Option: To use the Anaconda Distribution (for Windows, Mac, Linux)",
        "Master the Python 3 programming language for Data Handling",
        "Master Pandas 2 and 3 for Advanced Data Handling"
      ],
      "course_content": {
        "Introduction to Data Science Methods and Algorithms": [
          "Introduction",
          "Setup of the Anaconda Cloud Notebook",
          "Download and installation of the Anaconda Distribution (optional)",
          "The Conda Package Management System (optional)"
        ],
        "Master Python for Data Handling": [
          "Overview",
          "Python Integers",
          "Python Floats",
          "Python Strings",
          "Python String Methods",
          "Python Strings and DateTime Objects",
          "Python Data Storage Overview",
          "Python Set",
          "Python Tuple",
          "Python Dictionary",
          "Python List",
          "Data Transformers and Functions Overview",
          "Python While-loop",
          "Python For-loop",
          "Python Conditional Code Branching and Logic Operators",
          "Python Function Theory",
          "Python Functions: create your own functions",
          "Python Object Oriented Programming: Some theory",
          "Python Object Oriented Programming II: create your own custom objects",
          "Python Object Oriented Programming III: Files and Tables",
          "Python Object Oriented Programming IV: Recap and More"
        ],
        "Master Pandas for Data Handling": [
          "Master Pandas for Data Handling: Overview",
          "Pandas Theory and Terminology",
          "Creating a Pandas DataFrame from scratch",
          "Pandas File Handling: Overview",
          "Pandas File Handling: The .csv file format",
          "Pandas File Handling: The .xlsx file format",
          "Pandas File Handling: SQL-database files and Pandas DataFrame",
          "Pandas Operations & Techniques: Overview",
          "Pandas Operations & Techniques: Object Inspection",
          "Pandas Operations & Techniques: DataFrame Inspection",
          "Pandas Operations & Techniques: Column Selections",
          "Pandas Operations & Techniques: Row Selections",
          "Pandas Operations & Techniques: Conditional Selections",
          "Pandas Operations & Techniques: Scalers and Standardization",
          "Pandas Operations & Techniques: Concatenate DataFrames",
          "Pandas Operations & Techniques: Joining DataFrames",
          "Pandas Operations & Techniques: Merging DataFrames",
          "Pandas Operations & Techniques: Transpose & Pivot Functions",
          "Pandas Data Preparation: Overview & workflow",
          "Pandas Data Preparation II: Edit DataFrame labels",
          "Pandas Data Preparation III: Duplicates",
          "Pandas Data Preparation IV: Missing Data & Imputation",
          "Pandas Data Preparation V: Data Binnings [Extra Video]",
          "Pandas Data Preparation VI: Indicator Features [Extra Video]",
          "Pandas Data Description: Overview",
          "Pandas Data Description II: Sorting and Ranking",
          "Pandas Data Description III: Descriptive Statistics",
          "Pandas Data Description IV: Crosstabulations & Groupings",
          "Pandas Data Visualization: Overview",
          "Pandas Data Visualization II: Histograms",
          "Pandas Data Visualization III: Boxplots",
          "Pandas Data Visualization IV: Scatterplots",
          "Pandas Data Visualization V: Pie Charts",
          "Pandas Data Visualization VI: Line plots"
        ],
        "Master Regression, Prediction & Supervised Learning": [
          "Regression, Prediction, and Supervised Learning. Section Overview (I)",
          "The Traditional Simple Regression Model (II)",
          "The Traditional Simple Regression Model (III)",
          "Some practical and useful modelling concepts (IV)",
          "Some practical and useful modelling concepts (V)",
          "Linear Multiple Regression model (VI)",
          "Linear Multiple Regression model (VII)",
          "Multivariate Polynomial Multiple Regression models (VIII)",
          "Multivariate Polynomial Multiple Regression models (VIIII)",
          "Regression Regularization, Lasso and Ridge models (X)",
          "Decision Tree Regression models (XI)",
          "Random Forest Regression (XII)",
          "Voting Regression (XIII)"
        ],
        "Master Classification & Supervised Learning": [
          "Classification and Supervised Learning, overview",
          "Logistic Regression Classifier",
          "The Naive Bayes Classifier",
          "K-Nearest Neighbor Classifier (KNN) [Extra Video]",
          "The Decision Tree Classifier",
          "The Random Forest Classifier",
          "Linear Discriminant Analysis (LDA) [Extra Video]",
          "The Voting Classifier"
        ],
        "Master Cluster Analysis and Unsupervised Learning": [
          "Overview",
          "K-Means Cluster Analysis",
          "Auto-updated K-Means Cluster Analysis, introduction and simulation",
          "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
          "Four Hierarchical Clustering algorithms",
          "Principal Component Analysis (PCA)"
        ]
      },
      "requirements": [
        "The four ways of counting (+-*/)",
        "Everyday experience using a computer with either Windows, MacOS, iOS, Android, ChromeOS, or Linux is recommended",
        "Access to a computer with an internet connection",
        "Programming experience is not needed and you will be taught everything you need",
        "The course only uses costless software",
        "Walk-you-through installation and setup videos for Cloud computing and Windows 10/11 is included"
      ],
      "description": "Welcome to the course Data Science Methods and Algorithms with Pandas and Python!\nData Science is expanding and developing on a massive and global scale. Everywhere in society, there is a movement to implement and use Data Science Methods and Algorithms to develop and optimize all aspects of our lives, businesses, societies, governments, and states.\nThis course will teach you a large selection of Data Science methods and algorithms, which will give you an excellent foundation for Data Science jobs and studies. This course has exclusive content that will teach you many new things regardless of if you are a beginner or an experienced Data Scientist.\n\n\nThis is a five-in-one master class video course which will teach you to master Regression, Prediction, Classification, Supervised Learning, Cluster analysis, Unsupervised Learning, Python 3, Pandas 2 + 3, and advanced Data Handling.\nYou will learn to master Regression, Regression analysis, Prediction and supervised learning. This course has the most complete and fundamental master-level regression content packages on Udemy, with hands-on, useful practical theory, and also automatic Machine Learning algorithms for model building, feature selection, and artificial intelligence. You will learn about models ranging from linear regression models to advanced multivariate polynomial regression models.\nYou will learn to master Classification and supervised learning. You will learn about the classification process, classification theory, and visualizations as well as some useful classifier models, including the very powerful Random Forest Classifiers Ensembles and Voting Classifier Ensembles.\nYou will learn to master Cluster Analysis and unsupervised learning. This part of the course is about unsupervised learning, cluster theory, artificial intelligence, explorative data analysis, and some useful Machine Learning clustering algorithms ranging from hierarchical cluster models to density-based cluster models.\nYou will learn to master the Python 3 programming language, which is one of the most popular and useful programming languages in the world, and you will learn to use it for Data Handling.\nYou will learn to master the Pandas 2 and future 3 library and to use Pandas powerful Data Handling techniques for advanced Data Handling tasks. The Pandas library is a fast, powerful, flexible, and easy-to-use open-source data analysis and data manipulation tool, which is directly usable with the Python programming language, and combined creates the world’s most powerful coding environment for Data Handling and Advanced Data Handling…\n\n\nYou will learn\nKnowledge about Data Science methods, algorithms, theory, best practices, and tasks\nDeep hands-on knowledge of Data Science and know how to handle common Data Science tasks with confidence\nDetailed and deep Master knowledge of Regression, Regression analysis, Prediction, Classification, Supervised Learning, Cluster Analysis, and Unsupervised Learning\nHands-on knowledge of Scikit-learn, Statsmodels, Matplotlib, Seaborn, and some other Python libraries\nAdvanced knowledge of A.I. prediction models and automatic model creation\nCloud computing: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources\nOption: To use the Anaconda Distribution (for Windows, Mac, Linux)\nOption: Use Python environment fundamentals with the Conda package management system and command line installing/updating of libraries and packages – golden nuggets to improve your quality of work life\nMaster the Python 3 programming language for Data Handling\nMaster Pandas 2 and 3 for Advanced Data Handling\nAnd much more…\nThis course includes\na comprehensive and easy-to-follow teaching package for Mastering Python and Pandas for Data Handling, which makes anyone able to learn the course contents regardless of beforehand knowledge of programming, tabulation software, Python, Data Science, or Machine Learning\nan easy-to-follow guide for using the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). You may learn to use Cloud Computing resources in this course\nan easy-to-follow optional guide for downloading, installing, and setting up the Anaconda Distribution, which makes anyone able to install a Python Data Science environment useful for this course or for any Data Science or coding task\ncontent that will teach you many new things, regardless of if you are a beginner or an experienced Data Scientist\na large collection of unique content, and this course will teach you many new things that only can be learned from this course on Udemy\nA course structure built on a proven and professional framework for learning.\nA compact course structure and no killing time\n\n\nThis course is an excellent way to learn to master Regression, Prediction, Classification, Cluster analysis, Python, Pandas and Data Handling! These are the most important and useful tools for modeling, AI, and forecasting. Data Handling is the process of making data useful and usable for regression, prediction, classification, cluster analysis, and data analysis.\nMost Data Scientists and Machine Learning Engineers spends about 80% of their working efforts and time on Data Handling tasks. Being good at Python, Pandas, and Data Handling are extremely useful and time-saving skills that functions as a force multiplier for productivity.\n\n\nIs this course for you?\nThis course is for you, regardless if you are a beginner or an experienced Data Scientist\nThis course is for you, regardless if you have a Ph.D. or no education or experience at all\nThis course is the course we ourselves would want to be able to enroll in if we could time-travel and become new students. In our opinion, this course is the best course to learn to Master Regression, Prediction, Python, Pandas, and Data Handling.\n\n\nCourse requirements\nThe four ways of counting (+-*/)\nEveryday experience using a computer with either Windows, MacOS, iOS, Android, ChromeOS, or Linux is recommended\nAccess to a computer with an internet connection\nProgramming experience is not needed and you will be taught everything you need\nThe course only uses costless software\nWalk-you-through installation and setup videos for Cloud computing and Windows 10/11 is included\n\n\nEnroll now to receive 35+ hours of video tutorials with manually edited English captions, and a certificate of completion after completing the course!",
      "target_audience": [
        "This course is for you, regardless if you are a beginner or an experienced Data Scientist",
        "This course is for you, regardless if you have a Ph.D. or no education or experience at all"
      ]
    },
    {
      "title": "ChatGPT for Data Science and Data Analysis in Python",
      "url": "https://www.udemy.com/course/chatgpt-for-data-science-and-data-analysis-in-python/",
      "bio": "Learn to Leverage AI to Fast-Track Your Data Science Project Execution, Data Analysis, Data Visualization and Reporting",
      "objectives": [
        "Learn to design efficient prompts in ChatGPT to achieve desired results with fewer steps.",
        "Gain skills to employ ChatGPT and other GenAI tools to initiate data science projects, reducing start-up time by up to 90%.",
        "Develop proficiency in using ChatGPT and GenAI technologies to carry out data science projects, potentially halving project execution time.",
        "Create stunning data visualizations (in Python, Tableau and PowerBI) and reports in no time",
        "You will be able to comprehend and explain the basic principles of promo engineering and its relevance in the field of data science.",
        "Learn to leverage Google Colab for efficient Python programming and data analysis.",
        "Develop skills in using ChatGPT to prepare and clean data for analysis, simplifying the preprocessing stage.",
        "You will be able to transform categorical data into numerical form using ChatGPT effectively.",
        "Learn to conduct descriptive data analysis using ChatGPT, and interpret the results for informed decision-making.",
        "Develop expertise in implementing feature engineering techniques with ChatGPT",
        "Learn to perform exploratory data analysis using ChatGPT, uncovering hidden patterns and insights from the data.",
        "Gain proficiency in analyzing qualitative data with ChatGPT, extracting valuable interpretations from non-numerical data.",
        "You will be able to use ChatGPT to produce clear and comprehensive reports based on the findings from data analysis."
      ],
      "course_content": {
        "Intro to Prompt Engineering": [
          "Get excited! Derive insights from the data, create report and send email to your",
          "Intro into Google Colab",
          "Key Prompt Engineering Principles for Mastering ChatGPT",
          "Download Prompt Engineering Principles Cheatsheet",
          "Hands-On: Enhancing Business Data Analysis with ChatGPT's Prompt Engineering",
          "Course survey for the next topics"
        ],
        "Survey analysis with ChatGPT": [
          "Data Preparation Techniques for Improved Decision-Making",
          "Use ChatGPT to Prepare Data",
          "Mastering Categorical Data Encoding with ChatGPT for Data Analytics",
          "Hands-On Data Processing: Applying ChatGPT for Efficient Business Data Prep.",
          "Descriptive Data Analysis for Deriving Business Insights",
          "Iterative Feature Engineering with ChatGPT and Python",
          "Feature Creation using ChatGPT: Decoding data",
          "ChatGPT as a Co-Pilot in Exploratory Data Analysis and Simplifying Insights",
          "Streamlining Qualitative Data Analysis using ChatGPT: Text Categorization",
          "Using ChatGPT for Text Classification: A Deep Dive into Sentiment Analysis",
          "Hands On: Exploring the SMILE Twitter Emotion Dataset with ChatGPT",
          "Creating Data Analysis Reports and Communicating with Stakeholder with ChatGPT"
        ],
        "OpenAI API": [
          "Setting Up and Utilizing ChatGPT APIs for Data Analysis",
          "OpenAI API price",
          "Hands On: Setting up your OpenAI API and executing your first API prompt",
          "Optimizing ChatGPT Prompts for Predictable Results: API Testing w. Google Cloud",
          "Applying Function Calling in GPT-4 API for Structured Data Extraction"
        ],
        "Case-Study - Ecommerce Churn Prediction": [
          "Predicting E-commerce Churn Using ChatGPT: A Step-by-step Guide",
          "Understanding Churn and Identifying Churn Users with ChatGPT",
          "Applying the Tree of Thoughts Technique for Enhanced Feature Engineering",
          "Hands-On: Engineering Features with Tree of Thoughts approach",
          "Streamlining Repetitive Tasks and Verifying ChatGPT Results during Feature Eng.",
          "Have ChatGPT engineer a feature",
          "Exploratory Data Analysis: Visual Inspection and Spot Checking with ChatGPT",
          "Model Building with ChatGPT: Data Preprocessing and Handling Class Imbalance",
          "ChatGPT for Predictive Modeling: Choosing Metrics and Establishing a Baseline",
          "Model Selection with ChatGPT: Overcoming Redundancy and Enhancing Efficiency",
          "Hands-On: Leveraging ChatGPT for Metric Selection, Baseline Establishment",
          "Cross-Validation and Model Selection with ChatGPT",
          "Plotting Learning Curves and Diagnosing Model Performance with ChatGPT",
          "Final Model Training, Interpretation, and Trade-Offs in E-Commerce Churn Predic.",
          "Section Practice"
        ],
        "Extra lessons": [
          "LangChain - Get Excited! - Unlocking Efficient Data Analysis with Langchain"
        ]
      },
      "requirements": [
        "Basic Python knowledge.",
        "ChatGPT account.",
        "Connection to the internet."
      ],
      "description": "Are you interested in leveraging the power of AI to streamline your Data Science projects?\nDo you want to learn how to use ChatGPT and GenAI technologies to design efficient data science workflows and create stunning data visualizations?\nAre you a data scientist, project manager, or entrepreneur keen on leveraging AI tools to kick-start and execute data science projects efficiently?\n\n\nIf the answer is yes to any of these questions, this course is tailor-made for you!\nChatGPT, developed by OpenAI, is an advanced language model that can be applied to various data science tasks, including data preparation, feature engineering, data analysis, and report generation. This course, \"ChatGPT for Data Science and Data Analysis in Python\", will help you significantly use ChatGPT to speed up your data science projects.\nData Science continues to be one of the most in-demand fields, offering numerous career opportunities across sectors. With the advent of AI technologies like ChatGPT, it's now possible to execute data science projects more efficiently, reducing time and effort significantly. And we will teach you how.\nHere's what sets this course apart:\n\n\nA focus on practical application: From prompt engineering to text classification, you will learn to apply ChatGPT in real-world data science contexts.\nStep-by-step guide: Each module is designed to build on the previous one, ensuring a comprehensive understanding of how to use ChatGPT for various stages of a data science project.\nCollaborative learning: Learn how to use ChatGPT to improve team communication, a critical skill in any data science project.\nWhat will you learn?\n\n\nHow to design efficient prompts in ChatGPT for optimal results.\nTechniques to initiate data science projects using ChatGPT, potentially reducing start-up time by up to 90%.\nMethods to utilize ChatGPT and GenAI technologies to carry out data science projects, potentially halving project execution time.\nCreating stunning data visualizations and reports in Python, Tableau, and PowerBI in no time.\n\n\nWith this course, you'll get:\n\n\nAccess to all the codes and course materials.\nFour hours of content plus coding exercises and practical assignments.\nA certificate of completion that you can post on your LinkedIn profile, showcasing your skills in using ChatGPT for Data Science.\nA 30-day money-back guarantee, allowing you to try the course risk-free!\nExplore the preview videos and the outline to understand the exciting journey ahead. Enroll today, and let's revolutionize how we do Data Science together!",
      "target_audience": [
        "Aspiring data analysts and data scientists.",
        "Entrepreneurs, digital marketers, social media managers, and product managers who use data to drive decisions."
      ]
    },
    {
      "title": "Mathematics-Basics to Advanced for Data Science And GenAI",
      "url": "https://www.udemy.com/course/mathematics-basics-to-advanced-for-data-science-and-ml/",
      "bio": "Build Strong math foundation with linear algebra,stats,probability,differential calculus for mastering data science",
      "objectives": [
        "Master Calculus: Understand derivatives and integrals, and apply them in optimizing machine learning algorithms and data analysis tasks.",
        "Learn Linear Algebra: Grasp vectors, matrices, and eigenvalues, essential for building and understanding advanced data science models.",
        "Understand Probability: Dive into probability theory, crucial for making informed predictions and working with uncertainty in data.",
        "Apply Statistics: Gain practical skills in statistical analysis, helping you make data-driven decisions and interpret results effectively."
      ],
      "course_content": {
        "Welcome To This Course": [
          "What We are Going To Learn"
        ],
        "Introduction To Linear Algebra": [
          "Introduction",
          "Scalars And Vectors",
          "Addition Of Vectors",
          "Multiplication Of Vectors",
          "Vector Databases- Examples Of Cosines similarity",
          "Vectors Multiplication-Element Wise Multiplication",
          "Vectors Multiplication-Scaler Multiplication",
          "Introduction To Matrices And Application",
          "Matrices Operation"
        ],
        "Introduction To Functions And Transformation": [
          "Introduction To Function And Linear Transformation",
          "Vector Transformations",
          "Linear Transformation",
          "Why Linear Transformation?",
          "Linear Transformation Visualization",
          "Vector Length And Vector Unit",
          "Introduction To Projection"
        ],
        "Inverse Functions Or Transformation": [
          "Inversion Functions",
          "Applications of Function And Inverse Function",
          "How to find Inverse Of A Matrix"
        ],
        "Eigen Vectors And Eigen Values": [
          "All You need to know about Eigen Values And Eigen Vectors"
        ],
        "Equation Of a Line,Plane,Hyperplane": [
          "Equation OF a Line,Plane And Hyperplane"
        ],
        "Introduction To Statistics": [
          "Introduction To Statistics",
          "Types Of Statistics",
          "Population And Sample Data",
          "Types Of Sampling",
          "Types Of Data",
          "Scales OF Measurement Of Data"
        ],
        "Descriptive Statistics": [
          "Measure Of Central Tendency",
          "Measure Of Dispersion",
          "Why Sample Variance is Divided By N-1",
          "Random Variables",
          "Percentile And Quartiles",
          "5 Number Summary",
          "Histogram And Skewness",
          "Correlation And Covariance"
        ],
        "Introduction To Probability": [
          "Addition Rule In Probability",
          "Multiplication Rule In Probability"
        ],
        "Probability Distribution function And Types Of Distribution": [
          "PDF,PMF,CDF",
          "Types Of Probability Distribution",
          "Bernoulli Distribution",
          "Binomial Distribution",
          "Poisson Distribution",
          "Normal Gaussian Distribution",
          "Standard Normal Distribution and Z score",
          "Uniform Distribution",
          "Log Normal Distribution",
          "Power Law Distribution",
          "Pareto Distribution",
          "Central Limit Theorem",
          "Estimates"
        ]
      },
      "requirements": [
        "Basic Understanding of High School Math: Familiarity with basic algebra, geometry, and arithmetic operations will help you grasp the course material more effectively.",
        "Curiosity About Data Science: A keen interest in learning how mathematics is applied in data science will enhance your engagement with the content.",
        "No Prior Programming Experience Required: While some familiarity with programming is helpful, it's not mandatory. Mathematical concepts will be the main focus.",
        "Willingness to Practice: Be ready to work through exercises, quizzes, and projects to reinforce your understanding and gain practical experience."
      ],
      "description": "Are you eager to dive into the world of data science but feel overwhelmed by the mathematical concepts involved? Welcome to the \"Complete Maths to Learn Data Science\" course, your comprehensive guide to mastering the essential mathematical foundations needed to excel in data science and machine learning.\nThis course is designed to bridge the gap between your current math skills and the level required to understand and implement data science algorithms effectively. Whether you are a beginner or an experienced professional looking to strengthen your mathematical understanding, this course will equip you with the tools you need to succeed.\nWhat You Will Learn:\nCalculus for Data Science:\nUnderstand the fundamentals of calculus, including derivatives, integrals, and limits.\nLearn how these concepts are applied in optimizing machine learning algorithms, such as gradient descent, and in understanding complex data transformations.\nLinear Algebra Essentials:\nGain a deep understanding of vectors, matrices, eigenvalues, and eigenvectors.\nDiscover how these linear algebra concepts are crucial for data manipulation, dimensionality reduction (like PCA), and building advanced machine learning models.\nProbability Theory and Its Applications:\nDive into the world of probability, including concepts like random variables, distributions, and Bayes’ Theorem.\nExplore how probability forms the backbone of predictive modeling, classification algorithms, and risk assessment in data science.\nStatistics for Data Analysis:\nMaster key statistical techniques such as hypothesis testing, regression analysis, and statistical inference.\nLearn to make data-driven decisions by understanding and applying statistical methods to real-world datasets.\nWhy This Course?\nThis course stands out by focusing on the clarity and practical application of mathematical concepts in data science. Each topic is broken down into simple, easy-to-understand modules that build on one another. You will not only learn the theory but also see exactly how these mathematical tools are used in real data science scenarios.\nThroughout the course, you’ll engage with interactive quizzes, assignments, and hands-on projects designed to reinforce your understanding. By applying what you learn in real-world projects, you’ll gain practical experience and build a portfolio that showcases your newly acquired skills.\nWho Is This Course For?\nAspiring Data Scientists: Individuals looking to build a strong mathematical foundation essential for mastering data science and machine learning.\nData Science Beginners: Those new to the field who want to understand the core mathematical concepts that drive data science algorithms.\nProfessionals Transitioning into Data Science: Engineers, analysts, or professionals from other fields seeking to acquire the mathematical skills necessary for a career shift into data science.\nStudents and Academics: Students pursuing studies in data science, computer science, or related fields who need a comprehensive understanding of mathematics for data science applications.\nLifelong Learners: Anyone with a passion for learning and a desire to understand how mathematics powers the world of data science, even without prior experience in the field.\nEnroll Today!\nJoin thousands of learners who have transformed their careers by mastering the mathematics behind data science. Whether you’re aiming to start a new career, enhance your skills, or simply satisfy your curiosity, this course will provide the solid mathematical foundation you need to succeed. Enroll now and take the first step towards becoming a confident and skilled data scientist!",
      "target_audience": [
        "Aspiring Data Scientists: Individuals looking to build a strong mathematical foundation essential for mastering data science and machine learning.",
        "Data Science Beginners: Those who are new to data science and want to understand the core mathematical concepts that drive data science algorithms.",
        "Professionals Transitioning into Data Science: Engineers, analysts, or professionals from other fields seeking to acquire the mathematical skills necessary for a career shift into data science.",
        "Students and Academics: Students pursuing studies in data science, computer science, or related fields who need a comprehensive understanding of mathematics for data science applications.",
        "Lifelong Learners: Anyone with a passion for learning and a desire to understand how mathematics powers the world of data science, even without prior experience in the field.",
        "This course is tailored to equip learners with the essential mathematical tools needed to excel in data science, regardless of their current level of expertise."
      ]
    },
    {
      "title": "Reinforcement Learning with Pytorch",
      "url": "https://www.udemy.com/course/reinforcement-learning-with-pytorch/",
      "bio": "Learn to apply Reinforcement Learning and Artificial Intelligence algorithms using Python, Pytorch and OpenAI Gym",
      "objectives": [
        "Reinforcement Learning basics",
        "Tabular methods",
        "Bellman equation",
        "Q Learning",
        "Deep Reinforcement Learning",
        "Learning from video input"
      ],
      "course_content": {
        "Welcome to the course": [
          "Welcome!",
          "Before you start - Videos quality!",
          "Resources"
        ],
        "Introduction": [
          "Introduction #1",
          "Introduction #2",
          "Introduction #3",
          "Introduction #4",
          "Environment setup / Installation",
          "Lab. OpenAI Gym #1",
          "Lab. OpenAI Gym #2",
          "Lab. OpenAI Gym #3",
          "Lab. OpenAI Gym #4"
        ],
        "Tabular methods": [
          "Deterministic & Stochastic environments",
          "Rewards",
          "Bellman equation #1",
          "Bellman equation #2",
          "Resource - code",
          "Lab. Algorithm for deterministic environments #1",
          "Lab. Algorithm for deterministic environments #2",
          "Lab. Algorithm for deterministic environments #3",
          "Lab. Algorithm for deterministic environments #4",
          "Lab. Test with stochastic environment",
          "Q-Learning",
          "Lab. Algorithm for stochastic environments",
          "Exploration vs Exploitation",
          "Lab. Egreedy",
          "Lab. Adaptive egreedy",
          "Bonus Lab. Value iteration",
          "Homework",
          "Homework. Solution",
          "Homework. Tuning"
        ],
        "Scaling up": [
          "Scaling up",
          "Neural Networks review",
          "Lab. Neural Networks review #1",
          "Lab. Neural Networks review #2",
          "Lab. Random CartPole",
          "Lab. Epsilon egreedy revisited",
          "Lab. Pytorch updated ( version 0.4.0 )",
          "Article. Pytorch updated! (further versions)",
          "Lab. OpenAI Gym + Neural Network #1",
          "Lab. OpenAI Gym + Neural Network #2",
          "Lab. OpenAI Gym + Neural Network #3",
          "Lab. Extended logging"
        ],
        "DQN": [
          "Deep Reinforcement Learning",
          "Lab. Deep Reinforcement Learning",
          "Lab. Tuning challenge",
          "Experience Replay",
          "Lab. Experience Replay #1",
          "Lab. Experience Replay #2",
          "Lab. Experience Replay #3",
          "DQN",
          "Lab. DQN"
        ],
        "DQN Improvements": [
          "Double DQN",
          "Lab. Double DQN",
          "Dueling DQN",
          "Lab. Dueling DQN",
          "Lab. Dueling DQN Challenge"
        ],
        "DQN with video output": [
          "CNN Review",
          "Lab. Random Pong",
          "Saving & Loading the Model",
          "Lab. Pong from video output #1",
          "Lab. Pong from video output #2",
          "Lab. Pong from video output #3",
          "Lab. Pong from video output #4",
          "Lab. Pong from video output #5",
          "Lab. Pong from video output #6",
          "Potential improvements",
          "Article. Stacking 4 images together"
        ],
        "Final notes": [
          "What's next?"
        ]
      },
      "requirements": [
        "Basic python knowledge is needed. AI / Machine Learning / Pytorch basics - nice to have but not fully necessary. Only open source tools will be in use."
      ],
      "description": "UPDATE:\nAll the code and installation instructions have been updated and verified to work with Pytorch 1.6 !!\n\n\nArtificial Intelligence is dynamically edging its way into our lives. It is already broadly available and we use it - sometimes even not knowing it  - on daily basis. Soon it will be our permanent, every day companion.\nAnd where can we place Reinforcement Learning in AI world? Definitely this is one of the most promising and fastest growing technologies that can eventually lead us to General Artificial Intelligence! We can see multiple examples where AI can achieve amazing results - from reaching super human level while playing games to solving real life problems (robotics, healthcare, etc).\nWithout a doubt it's worth to know and understand it!\nAnd that's why this course has been created.\nWe will go through multiple topics, focusing on most important and practical details. We will start from very basic information, gradually building our understanding, and finally reaching the point where we will make our agent learn in human-like way - only from video input!\nWhat's important - of course we need to cover some theory - but we will mainly focus on practical part. Goal is to understand WHY and HOW.\n\nIn order to evaluate our algorithms we will use environments from - very popular - OpenAI Gym. We will start from basic text games, through more complex ones, up to challenging Atari games\nWhat will be covered during the course ?\n- Introduction to Reinforcement Learning\n- Markov Decision Process\n- Deterministic and stochastic environments\n- Bellman Equation\n- Q Learning\n- Exploration vs Exploitation\n- Scaling up\n- Neural Networks as function approximators\n- Deep Reinforcement Learning\n- DQN\n- Improvements to DQN\n- Learning from video input\n- Reproducing some of most popular RL solutions\n- Tuning parameters and  general recommendations\nSee you in the class!",
      "target_audience": [
        "Anyone interested in artificial intelligence, data science, machine learning, deep learning and reinforcement learning."
      ]
    },
    {
      "title": "ChatGPT + Bing (Copilot): Prompt Engineering Masterclass",
      "url": "https://www.udemy.com/course/chatgpt-bing-chat-prompt-engineering-2023-marterclass-course/",
      "bio": "Combined Prompt Engineering AI Course for ChatGPT + Bing Chat (Copilot). Beginner to Advance. 1000+ prompts, Templates",
      "objectives": [
        "You will understand ChatGPT and Bing Chat Prompt Engineering from basics to advanced concepts and applications.",
        "How to quickly generate images with Bing Chat using the 3-elements technique.",
        "The basics of AI, machine learning, deep learning and NLP.",
        "How to train ChatGPT and Bing Chat to improve the output",
        "How to use ChatGPT and Bing Chat for brainstorming and creating presentations.",
        "How to generate stories and novels in ChatGPT using a template.",
        "How to use ChatGPT and Bing Chat for summarizing texts.",
        "How to use ChatGPT and Bing Chat for solving complex problems.",
        "How to use ChatGPT and Bing Chat for extracting content.",
        "Understand chain of thought prompting and it's application.",
        "How to use ChatGPT and Bing Chat for personalization.",
        "How to use ChatGPT and Bing Chat for completing stories and emails.",
        "How to use ChatGPT and Bing Chat for sentiment analysis.",
        "How to use the ‘STAR’ prompt system for generating great output.",
        "How to use advanced stepwise prompting.",
        "How to use the image generation template for creating artistic images in Bing Chat."
      ],
      "course_content": {
        "The Power of Prompt Engineering & AI": [
          "Magic of Prompt Engineering: Unleashing ChatGPT and Bing's AI Power",
          "Why Learning ChatGPT + Bing Prompt Engineering is A Game Changer",
          "Demystifying AI: Understanding the Basics",
          "Inside the AI Mind: Machine Learning, Deep Learning & NLP",
          "Prompt engineering & AI Groundwork Quiz"
        ],
        "ChatGPT & Bing ChatBot: Prompt Engineering Building Blocks": [
          "Setting Up ChatGPT + Bing Chat",
          "Understanding ChatGPT and Bing Chat: How They Work",
          "Real-world Applications 1: Content Creation, Rewrite & Primed Prompt",
          "Real-world Applications 2: Summarizing Made Easy",
          "Real-world Applications 3: Solve Complex Problems & CoT Prompting",
          "Real-world Applications 4: Extraction & Output",
          "Real-world Applications 5: Personalization & Few-shot Prompting",
          "Real-world Applications 6: Translation, Completion & Ignore",
          "Real-world Applications 7: Power Of Sentiment Analysis",
          "Prompt Engineering Building Blocks Quiz"
        ],
        "Advanced Prompting Techniques & Class Project": [
          "Unlocking Imagination: The 'STAR' Prompt System For Great Output",
          "Advanced Stepwise Prompting: Practical Example for Blog Writing",
          "Brainstorming and Presentations: Advanced Techniques for ChatGPT",
          "Quickly Generate Images with Bing Using the 3-Elements Technique",
          "Image Generation Template: Creating Artistic Images with Ease",
          "Creating the Perfect Prompt: Endless Possibilities with ChatGPT & Bing",
          "Advanced Prompting Quiz",
          "Class Project: Generating An AI Fantasy Novel Using Template"
        ]
      },
      "requirements": [
        "No coding or previous knowledge is required; just have an internet connection.",
        "Only free ChatGPT & Bing Chat are used, so the ChatGPT paid plan is not required."
      ],
      "description": "\"Why learn ChatGPT and Bing Chat together?\"\nYou can unleash your creativity and productivity with the power of ChatGPT and Bing Chat, two of the most advanced AI systems in the world. Both are complementary and powerful AI systems that can help you achieve more. ChatGPT can write anything from essays to stories to codes, while Bing Chat can answer questions and provide up-to-date information from the web. Together, they can give you the best of both worlds: creativity and accuracy, imagination and facts, fiction and reality. And Bing chat can generate stunning images with just words. By learning how to use both ChatGPT and Bing Chat, you will be able to create amazing things with AI that no one else can.\n\n\n\"Why learn Prompt Engineering?\"\nAIs are like the smartest person you have ever met, but at the same time, they are like a five-year-old kid - they can do a lot of things, but they also need careful instruction to do them well.\nThus, being able to communicate effectively with AIs is a crucial skill and I believe at this point anyone who wants to advance in their career, school, business or anything, or just stand apart from the competition, must definitely possess this important skill, called Prompt Engineering.\nAs prompt engineering teaches you how to get interact and generated desired output from the AI because the quality of your input determines the quality of the output generated by these AIs.\n\n\n\"What I will learn in this course?\"\nAI ChatBots like ChatGPT and Bing Chat can write essays, articles, and stories, solve math problems, write sales emails, draft a contract, write codes for games, websites, and apps, and can even generate amazing images just by typing a few words.\nThe possibilities for using AIs are endless.\nThat's the reason I have crafted this course, so anyone can learn and master ChatGPT and Bing's AI ChatBots with the skill of Prompt Engineering.\nYou will be learning from Basics concepts to Real Life Usages of ChatGPT & Bing Chat.\nAlso, we will cover advanced topics like:\nBrainstorming\nStepwise prompting\n'Star' Prompt System\nImage Prompting\nZero-shot, Few-shot prompting, Temperature etc.\n\n\n\"Will you also teach Image prompting in Bing Chat?\"\nYes, you will be taught image prompting in the course and how to generate images from Bing Chat.\n\n\n\"What resources will be provided in the course?\"\nIn this course, you will receive -\n- 1000+ Prompts Ebook\n- Prompt Templates Ebook.\n- Prompt Response Handbook\n- 'Perfect Prompt' Cheatsheet\n- Prompt Style Sheet (Excel)\n- Short Fantasy Novel \"Seasons, Superpowers, and Dreams Three Short Stories by ChatGPT\"\n- PPT Presentation \"Blockbuster GamingPass\"\n\n\n\"Do I have to get ChatGPT paid plan to use this course?\"\nNo. We will only be using the free ChatGPT plan throughout this course.\n\n\n\"What if I don't like the course?\"\nThis course comes with a 30-day money-back guarantee, so don't worry, just enter the course to fully grasp the power of Prompt Engineering for ChatGPT and Bing Chat.\n\n\nIf you eager to enhance your skills and become a part of AI revolution, then come and join me in this exciting and important course to learn and master the vast potential of Prompt Engineering.\n\n\nI look forward to seeing you in the virtual classroom, with a passion to become the best Prompt Engineer.\nLet's master ChatGPT and Bing Chat",
      "target_audience": [
        "This ChatGPT & Bing Chat Prompt Engineering course is for EVERYONE.",
        "You can be a student, businessperson or professional, or you might be working in finance, HR, marketing or any other field. Prompt Engineering is a skill that you can acquire and master."
      ]
    },
    {
      "title": "Natural Language Processing: NLP With Transformers in Python",
      "url": "https://www.udemy.com/course/nlp-with-transformers/",
      "bio": "Learn next-generation NLP with transformers for sentiment analysis, Q&A, similarity search, NER, and more",
      "objectives": [
        "Industry standard NLP using transformer models",
        "Build full-stack question-answering transformer models",
        "Perform sentiment analysis with transformers models in PyTorch and TensorFlow",
        "Advanced search technologies like Elasticsearch and Facebook AI Similarity Search (FAISS)",
        "Create fine-tuned transformers models for specialized use-cases",
        "Measure performance of language models using advanced metrics like ROUGE",
        "Vector building techniques like BM25 or dense passage retrievers (DPR)",
        "An overview of recent developments in NLP",
        "Understand attention and other key components of transformers",
        "Learn about key transformers models such as BERT",
        "Preprocess text data for NLP",
        "Named entity recognition (NER) using spaCy and transformers",
        "Fine-tune language classification models"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course Overview",
          "Hello! and Further Resources",
          "Environment Setup",
          "Alternative Local Setup",
          "Alternative Colab Setup",
          "CUDA Setup",
          "Apple Silicon Setup"
        ],
        "NLP and Transformers": [
          "The Three Eras of AI",
          "Pros and Cons of Neural AI",
          "Word Vectors",
          "Recurrent Neural Networks",
          "Long Short-Term Memory",
          "Encoder-Decoder Attention",
          "Self-Attention",
          "Multi-head Attention",
          "Positional Encoding",
          "Transformer Heads"
        ],
        "Preprocessing for NLP": [
          "Stopwords",
          "Tokens Introduction",
          "Model-Specific Special Tokens",
          "Stemming",
          "Lemmatization",
          "Unicode Normalization - Canonical and Compatibility Equivalence",
          "Unicode Normalization - Composition and Decomposition",
          "Unicode Normalization - NFD and NFC",
          "Unicode Normalization - NFKD and NFKC"
        ],
        "Attention": [
          "Attention Introduction",
          "Alignment With Dot-Product",
          "Dot-Product Attention",
          "Self Attention",
          "Bidirectional Attention",
          "Multi-head and Scaled Dot-Product Attention"
        ],
        "Language Classification": [
          "Introduction to Sentiment Analysis",
          "Prebuilt Flair Models",
          "Introduction to Sentiment Models With Transformers",
          "Tokenization And Special Tokens For BERT",
          "Making Predictions"
        ],
        "[Project] Sentiment Model With TensorFlow and Transformers": [
          "Project Overview",
          "Getting the Data (Kaggle API)",
          "Preprocessing",
          "Building a Dataset",
          "Dataset Shuffle, Batch, Split, and Save",
          "Build and Save",
          "Loading and Prediction"
        ],
        "Long Text Classification With BERT": [
          "Classification of Long Text Using Windows",
          "Window Method in PyTorch"
        ],
        "Named Entity Recognition (NER)": [
          "Introduction to spaCy",
          "Extracting Entities",
          "NER Walkthrough",
          "Authenticating With The Reddit API",
          "Pulling Data With The Reddit API",
          "Extracting ORGs From Reddit Data",
          "Getting Entity Frequency",
          "Entity Blacklist",
          "NER With Sentiment",
          "NER With roBERTa"
        ],
        "Question and Answering": [
          "Open Domain and Reading Comprehension",
          "Retrievers, Readers, and Generators",
          "Intro to SQuAD 2.0",
          "Processing SQuAD Training Data",
          "(Optional) Processing SQuAD Training Data with Match-Case",
          "Processing SQuAD Dev Data",
          "Our First Q&A Model"
        ],
        "Metrics For Language": [
          "Q&A Performance With Exact Match (EM)",
          "Introducing the ROUGE Metric",
          "ROUGE in Python",
          "Applying ROUGE to Q&A",
          "Recall, Precision and F1",
          "Longest Common Subsequence (LCS)"
        ]
      },
      "requirements": [
        "Knowledge of Python",
        "Experience in data science a plus",
        "Experience in NLP a plus"
      ],
      "description": "Transformer models are the de-facto standard in modern NLP. They have proven themselves as the most expressive, powerful models for language by a large margin, beating all major language-based benchmarks time and time again.\nIn this course, we cover everything you need to get started with building cutting-edge performance NLP applications using transformer models like Google AI's BERT, or Facebook AI's DPR.\nWe cover several key NLP frameworks including:\nHuggingFace's Transformers\nTensorFlow 2\nPyTorch\nspaCy\nNLTK\nFlair\nAnd learn how to apply transformers to some of the most popular NLP use-cases:\nLanguage classification/sentiment analysis\nNamed entity recognition (NER)\nQuestion and Answering\nSimilarity/comparative learning\nThroughout each of these use-cases we work through a variety of examples to ensure that what, how, and why transformers are so important. Alongside these sections we also work through two full-size NLP projects, one for sentiment analysis of financial Reddit data, and another covering a fully-fledged open domain question-answering application.\nAll of this is supported by several other sections that encourage us to learn how to better design, implement, and measure the performance of our models, such as:\nHistory of NLP and where transformers come from\nCommon preprocessing techniques for NLP\nThe theory behind transformers\nHow to fine-tune transformers\nWe cover all this and more, I look forward to seeing you in the course!",
      "target_audience": [
        "Aspiring data scientists and ML engineers interested in NLP",
        "Practitioners looking to upgrade their skills",
        "Developers looking to implement NLP solutions",
        "Data scientist",
        "Machine Learning Engineer",
        "Python Developers"
      ]
    },
    {
      "title": "dbt on Databricks",
      "url": "https://www.udemy.com/course/dbt-on-databricks/",
      "bio": "Building Scalable, Modular, Testable, and Version-Controlled Data Transformation Pipelines with dbt on Databricks",
      "objectives": [
        "Gain hands-on experience with dbt Cloud and dbt Core, including setup, orchestration, and deployment in real-world scenarios.",
        "Understand and implement dbt models, sources, and seeds within Databricks for efficient data transformations.",
        "Learn to use Jinja functions and macros to create dynamic, reusable SQL code in dbt projects.",
        "Master table materializations and incremental data loading to optimize data workflows and performance.",
        "Develop comprehensive testing and documentation practices to ensure data quality and project transparency in dbt."
      ],
      "course_content": {},
      "requirements": [
        "Solid Understanding of Databricks, including Lakehouse Architecture",
        "Familiarity with SQL",
        "Knowledge of Cloud Computing (either Azure, GCP or AWS)",
        "Access to Cloud Platform Account (either Azure, GCP or AWS)",
        "Awareness of Version Control (Optional)"
      ],
      "description": "Are you ready to unlock the full potential of your data analytics pipelines? dbt on Databricks is a comprehensive course tailored for data professionals aiming to master data transformation using dbt (data build tool) on the Databricks platform, harnessing the power of Apache Spark for scalable and efficient workflows.\n\n\nWhile Databricks offers robust data processing capabilities, dbt enhances the experience by providing a framework for version-controlled, modular, and testable SQL-based transformations. This combination leverages Apache Spark's power for scalable workflows while maintaining cleaner, more maintainable, and reusable code.\n\n\nThe course covers both dbt Cloud and dbt Core, equipping learners with versatile skills for any environment.\n\n\nWhat This Course Covers:\nIntroduction to dbt and Key Concepts: Begin with an in-depth overview of dbt, Jinja templating, and YAML for configuration. Understand how these tools come together to streamline data transformation.\nSetting Up the Environment: Follow step-by-step guidance on configuring dbt Cloud with Databricks, version control essentials, and an introduction to core components and data pipelines.\nData Modeling and Transformations: Explore multi-layer data architecture, including Bronze, Silver, and Gold models. Learn practical approaches for source referencing, schema configuration, and building efficient data pipelines using dbt commands.\nAdvanced Testing and Validation: Implement robust data quality checks through generic and singular tests, transitioning from tests: syntax to data_tests:, and integrate dbt packages like dbt_utils for enhanced functionality.\nJinja, Macros, and Custom Functions: Master the art of reusable, scalable code with Jinja syntax and macros. Gain the skills to manipulate data models dynamically, change schemas, and develop custom logic for specific use cases.\nMaterializations Explained: Understand various materialization strategies including tables, views, incremental loads, and snapshots. Delve into specific scenarios like SCD Type 2 for dimension tables and incremental updates for fact tables.\nDeployment and Production Workflows: Set up a production-ready dbt environment on Databricks, manage jobs, and deploy models seamlessly. Learn to configure environment and target variables for streamlined CI/CD workflows.\nDeveloping with dbt Core: Experience the flexibility of dbt Core through local project setups, GitHub integration, and command-line navigation, while learning best practices for version control and collaboration.\nTroubleshooting and Advanced Techniques: Gain insights into handling common connection issues, optimizing project performance, and scaling workloads on Databricks.\n\n\nTarget Audience:\nThis course is designed for data engineers, analysts, and architects who are already familiar with SQL and want to elevate their skills in data transformation using dbt on the Databricks platform. Basic knowledge of Python, Git, and cloud-based data environments is recommended.\n\n\nWhy Take This Course?\nWith hands-on projects, guided exercises, and downloadable resources, this course builds practical skills that can be applied to real-world data challenges. By the end of the course, proficiency in building, testing, and deploying robust data pipelines will set learners apart as skilled data professionals equipped to handle complex analytics workflows.",
      "target_audience": [
        "This course is for Databricks developers, data engineers, and analytics professionals who want to harness dbt for efficient data transformations and enhance their expertise in modern data workflows."
      ]
    },
    {
      "title": "Apache Spark 3 - Databricks Certified Associate Developer",
      "url": "https://www.udemy.com/course/apache-spark-3-databricks-certified-associate-developer/",
      "bio": "Learn Apache Spark 3 With Scala & Earn the Databricks Associate Certification to prove your skills as data professional",
      "objectives": [
        "How to prepare for the Databricks Certified Associate Developer For Apache Spark 3 Certification Exam",
        "The Architecture of an Apache Spark Application",
        "Learn how Apache Spark runs on a cluster of computer",
        "Learn the Execution Hierarchy of Apache Spark",
        "Create DataFrame from files and Scala Collections",
        "Spark DataFrame API and SQL functions",
        "Learn the different techniques to select the columns of a DataFrame",
        "How to define the schema of a DataFrame and set the data types of the columns",
        "Apply various methods to manipulate the columns of a DataFrame",
        "How to filter your DataFrame based on specifics rules",
        "Learn how to sort data in a specific order",
        "Learn how to sort rows of a DataFrame in a specific order",
        "How to arrange the rows of DataFrame as groups",
        "How to handle NULL Values in a DataFrame",
        "How to use JOIN or UNION to combine two data sets",
        "How you can save the result of complex data transformations to an external storage system",
        "The different deployment modes of an Apache Spark Application",
        "working with UDFs and Spark SQL functions",
        "How to use Databricks Community Edition to write Apache Spark Code"
      ],
      "course_content": {
        "Apache Spark Architecture: Distributed Processing": [
          "What You Will Learn In This Section",
          "Distributed Processing: How Apache Spark Runs On A Cluster",
          "Azure Databricks: How To Create A Cluster",
          "Databricks Community Edition: How To Create A Cluster",
          "How does Apache Spark run on a cluster ?"
        ],
        "Apache Spark Architecture: Distributed Data": [
          "Install the course Dataset and Notbooks",
          "Distributed Data: The DataFrame",
          "How To Define The Structure Of A DataFrame",
          "What is a DataFrame (Scala)"
        ],
        "DataFrame Transformations": [
          "Selecting Columns",
          "Renaming Columns",
          "Change Columns data type",
          "How to access columns",
          "Adding Columns to a DataFrame",
          "Removing Columns from a DataFrame",
          "Test your understanding",
          "Basics Arithmetic with DataFrame",
          "Apache Spark Architecture: DataFrame Immutability",
          "How To Filter A DataFrame",
          "Test Your Knowledge",
          "Apache Spark Architecture: Narrow Transformations",
          "Dropping Rows",
          "How to Drop rows and columns",
          "Handling Null Values Part I - Null Functions",
          "Handling Null Values Part II - DataFrameNaFunctions",
          "Sort and Order Rows - Sort & OrderBy",
          "Can You Handle Null Values?",
          "Create Group of Rows: GroupBy",
          "DataFrame Statistics",
          "Group and Order",
          "Joining DataFrames - Inner Join",
          "Joining DataFrames - Right Outer Join",
          "Joining DataFrames - Left Outer Join",
          "Appending Rows to a DataFrame - Union",
          "Can you Join two DataFrames?",
          "Cahing a DataFrame",
          "DataFrameWriter Part I",
          "DataFrameWriter Part II - PartitionBy",
          "User Defined Functions",
          "Do you know how to save the result of your work?"
        ],
        "Apache Spark Architecture: Execution": [
          "Query Planning",
          "Execution Hierarchy",
          "Partioning a DataFrame",
          "Adaptive Query Execution - An Introductuction",
          "How Apache Spark Runs"
        ],
        "Exam Logistics": [
          "Exam Logistics"
        ],
        "Practice Exams Scala": [
          "Practice Exam - Scala"
        ]
      },
      "requirements": [
        "Basic Scala Knowledge",
        "Basic data skills",
        "NO Previous Spark Knowledge"
      ],
      "description": "Do you want to learn how to handle massive amounts of data at scale?\nLearn Apache Spark 3 and pass the Databricks Certified Associate Developer for Apache Spark 3.0\nHi, My name is Wadson, and I’m a Databricks Certified Associate Developer for Apache Spark.\nApache Spark has become the standard big-data cluster processing framework in today's data-driven world.\nApache Spark is used for Data Engineering, Data Science, and Machine Learning.\nI will teach you everything you need to know about starting with Apache Spark.\n\nYou will learn the Architecture of Apache Spark and use its Core APIs to manipulate complex data.\nYou will write queries to perform transformations such as Join, Union, GroupBy, and more.\nThis course is for beginners.\nYou don't need any previous knowledge of Apache Spark.\nNotebooks are available to download so that you can follow along with me in the videos.\n\nThe Notebooks contain all the source code I use in the course.\n\nThere are also Quizzes to help you assess your understanding of the topics.\n\n\nCheck Out some of the top reviews and enroll in the course.\n\"This course is really helpful with all the necessary details needed for the Certification: Databricks Certified Associate Developer for Apache Spark 3.0.\nI've cleared the certification with 80% score and I'd suggest to check all the Course contents thoroughly\"\n\n\n\"Very good course. Gives a good overview of all the necessary components of the spark application which are required for the test and that too in very short span of time. will highly recommend this course.\nworth spending time !!\"",
      "target_audience": [
        "Any Developer who wants to start using Apache Spark in their career",
        "Beginner Spark Developer seeking Big Data Certification",
        "Developer curious about Data Engineering and Data Science"
      ]
    },
    {
      "title": "ChatGPT Masterclass: A Complete ChatGPT Zero to Hero!",
      "url": "https://www.udemy.com/course/chatgpt-masterclass-a-complete-chatgpt-zero-to-hero-openai/",
      "bio": "ChatGPT for Beginners: Use ChatGPT to WIN in your CAREER as a professional or Make a Business Using ChatGPT OpenAI!",
      "objectives": [
        "Become a Pro in text communication with ChatGPT AI",
        "Learn to Write algorithms or complex logic using ChatGPT",
        "How to master problem-solving skills using Generative AI",
        "Create code and apps using ChatGPT Open AI",
        "Build websites with stunning landing pages for your business needs",
        "How to create high-quality graphic images using Dall-e AI",
        "Produce high-quality ChatGPT text faster with our pro tips for ChatGPT usage",
        "Techniques for using ChatGPT to create personalized, profitable, and engaging content",
        "How to use without paying for ChatGPT Plus even when it's at capacity",
        "Create Online courses and pro tips from ChatGPT to start your own Business",
        "Use ChatGPT to WIN in your CAREER as a Software Engineer",
        "Learn How to Ask ChatGPT to prepare an Excel formula for your Data Analytics to save your work time",
        "Participate in Practice test to test your learning skills"
      ],
      "course_content": {},
      "requirements": [
        "No programming experience needed, You will learn everything you need to know",
        "None! Just a willingness to learn and a desire to take advantage of the amazing technology that is ChatGPT and Dalle!"
      ],
      "description": "The ChatGPT Masterclass Zero to Hero is a comprehensive course designed to equip you with the necessary skills and knowledge to master the art of communication through text. This course is ideal for individuals who want to improve their software development skills, and writing skills, enhance their online presence, or simply communicate more effectively in today's digital world.\n\n\nThroughout this course, you will be guided by ChatGPT and also applying Generative AI, a large language model trained by OpenAI, based on the GPT-3.5 architecture. ChatGPT will serve as your personal tutor, providing you with customized feedback and guidance every step of the way.\n\n\nParticipate in Practice test to test your learning skills\n\n\nThe course is divided into several modules, each covering a specific topic related to the development and text communication. These modules include:\n\n\nIntroduction to ChatGPT - In this module, you will learn the basics of ChatGPT text communication and its importance in today's world. You will also learn about the different types of text communication and their respective applications.\n\n\nDeveloper Techniques - In this module, you will learn about the different techniques that can be used to improve your software development skills. You will learn about programming, website creation, code debugging, and other aspects of writing that can make your code development more effective.\n\n\nUnderstanding your Business - In this module, you will learn about the importance of understanding your SEO when communicating through a website. You will learn how to tailor your message to different audiences and how to use language to effectively convey your message to scale your business.\n\n\nChatGPT for Students - In this module, you will learn how to create a resume and compelling content that engages your audience and motivates them to take action. You will learn about the different types of content and how to create content that resonates with your audience.\n\n\nScale your Business - In this module, you will learn about the different social media platforms and how to effectively communicate on them. You will learn about the best practices for posting on social media and how to use social media to promote your brand or message.\n\n\nChatGPT for Excel Professionals - In this module, you will learn about the different Excel formulas and how to effectively communicate to ChatGPT to ask to write a formula for your business needs. You will learn about the best practices for writing formulas and applying your Excel spreadsheets.\n\n\nGenerate Graphic Art: In this module, you will learn about the different Graphic Art creations as your requirements. You will learn about the best practices for creating images and editing personal images as your need.\nAt the end of this course, you will have a thorough understanding of the development, text communication, and the skills to effectively communicate through text in any situation. You will also receive a certificate of completion that you can use to showcase your newfound skills to potential employers or clients.\n\n\nEnroll in the ChatGPT Masterclass Zero to Hero today and take the first step towards becoming a master of Software development and text communication!",
      "target_audience": [
        "Anyone who wants to win a career as a software professional",
        "Professionals who want to improve their online presence and communication skills, including software engineers, marketers, social media managers, customer service representatives, and public relations specialists.",
        "Writers who want to enhance their writing skills and learn how to write compelling content that engages their audience.",
        "Students who want to improve their academic career skills and learn how to develop effectively in their coursework.",
        "Entrepreneurs who want to promote their brand or message through effective text communication.",
        "Anyone who wants to improve their communication skills and learn how to effectively convey their message through text, including bloggers, journalists, and content creators."
      ]
    },
    {
      "title": "Complete MLOps Bootcamp With 10+ End To End ML Projects",
      "url": "https://www.udemy.com/course/complete-mlops-bootcamp-with-10-end-to-end-ml-projects/",
      "bio": "End-to-End MLOps Bootcamp: Build, Deploy, and Automate ML with Data Science Projects",
      "objectives": [
        "Build scalable MLOps pipelines with Git, Docker, and CI/CD integration.",
        "Implement MLFlow and DVC for model versioning and experiment tracking.",
        "Deploy end-to-end ML models with AWS SageMaker and Huggingface.",
        "Automate ETL pipelines and ML workflows using Apache Airflow and Astro.",
        "Monitor ML systems using Grafana and PostgreSQL for real-time insights."
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "IDE's And Code Editors You Can Use": [
          "Getting Started With Google Colab",
          "Getting Started With Github Codespace",
          "Anaconda And VS Code Installation"
        ],
        "Python Prerequisites": [
          "Getting Started With VS Code And Environment",
          "Python Basics-Syntax and Semantics",
          "Variables In Python",
          "Basics Data Types",
          "Operators In Python",
          "Conditional Statements In Python",
          "Loops In Python",
          "Practical Examples Of List",
          "Sets In Python",
          "Tuples In Python",
          "Dictionaries In Python",
          "Functions In Python",
          "Python Function Examples",
          "Lambda Functions In Python",
          "Map functions In Python",
          "Python Filter Function",
          "Import Modules And Packages In Python",
          "Standard Library Overview",
          "File Operation In Python",
          "Working With File Paths",
          "Exception Handling In Python",
          "OOPS In Python",
          "Inheritance In Python",
          "Polymorphism In Python",
          "Encapsulation In Python",
          "Abstraction In Python",
          "Magic Methods In Python",
          "Custom Exception In Python",
          "Operator OverLoading In Python",
          "Iterators In Python",
          "Generators In Python",
          "Decorators In Python",
          "Working With Numpy In Python",
          "Pandas DataFrame And Series",
          "Data Manipulation And Analysis",
          "Data Source Reading",
          "Logging In Python",
          "Logging With Multiple Loggers",
          "Logging In a Real World Examples"
        ],
        "Complete Flask Tutorial": [
          "Introduction To Flask Framework",
          "Understanding A Sample Flask Application",
          "Integrating HTML With Flask Framework",
          "HTTP Verbs Get And Post",
          "Building Dynamic Url With Jinja 2",
          "Put Delete And API's In Flask"
        ],
        "Git and Github": [
          "Getting Started With Git And Github",
          "Part 2- Git Merge,Push, Checkout And Log With Commands",
          "Part 3- Resolving Git Branch Merge Conflict"
        ],
        "Complete MLFLOW Tutorials": [
          "Introduction To MLFLOW",
          "Getting Started With MLFLOW",
          "Creating MLFLOW Environment",
          "Getting Started With MLFLow Tracking Server",
          "Deep Diving Into MLFlow Experiments",
          "Getting Started With MLFlow ML Project",
          "First ML Project With MLFLOW",
          "Inferencing Model Artifacts With MLFlow Inferencing",
          "MLFLOW Model Registry Tracking"
        ],
        "ML Project Integration With MLFLOW Tracking": [
          "Data Preparation House Price Prediction",
          "Model Building And MLFLOW Tracking"
        ],
        "Deep Learning ANN Model Building Integration With MLFLOW": [
          "ANN With MLFLOW- Part 1",
          "ANN with MLFLOW-Part 2"
        ],
        "Getting Started With DVC- Data Version Control": [
          "Introduction To DVC With Practical Implementation"
        ],
        "Getting Started With Dagshub": [
          "Introduction To Dagshub Remote Repository",
          "Creating First Remote Repo Using Dagshub",
          "DVC With Dagshub Remote Repository"
        ]
      },
      "requirements": [
        "Basic understanding of Python programming.",
        "Familiarity with machine learning concepts and algorithms.",
        "Basic knowledge of Git and GitHub for version control.",
        "Understanding of Docker for containerization (optional but helpful).",
        "Awareness of cloud computing concepts (AWS preferred, but not mandatory)."
      ],
      "description": "Welcome to the Complete MLOps Bootcamp With End to End Data Science Project, your one-stop guide to mastering MLOps from scratch! This course is designed to equip you with the skills and knowledge necessary to implement and automate the deployment, monitoring, and scaling of machine learning models using the latest MLOps tools and frameworks.\nIn today’s world, simply building machine learning models is not enough. To succeed as a data scientist, machine learning engineer, or DevOps professional, you need to understand how to take your models from development to production while ensuring scalability, reliability, and continuous monitoring. This is where MLOps (Machine Learning Operations) comes into play, combining the best practices of DevOps and ML model lifecycle management.\nThis bootcamp will not only introduce you to the concepts of MLOps but will take you through real-world, hands-on data science projects. By the end of the course, you will be able to confidently build, deploy, and manage machine learning pipelines in production environments.\nWhat You’ll Learn:\nPython Prerequisites: Brush up on essential Python programming skills needed for building data science and MLOps pipelines.\nVersion Control with Git & GitHub: Understand how to manage code and collaborate on machine learning projects using Git and GitHub.\nDocker & Containerization: Learn the fundamentals of Docker and how to containerize your ML models for easy and scalable deployment.\nMLflow for Experiment Tracking: Master the use of MLFlow to track experiments, manage models, and seamlessly integrate with AWS Cloud for model management and deployment.\nDVC for Data Versioning: Learn Data Version Control (DVC) to manage datasets, models, and versioning efficiently, ensuring reproducibility in your ML pipelines.\nDagsHub for Collaborative MLOps: Utilize DagsHub for integrated tracking of your code, data, and ML experiments using Git and DVC.\nApache Airflow with Astro: Automate and orchestrate your ML workflows using Airflow with Astronomer, ensuring your pipelines run seamlessly.\nCI/CD Pipeline with GitHub Actions: Implement a continuous integration/continuous deployment (CI/CD) pipeline to automate testing, model deployment, and updates.\nETL Pipeline Implementation: Build and deploy complete ETL (Extract, Transform, Load) pipelines using Apache Airflow, integrating data sources for machine learning models.\nEnd-to-End Machine Learning Project: Walk through a full ML project from data collection to deployment, ensuring you understand how to apply MLOps in practice.\nEnd-to-End NLP Project with Huggingface: Work on a real-world NLP project, learning how to deploy and monitor transformer models using Huggingface tools.\nAWS SageMaker for ML Deployment: Learn how to deploy, scale, and monitor your models on AWS SageMaker, integrating seamlessly with other AWS services.\nGen AI with AWS Cloud: Explore Generative AI techniques and learn how to deploy these models using AWS cloud infrastructure.\nMonitoring with Grafana & PostgreSQL: Monitor the performance of your models and pipelines using Grafana dashboards connected to PostgreSQL for real-time insights.\nWho is this Course For?\nData Scientists and Machine Learning Engineers aiming to scale their ML models and automate deployments.\nDevOps professionals looking to integrate machine learning pipelines into production environments.\nSoftware Engineers transitioning into the MLOps domain.\nIT professionals interested in end-to-end deployment of machine learning models with real-world data science projects.\nWhy Enroll?\nBy enrolling in this course, you will gain hands-on experience with cutting-edge tools and techniques used in the industry today. Whether you’re a data science professional or a beginner looking to expand your skill set, this course will guide you through real-world projects, ensuring you gain the practical knowledge needed to implement MLOps workflows successfully.\nEnroll now and take your data science skills to the next level with MLOps!",
      "target_audience": [
        "Data Scientists and Machine Learning Engineers looking to scale and deploy ML models.",
        "DevOps professionals wanting to integrate ML pipelines.",
        "Software Engineers interested in transitioning to MLOps.",
        "Beginners with basic ML knowledge aiming to learn end-to-end deployment.",
        "IT professionals eager to understand MLOps tools and practices for real-world projects."
      ]
    },
    {
      "title": "Spark and Python for Big Data with PySpark",
      "url": "https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/",
      "bio": "Learn how to use Spark with Python, including Spark Streaming, Machine Learning, Spark 2.0 DataFrames and more!",
      "objectives": [
        "Use Python and Spark together to analyze Big Data",
        "Learn how to use the new Spark 2.0 DataFrame Syntax",
        "Work on Consulting Projects that mimic real world situations!",
        "Classify Customer Churn with Logisitic Regression",
        "Use Spark with Random Forests for Classification",
        "Learn how to use Spark's Gradient Boosted Trees",
        "Use Spark's MLlib to create Powerful Machine Learning Models",
        "Learn about the DataBricks Platform!",
        "Get set up on Amazon Web Services EC2 for Big Data Analysis",
        "Learn how to use AWS Elastic MapReduce Service!",
        "Learn how to leverage the power of Linux with a Spark Environment!",
        "Create a Spam filter using Spark and Natural Language Processing!",
        "Use Spark Streaming to Analyze Tweets in Real Time!"
      ],
      "course_content": {
        "Introduction to Course": [
          "Introduction",
          "Course Overview",
          "Frequently Asked Questions",
          "What is Spark? Why Python?"
        ],
        "Setting up Python with Spark": [
          "Set-up Overview",
          "Note on Installation Sections"
        ],
        "Databricks Setup": [
          "Recommended Setup",
          "Databricks Setup"
        ],
        "Local VirtualBox Set-up": [
          "Local Installation VirtualBox Part 1",
          "Local Installation VirtualBox Part 2",
          "Setting up PySpark"
        ],
        "AWS EC2 PySpark Set-up": [
          "AWS EC2 Set-up Guide",
          "Creating the EC2 Instance",
          "SSH with Mac or Linux",
          "Installations on EC2"
        ],
        "AWS EMR Cluster Setup": [
          "AWS EMR Setup"
        ],
        "Python Crash Course": [
          "Introduction to Python Crash Course",
          "Jupyter Notebook Overview",
          "Python Crash Course Part One",
          "Python Crash Course Part Two",
          "Python Crash Course Part Three",
          "Python Crash Course Exercises",
          "Python Crash Course Exercise Solutions"
        ],
        "Spark DataFrame Basics": [
          "Introduction to Spark DataFrames",
          "Spark DataFrame Basics",
          "Spark DataFrame Basics Part Two",
          "Spark DataFrame Basic Operations",
          "Groupby and Aggregate Operations",
          "Missing Data",
          "Dates and Timestamps"
        ],
        "Spark DataFrame Project Exercise": [
          "DataFrame Project Exercise",
          "DataFrame Project Exercise Solutions"
        ],
        "Introduction to Machine Learning with MLlib": [
          "Introduction to Machine Learning and ISLR",
          "Machine Learning with Spark and Python with MLlib"
        ]
      },
      "requirements": [
        "General Programming Skills in any Language (Preferrably Python)",
        "20 GB of free space on your local computer (or alternatively a strong internet connection for AWS)"
      ],
      "description": "Learn the latest Big Data Technology - Spark! And learn to use it with one of the most popular programming languages, Python!\nOne of the most valuable technology skills is the ability to analyze huge data sets, and this course is specifically designed to bring you up to speed on one of the best technologies for this task, Apache Spark! The top technology companies like Google, Facebook, Netflix, Airbnb, Amazon, NASA, and more are all using Spark to solve their big data problems!\nSpark can perform up to 100x faster than Hadoop MapReduce, which has caused an explosion in demand for this skill! Because the Spark 2.0 DataFrame framework is so new, you now have the ability to quickly become one of the most knowledgeable people in the job market!\nThis course will teach the basics with a crash course in Python, continuing on to learning how to use Spark DataFrames with the latest Spark 2.0 syntax! Once we've done that we'll go through how to use the MLlib Machine Library with the DataFrame syntax and Spark. All along the way you'll have exercises and Mock Consulting Projects that put you right into a real world situation where you need to use your new skills to solve a real problem!\nWe also cover the latest Spark Technologies, like Spark SQL, Spark Streaming, and advanced models like Gradient Boosted Trees! After you complete this course you will feel comfortable putting Spark and PySpark on your resume! This course also has a full 30 day money back guarantee and comes with a LinkedIn Certificate of Completion!\nIf you're ready to jump into the world of Python, Spark, and Big Data, this is the course for you!",
      "target_audience": [
        "Someone who knows Python and would like to learn how to use it for Big Data",
        "Someone who is very familiar with another programming language and needs to learn Spark"
      ]
    },
    {
      "title": "Deep Learning: Convolutional Neural Networks in Python",
      "url": "https://www.udemy.com/course/deep-learning-convolutional-neural-networks-theano-tensorflow/",
      "bio": "Tensorflow 2 CNNs for Computer Vision, Natural Language Processing (NLP) +More! For Data Science & Machine Learning",
      "objectives": [
        "Understand convolution and why it's useful for Deep Learning",
        "Understand and explain the architecture of a convolutional neural network (CNN)",
        "Implement a CNN in TensorFlow 2",
        "Apply CNNs to challenging Image Recognition tasks",
        "Apply CNNs to Natural Language Processing (NLP) for Text Classification (e.g. Spam Detection, Sentiment Analysis)",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "Where to get the code",
          "How to Succeed in this Course"
        ],
        "Google Colab": [
          "Intro to Google Colab, how to use a GPU or TPU for free",
          "Uploading your own data to Google Colab",
          "Where can I learn about Numpy, Scipy, Matplotlib, Pandas, and Scikit-Learn?",
          "Temporary 403 Errors"
        ],
        "Machine Learning and Neurons": [
          "Review Section Introduction",
          "What is Machine Learning?",
          "Code Preparation (Classification Theory)",
          "Classification Notebook",
          "Code Preparation (Regression Theory)",
          "Regression Notebook",
          "The Neuron",
          "How does a model \"learn\"?",
          "Making Predictions",
          "Saving and Loading a Model",
          "Suggestion Box"
        ],
        "Feedforward Artificial Neural Networks": [
          "Artificial Neural Networks Section Introduction",
          "Forward Propagation",
          "The Geometrical Picture",
          "Activation Functions",
          "Multiclass Classification",
          "How to Represent Images",
          "Color Mixing Clarification",
          "Code Preparation (ANN)",
          "ANN for Image Classification",
          "ANN for Regression"
        ],
        "Convolutional Neural Networks": [
          "What is Convolution? (part 1)",
          "What is Convolution? (part 2)",
          "What is Convolution? (part 3)",
          "Why use 0-indexing?",
          "Convolution on Color Images",
          "CNN Architecture",
          "CNN Code Preparation",
          "CNN for Fashion MNIST",
          "CNN for CIFAR-10",
          "Data Augmentation",
          "Batch Normalization",
          "Improving CIFAR-10 Results (Legacy)"
        ],
        "Natural Language Processing (NLP)": [
          "Embeddings",
          "Code Preparation (NLP)",
          "Text Preprocessing",
          "CNNs for Text",
          "Text Classification with CNNs"
        ],
        "Convolution In-Depth": [
          "Real-Life Examples of Convolution",
          "Beginner's Guide to Convolution",
          "Alternative Views on Convolution"
        ],
        "Convolutional Neural Network Description": [
          "Convolution on 3-D Images",
          "Tracking Shapes in a CNN"
        ],
        "Practical Tips": [
          "Advanced CNNs and how to Design your Own"
        ],
        "In-Depth: Loss Functions": [
          "Mean Squared Error",
          "Binary Cross Entropy",
          "Categorical Cross Entropy"
        ]
      },
      "requirements": [
        "Basic math (taking derivatives, matrix arithmetic, probability) is helpful",
        "Python, Numpy, Matplotlib"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nLearn about one of the most powerful Deep Learning architectures yet!\nThe Convolutional Neural Network (CNN) has been used to obtain state-of-the-art results in computer vision tasks such as object detection, image segmentation, and generating photo-realistic images of people and things that don't exist in the real world!\nThis course will teach you the fundamentals of convolution and why it's useful for deep learning and even NLP (natural language processing).\nYou will learn about modern techniques such as data augmentation and batch normalization, and build modern architectures such as VGG yourself.\nThis course will teach you:\nThe basics of machine learning and neurons (just a review to get you warmed up!)\nNeural networks for classification and regression (just a review to get you warmed up!)\nHow to model image data in code\nHow to model text data for NLP (including preprocessing steps for text)\nHow to build an CNN using Tensorflow 2\nHow to use batch normalization and dropout regularization in Tensorflow 2\nHow to do image classification in Tensorflow 2\nHow to do data preprocessing for your own custom image dataset\nHow to use Embeddings in Tensorflow 2 for NLP\nHow to build a Text Classification CNN for NLP (examples: spam detection, sentiment analysis, parts-of-speech tagging, named entity recognition)\nAll of the materials required for this course can be downloaded and installed for FREE. We will do most of our work in Numpy, Matplotlib, and Tensorflow. I am always available to answer your questions and help you along your data science journey.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\nSuggested Prerequisites:\nmatrix addition and multiplication\nbasic probability (conditional and joint distributions)\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students, professionals, and anyone else interested in Deep Learning, Computer Vision, or NLP",
        "Software Engineers and Data Scientists who want to level up their career"
      ]
    },
    {
      "title": "Machine Learning for Absolute Beginners - Level 3",
      "url": "https://www.udemy.com/course/machine-learning-for-absolute-beginners-level-3/",
      "bio": "Learn to Master Data Visualization and perform Exploratory Data Analysis (EDA) using Python, Matplotlib and Seaborn",
      "objectives": [
        "Perform Exploratory Data Analysis (EDA) for any Dateset",
        "Visualise Data using a Variety of Chart Types",
        "Learn Matplotlib and Seaborn Fundamentals",
        "Creating Bar, Grouped Bar, Stacked Bar, Lollipop charts",
        "Creating Pie, Tree-map charts",
        "Creating Line, Area, Stacked Area charts",
        "Creating Histogram, Density, Box-and-Whisker, Swarm charts",
        "Creating Scatter, Correlogram, Heat-Map, Hexbin-Map charts"
      ],
      "course_content": {
        "Getting Started with Level 3!": [
          "Welcome!",
          "Our Overall Learning Path",
          "How to Practice?"
        ],
        "Data Visualization with Matplotlib and Seaborn": [
          "Overview",
          "Matplotlib – Overview",
          "Matplotlib – Figures, Axes",
          "Matplotlib – The OO and Pyplot Interfaces",
          "Matplotlib – APIs Reference Review",
          "Seaborn – Overview",
          "Seaborn – Figure and Axes-level Functions",
          "Seaborn - Chart Customization",
          "Seaborn – APIs Reference Review",
          "A little bit about NumPy",
          "The Right Chart for the Right Job"
        ],
        "Ranking and Proportion Charts": [
          "Overview",
          "Bar Chart",
          "Grouped Bar Chart",
          "Lollipop Chart",
          "Stacked Bar Chart",
          "Pie Chart",
          "Treemap Chart",
          "Optimizing Colors",
          "Exercise 1 - Ranking and Proportion Charts"
        ],
        "Trend and Distribution Charts": [
          "Overview",
          "Line Chart",
          "Area Chart",
          "Stacked Area Chart",
          "Histogram Chart",
          "Density Curve Chart",
          "Box-and-Whisker Chart",
          "Bee-swarm Chart",
          "Exercise 2 – Trend and Distribution Charts"
        ],
        "Correlation Charts": [
          "Overview",
          "Scatter Chart",
          "Correlogram",
          "Heat-Map",
          "Hexbin-Map",
          "Exercise 3 – Correlation Charts"
        ],
        "Course Summary": [
          "Let’s Recap and Thank You!",
          "*** BONUS ***"
        ]
      },
      "requirements": [
        "Python Basic Syntax",
        "Highly skilled working with the Pandas Library - Loading Datasets and Manipulating Data in a Data Frame.",
        "It is recommended to start with the Level 1 and Level 2 of the \"Machine Learning for Absolute Beginners\" training program"
      ],
      "description": "Unleash the Power of ML\nMachine Learning is one of the most exciting fields in the hi-tech industry, gaining momentum in various applications. Companies are looking for data scientists, data engineers, and ML experts to develop products, features, and projects that will help them unleash the power of machine learning. As a result, a data scientist is one of the top ten wanted jobs worldwide!\nMachine Learning for Absolute Beginners\nThe “Machine Learning for Absolute Beginners” training program is designed for beginners looking to understand the theoretical side of machine learning and to enter the practical side of data science. The training is divided into multiple levels, and each level is covering a group of related topics for a continuous step-by-step learning path.\nLevel 3 – Data Visualization with Matplotlib and Seaborn\nThe third course, as part of the training program, aims to help you to perform Exploratory Data Analysis (EDA) by visualizing a dataset using a variety of charts. You will learn the fundamentals of data visualization in Python using the well-known Matplotlib and Seaborn data science libraries, including:\nMatplotlib fundamentals\nSeaborn fundamentals\nSelecting the right chart for the right job\nBar, Grouped Bar, Stacked Bar, Lollipop charts\nPie, Three-map charts\nLine, Area, Stacked Area charts\nHistogram, Density, Box-and-Whisker, Swarm charts\nScatter, Correlogram, Heatmap, Hexbin charts\nEach section has a summary exercise as well as a complete solution to practice new knowledge.\nThe Game just Started!\nEnroll in the training program and start your journey to become a data scientist!",
      "target_audience": [
        "Developers curious about Data Science projects",
        "Beginner Data Scientists",
        "AI Product Managers",
        "ML Engineers",
        "AI/ML Consultants"
      ]
    },
    {
      "title": "Machine Learning & Self-Driving Cars: Bootcamp with Python",
      "url": "https://www.udemy.com/course/machine-learning-self-driving-cars/",
      "bio": "Combine the power of Machine Learning, Deep Learning and Computer Vision to make a Self-Driving Car!",
      "objectives": [
        "Master Machine Learning and Python",
        "Learn how to apply Machine Learning algorithms to develop a Self-Driving Car from scratch",
        "Understand why Deep Learning is such a revolution and use it to make the car drive like a human (Behavioural Cloning)",
        "Simulate a Self-Driving car in a realistic environment using multiple techniques (Computer Vision, Convolution Neural Networks, ...)",
        "Create strong added value to your business",
        "Gentle introduction to Machine Learning where all the key concepts are presented in an intuitive way",
        "Code Deep Convolutional Neural Networks with Keras (the most popular library)",
        "Learn to apply Computer Vision and Deep Learning techniques to build automotive related algorithms",
        "Understand how Self Driving Cars work (sensors, actuators, speed control, ...)",
        "Learn to code in Python starting from the very beginning",
        "Python libraires: NumPy, Sklearn (Scikit-Learn), Keras, OpenCV, Matplotlib"
      ],
      "course_content": {
        "Introduction": [
          "Why This Course?",
          "How to Approach This Course?",
          "Make it Engaging"
        ],
        "Python [Optional]": [
          "Installation",
          "Types in Python",
          "Types in Python",
          "List & Map",
          "List & Map",
          "Operations",
          "Operations",
          "Statements",
          "Statements",
          "Functions",
          "Functions",
          "Classes",
          "Classes Basic",
          "Classes Inheritance",
          "Object Oriented Programming",
          "Libraries / Modules",
          "Modules"
        ],
        "Python's Essential Libraries [Optional]": [
          "Introduction to Python Libraries",
          "Numpy",
          "Numpy",
          "Matplotlib",
          "Matplotlib Quick Test",
          "OpenCV",
          "OpenCV Test",
          "Other Libraries",
          "Other Libraries Quiz"
        ],
        "Computer Vision": [
          "Introduction to Computer Vision",
          "How Computers \"See\" Images?",
          "Kernel & Convolution",
          "Image Processing with Kernels",
          "Thresholding",
          "Road Segmentation",
          "Why Webots?",
          "How to Install Webots in Windows? [Complete Video]",
          "How to Install Webots in Linux?",
          "Get The Code!",
          "Webots too slow?",
          "Webots Code: Explained",
          "[Exercise]: Your Line Following Algorithm!",
          "[Advanced] How to Read a Paper?",
          "[Advanced] Paper: SIFT",
          "[Advanced] Read Paper Example"
        ],
        "Machine Learning": [
          "What's Machine Learning?",
          "Train, Predict & Evaluate",
          "Types of Machine Learning",
          "ML for Self-Driving Cars"
        ],
        "Machine Learning Hands-On": [
          "Machine Learning Hands-On: Introduction",
          "Feature Engineering",
          "HOG",
          "SVM",
          "Performance Metrics",
          "Download the Dataset",
          "Code Explanation",
          "[Exercise]: Modify the code",
          "Useful ML Models",
          "Bias Vs Variance",
          "[Advanced] Paper: SVM"
        ],
        "Collision Avoidance": [
          "Collision Avoidance: Introduction",
          "Ranging Sensors",
          "Cameras",
          "Simulation",
          "My Solution",
          "[Exercise]: Your Solution",
          "Path Planning",
          "[Advanced] RRT Code"
        ],
        "Deep Learning": [
          "Deep Learning: Introduction",
          "How do Neural Networks Work?",
          "How does a Neural Network Learn?",
          "Convolutional Neural Networks",
          "Code Example"
        ],
        "Deep Learning: Hands-On": [
          "Deep Learning Hands-On: Introduction",
          "Creating a Dataset",
          "Training",
          "See it drive!",
          "[Exercise]: Train it yourself!",
          "[Advanced] AlexNet"
        ],
        "Control Theory": [
          "Why Learn Control Theory",
          "Control Systems Map",
          "Stability - Introduction",
          "Stability - Missing in Machine Learning",
          "Open and Closed Loop Control",
          "Closed Loop Control - Cruise Control",
          "PID - Introduction",
          "PID Controller - Deep Dive",
          "PID Controller - How to Tune it?",
          "PID Controller - Why is it use SO much?",
          "[Advanced] Paper: PID Controller Design"
        ]
      },
      "requirements": [
        "Any student with basic physics and mathematics knowledge can join (all skill levels are welcome)",
        "Prior programming experience is NOT necessary"
      ],
      "description": "Interested in Machine Learning or Self-Driving Cars (i.e. Tesla)? Then this course is for you!\nThis course has been designed by a professional Data Scientist, expert in Autonomous Vehicles, with the goal of sharing my knowledge and help you understand how Self-Driving Cars work in a simple way.\nEach topic is presented at three levels:\nIntroduction [Beginner]: the topic will be presented, initial intuition about it\nHands-On [Intermediate]: practical lectures where we will learn by doing\nDeep dive [Expert/Optional]: going deep into the maths to fully understand the topic\nWhat tools will we use in the course?\nPython: probably the most versatile programming language in the world, from websites to Deep Neural Networks, all can be done in Python\nPython libraries: matplotlib, OpenCV, numpy, scikit-learn, keras, ... (those libraries make the possibilities of Python limitless)\nWebots: a very powerful simulator, which free and open source but can provide a wide range of simulation scenarios (Self-Driving Cars, drones, quadrupeds, robotic arms, production lines, ...)\nWho this course is for?\nAll-levels: there is no previous knowledge required, there is a section that will teach you how to program in Python\nMaths/logic: High-school level is enough to understand everything!\nSections:\n[Optional] Python sections: How to program in python, and how to use essential libraries\nComputer Vision: teaches a computer how to see, and introduces key concepts for Neural Networks\nMachine Learning: introduction, key concepts, and road sign classification\nCollision Avoidance: so far we have used cameras, in this section we understand how radar and lidar sensors are used for self-driving cars, use them for collision avoidance, path planning\nHelp us understand the difference between Tesla and other car manufacturers, because Tesla doesn’t use radar sensors\nDeep learning: we will use all the concepts that we have seen before in CV, in ML and CA, neural networks introduction, Behavioural Cloning\nControl Theory: control systems is the glue that stitches all engineering fields together\nIf you are mainly interested in ML, you can only listen to the introduction for this section, but you should know that the initial Neural Networks were heavily influenced by CT\nWho am I, and why am I qualified to talk about Self-driving cars?\nWorked in self-driving motorbikes, boats and cars\nSome of the biggest companies in the world\nOver 8 years experience in the industry and a master in Robotic & CV\nAlways been interested in efficient learning, and used all the techniques that I’ve learned in this course",
      "target_audience": [
        "All-levels, every section is separated with three levels: Introduction, Hands-On, Deep Dive",
        "Any student who wants to transition into the field of artificial intelligence",
        "Entrepreneurs with an interest in working on some of the most cutting edge technologies",
        "To upgrade or get a job in the Automotive / Data Science domain",
        "Any people who want to create added value to their business by using powerful Machine Learning tools"
      ]
    },
    {
      "title": "Machine Learning: Modern Computer Vision & Generative AI",
      "url": "https://www.udemy.com/course/computer-vision-kerascv/",
      "bio": "Use KerasCV, Python, Tensorflow, PyTorch, & JAX for Image Recognition, Object Detection, and Stable Diffusion",
      "objectives": [
        "Computer vision with KerasCV",
        "How to do image classification / image recognition with a pretrained model and fine-tuning / transfer learning",
        "How to do object detection with a pretrained model and fine-tuning / transfer learning",
        "How to generate images with Stable Diffusion in KerasCV"
      ],
      "course_content": {
        "Introduction": [
          "Introduction & Outline",
          "How to Succeed in This Course",
          "Where to Get the Code"
        ],
        "Image Classification, Fine-Tuning and Transfer Learning": [
          "Classification Section Outline",
          "Concepts: Pre-trained Image Classifier",
          "Pre-trained Image Classifier in Python",
          "Transfer Learning and Fine-Tuning",
          "Fine-Tuning an Image Classifier in Python",
          "Classification Exercise",
          "Suggestion Box"
        ],
        "Object Detection": [
          "Object Detection Outline",
          "Concepts: Object Detection",
          "Decoding the Output: IoU, Non-Max Suppression, Confidence Score",
          "Pre-trained Object Detection in Python",
          "Focal Loss & Smooth L1 Loss",
          "Object Detection Dataset Formats (COCO & Pascal VOC)",
          "LabelImg Setup",
          "LabelImg Demo",
          "Data Augmentation",
          "KerasCV Object Detection Dataset Format",
          "Fine-Tuning Object Detection in Python (Built-In Dataset)",
          "Fine-Tuning Object Detection in Python (Custom Dataset)",
          "Object Detection Exercise"
        ],
        "Generative AI with Stable Diffusion": [
          "Stable Diffusion Outline",
          "Generate Images with Stable Diffusion in Python",
          "How Do Diffusion Models Work? (Optional)",
          "Diffusion Model Architecture (Optional)",
          "How Diffusion Models Condition on Prompts (Optional)",
          "A Look at the Diffusion Model Source Code (Optional)"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (Appendix/FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (Appendix/FAQ by Student Request)": [
          "Beginner's Coding Tips",
          "How to Code Yourself (part 1)",
          "How to Code Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it"
        ],
        "Effective Learning Strategies for Machine Learning (Appendix/FAQ)": [
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "What order should I take your courses in? (part 1)",
          "What order should I take your courses in? (part 2)"
        ],
        "Appendix / FAQ Finale": [
          "BONUS"
        ]
      },
      "requirements": [
        "Experience with Keras"
      ],
      "description": "Welcome to \"Machine Learning: Modern Computer Vision & Generative AI,\" a cutting-edge course that explores the exciting realms of computer vision and generative artificial intelligence using the KerasCV library in Python. This course is designed for aspiring machine learning practitioners who wish to explore the fusion of image analysis and generative modeling in a streamlined and efficient manner.\n\n\nCourse Highlights:\nKerasCV Library: We start by harnessing the power of the KerasCV library, which seamlessly integrates with popular deep learning backends like Tensorflow, PyTorch, and JAX. KerasCV simplifies the process of writing deep learning code, making it accessible and user-friendly.\nImage Classification: Gain proficiency in image classification techniques. Learn how to leverage pre-trained models with just one line of code, and discover the art of fine-tuning these models to suit your specific datasets and applications.\nObject Detection: Dive into the fascinating world of object detection. Master the art of using pre-trained models for object detection tasks with minimal effort. Moreover, explore the process of fine-tuning these models and learn how to create custom object detection datasets using the LabelImg GUI program.\nGenerative AI with Stable Diffusion: Unleash the creative potential of generative artificial intelligence with Stable Diffusion, a powerful text-to-image model developed by Stability AI. Explore its capabilities in generating images from textual prompts and understand the advantages of KerasCV's implementation, such as XLA compilation and mixed precision support, which push the boundaries of generation speed and quality.\n\n\nCourse Objectives:\nDevelop a strong foundation in modern computer vision techniques, including image classification and object detection.\nAcquire hands-on experience in using pre-trained models and fine-tuning them for specific tasks.\nLearn to create custom object detection datasets to tackle real-world problems effectively.\nUnlock the world of generative AI with Stable Diffusion, enabling you to generate images from text with state-of-the-art speed and precision.\nEnhance your machine learning skills and add valuable tools to your toolkit for various applications, from computer vision projects to generative art and content generation.\n\n\nJoin us on this captivating journey into the realms of modern computer vision and generative AI. Whether you're a seasoned machine learning practitioner or just starting, this course will equip you with the knowledge and skills to tackle complex image analysis and creative AI projects with confidence. Explore the cutting-edge possibilities that KerasCV and Stable Diffusion offer, and bring your AI aspirations to life.\n\n\nPrerequisites: Basic knowledge of machine learning and Python programming. Familiarity with deep learning concepts is beneficial but not mandatory.",
      "target_audience": [
        "Beginner to advanced students and professionals interested in computer vision with KerasCV"
      ]
    },
    {
      "title": "Intro to Data Science: QuickStart Guide + AI & ChatGPT Prize",
      "url": "https://www.udemy.com/course/intro-to-data-science/",
      "bio": "Learn the critical elements of Data Science, from visualization to databases to Python and more, in just 6 weeks!",
      "objectives": [
        "The entire Data Science process",
        "Cloud concepts & application in Data Science",
        "Database concepts",
        "Statistics fundamentals as needed in Data Science",
        "Visualizations for data mining and presentation",
        "An overview on Statistical Learning",
        "The essentials of Machine Learning",
        "More advanced Python to apply to Data Science"
      ],
      "course_content": {
        "Section 0: -------------------- Introduction ----------------": [
          "Welcome to the Course",
          "Learning Paths",
          "Get the Materials",
          "Prizes $$ for Learning"
        ],
        "Section 1: -------------------- The Ultimate Data Science Guide ----------------": [
          "The 6-Week Data Scientist",
          "Statistics",
          "The Data Science Process",
          "Visualization",
          "Interview with Nadieh Bremer",
          "Databases",
          "Statistical Learning & Meet Hadelin de Ponteves",
          "Machine Learning",
          "Deep Learning",
          "Deep Learning and Computer Vision",
          "Deep Learning and NLP",
          "Artificial Intelligence"
        ],
        "Section 2: -------------------- Data Science with Tableau --------------------": [
          "Intro to Tableau - Step 1",
          "Intro to Tableau - Step 2",
          "Intro to Tableau - Step 3",
          "Intro to Tableau - Step 4",
          "Intro to Tableau - Step 5",
          "Intro to Tableau - Step 6",
          "Intro to Tableau - Step 7",
          "Intro to Tableau - Step 8",
          "Intro to Tableau - Step 9"
        ],
        "Section 3: -------------------- Data Science with Python --------------------": [
          "Python - Step 1",
          "Python - Step 2",
          "Python - Step 3",
          "Python - Step 4",
          "Python - Step 5",
          "Python - Step 6",
          "Python - Step 7",
          "Python - Step 8",
          "Python - Step 9",
          "Python - Step 10"
        ],
        "Section 4: -------------------- Data Science with the Cloud --------------------": [
          "Cloud - Step 1",
          "Cloud - Step 2",
          "Cloud - Step 3",
          "Cloud - Step 4"
        ],
        "Section 5: -------------------- Course Conclusion --------------------": [
          "Course Conclusion",
          "THANK YOU Video",
          "Huge Congrats for completing the challenge!",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Only a passion to build a successful Data Science career"
      ],
      "description": "The demand for Data Scientists is immense. In this course, you'll learn how you can play a part in fulfilling this demand and build a long, successful career for yourself.\nThe #1 goal of this course is clear: give you all the skills you need to be a Data Scientist who could start the job tomorrow... within 6 weeks.\nWith so much ground to cover, we've stripped out the fluff and geared the lessons to focus 100% on preparing you as a Data Scientist. You’ll discover:\nThe structured path for rapidly acquiring Data Science expertise\nHow to build your ability in statistics to help interpret and analyse data more effectively\nHow to perform visualizations using one of the industry's most popular tools\nHow to apply machine learning algorithms with Python to solve real world problems\nWhy the cloud is important for Data Scientists and how to use it\nAlong with much more. You'll pick up all the core concepts that veteran Data Scientists understand intimately. Use common industry-wide tools like SQL, Tableau and Python to tackle problems. And get guidance on how to launch your own Data Science projects.\nIn fact, it might seem like too much at first. And there is a lot of content, exercises, study and challenges to get through. But with the right attitude, becoming a Data Scientist this quickly IS possible!\nOnce you've finished Introduction to Data Science, you’ll be ready for an incredible career in a field that's expanding faster than almost anything else in the world.\nComplete this course, master the principles, and join the ranks of Data Scientists all around the world.",
      "target_audience": [
        "Anyone who's generally interested in Data Science",
        "Anyone not satisfied with their job and wanting to transition into Data Science",
        "Students in college wanting to start a career in Data Science",
        "Students unsure of their future career wanting to see what Data Science is about",
        "Junior Data Scientists aiming to boost their career prospects"
      ]
    },
    {
      "title": "No-Code Machine Learning Using Amazon AWS SageMaker Canvas",
      "url": "https://www.udemy.com/course/no-code-machine-learning-using-amazon-aws-sagemaker-canvas/",
      "bio": "Build your Machine Learning Model and get accurate predictions without writing any Code using AWS SageMaker Canvas",
      "objectives": [
        "Machine Learning on Amazon's AWS Sagemaker Canvas without writing any Code",
        "4 Live Projects with Sample Dataset",
        "Training and Testing ML Models, Improving Accuracy",
        "Basics of Machine Learning"
      ],
      "course_content": {
        "Introduction to Machine Learning": [
          "What is Machine Learning?"
        ],
        "Introduction to AWS": [
          "What is Amazon Web Services (AWS) ?",
          "Signing into AWS Console"
        ],
        "Introduction to SageMaker": [
          "What is SageMaker and how it is used for Machine Learning?",
          "What is SageMaker Canvas?"
        ],
        "Setup": [
          "SageMaker Domain and User Setup",
          "Setup Data in S3 Buckets for use in SageMaker"
        ],
        "SageMaker Canvas Interface Walkthrough": [
          "Navigating in SageMaker Canvas"
        ],
        "Project 1 : Banknote Authentication": [
          "Adding Training Data",
          "Building and Using Model for Prediction",
          "Predict Single & Batch Dataset",
          "Validating Accuracy of Batch Predictions"
        ],
        "Project 2 : Spam SMS Detection": [
          "Adding Train & Test Data",
          "Building and Using Model for Prediction",
          "Predicting Data and Validating Accuracy"
        ],
        "Project 3 : Customer Churn Prediction": [
          "Adding Data",
          "Building Model",
          "Performing & Validating Predictions"
        ],
        "Project 4 : Wine Quality Prediction": [
          "Adding & Joining Datasets",
          "Building Model",
          "Predicting Test Data"
        ],
        "Assignment": [
          "White Wine Quality Prediction"
        ]
      },
      "requirements": [
        "Basic Awareness of Machine Learning",
        "No Coding Expertise needed.",
        "No Advanced Machine Learning knowledge required",
        "No heavy Software required"
      ],
      "description": "This AWS SageMaker Canvas Course will help you to become a Machine Learning Expert and will enhance your skills by offering you comprehensive knowledge, and the required hands-on experience on this newly launched Cloud based ML tool, by solving real-time industry-based projects, without needing any complex coding expertise.\n\nTop Reasons why you should learn AWS SageMaker Canvas :\nAWS is the #1 cloud based tool used industry wide for Machine Learning Projects.\nYou do not need Advanced Coding expertise generally required in the field of Machine Learning.\nComplex knowledge of Statistics, Algorithms, Mathematics that is difficult to master is also not required.\nMachine Learning Models that usually takes many days to build, are available very quickly in just a few minutes.\nThe demand for ML professionals is on the rise. This is one of the most sought-after profession currently in the lines of Data Science.\nThere are multiple opportunities across the Globe for everyone with Machine Learning skills.\nSageMaker Canvas has a small learning curve and you can pick up even advanced concepts very quickly.\nThis Tool is available as a part of AWS Free Tier.\nYou do not need high configuration computer to learn this tool. All you need is any system with internet connectivity.\nTop Reasons why you should choose this Course :\nThis course is designed keeping in mind the students from all backgrounds - hence we cover everything from basics, and gradually progress towards advanced topics.\nWe take live Industry Projects and do each and every step from start to end in the course itself.\nThis course can be completed in a Day !\nAll Doubts will be answered.\nMost Importantly, Guidance is offered beyond the Tool - You will not only learn the Software, but important Machine Learning principles. Also, I will share the resources where to get the best possible help from, & also the sources to get public datasets to work on to get mastery in the ML domain.\nA Verifiable Certificate of Completion is presented to all students who undertake this AWS SageMaker Canvas course.",
      "target_audience": [
        "Machine Learning Enthusiasts",
        "Data Science Professionals",
        "Students who want to enter ML domain but don't have Coding expertise",
        "Anybody in General who want to know what is Machine Learning and apply it practically"
      ]
    },
    {
      "title": "Deep Learning: Advanced Natural Language Processing and RNNs",
      "url": "https://www.udemy.com/course/deep-learning-advanced-nlp/",
      "bio": "Natural Language Processing (NLP) with Sequence-to-sequence (seq2seq), Attention, CNNs, RNNs, and Memory Networks!",
      "objectives": [
        "Build a text classification system (can be used for spam detection, sentiment analysis, and similar problems)",
        "Build a neural machine translation system (can also be used for chatbots and question answering)",
        "Build a sequence-to-sequence (seq2seq) model",
        "Build an attention model",
        "Build a memory network (for question answering based on stories)",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Welcome": [
          "Introduction",
          "Outline",
          "Where to get the code",
          "How to Succeed in this Course"
        ],
        "Recurrent Neural Networks, Convolutional Neural Networks, and Word Embeddings": [
          "Review Section Introduction",
          "How to Open Files for Windows Users",
          "What is a word embedding?",
          "Using word embeddings",
          "What is a CNN?",
          "Where to get the data",
          "CNN Code (part 1)",
          "CNN Code (part 2)",
          "What is an RNN?",
          "GRUs and LSTMs",
          "Different Types of RNN Tasks",
          "A Simple RNN Experiment",
          "RNN Code",
          "Review Section Summary",
          "Suggestion Box"
        ],
        "Bidirectional RNNs": [
          "Bidirectional RNNs Motivation",
          "Bidirectional RNN Experiment",
          "Bidirectional RNN Code",
          "Image Classification with Bidirectional RNNs",
          "Image Classification Code",
          "Bidirectional RNNs Section Summary"
        ],
        "Sequence-to-sequence models (Seq2Seq)": [
          "Seq2Seq Theory",
          "Seq2Seq Applications",
          "Decoding in Detail and Teacher Forcing",
          "Poetry Revisited",
          "Poetry Revisited Code 1",
          "Poetry Revisited Code 2",
          "Seq2Seq in Code 1",
          "Seq2Seq in Code 2",
          "Seq2Seq Section Summary"
        ],
        "Attention": [
          "Attention Section Introduction",
          "Attention Theory",
          "Teacher Forcing",
          "Helpful Implementation Details",
          "Attention Code 1",
          "Attention Code 2",
          "Visualizing Attention",
          "Building a Chatbot without any more Code",
          "Attention Section Summary"
        ],
        "Memory Networks": [
          "Memory Networks Section Introduction",
          "Memory Networks Theory",
          "Memory Networks Code 1",
          "Memory Networks Code 2",
          "Memory Networks Code 3",
          "Memory Networks Section Summary"
        ],
        "Keras and Tensorflow 2 Basics": [
          "(Review) Keras Discussion",
          "(Review) Keras Neural Network in Code",
          "(Review) Keras Functional API",
          "(Review) How to easily convert Keras into Tensorflow 2.0 code"
        ],
        "Course Conclusion": [
          "What to Learn Next"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to How to install Numpy, Theano, Tensorflow, etc..."
        ]
      },
      "requirements": [
        "Understand what deep learning is for and how it is used",
        "Decent Python coding skills, especially tools for data science (Numpy, Matplotlib)",
        "Preferable to have experience with RNNs, LSTMs, and GRUs",
        "Preferable to have experience with Keras",
        "Preferable to understand word embeddings"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nIt’s hard to believe it's been been over a year since I released my first course on Deep Learning with NLP (natural language processing).\nA lot of cool stuff has happened since then, and I've been deep in the trenches learning, researching, and accumulating the best and most useful ideas to bring them back to you.\nSo what is this course all about, and how have things changed since then?\nIn previous courses, you learned about some of the fundamental building blocks of Deep NLP. We looked at RNNs (recurrent neural networks), CNNs (convolutional neural networks), and word embedding algorithms such as word2vec and GloVe.\nThis course takes you to a higher systems level of thinking.\nSince you know how these things work, it’s time to build systems using these components.\nAt the end of this course, you'll be able to build applications for problems like:\ntext classification (examples are sentiment analysis and spam detection)\n\nneural machine translation\n\nquestion answering\n\n\nWe'll take a brief look chatbots and as you’ll learn in this course, this problem is actually no different from machine translation and question answering.\nTo solve these problems, we’re going to look at some advanced Deep NLP techniques, such as:\nbidirectional RNNs\n\nseq2seq (sequence-to-sequence)\n\nattention\n\nmemory networks\n\n\nAll of the materials of this course can be downloaded and installed for FREE. We will do most of our work in Python libraries such as Keras, Numpy, Tensorflow, and Matpotlib to make things super easy and focus on the high-level concepts. I am always available to answer your questions and help you along your data science journey.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\nSee you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nDecent Python coding skills\nUnderstand RNNs, CNNs, and word embeddings\nKnow how to build, train, and evaluate a neural network in Keras\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students in machine learning, deep learning, artificial intelligence, and data science",
        "Professionals in machine learning, deep learning, artificial intelligence, and data science",
        "Anyone interested in state-of-the-art natural language processing"
      ]
    },
    {
      "title": "Apache Spark Hands on Specialization for Big Data Analytics",
      "url": "https://www.udemy.com/course/apache-spark-hands-on-course-big-data-analytics/",
      "bio": "In-depth course to master Apache Spark Development using Scala for Big Data (with 30+ real-world & hands-on examples)",
      "objectives": [
        "Understand the relationship between Apache Spark and Hadoop Ecosystem",
        "Understand Apache Spark use-cases and advanced characteristics",
        "Understand Apache Spark Architecture and how it works",
        "Understand how Apache Spark on YARN (Hadoop) works in multiple modes",
        "Understand development life-cycle of Apache Spark Applications in Python and Scala",
        "Learn the foundations of Scala programming language",
        "Understand Apache Spark's primary data abstraction (RDDs)",
        "Understand and use RDDs advanced characteristics (e.g. partitioning)",
        "Learn nuances in loading files in Hadoop Distributed File system in Apache Spark",
        "Learn implications of delimiters in text files and its processing in Spark",
        "Create and use RDDs by parallelizing Scala's collection objects and implications",
        "Learn the usage of Spark and YARN Web UI to gain in-depth operational insights",
        "Understand Spark's Direct Acyclic Graph (DAG) based execution model and implications",
        "Learn Transformations and their lazy execution semantics",
        "Learn Map transformation and master its applications in real-world challenges",
        "Learn Filter transformation and master its usage in real-world challenges",
        "Learn Apache Spark's advanced Transformations and Actions",
        "Learn and use RDDs of different JVM objects including collections and understanding critical nuances",
        "Learn and use Apache Spark for statistical analysis",
        "Learn and master Key Value Pair RDDs and their applications in complex Big Data problems",
        "Learn and master Join Operations on complex Key Value Pair RDDs in Apache Spark",
        "Learn how RDDs caching works and use it for advanced performance optimization",
        "Learn how to use Apache Spark for Data Ranking problems",
        "Learn how to use Apache Spark for handling and processing structured and unstructured data",
        "Learn how to use Apache Spark for advanced Business Analytics",
        "Learn how to use Apache Spark for advanced data integrity and quality checks",
        "Learn how to use Scala's advanced features like functional programming and pattern matching",
        "Learn how to use Apache Spark for logs processing"
      ],
      "course_content": {
        "Introduction": [
          "Breaking the Ice with Warm Welcome!",
          "Course's Curriculum - Journey to the excellence!"
        ],
        "Section 1 - Apache Spark Introduction and Architecture Deep Dive": [
          "Apache Spark in the context of Hadoop Evolution",
          "Say Hello to Apache Spark - Thorough Dissemination of Capabilities",
          "In-Depth Understanding of Spark's Ecosystem of High Level Libraries",
          "Apache Spark and its integration within Enterprise Lambda Architecture",
          "Apache Spark and where it fits in whole Hadoop Ecosystem"
        ],
        "Working with Text Files to create Resilient Distributed Datasets (RDDs) in Spark": [
          "Setting up development Environment",
          "Better Development Environment Employing DataBricks - Part 1 (**New Lecture**)",
          "Better Development Environment Employing Databricks - Part 2 (**New Lecture**)",
          "Loading Text Files (in HDFS) in Spark to create RDDs",
          "Loading All Directory Files (in HDFS) simultaneously in Spark and implications",
          "Loading Text Files (in HDFS) in Spark - Continued",
          "Using Wildcards to selectively load text files (in HDFS) in Spark and use-cases",
          "Real Life Challenge: Different Record Delimiters in Text Files in Spark",
          "Solution: Handling Different Record Delimiters in Text Files in Spark",
          "T1"
        ],
        "Creating RDDs by Distributing Scala Collections in Spark": [
          "The semantics and implications behind parallelizing Scala Collections",
          "Hands-on: Distributing/Parallelizing Scala Collections"
        ],
        "Understanding the Partitioning and Distributed Nature of RDDs in Spark": [
          "How Data gets Partitioned and Distributed in Spark Cluster",
          "Accessing Hadoop YARN RM and AM Web UIs to understand RDDs Partitioning",
          "Manually Changing Partitions of RDDs in Spark and Implications"
        ],
        "Developing Mastery in Spark's Map Transformations and lazy DAG Execution Model": [
          "Demystifying Spark's Direct Acyclic Graph (DAG) and Lazy Execution Model",
          "Introducing Map Transformation - the Swiss Army Knife of Transformations",
          "Hands-on: Map Transformation via Scala's Functional Programming constructs",
          "Understanding the Potential of Map Transformation to alter RDDs Types",
          "Using Your Own Functions, in addition to Anonymous ones, in Map Transformations"
        ],
        "Assignment - Using Map Transformation on Real World Big Data Retail Analytics": [
          "Introducing the Real World Online Retail Data-set and Assignment Challenges",
          "Detailed Hands-on Comprehension of Assignment Challenges' Solutions",
          "Conceptual Understanding of Distributing Scala Collections and Implications",
          "Hands-on Understanding of Distributing Scala Collections and use-cases"
        ],
        "Developing Mastery in Spark's Filter Transformation": [
          "Introducing Filter Transformation and its Powerful Use-Cases",
          "Hands on: Spark's Filter Transformation in Action"
        ],
        "Assignment - Using Filter and Map on Apache Web Server Logs and Retail Dataset": [
          "Introducing the Data-sets and Real-World Assignment Challenges",
          "Challenge 1: Removing Empty Lines in Web Logs Data-set",
          "Challenge 2: Removing Header Line in Retail Data-set",
          "Challenge 3: Selecting rows in Retail Data-set Containing Specific Countries"
        ],
        "Developing Mastery in RDD of Scala Collections": [
          "Introducing RDDs of Scala Collections and their Relational Analytics use-cases",
          "Transforming Scala Collections using Functional Programming Constructs",
          "Creating and Manipulating RDDs of Arrays of String from Different Data Sources"
        ]
      },
      "requirements": [
        "Background knowledge of Big Data would be helpful but not necessary as everything will be taught from scratch",
        "Past experience of programming language would be helpful but not necessary as everything will be taught from scratch",
        "A computer system (Laptop/Desktop) with either Windows, Linux or Mac installed for hands-on practice",
        "All the software and tools used are freely available",
        "The most important requirement: Thirst and commitment to learn!"
      ],
      "description": "What if you could catapult your career in one of the most lucrative domains i.e. Big Data by learning the state of the art Hadoop technology (Apache Spark) which is considered mandatory in all of the current jobs in this industry?\n\nWhat if you could develop your skill-set in one of the most hottest Big Data technology i.e. Apache Spark by learning in one of the most comprehensive course  out there (with 10+ hours of content) packed with dozens of hands-on real world examples, use-cases, challenges and best-practices?\nWhat if you could learn from an instructor who is working in the world's largest consultancy firm, has worked, end-to-end, in Australia's biggest Big Data projects to date and who has a proven track record on Udemy with highly positive reviews and thousands of students already enrolled in his previous course(s)?\n\nIf you have such aspirations and goals, then this course and you is a perfect match made in heaven!\nWhy Apache Spark?\nApache Spark has revolutionised and disrupted the way big data processing and machine learning were done by virtue of its unprecedented in-memory and optimised computational model. It has been unanimously hailed as the future of Big Data. It's the tool of choice all around the world which allows data scientists, engineers and developers to acquire and process data for a number of use-cases like scalable machine learning, stream processing and graph analytics to name a few. All of the leading organisations like Amazon, Ebay, Yahoo among many others have embraced this technology to address their Big Data processing requirements.\nAdditionally, Gartner has repeatedly highlighted Apache Spark as a leader in Data Science platforms. Certification programs of Hadoop vendors like Cloudera and Hortonworks, which have high esteem in current industry, have oriented their curriculum to focus heavily on Apache Spark. Almost all of the jobs in Big Data and Machine Learning space demand proficiency in Apache Spark.\nThis is what John Tripier, Alliances and Ecosystem Lead at Databricks has to say, “The adoption of Apache Spark by businesses large and small is growing at an incredible rate across a wide range of industries, and the demand for developers with certified expertise is quickly following suit”.\nAll of these facts correlate to the notion that learning this amazing technology will give you a strong competitive edge in your career.\nWhy this course?\nFirstly, this is the most comprehensive and in-depth course ever produced on Apache Spark. I've carefully and critically surveyed all of the resources out there and almost all of them fail to cover this technology in the depth that it truly deserves. Some of them lack coverage of Apache Spark's theoretical concepts like its architecture and how it works in conjunction with Hadoop, some fall short in thoroughly describing how to use Apache Spark APIs optimally for complex big data problems, some ignore the hands-on aspects to demonstrate how to do Apache Spark programming to work on real-world use-cases and almost all of them don't cover the best practices in industry and the mistakes that many professionals make in field.\nThis course addresses all of the limitations that's prevalent in the currently available courses. Apart from that, as I have attended trainings from leading Big Data vendors like Cloudera (for which they charge thousands of dollars), I've ensured that the course is aligned with the educational patterns and best practices followed in those training to ensure that you get the best and most effective learning experience.\nEach section of the course covers concepts in extensive detail and from scratch so that you won't find any challenges in learning even if you are new to this domain. Also, each section will have an accompanying assignment section where we will work together on a number of real-world challenges and use-cases employing real-world data-sets. The data-sets themselves will also belong to different niches ranging from retail, web server logs, telecommunication and some of them will also be from Kaggle (world's leading Data Science competition platform).\nThe course leverages Scala instead of Python. Even though wherever possible, reference to Python development is also given but the course is majorly based on Scala. The decision was made based on a number of rational factors. Scala is the de-facto language for development in Apache Spark. Apache Spark itself is developed in Scala and as a result all of the new features are initially made available in Scala and then in other languages like Python. Additionally, there is significant performance difference when it comes to using Apache Spark with Scala compared to Python. Scala itself is one of the most highest paid programming languages and you will be developing strong skill in that language along the way as well.\nThe course also has a number of quizzes to further test your skills. For further support, you can always ask questions to which you will get prompt response. I will also be sharing best practices and tips on regular basis with my students.\n\nWhat you are going to learn in this course?\nThe course consists of majorly two sections:\nSection - 1:\nWe'll start off with the introduction of Apache Spark and will understand its potential and business use-cases in the context of overall Hadoop ecosystem. We'll then focus on how Apache Spark actually works and will take a deep dive of the architectural components of Spark as its crucial for thorough understanding.\nSection  - 2:\nAfter developing understanding of Spark architecture, we will move to the next section of this course where we will employ Scala language to use Apache Spark APIs to develop distributed computation programs. Please note that you don't need to have prior knowledge of Scala for this course as I will start with the very basics of Scala and as a result you will also be developing your skills in this one of the highest paying programming languages.\nIn this section, We will comprehensively understand how spark performs distributed computation using abstractions like RDDs, what are the caveats in loading data in Apache Spark, what are the different ways to create RDDs and how to leverage parallelism and much more.\nFurthermore, as transformations and action constitute the gist of Apache Spark APIs thus its imperative to have sound understanding of these. Thus, we will then focus on a number of Spark transformations and Actions that are heavily being used in Industry and will go into detail of each. Each API usage will be complimented with a series of real-world examples and datasets e.g. retail, web server logs, customer churn and also from kaggle. Each section of the course will have a number of assignments where you will be able to practically apply the learned concepts to further consolidate your skills.\nA significant section of the course will also be dedicated to key value RDDs which form the basis of working optimally on a number of big data problems.\nIn addition to covering the crux of Spark APIs, I will also highlight a number of valuable best practices based on my experience and exposure and will also intuit on mistakes that many people do in field. You will rarely such information anywhere else.\nEach topic will be covered in a lot of detail with strong emphasis on being hands-on thus ensuring that you learn Apache Spark in the best possible way.\nThe course is applicable and valid for all versions of Spark i.e. 1.6 and 2.0.\nAfter completing this course, you will develop a strong foundation and extended skill-set to use Spark on complex big data processing tasks. Big data is one of the most lucractive career domains where data engineers claim salaries in high numbers. This course will also substantially help in your job interviews. Also, if you are looking to excel further in your big data career, by passing Hadoop certifications like of Cloudera and Hortonworks, this course will prove to be extremely helpful in that context as well.\nLastly, once enrolled, you will have life-time access to the lectures and resources. Its a self-paced course and you can watch lecture videos on any device like smartphone or laptop. Also, you are backed by Udemy's rock-solid 30 days money back guarantee. So if you are serious about learning about learning Apache Spark, enrol in this course now and lets start this amazing journey together!",
      "target_audience": [
        "Anyone who has the passion to develop expertise in Big Data and specifically Apache Spark",
        "Software Engineers or Developers",
        "Data Warehousing or Business Intelligence Professionals",
        "Data Scientist and Machine Learning Enthusiasts",
        "Data Engineers and Big Data Architects"
      ]
    },
    {
      "title": "Complete Generative AI Course: RAG, AI Agents & Deployment",
      "url": "https://www.udemy.com/course/generative-ai-for-beginners-chatbots-rag-mcp-ai-agents/",
      "bio": "Learn Generative AI from scratch – Build RAG, AI Agents & Chatbots, master MCP, and deploy real-world projects",
      "objectives": [
        "Master the foundations of Generative AI, Large Language Models, and Transformer architecture.",
        "Build real-world AI applications including chatbots, RAG systems, MCP servers, and multi-agent systems.",
        "Deploy LLM-powered solutions on the cloud using Docker, Streamlit, Ollama, vLLM, and AWS EC2.",
        "Gain the knowledge and hands-on skills required to step into a Generative AI Engineer role."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "What you will learn",
          "Environment Setup: Python, IDEs & Dev Tools"
        ],
        "Generative AI – Foundation": [
          "AI vs ML vs DL vs GenAI",
          "Large Language Models",
          "Transformer - architecture",
          "Section 2- Quiz (GenAI Foundation)"
        ],
        "Accessing LLMs in Python": [
          "OpenAI LLMs (Proprietary)",
          "Gemini LLMs (Proprietary)",
          "Groq LLMs (Open-Source)",
          "Ollama (Open-Source & Local)",
          "Accessing LLMs via LangChain",
          "Accessing LLMs via LlamaIndex",
          "Section 3 - Quiz (Accessing LLMs)",
          "Practice Exercise - Section 3"
        ],
        "Prompt Engineering": [
          "Using Prompt Template",
          "Zero-shot Prompting",
          "Few-shot Prompting",
          "Section 4 - Quiz (Prompt Engineering)",
          "Practice Exercise - Section 4"
        ],
        "Building Generative AI Chatbots": [
          "Building a chatbot with LangChain",
          "Building a chatbot with Llamaindex",
          "GenAI Chatbot with Streamlit UI",
          "Deploy GenAI Chatbot on Streamlit Cloud",
          "Section 5 - Quiz (GenAI Chatbots)",
          "Practice Exercise - Section 5"
        ],
        "RAG - Retrieval-Augmented Generation": [
          "Understanding RAG",
          "Building a RAG system in Python with LangChain",
          "Building a RAG system in Python with Llamaindex",
          "Build a PDF question-answering RAG app with Streamlit",
          "Section 6 - Quiz (RAG)",
          "Practice Exercise - Section 6"
        ],
        "AI Agents": [
          "Understanding AI Agents",
          "Build AI Agent with PydanticAI",
          "Build AI Agent with Microsoft's AutoGen",
          "Multi-Agent system with CrewAI",
          "Section 7 - Quiz (AI Agents)",
          "Practice Exercise - Section 7"
        ],
        "LLM Deployment": [
          "Running LLMs Locally with Ollama & Docker",
          "Launching an AWS EC2 Instance",
          "Deploying Ollama LLMs on EC2 with Docker",
          "vLLM - High-Performance Serving on EC2",
          "Serve Local LLMs (Ollama) via FastAPI",
          "Deploying LLMs on RunPod (Cost-effective GPU)"
        ],
        "MCP – Model Context Protocol": [
          "Understanding MCP",
          "Build an MCP Server",
          "Pydantic AI Agent with MCP tool",
          "CrewAI Agent with MCP tool",
          "Section 9 - Quiz (MCP)",
          "Practice Exercise - Section 9"
        ],
        "Capstone Projects – Build and Deploy Real-World AI Solutions": [
          "Section 10 - Capstone Projects - Real-World GenAI Applications",
          "Project 1 - ConvoPro – Private ChatGPT Clone",
          "Project 1 - ConvoPro - DB & Environment Setup",
          "Project 1 - ConvoPro - Implementation",
          "Project 1 - ConvoPro - Deploy on EC2",
          "Project 2 - StudyPal – RAG-Powered AI Study Assistant",
          "Project 2 - StudyPal - Environment Setup",
          "Project 2 - StudyPal - Document Ingestion",
          "Project 2 - StudyPal - RAG Pipeline Implementation",
          "Project 2 - StudyPal - EC2 Deployment",
          "Project 3 - AstraRAG - Agentic RAG Chatbot - Production-Grade",
          "Project 3 - AstraRAG - Environment Setup",
          "Project 3 - AstraRAG - Document Ingestion Pipeline",
          "Project 3 - AstraRAG - Build RAG Agent",
          "Project 3 - AstraRAG - Build Backend & Frontend",
          "Project 3 - AstraRAG - Deploy locally with Docker",
          "Project 3 - AstraRAG - EC2 Deployment with Docker",
          "Conclusion"
        ]
      },
      "requirements": [
        "This course requires only a basic understanding of Python and Machine Learning. No prior knowledge of Generative AI is needed — we start from the fundamentals and progress to advanced concepts. All you need is the curiosity to learn by building real-world projects."
      ],
      "description": "This complete Generative AI course takes you from beginner to advanced with hands-on projects, real-world applications, and career-ready skills. You’ll learn the foundations of Generative AI, explore Large Language Models (LLMs), master frameworks like LangChain, LlamaIndex, CrewAI, and PydanticAI, and deploy your own AI solutions on the cloud. The course is tailored to equip you with both the knowledge and practical experience required to step into a Generative AI Engineer role.\n\n\nEach section includes quizzes & coding exercises to help you test your knowledge and reinforce your skills.\n\n\nWhat you’ll learn in each section\n\n\n1. Introduction – Get started with the course, understand what you will learn & set up Python environments (Colab, Jupyter, PyCharm).\n2. Generative AI – Foundation – Understand AI vs ML vs DL vs GenAI, dive into Large Language Models, and learn the Transformer architecture.\n3. Accessing LLMs in Python – Use OpenAI, Gemini, Groq, and Ollama LLMs, and connect them through LangChain and LlamaIndex.\n4. Prompt Engineering – Explore prompt templates, zero-shot, and few-shot prompting to effectively interact with LLMs.\n5. Building GenAI Chatbots – Build and deploy chatbots step by step using LangChain, LlamaIndex, Streamlit UI, and Streamlit Cloud.\n6. Retrieval-Augmented Generation (RAG) – Understand RAG, build RAG pipelines with LangChain and LlamaIndex, and create a PDF Q&A bot.\n7. AI Agents – Learn what AI agents are and build agents with PydanticAI, AutoGen, and CrewAI for multi-agent workflows.\n8. LLM Deployment – Deploy open-source LLMs with Ollama, Docker, and vLLM, and set them up on AWS EC2 for real-world usage.\n9. Model Context Protocol (MCP) – Understand MCP, build an MCP server, and integrate MCP tools with PydanticAI and CrewAI agents.\n10. Capstone Projects – Apply everything learned to build real-world AI projects: Enterprise Chatbots, RAG Assistants, and Intelligent AI Agents with Full Cloud Deployment.",
      "target_audience": [
        "This course is for students, developers, and professionals with basic Python/ML knowledge who want to become Generative AI Engineers through hands-on projects."
      ]
    },
    {
      "title": "NLP - Natural Language Processing with Python",
      "url": "https://www.udemy.com/course/nlp-natural-language-processing-with-python/",
      "bio": "Learn to use Machine Learning, Spacy, NLTK, SciKit-Learn, Deep Learning, and more to conduct Natural Language Processing",
      "objectives": [
        "Learn to work with Text Files with Python",
        "Learn how to work with PDF files in Python",
        "Utilize Regular Expressions for pattern searching in text",
        "Use Spacy for ultra fast tokenization",
        "Learn about Stemming and Lemmatization",
        "Understand Vocabulary Matching with Spacy",
        "Use Part of Speech Tagging to automatically process raw text files",
        "Understand Named Entity Recognition",
        "Visualize POS and NER with Spacy",
        "Use SciKit-Learn for Text Classification",
        "Use Latent Dirichlet Allocation for Topic Modelling",
        "Learn about Non-negative Matrix Factorization",
        "Use the Word2Vec algorithm",
        "Use NLTK for Sentiment Analysis",
        "Use Deep Learning to build out your own chat bot"
      ],
      "course_content": {
        "Introduction": [
          "Course Overview - DO NOT SKIP THIS LECTURE PLEASE. IMPORTANT INFO HERE!",
          "Quick Check",
          "Curriculum Overview",
          "Installation and Setup Lecture",
          "FAQ - Frequently Asked Questions"
        ],
        "Python Text Basics": [
          "Introduction to Python Text Basics",
          "Working with Text Files with Python - Part One",
          "Working with Text Files with Python - Part Two",
          "Working with PDFs",
          "Regular Expressions Part One",
          "Regular Expressions Part Two",
          "Python Text Basics - Assessment Overview",
          "Python Text Basics - Assessment Solutions"
        ],
        "Natural Language Processing Basics": [
          "Introduction to Natural Language Processing",
          "Spacy Setup and Overview",
          "What is Natural Language Processing?",
          "Spacy Basics",
          "Tokenization - Part One",
          "Tokenization - Part Two",
          "Stemming",
          "Lemmatization",
          "Stop Words",
          "Phrase Matching and Vocabulary - Part One",
          "Phrase Matching and Vocabulary - Part Two",
          "NLP Basics Assessment Overview",
          "NLP Basics Assessment Solution"
        ],
        "Part of Speech Tagging and Named Entity Recognition": [
          "Introduction to Section on POS and NER",
          "Part of Speech Tagging",
          "Visualizing Part of Speech",
          "Named Entity Recognition - Part One",
          "Named Entity Recognition - Part Two",
          "Visualizing Named Entity Recognition",
          "Sentence Segmentation",
          "Part Of Speech Assessment",
          "Part Of Speech Assessment - Solutions"
        ],
        "Text Classification": [
          "Introduction to Text Classification",
          "Machine Learning Overview",
          "Classification Metrics",
          "Confusion Matrix",
          "Scikit-Learn Primer - How to Use SciKit-Learn",
          "Scikit-Learn Primer - Code Along Part One",
          "Scikit-Learn Primer - Code Along Part Two",
          "Text Feature Extraction Overview",
          "Text Feature Extraction - Code Along Implementations",
          "Text Feature Extraction - Code Along - Part Two",
          "Text Classification Code Along Project",
          "Text Classification Assessment Overview",
          "Text Classification Assessment Solutions"
        ],
        "Semantics and Sentiment Analysis": [
          "Introduction to Semantics and Sentiment Analysis",
          "Overview of Semantics and Word Vectors",
          "Semantics and Word Vectors with Spacy",
          "Sentiment Analysis Overview",
          "Sentiment Analysis with NLTK",
          "Sentiment Analysis Code Along Movie Review Project",
          "Sentiment Analysis Project Assessment",
          "Sentiment Analysis Project Assessment - Solutions"
        ],
        "Topic Modeling": [
          "Introduction to Topic Modeling Section",
          "Overview of Topic Modeling",
          "Latent Dirichlet Allocation Overview",
          "Latent Dirichlet Allocation with Python - Part One",
          "Latent Dirichlet Allocation with Python - Part Two",
          "Non-negative Matrix Factorization Overview",
          "Non-negative Matrix Factorization with Python",
          "Topic Modeling Project - Overview",
          "Topic Modeling Project - Solutions"
        ],
        "Deep Learning for NLP": [
          "Introduction to Deep Learning for NLP",
          "The Basic Perceptron Model",
          "Introduction to Neural Networks",
          "Keras Basics - Part One",
          "Keras Basics - Part Two",
          "Recurrent Neural Network Overview",
          "LSTMs, GRU, and Text Generation",
          "Text Generation with LSTMs with Keras and Python - Part One",
          "Text Generation with LSTMs with Keras and Python - Part Two",
          "Text Generation with LSTMS with Keras - Part Three",
          "Chat Bots Overview",
          "Creating Chat Bots with Python - Part One",
          "Creating Chat Bots with Python - Part Two",
          "Creating Chat Bots with Python - Part Three",
          "Creating Chat Bots with Python - Part Four"
        ],
        "BONUS SECTION: THANK YOU!": [
          "BONUS LECTURE"
        ]
      },
      "requirements": [
        "Understand general Python",
        "Have permissions to install python packages onto computer",
        "Internet connection"
      ],
      "description": "Welcome to the best Natural Language Processing course on the internet! This course is designed to be your complete online resource for learning how to use Natural Language Processing with the Python programming language.\nIn the course we will cover everything you need to learn in order to become a world class practitioner of NLP with Python.\nWe'll start off with the basics, learning how to open and work with text and PDF files with Python, as well as learning how to use regular expressions to search for custom patterns inside of text files.\nAfterwards we will begin with the basics of Natural Language Processing, utilizing the Natural Language Toolkit library for Python, as well as the state of the art Spacy library for ultra fast tokenization, parsing, entity recognition, and lemmatization of text.\nWe'll understand fundamental NLP concepts such as stemming, lemmatization, stop words, phrase matching, tokenization and more!\nNext we will cover Part-of-Speech tagging, where your Python scripts will be able to automatically assign words in text to their appropriate part of speech, such as nouns, verbs and adjectives, an essential part of building intelligent language systems.\nWe'll also learn about named entity recognition, allowing your code to automatically understand concepts like money, time, companies, products, and more simply by supplying the text information.\nThrough state of the art visualization libraries we will be able view these relationships in real time.\nThen we will move on to understanding machine learning with Scikit-Learn to conduct text classification, such as automatically building machine learning systems that can determine positive versus negative movie reviews, or spam versus legitimate email messages.\nWe will expand this knowledge to more complex unsupervised learning methods for natural language processing, such as topic modelling, where our machine learning models will detect topics and major concepts from raw text files.\nThis course even covers advanced topics, such as sentiment analysis of text with the NLTK library, and creating semantic word vectors with the Word2Vec algorithm.\nIncluded in this course is an entire section devoted to state of the art advanced topics, such as using deep learning to build out our own chat bots!\nNot only do you get fantastic technical content with this course, but you will also get access to both our course related Question and Answer forums, as well as our live student chat channel, so you can team up with other students for projects, or get help on the course content from myself and the course teaching assistants.\nAll of this comes with a 30 day money back garuantee, so you can try the course risk free.\nWhat are you waiting for? Become an expert in natural language processing today!\nI will see you inside the course,\nJose",
      "target_audience": [
        "Python developers interested in learning how to use Natural Language Processing."
      ]
    },
    {
      "title": "Generative AI - Natural Language Processing Bootcamp 2025",
      "url": "https://www.udemy.com/course/practical-natural-language-processing-go-from-zero-to-hero/",
      "bio": "Generative AI Mastery with NLP LLM: Create Chatbots, RASA, ChatGPT, BERT, Transformers, Prompt Engineering Mastery",
      "objectives": [
        "Text Preprocessing using NLTK and Spacy",
        "How to work on NLP pipeline",
        "Perform Tokenization",
        "Stemming & Lemmatization",
        "Apply Word Embeddings",
        "NLP Pipeline for various tasks",
        "Named Entity Recognition",
        "Text Summarization",
        "Building an Enterprise Grade Chatbot with Dialogflow",
        "Building a project on Twitter Tweets",
        "Build Chatbot with RASA with Advanced Integration",
        "Deep Learning for Sequence Data",
        "Transformer NLP Architecture",
        "ChatGPT",
        "BERT Model",
        "Hugging Face Transformers"
      ],
      "course_content": {
        "Introduction to Natural Language Processing": [
          "Why NLP and how its different from Normal ML ?",
          "Knowledge Test of Basics",
          "Understanding Human Language",
          "Quiz - Learning from Basics of Language",
          "Challenges of NLP",
          "Quiz - Test the learning",
          "Summary",
          "Source code link for the course",
          "Connect with Instructor"
        ],
        "Pipeline of NLP": [
          "Attachments of this section - Code Reference",
          "NLP Pipeline",
          "Data Extraction and Text Cleaning hands On",
          "Introduction to NLTK library",
          "Tokenization , bigrams, trigrams, and N gram - Hands on",
          "POS Tagging & Stop Words Removal",
          "Stemming & Lemmatization",
          "NER and Wordsense Disambiguation",
          "Introduction to Spacy Library",
          "Hands On Spacy",
          "Summary"
        ],
        "NLP -Text Vectorization": [
          "Attachments of this section - Code Reference",
          "Vector Representation of Text - One Hot Encoding",
          "Understanding BoW Technique",
          "BoW Hands On",
          "TF-IDF",
          "TF-IDF Hands On",
          "Tf-idf from Scratch Implementation"
        ],
        "Word Embeddings": [
          "Attachments of this section - code reference",
          "Introduction to Word Embeddings",
          "Intuition of Vector Representation",
          "Hands On Word Embeddings - Usage of Pre-trained models",
          "Skip-gram Word Embeddings - Understanding Data Preperation",
          "Skip Gram Model Architecture",
          "Skip Gram Hands On - Deep Dive",
          "CBOW Model Architecture & Hands On",
          "Hyperparameters - Negative Sampling and Sub Sampling",
          "Practical Difference between CBOW and Skip-gram",
          "Bonus : How does a Network is trained - Back-propagation",
          "Section Summary"
        ],
        "End to End Pipeline for Text Classification": [
          "Code Attachments for this section",
          "General Pipeline for Classification",
          "Approaches to Classification",
          "Loading the Dataset",
          "Exploratory Data Analysis & Text Preprocessing",
          "Remove Low Frequency Words",
          "Remove Stop Words with Stemming & Lemmatisation",
          "Application of Model",
          "TfIDF Approach",
          "Challenges of NLP & N-grams"
        ],
        "Information Extraction": [
          "Introduction to NER",
          "Understanding CRF - Introduction"
        ],
        "Chatbots - Build with Google Cloud Service - Dialogflow": [
          "Attachments for the section - Code Reference",
          "Understanding Chatbots",
          "Building a Simple Chatbot",
          "Hands On Building a Simple FAQ Chatbot",
          "Types of Chatbot and Pipeline for Chatbot",
          "Terminologies in Chatbot",
          "Dialog flow - Introduction",
          "Basics of Dialogflow",
          "Dialogflow system setup",
          "Create Dialogflow chatbot",
          "Dialogflow Fulfilment",
          "Dialogflow Integrations/Deployment",
          "Dialogflow Miscellaneous Tools"
        ],
        "Deep Dive into the Dialog Systems (Chatbot)": [
          "Attachments for the section - code reference",
          "Deep Dive into the components of Dialog System",
          "Dialog Intent Prediction",
          "Deep Learning based intent Classification"
        ],
        "Project - Build Chatbot using RASA": [
          "Project Files for RASA",
          "Introduction to RASA Chatbot",
          "Installation of RASA",
          "RASA project Structure",
          "RASA Files",
          "Basics of YAML",
          "Building the chatbot - Add intents and Response",
          "Building the chatbot - Extract Entity & working with Slots",
          "Create API Key from NyTimes",
          "Working with Action File - Demo",
          "Building Custom Action File",
          "Test the Action Server",
          "RASA Pipeline file",
          "RASA Deployment - Integration with RASA Chatbot - Pre-requisites",
          "Run Ngork on RASA Chatbot with Actions",
          "Slack Settings for Connection to RASA Chatbot",
          "Practice Project Concert Chatbot & Summary"
        ],
        "Text Summarization": [
          "Code File for Reference",
          "Text Summarization - Introduction",
          "Hands On Text Summarization"
        ]
      },
      "requirements": [
        "Access to Google Colab/Jupyter Notebook",
        "Basic to Intermediate Python Programming skills",
        "Optional – GCP free trial account"
      ],
      "description": "Embark on an Exhilarating NLP Journey: From Zero to Hero in Practical Natural Language Processing\nUnleash the immense potential of Natural Language Processing (NLP) with our comprehensive course, meticulously crafted to transform you from a novice to a master in text processing, chatbot development, and cutting-edge techniques like BERT and Transformer NLP Architecture.\nWhy Choose This Course?\n1. Empowering Your NLP Mastery:\nNavigate the intricate world of NLP, understanding its profound impact on human communication.\nHarness NLP's potential to build applications for seamless human interaction, enhanced efficiency, and automation of tedious tasks.\n2. Accelerating Career Opportunities:\nElevate your career in software development, data science, and marketing by mastering the interdisciplinary and rapidly growing field of NLP.\nOpen doors to diverse industries seeking NLP expertise, from software development to marketing.\n3. Benefits of NLP Learning:\nGain a profound understanding of human communication dynamics through NLP.\nDevelop applications that enhance human interaction and task automation.\nAutomate tedious tasks such as information extraction from unstructured text data.\nImprove the usability of search engines and information retrieval systems.\nUnlock career opportunities in software development, data science, marketing, and more.\n4. A Unique Learning Experience:\nExplore a meticulously designed curriculum that caters to practitioners seeking in-depth knowledge of NLP.\nProgress through essential topics, building a solid foundation for advanced techniques.\n5. Key Topics Covered:\nText Processing Fundamentals:\nDive into crucial aspects such as text preprocessing, NLP pipeline, tokenization, stemming, lemmatization, word embeddings, and NLP pipeline for various tasks.\nEnterprise-Grade Chatbot Development with DialogFlow:\nBuild a robust chatbot using Google Cloud Platform's service - DialogFlow.\nLeverage advanced machine learning models seamlessly with a few clicks for immediate implementation.\nTwitter Project - Real-time Data Extraction and Analysis:\nEngage in an end-to-end project using the Tweepy library to extract, mine, preprocess text data from Twitter.\nCreate real-time word clouds based on live tweets, providing practical, hands-on experience.\nRASA Chatbot with Advanced Integration:\nExplore Rasa, an open-source chatbot framework, for building contextual assistants.\nImplement Rasa from scratch, integrating it with Slack channels and enabling functionalities such as retrieving news from the New York Times website.\nDeep Learning for Sequence Data:\nMove beyond traditional machine learning, delving into deep learning neural networks.\nExplore the latest advancements in NLP research, including recurrent neural networks, LSTM neural networks, and attention mechanisms for encoder-decoder architecture.\nTransformer NLP Architecture:\nWitness the transformative impact of transformer NLP architecture on solving natural language tasks.\nUnderstand how businesses leverage this technology for extracting meaningful insights from large volumes of text data.\nRevolutionary ChatGPT Technology:\nGrasp the essence of ChatGPT, a revolutionary AI technology automating mundane business tasks.\nLearn the intuition behind ChatGPT and its applications in customer support, onboarding, training, sales, and marketing.\nBERT Model - Unveiling Core Architectures:\nExplore BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking AI designed for profound understanding of natural language.\nDelve into sentiment analysis, question-answering, and text summarization using the core architecture of BERT.\nHugging Face Transformers:\nNavigate the realm of Hugging Face transformers, a platform offering access to state-of-the-art pre-trained models.\nImplement these models hands-on, gaining insights into the latest advancements in NLP.\n6. Seize the Learning Advantage:\nBenefit from a practitioner-led course that ensures hands-on learning and practical insights.\nStay ahead in the ever-evolving NLP landscape with a comprehensive curriculum designed for seamless progression.\n7. Take the Leap into a Transformative NLP Future:\nExperience the thrill of going from zero to hero in Practical Natural Language Processing.\nInvest in your future with a course that not only imparts knowledge but also cultivates practical skills.\nDon't miss this chance to elevate your proficiency in NLP. Click the \"Enroll Now\" button and be part of a transformative learning journey. Every second counts, and the opportunity to become an NLP hero awaits.",
      "target_audience": [
        "Anyone who wants to learn natural language processing (NLP)",
        "Anyone interested in artificial intelligence, machine learning, deep learning, or data science",
        "Anyone who wants to build Advanced NLP models and implement in a project",
        "Anyone who wants to create Enterprise Grade Chatbots"
      ]
    },
    {
      "title": "Machine Learning Essentials - Master core ML concepts",
      "url": "https://www.udemy.com/course/machine-learning-artificial-intelligence-essentials/",
      "bio": "Kickstart Machine Learning, understand maths behind essential algorithms, implement them in python & build 8+ projects!",
      "objectives": [
        "Jumpstart the world of AI & ML",
        "Maths of Machine Learning",
        "Regression & Classification Techniques",
        "Linear & Logistic Regression",
        "K-Nearest Neighbours, K-Means",
        "Naive Bayes, Text Classification",
        "Decision Trees & Random Forests",
        "Ensemble Learning - Bagging & Boosting",
        "Dimensionality Reduction",
        "Neural Networks",
        "8+ Hands on Projects"
      ],
      "course_content": {
        "Introduction": [
          "Course Overview",
          "Artificial Intelligence",
          "Machine Learning",
          "Deep Learning",
          "Computer Vision",
          "Natural Language Processing",
          "Automatic Speech Recognition",
          "Reinforcement Learning",
          "Pre-requisites",
          "Code Repository",
          "Quiz Time!"
        ],
        "Supervised vs Unsupervised Learning": [
          "Supervised Learning Introduction",
          "Supervised Learning Example",
          "Unsupervised Learning",
          "Quiz Time!"
        ],
        "Linear Regression": [
          "Introduction to Linear Regression",
          "Notation",
          "Hypothesis",
          "Loss / Error Function",
          "Training Idea",
          "Gradient Descent Optimisation",
          "Gradient Descent Code",
          "Gradient Descent - for Linear Regression",
          "The Math of Training",
          "Code 01 - Data Generation",
          "Code 02 - Data Normalisation",
          "Code 03 - Train Test Split",
          "Code 04 - Modelling",
          "Code 05 - Predictions",
          "R2 Score",
          "Code 06 - Evaluation",
          "Code 07 - Visualisation",
          "Code 08 - Trajectory [Optional]"
        ],
        "Linear Regression - Multiple Features": [
          "Introduction",
          "Hypothesis",
          "Loss Function",
          "Training & Gradient Updates",
          "Code 01 - Data Prep",
          "Code 02 - Hypothesis",
          "Code 03 - Loss Function",
          "Code 04 - Gradient Computation",
          "Code 05 - Training Loop",
          "A Note about Shapes",
          "Code 06 - Evaluation",
          "Linear Regression using Sk-Learn"
        ],
        "Logistic Regression": [
          "Binary Classification Introduction",
          "Notation",
          "Hypothesis Function",
          "Binary Cross-Entropy / Loss Function",
          "Gradient Update Rule",
          "Code 01 - Data Prep",
          "Code 02 - Hypothesis / Logit Model",
          "Code 03 - Binary Cross Entropy Loss",
          "Code 04 - Gradient Computation",
          "Code 05 - Training Loop",
          "Code 06 - Visualise Decision Boundary",
          "Code 07 - Predictions & Accuracy",
          "Logistic Regression using Sk-Learn",
          "Multiclass Classification : One Vs Rest",
          "Multiclass Classification : One Vs One"
        ],
        "Dimensionality Reduction/ Feature Selection": [
          "Curse of Dimensionality",
          "Feature Selection Vs. Feature Extraction",
          "Filter Method",
          "Wrapper Method",
          "Embedded Method",
          "Feature Selection - Code"
        ],
        "Principal Component Analysis (PCA)": [
          "Introduction to PCA",
          "Conceptual Overview of PCA",
          "Maximising Variance",
          "Minimising Distances",
          "Eigen Values & Eigen Vectors",
          "PCA Summary",
          "Understanding Eigen Values",
          "PCA Code",
          "Choosing the right dimensions"
        ],
        "K-Nearest Neigbours": [
          "Introduction",
          "KNN Idea",
          "KNN Data Prep",
          "KNN Algorithm Code",
          "Euclidean and Manhattan Distance",
          "Deciding value of K",
          "KNN and Data Standardisation",
          "KNN Pros and Cons",
          "KNN using Sk-Learn"
        ],
        "PROJECT - Face Recognition": [
          "OpenCV - Working with Images",
          "OpenCV - Video Input from WebCam",
          "Object Detection using Haarcascades",
          "Face Detection in Images",
          "Face Detection in Live Video",
          "Face Recognition Project Intro",
          "Face Recognition 01 - Data Collection",
          "Face Recognition 02 - Loading Data",
          "Face Recognition 03 - Predictions using KNN"
        ],
        "K-Means": [
          "K-Means Algorithm",
          "Code 01 - Data Prep",
          "Code 02 - Init Centers",
          "Code 03 - Assigning Points",
          "Code 04 - Updating Centroids",
          "Code 05 - Visualizing K-Means & Results"
        ]
      },
      "requirements": [
        "Python Programming",
        "Basics of Numpy, Pandas, Matplotlib"
      ],
      "description": "Read to jumpstart the world of Machine Learning & Artificial intelligence?\n\nThis hands-on course is designed for absolute beginners as well as for proficient programmers who want kickstart Machine Learning for solving real life problems. You will learn how to work with data, and train models capable of making \"intelligent decisions\"\n\nData Science has one of the most rewarding jobs of the 21st century and fortune-500 tech companies are spending heavily on data scientists! Data Science as a career is very rewarding and offers one of the highest salaries in the world. Unlike other courses, which cover only library-implementations this course is designed to give you a solid foundation in Machine Learning by covering maths and implementation from scratch in Python for most statistical techniques.\n\nThis comprehensive course is taught by Prateek Narang & Mohit Uniyal, who not just popular instructors but also have worked in Software Engineering and Data Science domains with companies like Google. They have taught thousands of students in several online and in-person courses over last 3+ years.\n\nWe are providing you this course to you at a fraction of its original cost! This is action oriented course, we not just delve into theory but focus on the practical aspects by building 8+ projects.\n\nWith over 170+ high quality video lectures, easy to understand explanations and complete code repository this is one of the most detailed and robust course for learning data science.\n\nSome of the topics that you will learn in this course.\n\nLogistic Regression\nLinear Regression\nPrincipal Component Analysis\nNaive Bayes\nDecision Trees\nBagging and Boosting\nK-NN\nK-Means\nNeural Networks\n\n\nSome of the concepts that you will learn in this course.\n\nConvex Optimisation\nOverfitting vs Underfitting\nBias Variance Tradeoff\nPerformance Metrics\nData Pre-processing\nFeature Engineering\nWorking with numeric data, images & textual data\nParametric vs Non-Parametric Techniques\n\nSign up for the course and take your first step towards becoming a machine learning engineer! See you in the course!",
      "target_audience": [
        "Programmers who are curious to about Machine Learning and Artificial Intellgence",
        "Working professionals who want to build a career in data science",
        "Developers who wants to learn a new skill and build ML based projects",
        "University and college students who want to strengthen their understanding of Machine Learning"
      ]
    },
    {
      "title": "Learn Hugging Face Bootcamp",
      "url": "https://www.udemy.com/course/complete-hugging-face-bootcamp/",
      "bio": "Discover the power of open-source machine learning with Hugging Face! Explore transformers, diffusers, and more!",
      "objectives": [
        "Master Hugging Face's platform, including models, datasets, and spaces.",
        "Set up AI development environments with Hugging Face and Google Colab.",
        "Use Transformers for NLP tasks like text classification and entity recognition.",
        "Develop and train image generation models using the Diffusers library.",
        "Apply cutting-edge video and audio models for media generation and analysis.",
        "Build interactive AI applications with Gradio, making ML models user-friendly.",
        "Understand and implement advanced techniques in large language models.",
        "Initiate, manage, and deploy comprehensive machine learning projects."
      ],
      "course_content": {
        "Introduction": [
          "FAQs and Resource Downloads"
        ],
        "Introduction to Hugging Face": [
          "What is Hugging Face?",
          "Creating a Hugging Face Account and Token",
          "Understanding Models and Spaces",
          "Datasets on Hugging Face Overview",
          "Hugging Face Setup, Online Options, and Versions",
          "Introduction Check-In"
        ],
        "HuggingFace - NLP with Transformers Library on Hugging Face": [
          "Introduction to Transformers Library",
          "Hugging Face Basics",
          "NLP Pipelines",
          "Understanding how LLMs Work",
          "LLMs - Text Tokenization and Probabilities",
          "LLMs - Text Generation"
        ],
        "Image Models - Diffusers Library with Hugging Face": [
          "Introduction to Image Models Section",
          "Understanding Image Data",
          "Image Data with Python",
          "Understanding Text to Image Models",
          "Diffusion Models with Python",
          "AutoPipeines with Diffusers"
        ],
        "Video Models": [
          "Understanding Video Models",
          "Stable Video Diffusion",
          "I2VGen-XL"
        ],
        "Audio Models on Hugging Face": [
          "Introduction to Audio",
          "Understanding Audio Data",
          "Audio Data with Python",
          "Audio Classification",
          "Audio Transcription",
          "Audio Generation"
        ],
        "Building Machine Learning GUIs with Gradio": [
          "Introduction to Gradio Section",
          "Gradio Components",
          "Gradio Components - Part Two",
          "Gradio - Multiple Components and Layouts",
          "Gradio - Components and Layouts - Part Two",
          "Gradio - Component Interactions",
          "Gradio - Interactions Part Two",
          "Gradio - Machine Learning GUI Example - ML Integration",
          "Gradio - Pipelines Integration",
          "Gradio - Errors, Warnings, and Info",
          "Gradio - Styling and Themes"
        ],
        "APPENDIX: Git": [
          "Understanding Git vs. GitHub",
          "Installing Git",
          "GitHub Profile",
          "Configuring Git (Note: GitHub Specific)",
          "Creating and Cloning Repositories",
          "Private Repositories",
          "Git Usage and Workflow",
          "Git add and Commit",
          "Git Remote Branch and Push",
          "Git Log",
          "Git Fetch and Pull"
        ]
      },
      "requirements": [
        "Understanding of Python and pip installing libraries",
        "Understanding of PyTorch and CUDA",
        "Understanding of Jupyter Notebooks"
      ],
      "description": "Unlock the Future of AI: Master Hugging Face & Open Source Machine Learning\nWelcome to the Ultimate Journey in Cutting-Edge AI Technologies!\nDive into the dynamic world of artificial intelligence with our comprehensive course designed to empower you with the knowledge and skills to harness the full potential of Hugging Face and other open-source machine learning tools. Whether you’re aiming to innovate in tech, enhance your career, or simply passionate about AI, this course is your gateway to becoming a part of the AI revolution.\nCourse Overview\nKickstart Your Adventure\nGet introduced to the world of AI with an in-depth look at the course structure and what you can expect to achieve.\nExploring Hugging Face\nDelve into Hugging Face, the cutting-edge platform revolutionizing AI development.\nLearn the essentials of setting up your Hugging Face account, managing tokens, understanding models, and more.\nGain practical insights into using datasets and the ecosystem of Python packages critical for AI development.\nMastery Over NLP with Transformers\nExplore the Transformers library to unleash powerful NLP capabilities.\nTackle real-world tasks like text classification, named entity recognition, and more using pipelines.\nDeep dive into large language models (LLMs), from tokenization to text generation, and discover their fascinating applications.\nThe Art of Diffusion with the Diffusers Library\nStep into the world of image generation with the Diffusers library.\nFrom setting up diffusion models to generating breathtaking images, get hands-on experience in the entire workflow.\nLearn the intricacies of models like U-net and techniques for effective image training.\nVenturing into Video Models\nUnderstand and apply cutting-edge video models like Stable Video Diffusion and AnimateDiff.\nDiscover innovative methods to bring static images to life and generate high-quality video content.\nThe Universe of Audio Models\nUncover the potential of audio in AI with modules dedicated to audio classification, transcription, and generation.\nLearn the essential skills to handle and process complex audio data effectively.\nBuilding Machine Learning GUIs with Gradio\nMaster the art of creating user-friendly machine learning interfaces using Gradio.\nFrom simple components to complex interactive GUIs, learn to build applications that make your ML models accessible and practical.\nReal-World Applications\nTech Innovators: Integrate advanced AI models into your projects or start-ups to drive innovation.\nBusiness Professionals: Enhance decision-making processes by implementing AI-driven solutions.\nCreative Minds: Create stunning art, generate music, or develop interactive media and games using the skills acquired.\nWhy Choose This Course?\nHands-On Learning: Each section includes practical tasks and projects to consolidate learning and build your portfolio.\nIndustry-Relevant Skills: The curriculum is designed to equip you with skills highly sought after in the tech industry.\nCommunity and Support: Gain exclusive access to a community of like-minded learners and industry experts.\nEmbrace the opportunity to transform the digital landscape with your creativity and expertise. Enroll now and start your journey towards mastering Hugging Face and open source machine learning!",
      "target_audience": [
        "Python developers wanting to learn Hugging Face platform"
      ]
    },
    {
      "title": "The Complete Agentic AI Engineering Course (2025)",
      "url": "https://www.udemy.com/course/the-complete-agentic-ai-engineering-course/",
      "bio": "Master AI Agents in 30 days: build 8 real-world projects with OpenAI Agents SDK, CrewAI, LangGraph, AutoGen and MCP.",
      "objectives": [
        "Project 1: Career Digital Twin. Build and deploy your own Agent to represent you to potential future employers.",
        "Project 2: SDR Agent. An instant business application: create Sales Representatives that craft and send professional emails .",
        "Project 3: Deep Research. Make your own version of the essential Agentic use case: a team of Agents that carry out extensive research on any topic you choose.",
        "Project 4: Build a Stock Picker Agent in minutes with CrewAI—automate your search for investment gems!",
        "Project 5: Deploy your own 4-Agent Engineering Team—manage, build, and test software apps with CrewAI and Coder Agents in Docker!",
        "Project 6: Build your own version of OpenAI’s Operator Agent—your Sidekick works with you inside your browser via LangGraph!",
        "Project 7: Agent Creator—an Agent that builds and launches new Agents using AutoGen, unlocking endless AI possibilities!",
        "Project 8: Capstone—build a Trading Floor with 4 Agents making autonomous trades, powered by 6 MCP servers and 44 tools!"
      ],
      "course_content": {
        "Week 1": [
          "Day 1 - Autonomous AI Agent Demo: Using N8n to Control Smart Home Devices",
          "Day 1 - AI Agent Frameworks Explained: OpenAI SDK, Crew AI, LangGraph & AutoGen",
          "Day 1 - Agent Engineering Setup: Understanding Cursor IDE, UV & API Options",
          "Day 1 - Windows Setup for AI Development: Git, Cursor IDE & UV Package Manager",
          "Day 1 - Setting Up Your Mac for AI Projects: GitHub, Cursor IDE & OpenAI API Key",
          "Day 1 - Building Your First Agentic AI Workflow with OpenAI API: Step-by-Step",
          "Day 1 - Introduction to Agentic AI: Creating Multi-Step LLM Workflows + Autonomy",
          "Day 2 - Building Effective Agents: LLM Autonomy & Tool Integration Explained",
          "Day 2 - 5 Essential LLM Workflow Design Patterns for Building Robust AI Systems",
          "Day 2 - Understanding Agent vs Workflow Patterns in LLM Application Design",
          "Day 3 - Orchestrating Multiple LLMs: Comparing GPT-4o, Claude, Gemini & DeepSeek",
          "Day 3 - Multi-LLM API Integration: Comparing OpenAI, Anthropic & Other Models",
          "Day 3 - Comparing LLM APIs: Using OpenAI Client Library with Claude, Gemini & ++",
          "Day 3 - Multi-Model Orchestration: Creating a System to Evaluate AI Responses",
          "Day 3 - Connecting Agentic Patterns to Tool Use: Essential AI Building Blocks",
          "Day 4 - Comparing AI Agent Frameworks: Simplicity vs Power in LLM Orchestration",
          "Day 4 - Resources vs. Tools: Two Ways to Enhance LLM Capabilities in Agentic AI",
          "Day 4 - Build a Web Chatbot That Acts Like You Using Gradio & OpenAI",
          "Day 4 - Using Gemini to Evaluate GPT-4 Responses: A Multi-LLM Pipeline",
          "Day 4 - Building Agentic LLM Workflows: Resources, Tools & Structured Outputs",
          "Day 5 - Building Your Career Alter Ego: LLM Function Calling with Push Alerts",
          "Day 5 - LLM Tool Calls Demystified: How to Process and Execute Function Requests",
          "Day 5- Building AI Assistants: Implementing Tools for Handling Unknown Questions",
          "Day 5 - Creating & Deploying an AI Agent: From Chat Loop to HuggingFace Spaces",
          "Day 5 - Deploying Career Conversation Chatbots to Gradio",
          "Day 5 - Foundation Week Wrap-up: Building Complete AI Agents with APIs & Tools"
        ],
        "Week 2": [
          "Day 1 - Understanding Async Python: The Foundation for OpenAI Agents SDK",
          "Day 1 - OpenAI Agents SDK Fundamentals: Creating, Tracing, and Running Agents",
          "Day 1 - Introduction to Agent, Runner, and Trace Classes in OpenAI Agents SDK",
          "Day 1 - Vibe Coding: 5 Essential Tips for Efficient Code Generation with LLMs",
          "Day 1 - OpenAI Agents SDK: Understanding Core Concepts for AI Development",
          "Day 2 - Build AI Sales Agents with SendGrid: Tools & Collaboration in Agent SDK",
          "Day 2 - Concurrent LLM Calls: Implementing Asyncio for Parallel Agent Execution",
          "Day 2 - Converting Agents into Tools: Building Hierarchical AI Systems",
          "Day 2 - Agent Control Flow: When to Use Handoffs vs. Agents as Tools",
          "Day 2 - From Function Calls to Agent Autonomy: Sales Automation with OpenAI SDK",
          "Day 2 - Agentic AI for Business: Creating Interactive Sales Outreach Tools",
          "Day 3- Multi-Model Integration: Using Gemini, DeepSeek & Grok with OpenAI Agents",
          "Day 3 - Implementing Guardrails & Structured Outputs for Robust AI Agent Systems",
          "Day 3- AI Safety in Practice: Implementing Guardrails for LLM Agent Applications",
          "Day 4 - Building Deep Research Agents: Implementing OpenAI's Web Search Tool",
          "Day 4 - Building a Planner Agent: Using Structured Outputs with Pydantic in AI",
          "Day 4 - Building an End-to-End Research Pipeline with GPT-4 Agents & Async Tasks",
          "Day 4 - Building a Deep Research Agent: Parallel Searches with AsyncIO",
          "Day 5 - Building a Modular AI Research System with Gradio UI Implementation",
          "Day 5 - Deep Research App: Gradio to Visualize & Monitor Autonomous AI Agents",
          "Day 5 - Deploying Smart Research Agents with Gradio and HuggingFace Spaces"
        ],
        "Week 3": [
          "Day 1 - Crew AI Framework: Creating Collaborative AI Agent Teams",
          "Day 1 - Crew AI Framework Explained: Agents, Tasks & Processing Modes Tutorial",
          "Day 1 - Crew AI & LightLLM: Flexible Framework for Integrating Multiple LLMs",
          "Day 1 - Crew AI Tutorial: Setting Up a Debate Project with GPT-4o mini",
          "Day 1 - How to Create an AI Debate System Using Crew AI and Multiple LLMs",
          "Day 1 - Building AI Debate Systems with CrewAI: Compare Different LLMs",
          "Day 2 - Building Crew AI Projects: Tools, Context & Google Search Integration",
          "Day 2 - Building Multi-Agent Financial Research Systems with Crew.ai",
          "Day 2- Enhancing AI Agents with Web Search: Solving the Knowledge Cutoff Problem",
          "Day 3 - Building a Crew AI Stock Picker: Multi-Agent System for Investments",
          "Day 3 - Implementing Pydantic Outputs in Crew AI: Stock Picker Agent Tutorial",
          "Day 3 - Custom Tool Development for Crew AI: JSON Schema & Push Notifications",
          "Day 4 - Crew AI Memory: Vector Storage & SQL Implementation for AI Agents",
          "Day 4 - Crew AI for Coding Tasks: Agents That Generate & Run Python Code",
          "Day 4 - Create a Python-Writing AI Agent: Practical Implementation with Crew AI",
          "Day 5 - Building AI Teams: Configure Crew AI for Collaborative Development",
          "Day 5 - Collaborative AI Agent Development for a Stock Trading Framework",
          "Day 5 - Building a Trading Application Using GPT-4o & Claude",
          "Day 5 - From Single Modules to Complete Systems: Advanced CrewAI Techniques"
        ],
        "Week 4": [
          "Day 1 - LangGraph Explained: Graph-Based Architecture for Robust AI Agents",
          "Day 1 - LangGraph Explained: Framework, Studio, and Platform Components Compared",
          "Day 1 - LangGraph Theory: Core Components for Building Advanced Agent Systems",
          "Day 2 - LangGraph Deep Dive: Managing State in Graph-Based Agent Workflows",
          "Day 2 - Mastering LangGraph: How to Define State Objects & Use Reducers",
          "Day 2 - LangGraph Fundamentals: Creating Nodes, Edges & Workflows Step-by-Step",
          "Day 2 - LangGraph Tutorial: Building an OpenAI Chatbot with Graph Structures",
          "Day 3 - LangGraph Advanced Tutorial: Super Steps & Checkpointing Explained",
          "Day 3 - Setting Up Langsmith & Creating Custom Tools for LangGraph Applications",
          "Day 3 - LangGraph Tool Calling: Working with Conditional Edges & Tool Nodes",
          "Day 3 - LangGraph Checkpointing: How to Maintain Memory Between Conversations",
          "Day 3 - Building Persistent AI Memory with SQLite: LangGraph State Management",
          "Day 4 - Playwright Integration with LangGraph: Creating Web-Browsing AI Agents",
          "Day 4 - Create AI Web Assistants: Playwright, LangChain & Gradio Implementation",
          "Day 4 - LLM Evaluator Agents: Creating Feedback Loops with Structured Outputs",
          "Day 4- Creating LLM Feedback Loops: Worker-Evaluator Implementation in LangGraph",
          "Day 4 - Building an AI Sidekick Using LangGraph, Gradio & Browser Automation",
          "Day 5 - Agentic AI: Add Web Search, File System & Python REPL to Your Assistant",
          "Day 5 - LangChain Tool Integration: Building a Powerful AI Sidekick from Scratch",
          "Day 5 - Creating AI Workflows: Graph Builders & Node Communication Techniques",
          "Day 5 - Creating Isolated User Sessions in Gradio Apps Using State Management",
          "Day 5 - Inside AI Feedback Loops: Seeing How AI Evaluates & Corrects Errors",
          "Day 5 - AI Assistant Upgrades: Memory, Clarifying Questions & Custom Tools"
        ],
        "Week 5": [
          "Day 1 - Microsoft Autogen 0.5.1: AI Agent Framework Explained for Beginners",
          "Day 1 - AutoGen vs Other Agent Frameworks: Features & Components Compared",
          "Day 1 - AutoGen Agent Chat Tutorial: Creating Tools and Database Integration",
          "Day 1 - Essential AI Components: Models, Messages & Agents Explained",
          "Day 2 - Advanced Autogen Agent Chat: Multimodal Features & Structured Outputs",
          "Day 2 - Implementing Primary and Evaluator Agents in AutoGen with Langchain",
          "Day 2 - Headless Web Scraping Tutorial: MCP Server Fetch Integration in AutoGen",
          "Day 3 - AutoGen Core: The Backbone of Distributed Agent Communications",
          "Day 3 - Agent Communication in Autogen Core: Message Handlers & Dispatching",
          "Day 3 - AutoGenCore Agent Registration and Message Handling: Practical Examples",
          "Day 3 - AutoGenCore Standalone Agents: Rock Paper Scissors with GPT-4o & Llama",
          "Day 4 - Autogen Core Distributed Runtime: Architecture & Components Explained",
          "Day 4 - Implementing Distributed AI Agents with AutoGen Core and gRPC Runtime",
          "Day 4 - Building Distributed Agent Systems: AutoGen Cross-Process Communication",
          "Day 5 - Creating Autonomous Agents That Write & Deploy Other Agents in AutoGen",
          "Day 5 - Implementing Agent-to-Agent Messaging with Autogen Core & Templates",
          "Day 5 - Creating Autonomous AI Agents that Collaborate Using Async Python"
        ],
        "Week 6 - MCP": [
          "Day 1 - Intro to MCP: The USB-C of Agentic AI",
          "Day 1 - Understanding MCP Hosts, Clients, and Servers",
          "Day 1 - Using MCP Servers with OpenAI Agents SDK",
          "Day 1 - Exploring Node-Based MCP Servers & Tool Access",
          "Day 1 - Building an Agent That Uses Multiple MCP Servers",
          "Day 1 - MCP Marketplaces & Security Considerations",
          "Day 2 - Intro to Week 6 Day 2: Building Your Own MCP Server",
          "Day 2 - Wiring Business Logic into Your MCP Server",
          "Day 2 - Creating Client Code to Use Your MCP Server",
          "Day 2 - Wrap-Up: Capabilities of Your Custom MCP Server",
          "Day 3 - Exploring Types of MCP Servers and Agent Memory",
          "Day 3 - Brave Search API: MCP Server Calling the Web",
          "Day 3 - Integrating Polygon API for Stock Market Data",
          "Day 3 - Advanced Market Tools Using Paid Polygon Plan",
          "Day 4 - What’s Next: Launching Our Agent Trading Floor",
          "Day 4 - Viewing the User Interface for Trading Activity",
          "Day 4 - How Trading Agents Operate and Make Decisions",
          "Day 4 - Portfolio Management with Four Autonomous Agents",
          "Day 5 - Which Agent Framework Should You Pick?",
          "Day 5 - Key Settings and Launching the Trading System",
          "Day 5 - Advice for Selecting Agentic Frameworks",
          "Day 5 - 10 Essential Lessons for Building Agent Solutions",
          "Day 5 - Course Recap and Final Goodbye – Keep Building!"
        ]
      },
      "requirements": [
        "While it’s ideal if you can code in Python and have some experience working with LLMs, this course is designed for a very wide audience, regardless of background. I’ve included a whole folder of self-study labs that cover foundational technical and programming skills. If you’re new to coding, there’s only one requirement: plenty of patience!",
        "The course runs best if you have a small budget for APIs, but it’s totally your choice. You can complete the entire course with no API spend. If you do wish to use frontier models, the typical spend would be under $5. You can choose to access more capabilities if you’re comfortable spending a little more."
      ],
      "description": "2025 is the year that Agents enter the workforce. This is nothing short of a watershed moment for Artificial Intelligence. It has never been more important to be an expert with Agentic AI. And that is precisely the goal of this course: to equip you with the skills and expertise to design, build and deploy Autonomous AI Agents, opening up new career and commercial opportunities.\n\n\nThis is an intensive 6-week program to master Agentic AI. We start by building foundational expertise, connecting LLMs using proven design patterns. Then, each week, we upskill with new frameworks: OpenAI Agents SDK, CrewAI, LangGraph and Autogen. The course culminates with a full week on the remarkable opportunities opened up by MCP.\n\n\nAbove all, this is a hands-on course. I’m a big believer that the best way to learn is by DOING. So please prepare to roll up your sleeves! We’ll build 8 real-world projects; some are astonishing, some are intriguing, and some are quite surreal. But one thing’s for sure: all are powerful demonstrations of Agentic AI’s potential to utterly transform the business landscape.\n\n\nSo come join me on this comprehensive 6-week journey. By the end, you will have mastered Agentic AI. You will have expertise in all the major frameworks. You’ll be well-versed in the strengths and traps of Agentic AI. You’ll confidently unleash Autonomous Agents to solve real-world commercial problems. And along the way, you’ll have had a whole lot of fun with the astounding, groundbreaking technology that is Agentic AI.",
      "target_audience": [
        "Well, perhaps I’m biased, but I’d say: anyone and everyone! If you’re fascinated in the potential of Agents and hungry to have the skills to create powerful Agentic AI – then you’ve come to the right place. While it’s most suited to those with programming experience, I’ve designed the course to work for all backgrounds."
      ]
    },
    {
      "title": "Crash course: Data analytics in Python using Pandas",
      "url": "https://www.udemy.com/course/python-analytics/",
      "bio": "Let's get to grips with the Python Pandas library for data analytics / analysis",
      "objectives": [
        "Perform data analysis with the Pandas library",
        "Learn about dataframes and how to conduct data analysis in Python",
        "Understand how to handle missing values in your data",
        "Understand how to handle & clean up messy data"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "A project to get us started": [
          "Project introduction: What's the data & what do we want to achieve?",
          "Ingesting data & cleaning it up",
          "Initial insight from the data",
          "Extracting a little more insight"
        ],
        "The Pandas Essentials": [
          "Importing data",
          "Inspecting our dataframe - what do we have to work with?",
          "Handling missing values",
          "Removing duplicated data",
          "Where Statements",
          "Selecting specific fields from the dataframe",
          "Replacing values in our dataframe",
          "Group By",
          "Ranking our dataframes on a specific field value",
          "The Apply function: data cleanup & additional insight",
          "Write dataframes back to files"
        ],
        "Practicing what we have learned": [
          "Your challenge, if you accept it",
          "Challenge solution"
        ],
        "Conclusion": [
          "A real world example of using Pandas",
          "Where do I go from here?",
          "Thank you"
        ]
      },
      "requirements": [
        "Basic Python - from my FREE No Nonsense Python Course",
        "The Pandas library should be installed - which you can do using 'pip install python'",
        "Python should already be installed"
      ],
      "description": "Unleash Data Magic with Python Pandas:\nReady to supercharge your data skills? Dive into our Python Pandas course, where data becomes your playground and insights are your treasure.\nWhy Choose This Course?\nElevate Your Data Game: Python Pandas is the secret sauce behind cutting-edge data analysis. Become a data virtuoso in just a few clicks!\nUnlock Hidden Insights: I'll teach you to cleanse, manipulate, and analyze data like a pro. Say goodbye to messy datasets, and hello to crystal-clear conclusions!\nReal-World Experience: Dive into captivating, real-world projects that put your newfound skills to the test. Learn by doing and conquer any data challenge.\nWhat's In Store For You?\nData Transformation Mastery: Harness Python Pandas to transform raw data into invaluable insights. You'll wonder how you ever lived without it!\nData Wizardry: Learn the art of data ingestion, where you'll effortlessly extract, clean, and prepare data for analysis. No more data headaches!\nData Brilliance: By the course's end, you'll have the power to wield Python and Pandas like a seasoned pro, extracting untold gems of wisdom from your own datasets.\n\n\nJoin me and embark on a journey where data isn't just numbers; it's your passport to making data-driven decisions with confidence. Get ready to chart your course in the data-driven world. Enroll now and let your data adventures begin!\n\n\nYou will need to have Python installed and the Pandas library installed - which you can do using 'pip install pandas'.",
      "target_audience": [
        "Beginner Python Developers",
        "Those interested in data analytics"
      ]
    },
    {
      "title": "Python for Linear Regression in Machine Learning",
      "url": "https://www.udemy.com/course/python-for-advanced-linear-regression-masterclass/",
      "bio": "Linear and Non-Linear Regression, Lasso Ridge Regression, SHAP, LIME, Yellowbrick, Feature Selection | Outliers Removal",
      "objectives": [
        "Analyse and visualize data using Linear Regression",
        "Plot the graph of results of Linear Regression to visually analyze the results",
        "Learn how to interpret and explain machine learning models",
        "Do in-depth analysis of various forms of Linear and Non-Linear Regression",
        "Use YellowBrick, SHAP, and LIME to interact with predictions of machine learning models",
        "Do feature selection and transformations to fine tune machine learning models",
        "Course contains result oriented algorithms and data explorations techniques"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Resources Folder | DO NOT SKIP!",
          "Install Anaconda and Python 3 on Windows 10",
          "Install Anaconda and Python 3 on Ubuntu Machine",
          "Jupyter Notebook Shortcuts"
        ],
        "Python Crash Course": [
          "Introduction",
          "Data Types",
          "Variable Assignment",
          "String Assignment",
          "List",
          "Set",
          "Tuple",
          "Dictionary",
          "Boolean and Comparison Operator",
          "Logical Operator",
          "If, Else, Elif",
          "Loops in Python",
          "Methods and Lambda Function"
        ],
        "Numpy Introduction [Optional]": [
          "Introduction",
          "Array",
          "NaN and INF",
          "Statistical Operations",
          "Shape, Reshape, Ravel, Flatten",
          "Sequence, Repetitions, and Random Numbers",
          "Where(), ArgMax(), ArgMin()",
          "File Read and Write",
          "Concatenate and Sorting",
          "Working with Dates"
        ],
        "Pandas Introduction": [
          "Introduction",
          "DataFrame and Series",
          "File Reading and Writing",
          "Info, Shape, Duplicated and Drop",
          "Columns",
          "NaN and Null Values",
          "Imputation",
          "Lambda Functions"
        ],
        "Matplotlib Introduction": [
          "Introduction",
          "Line Plot",
          "Label for X-Axis and Y-Axis",
          "Scatter Plot, Bar Plot, and Hist Plot",
          "Box Plot",
          "Subplot",
          "xlim, ylim, xticks, and yticks",
          "Pie Plot",
          "Pie Plot Text Color",
          "Nested Pie Plot",
          "Labeling a Pie Plot",
          "Bar Chart on Polar Axis",
          "Line Plot on a Polar Axis",
          "Scatter Plot on a Polar Axis",
          "Integral in Calculus Plot as Area Under the Curve"
        ],
        "Linear Regression Introduction": [
          "Linear Regression Introduction",
          "Regression Examples",
          "Types of Linear Regression",
          "Assessing the performance of the model",
          "Bias-Variance tradeoff",
          "What is sklearn and train-test-split",
          "Python Package Upgrade and Import",
          "Load Boston Housing Dataset",
          "Dataset Analysis",
          "Exploratory Data Analysis- Pair Plot",
          "Exploratory Data Analysis- Hist Plot",
          "Exploratory Data Analysis- Heatmap",
          "Train Test Split and Model Training",
          "How to Evaluate the Regression Model Performance",
          "Plot True House Price vs Predicted Price",
          "Plotting Learning Curves Part 1",
          "Plotting Learning Curves Part 2",
          "Machine Learning Model Interpretability- Residuals Plot",
          "Machine Learning Model Interpretability- Prediction Error Plot"
        ],
        "Data Preprocessing for Linear Regression": [
          "Linear Model Assumption for Linear Regression",
          "Definitions of Linear Model Assumptions",
          "Load Boston Dataset",
          "Create Reference Data",
          "Check Linear Assumption for Boston Dataset Part 1",
          "Check Linear Assumption for Boston Dataset Part 2",
          "Log Transformation of Variables",
          "Types of Variable Transformations",
          "Reciprocal Transformation",
          "sqrt and exp Transformation",
          "Box-Cox Transformation",
          "Yeo-Johnson Transformation",
          "Check Variables Normality with Histogram",
          "Check Variables Normality with Q-Q Plot",
          "Variable Transformation for Normality",
          "Check Variables Homocedasticity",
          "Variable Transformation for Homoscedasticity Part 1",
          "Variable Transformation for Homoscedasticity Part 2",
          "How to Check Multicolinearity",
          "Normalization and Standardization Introduction",
          "Normalization and Standardization Coding"
        ],
        "Machine Learning Models Interpretability and Explainer": [
          "Machine Learning Models Interpretability",
          "Recap",
          "Prediction Error Plot",
          "Residuals Plot",
          "Explain Machine Learning Models with LIME Part 1",
          "Explain Machine Learning Models with LIME Part 2",
          "Explain Machine Learning Models with LIME Part 3",
          "Machine Learning Models Explainer Summary with SHAP",
          "Explain Machine Learning Models with Dependence Plot",
          "Explain Machine Learning Models with The Individual Force Plot",
          "Explain Machine Learning Models with The Collective Force Plot",
          "Explain Machine Learning Models with Shap Heatmap",
          "Explain Machine Learning Models with with SHAP Waterfall Plots",
          "Explain Feature Importance in Machine Learning Models",
          "Explain Feature Selection with SHAP"
        ],
        "Linear Regression Model Optimization": [
          "Recap",
          "Check Linear Model Assumptions on Selected Features",
          "Check Linear Model Assumptions on Selected Features Part 2",
          "Detect Outliers in Machine Learning Datasets",
          "Outliers Visualization Plot",
          "Outlier Detection for Normal Variables",
          "Outlier Detection for Skewed Variables",
          "Types of Outliers Removal Techniques",
          "Outliers Removal by Using Feature-Engine",
          "Model Evaluation After Removing the Outliers",
          "Model Evaluation After Feature Transformations and Outliers Removal"
        ],
        "Feature Selection for Linear Regression": [
          "Introduction to Feature Selection",
          "Introduction to A Python API for Intelligent Visual Discovery",
          "Recap",
          "Visualization of Data with LUX API",
          "Select Most Correlated Features with House Price Part 1",
          "Select Most Correlated Features with House Price Part 2",
          "Model Performance Evaluation",
          "Remove Correlated Input Features (Multicollinearity)",
          "Recursive Feature Elimination (RFE) Introduction",
          "10 Recursive Feature Elimination (RFE) Coding",
          "Increamental RFE",
          "Exhaustive Feature Selection (EFS)",
          "Feature Selection by Linear Regression Coefficients"
        ]
      },
      "requirements": [
        "Basic Python Programming",
        "Desire to Learn!"
      ],
      "description": "Unlock the power of machine learning with our comprehensive Python course on linear regression. Learn how to use Python to analyze data and build predictive models. This course is perfect for beginners with little or no programming experience and experienced Python developers looking to expand their skill set.\nYou'll start with the basics of Python and work your way up to advanced techniques like linear regression, which is a powerful tool for predicting future outcomes based on historical data. Along the way, you'll gain hands-on experience with popular Python libraries such as NumPy, Pandas, and Matplotlib. We will also cover the important aspect of model optimization, interpretability, and feature selection. You will learn how to optimize your model to improve its performance and how to interpret the model results and understand the underlying relationships in your data. We will also discuss feature selection techniques that are used to identify the most essential features that drive the predictions.\nBy the end of the course, you'll have a solid understanding of how to use Python to build linear regression models and make accurate predictions. You'll also be able to apply your new skills to a wide range of machine learning and data science projects. So, if you're ready to take your Python skills to the next level and start using machine learning to analyze and predict real-world outcomes, this is the course for you!\nWhat is covered in this course?\nThis course teaches you, step-by-step coding for Linear Regression in Python. The Linear Regression model is one of the widely used in machine learning and it is one the simplest ones, yet there is so much depth that we are going to explore in 14+ hours of videos.\nBelow are the course contents of this course:\nSection 1- Introduction\nThis section gets you to get started with the setup. Download resources files for code along.\nSection 2- Python Crash Course\nThis section introduces you to the basics of Python programming.\nSection 3- Numpy Introduction\nThis section is optional, you may skip it but I would recommend you to watch it if you are not comfortable with NumPy.\nSection 4- Pandas Introduction\nThis section introduces you to the basic concepts of Pandas. It will help you later in the course to catch up on the coding.\nSection 5- Matplotlib Introduction\nDo not skip this section. We will be using matplotlib plots extensively in the coming sections. It builds a foundation for a strong visualization of linear regression results.\nSection 6- Linear Regression Introduction\nWe will kick-start our Linear Regression learning. You will learn the basics of linear regression. You will see some examples so that you can understand how Linear Regression works and how to analyze the results.\nSection 7- Data Preprocessing for Linear Regression\nThis section is the most important section. DO NOT SKIP IT. It builds the foundation of data preprocessing for linear regression and other linear machine learning models. You will be learning, what are the techniques which we can use to improve the performance of the model. You will also learn how to check if your data is satisfying the coding of Linear Model Assumptions.\nSection 8- Machine Learning Models Interpretability and Explainer\nThis section teaches you how to open-up any machine learning models. Now you don't need to treat machine learning models as black-box, you will get to learn how to open this box and how to analyze each and every component of machine learning models.\nSection 9- Linear Regression Model Optimization\nThis section extensively uses the knowledge of previous sections so don't skip those. You will learn various techniques to improve model performance. We will show you how to do outliers removal and feature transformations.\nSection 10- Feature Selection for Linear Regression\nThis section teaches you some of the best techniques of feature selection. Feature selection reduces the model complexity and chances of model overfitting. Sometimes the model also gets trained faster but mostly depends on how many features are selected and the types of machine learning models.\nSection 11- Ridge & Lasso Regression, ElasticNet, and Nonlinear Regression\nThis section covers, various types of regression techniques. You will be seeing how to achieve the best accuracy by using the above techniques.\n\n\nBy the end of this course, your confidence will boost in creating and analyzing the Linear Regression model in Python. You'll have a thorough understanding of how to use regression modeling to create predictive models and solve real-world business problems.",
      "target_audience": [
        "Beginners python programmers.",
        "Beginners Data Science programmers.",
        "Students of Data Science and Machine Learning.",
        "Anyone interested in learning Linear Regression and Feature Selection",
        "Anyone interested about the rapidly expanding world of data science!",
        "Developers who want to work in analytics and visualization project.",
        "Anyone who wants to explore and understand data before applying machine learning."
      ]
    },
    {
      "title": "Spark Streaming - Stream Processing in Lakehouse - PySpark",
      "url": "https://www.udemy.com/course/spark-streaming-using-python/",
      "bio": "Master Spark Structured Streaming using Python (PySpark) on Azure Databricks Cloud with a end-to-end Project",
      "objectives": [
        "Real-time Stream Processing Concepts",
        "Spark Structured Streaming APIs and Architecture",
        "Working with Streaming Sources and Sinks",
        "Kafka for Data Engineers",
        "Working With Kafka Source and Integrating Spark with Kafka",
        "State-less and State-full Streaming Transformations",
        "Windowing Aggregates using Spark Stream",
        "Watermarking and State Cleanup",
        "Streaming Joins and Aggregation",
        "Handling Memory Problems with Streaming Joins",
        "Working with Azure Databricks",
        "Capstone Project - Streaming application in Lakehouse"
      ],
      "course_content": {
        "Before you start": [
          "About the Course",
          "Course Prerequisite",
          "Source Code and Other Resources",
          "Note for Students - Before Start"
        ],
        "Setup your environment": [
          "Spark Development Environments",
          "Setup your Databricks Community Cloud Environment",
          "Working in Databricks Workspace"
        ],
        "Getting Started with Spark Streaming": [
          "Batch processing to stream processing",
          "Your Spark application - Applying Best Practice",
          "Your first streaming application - Implementing Stream",
          "Stream Processing Model in Spark",
          "Create Another Streaming Application",
          "Stream Triggers",
          "Incremental Batch Processing",
          "Streaming Sources and Sinks",
          "Creating Chain of Streams"
        ],
        "Kafka for Data Engineers": [
          "An Introduction to Kafka",
          "Creating Kafka Cluster in Cloud",
          "Kafka Core Concepts",
          "Producing Data to Kafka Topic",
          "Consuming Data from Kafka Topic",
          "Working with Kafka Topic Data",
          "How to Implement Idempotence",
          "Working with Kafka Sink"
        ],
        "Streaming Aggregates and State Management": [
          "Streaming Aggregates and State Store",
          "Incremental Aggregates and Update Mode",
          "Spark Streaming Output Modes",
          "Statefull Vs Stateless Aggregation",
          "Implementing Stateless Streaming Aggregation",
          "Timebound Stateful Tumbling Window Aggregation",
          "Watermarking and State Store Cleanup",
          "Sliding Window Aggregates"
        ],
        "Working with Databricks Platform": [
          "Introduction to Databricks",
          "Creating Azure Free Account",
          "Azure Portal Overview",
          "Creating Azure Databricks Service",
          "Introduction to Azure Databricks Workspace",
          "Azure Databricks Architecture",
          "Creating Azure Databricks Cluster",
          "Introduction to Databricks Notebooks",
          "Notebooks Magic Commands and Utilities",
          "Databricks Notebooks Utilities",
          "Introduction to Databricks Unity Catalog",
          "Introduction to Databricks Workflow Jobs",
          "Introduction to Databricks Rest API",
          "Introduction to Databricks CLI"
        ],
        "Capstone Project - Implementing Real-time Project in Lakehouse": [
          "Project Scope and Background",
          "Taking out the operational requirement",
          "Storage Design",
          "Implement Data Security",
          "Implement Resource Policies",
          "Decouple Data Ingestion",
          "Design Bronze Layer",
          "Design Silver and Gold Layer",
          "Setup your source control",
          "Setup your environment",
          "Create a development workspace",
          "Create and Configure Storage Layer",
          "Create Unity Catalog Metastore",
          "Create Catalog and External Locations",
          "Start Coding",
          "Test your code",
          "Load historical data",
          "Ingest into bronze layer",
          "Process the silver layer",
          "Handling multiple updates",
          "Implementing Gold Layer",
          "Creating a run script",
          "Preparing for Integration testing",
          "Creating Test Data Producer",
          "Creating Integration Test for Batch mode",
          "Creating Integration Test for Stream mode",
          "Implementing CI CD Pipeline",
          "Develop Build Pipeline",
          "Develop Release Pipeline",
          "Creating Databricks CLI Script"
        ],
        "Final Word": [
          "An End is a new Beginning"
        ],
        "Archived - Old Course Content": [
          "Spark Development Environment",
          "Windows User - Spark Installation Prerequisites",
          "Windows User - Installing Apache Spark",
          "Windows User - Setup and test your IDE",
          "Mac User - Installing Apache Spark",
          "Mac User - Setup and test your IDE",
          "Install and run Apache Kafka",
          "Stream processing model in Spark",
          "Working with Files and Directories",
          "Streaming Sources, Sinks and Output Mode",
          "Fault Tolerance and Restarts",
          "Introduction to Stream Processing",
          "Spark Streaming APIs - DStream Vs Structured Streaming",
          "Creating your first stream processing application",
          "Streaming from Kafka as a Source",
          "Working with Kafka Sinks",
          "Multi-query Streams Application",
          "Kafka Serialization and Deserialization for Spark",
          "Creating Kafka AVRO Sinks",
          "Working with Kafka AVRO Source",
          "Stateless Vs Statefull transformations",
          "Event time and Windowing",
          "Tumbling Window aggregate",
          "Watermarking your windows",
          "Watermark and output modes",
          "Sliding Window",
          "Joining Stream to static source",
          "Joining Stream to another Stream",
          "Streaming Watermark",
          "Streaming Outer Joins",
          "Final Word"
        ]
      },
      "requirements": [
        "Spark Fundamentals and exposure to Spark Dataframe APIs",
        "Programming Knowledge Using Python Programming Language"
      ],
      "description": "About the Course\nI am creating Apache Spark and Databricks - Stream Processing in Lakehouse using the Python Language and PySpark API. This course will help you understand Real-time Stream processing using Apache Spark and Databricks Cloud and apply that knowledge to build real-time stream processing solutions. This course is example-driven and follows a working session-like approach. We will take a live coding approach and explain all the needed concepts.\nCapstone Project\nThis course also includes an End-To-End Capstone project. The project will help you understand the real-life project design, coding, implementation, testing, and CI/CD approach.\nWho should take this Course?\nI designed this course for software engineers willing to develop a Real-time Stream Processing Pipeline and application using Apache Spark. I am also creating this course for data architects and data engineers who are responsible for designing and building the organization’s data-centric infrastructure. Another group of people is the managers and architects who do not directly work with Spark implementation. Still, they work with those implementing Apache Spark at the ground level.\nSpark Version used in the Course.\nThis Course is using the Apache Spark 3.5. I have tested all the source code and examples used in this Course on Azure Databricks Cloud using Databricks Runtime 14.1.",
      "target_audience": [
        "Software Engineers and Architects who are willing to design and develop a Bigdata Engineering Projects using Apache Spark and Databricks Cloud",
        "Programmers and developers who are aspiring to grow and learn Data Engineering using Apache Spark and Databricks Cloud"
      ]
    },
    {
      "title": "Computer Vision Masterclass",
      "url": "https://www.udemy.com/course/computer-vision-masterclass/",
      "bio": "Learn in practice everything you need to know about Computer Vision! Build projects step by step using Python!",
      "objectives": [
        "Understand the basic intuition about Cascade and HOG classifiers to detect faces",
        "Implement face detection using OpenCV and Dlib library",
        "Learn how to detect other objects using OpenCV, such as cars, clocks, eyes, and full body of people",
        "Compare the results of three face detectors: Haarcascade, HOG (Histogram of Oriented Gradients) and CNN (Convolutional Neural Networks)",
        "Detect faces using images and the webcam",
        "Understand the basic intuition about LBPH algorithm to recognize faces",
        "Implement face recognition using OpenCV and Dlib library",
        "Recognize faces using images and the webcam",
        "Understand the basic intuition about KCF and CSRT algorithms to perform object tracking",
        "Learn how to track objects in videos using OpenCV library",
        "Learn everything you need to know about the theory behind neural networks, such as: perceptron, activation functions, weight update, backpropagation, gradient descent and a lot more",
        "Implement dense neural networks to classify images",
        "Learn how to extract pixels and features from images in order to build neural networks",
        "Learn the theory behind convolutional neural networks and implement them using Python and TensorFlow",
        "Implement transfer learning and fine tuning to get incredible results when classifying images",
        "Use convolutional neural networks to classify the following emotions in images and videos: happy, anger, disgust, fear, surprise and neutral",
        "Compress images using linear and convolutional autoencoders",
        "Detect objects in images in videos using YOLO, one of the most powerful algorithms today",
        "Recognize gestures and actions in videos using OpenCV",
        "Learn how to create hallucinogenic images with Deep Dream",
        "Learn how to revive famous artists with style transfer",
        "Create images that don't exist in the real world with GANs (Generative Adversarial Networks)",
        "Implement image segmentation do extract useful information from images and videos"
      ],
      "course_content": {
        "Introduction": [
          "Course content",
          "Google Drive folder"
        ],
        "Face detection": [
          "Plan of attack",
          "Images and pixels",
          "Cascade classifier - intuition",
          "Loading and pre-processing the image",
          "Face detection with Haarcascade and OpenCV",
          "Haarcascades parameters 1",
          "Haarcascades parameters 2",
          "Eye detection with haarcascades",
          "HOMERWORK – detecting other objects",
          "Homework solution",
          "HOG (Histrograms of Oriented Gradients) – intuition",
          "Face detection with HOG and Dlib",
          "Face detection with CNN and Dlib",
          "HOMEWORK – Haarcascade x HOG x CNN",
          "Homework solution",
          "Anaconda and PyCharm",
          "Face detection on the webcam",
          "Additional reading"
        ],
        "Face recognition": [
          "Plan of attack",
          "LBPH algorithm - intuition",
          "Loading the faces dataset",
          "Preprocessing the images",
          "Training the LBPH classifier",
          "Recognizing faces with LBPH",
          "Evaluating the LBPH classifier",
          "LBPH parameters",
          "LBPH parameters – implementation",
          "Detecting facial points",
          "Detecting facial descriptors 1",
          "Detecting facial descriptors 2",
          "Calculating distances between faces",
          "Recognizing faces with Dlib 1",
          "Recognizing faces with Dlib 2",
          "HOMEWORK",
          "Homework solution",
          "Face recognition on the webcam",
          "Additional reading"
        ],
        "Object tracking": [
          "Plan of attack",
          "Object tracking vs. object detection",
          "KCF and CSRT algorithms",
          "Object tracking with KCF",
          "Object tracking with CSRT",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ],
        "Neural networks for image classification": [
          "Plan of attack",
          "Biological fundamentals",
          "Artificial neuron",
          "Perceptron",
          "Weight update 1",
          "Weight update 2",
          "Introduction to multilayer neural networks",
          "Activation functions",
          "Hidden layer activation 1",
          "Hidden layer activation 2",
          "Output layer activation",
          "Error calculation (loss function)",
          "Basic algorithm",
          "Gradient descent and derivative",
          "Output layer delta",
          "Hidden layer delta",
          "Backpropagation and learning rate",
          "Weight update with backprogation 1",
          "Weight update with backprogation 2",
          "Bias, error and multiple outputs",
          "Hidden layers",
          "Output layer with categorical data",
          "Stochastic gradient descent",
          "Deep learning",
          "Pixels and neural networks",
          "Importing the libraries",
          "Extracting pixels from images 1",
          "Extracting pixels from images 2",
          "Extracting pixels from images 3",
          "Extracting pixels from images 4",
          "Normalizing the data",
          "Creating the train and test sets",
          "Building and training the neural network",
          "Evaluating the neural network",
          "Saving and loading the network",
          "Classifying one single image",
          "Extracting features from images",
          "Feature extraction with OpenCV 1",
          "Feature extraction with OpenCV 2",
          "Feature extraction with OpenCV 3",
          "Feature extraction with OpenCV 4",
          "Feature extraction with OpenCV 5",
          "Creating the train and test sets",
          "Building and training the neural network",
          "Evaluating the neural network",
          "Saving, loading and classifying one single image",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ],
        "Convolutional neural networks for image classification": [
          "Plan of attack",
          "Introduction to convolutional neural networks",
          "Convolutional operation",
          "Pooling",
          "Flattening",
          "Dense neural network",
          "Importing the libraries",
          "Loading the images",
          "Creating the train and test dataset",
          "Building and training the neural network",
          "Evaluating the neural network",
          "Saving and loading the network",
          "Classifying one single image",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ],
        "Transfer learning and fine tuning": [
          "Plan of attack",
          "Transfer learning – intuition",
          "Importing the libraries and dataset",
          "Creating the train and test dataset",
          "Pre-trained neural network",
          "Creating the custom dense layer",
          "Building and training the neural network",
          "Evaluating the neural network",
          "Fine tuning – intuition",
          "Fine tuning – implementation and evaluation",
          "Saving, loading and classifying one single image",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ],
        "Neural networks for classification of emotions": [
          "Plan of attack",
          "Importing the libraries and images",
          "Creating the train and test dataset",
          "Building and training the neural network",
          "Saving and loading the model",
          "Evaluating the neural network",
          "Classifying one single image",
          "Classifying multiple images",
          "Classifying emotions in videos",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ],
        "Autoencoders": [
          "Plan of attack",
          "Autoencoders – intuition",
          "Importing the libraries and dataset",
          "Visualizing the images",
          "Preprocessing the images",
          "Building and training a linear autoencoder",
          "Encoding the images",
          "Decoding the images",
          "Encoding and decoding the test images",
          "Convolutional autoencoders 1",
          "Convolutional autoencoders 2",
          "Convolutional autoencoders 3",
          "Convolutional autoencoders 4",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ],
        "Object detection with YOLO": [
          "Plan of attack",
          "YOLO – intuition",
          "Downloading and compiling Darknet",
          "Testing the detector",
          "Darknet and GPU",
          "Threshold and ext_output parameters",
          "Detecting objects in videos",
          "HOMEWORK",
          "Homework solution",
          "Additional reading"
        ]
      },
      "requirements": [
        "Programming logic",
        "Basic Python programming"
      ],
      "description": "Computer Vision is a subarea of Artificial Intelligence focused on creating systems that can process, analyze and identify visual data in a similar way to the human eye. There are many commercial applications in various departments, such as: security, marketing, decision making and production. Smartphones use Computer Vision to unlock devices using face recognition, self-driving cars use it to detect pedestrians and keep a safe distance from other cars, as well as security cameras use it to identify whether there are people in the environment for the alarm to be triggered.\nIn this course you will learn everything you need to know in order to get in this world. You will learn the step-by-step implementation of the 14 (fourteen) main computer vision techniques. If you have never heard about computer vision, at the end of this course you will have a practical overview of all areas. Below you can see some of the content you will implement:\nDetect faces in images and videos using OpenCV and Dlib libraries\nLearn how to train the LBPH algorithm to recognize faces, also using OpenCV and Dlib libraries\nTrack objects in videos using KCF and CSRT algorithms\nLearn the whole theory behind artificial neural networks and implement them to classify images\nImplement convolutional neural networks to classify images\nUse transfer learning and fine tuning to improve the results of convolutional neural networks\nDetect emotions in images and videos using neural networks\nCompress images using autoencoders and TensorFlow\nDetect objects using YOLO, one of the most powerful techniques for this task\nRecognize gestures and actions in videos using OpenCV\nCreate hallucinogenic images using the Deep Dream technique\nCombine style of images using style transfer\nCreate images that don't exist in the real world with GANs (Generative Adversarial Networks)\nExtract useful information from images using image segmentation\nYou are going to learn the basic intuition about the algorithms and implement some project step by step using Python language and Google Colab",
      "target_audience": [
        "Beginners who are starting to learn Computer Vision",
        "Undergraduate students who are studying subjects related to Artificial Intelligence",
        "People who want to solve their own problems using Computer Vision",
        "Students who want to work in companies developing Computer Vision projects",
        "People who want to know all areas inside Computer Vision, as well as know the problems that these techniques are able to solve",
        "Anyone interested in Artificial Intelligence or Computer Vision",
        "Data scientists who want to grow their portfolio",
        "Professionals who want to understand how to apply Computer Vision to real projects"
      ]
    },
    {
      "title": "Artificial Intelligence for Simple Games",
      "url": "https://www.udemy.com/course/artificial-intelligence-for-simple-games/",
      "bio": "Learn how to use powerful Deep Reinforcement Learning and Artificial Intelligence tools on examples of AI simple games!",
      "objectives": [
        "SOLVE THE TRAVELLING SALESMAN PROBLEM",
        "Understand and implement Genetic Algorithms",
        "Get the general AI framework",
        "Understand how to use this tool to your own projects",
        "SOLVE A COMPLEX MAZE",
        "Understand and implement Q-Learning",
        "Get the right Q-Learning intuition",
        "Understand how to use this tool to your own projects",
        "SOLVE MOUNTAIN CAR FROM OPENAI GYM",
        "Understand and implement Deep Q-Learning",
        "Build Artificial Neural Networks with Keras",
        "Use the environments provided in OpenAI Gym",
        "Understand how to use this tool to your own projects",
        "SOLVE SNAKE",
        "Understand and implement Deep Convolutional Q-Learning",
        "Build Convolutional Neural Networks with Keras",
        "Understand how to use this tool to your own projects"
      ],
      "course_content": {
        "Installation": [
          "Installing Anaconda"
        ],
        "Get the materials": [
          "Get the materials",
          "EXTRA: Learning Path"
        ],
        "Genetic Algorithms Intuition": [
          "Plan of Attack",
          "The DNA",
          "The Fitness Function",
          "The Population",
          "The Selection",
          "The Crossover",
          "The Mutation"
        ],
        "Genetic Algorithms Practical": [
          "Step 1 - The Introduction",
          "Step 2 - Importing the libraries",
          "Step 3 - Creating the bots",
          "Step 4 - Initializing the random DNA",
          "Step 5 - Building the Crossover method",
          "Step 6 - Random Partial Mutations 1",
          "Step 7 - Random Partial Mutations 2",
          "Step 8 - Initializing the main code",
          "Step 9 - Creating the first population",
          "Step 10 - Starting the main loop",
          "Step 11 - Evaluating the population",
          "Step 12 - Sorting the population",
          "Step 13 - Adding best previous bots to the population",
          "Step 14 - Filling in the rest of the population",
          "Step 15 - Displaying the results",
          "Step 16 - Running the code"
        ],
        "Q-Learning": [
          "Q-Learning Intuition: Plan of Attack",
          "Q-Learning Intuition: What is Reinforcement Learning?",
          "Q-Learning Intuition: The Bellman Equation",
          "Q-Learning Intuition: The Plan",
          "Q-Learning Intuition: Markov Decision Process",
          "Q-Learning Intuition: Policy vs Plan",
          "Q-Learning Intuition: Living Penalty",
          "Q-Learning Intuition: Q-Learning Intuition",
          "Q-Learning Intuition: Temporal Difference",
          "Q-Learning Intuition: Q-Learning Visualization"
        ],
        "Q-Learning Practical": [
          "Step 1 - Introduction",
          "Step 2 - Importing the libraries",
          "Step 3 - Defining the parameters",
          "Step 4 - Environment and Q-Table initialization",
          "Step 5 - Preparing the Q-Learning process 1",
          "Step 6 - Preparing the Q-Learning process 2",
          "Step 7 - Starting the Q-Learning process",
          "Step 8 - Getting all playable actions",
          "Step 9 - Playing a random action",
          "Step 10 - Updating the Q-Value",
          "Step 11 - Displaying the results",
          "Step 12 - Running the code"
        ],
        "Deep Q-Learning with ANNs": [
          "Deep Q-Learning Intuition: Plan of Attack",
          "Deep Q-Learning Intuition: Step 1",
          "Deep Q-Learning Intuition: Step 2",
          "Deep Q-Learning Intuition: Experience Replay",
          "Deep Q-Learning Intuition: Action Selection Policies"
        ],
        "Deep Q-Learning Practical": [
          "Step 1 - Introduction",
          "Step 2 - Brain - Importing the libraries",
          "Step 3 - Brain - Building the Brain class",
          "Step 4 - Brain - Creating the Neural Network",
          "Step 5 - DQN Memory - Initializing the Experience Replay Memory",
          "Step 6 - DQN Memory - Remembering new experience",
          "Step 7 - DQN Memory - Getting the batches of inputs and targets",
          "Step 8 - DQN Memory - Initializing the inputs and the targets",
          "Step 9 - DQN Memory - Extracting transitions from random experiences",
          "Step 10 - DQN Memory - Updating the inputs and the targets",
          "Step 11 - Training - Importing the libraries",
          "Step 12 - Training - Setting the parameters",
          "Step 13 - Training - Initializing the environment, the brain and dqn",
          "Step 14 - Training - Starting the main loop",
          "Step 15 - Training - Starting to play the game",
          "Step 16 - Training - Taking an action",
          "Step 17 - Training - Updating the Environment",
          "Step 18 - Training - Adding new experience, training the AI, updating cur. state",
          "Step 19 - Training - Lowering epsilon and displaying the results",
          "Step 20 - Running the code"
        ],
        "Deep Convolutional Q-Learning": [
          "Deep Convolutional Q-Learning Intuition: Plan of Attack",
          "Deep Convolutional Q-Learning Intuition: Deep Convolutional Q-Learning Intuition",
          "Deep Convolutional Q-Learning Intuition: Eligibility Trace"
        ],
        "Deep Convolutional Q-Learning Practical": [
          "Step 1 - Introduction",
          "Step 2 - Brain - Importing the libraries",
          "Step 3 - Brain - Starting building the Brain class",
          "Step 4 - Brain - Creating the neural network",
          "Step 5 - Brain - Building a method that will load a model",
          "Step 6 - DQN - Building the Experience Replay Memory",
          "Step 7 - Training - Importing the libraries",
          "Step 8 - Training - Defining the parameters",
          "Step 9 - Training - Initializing the Environment the Brain and the DQN",
          "Step 10 - Training - Building a function to reset the current state",
          "Step 11 - Training - Starting the main loop",
          "Step 12 - Training - Resetting the Environment and starting to play the game",
          "Step 13 - Training - Selecting an action to play",
          "Step 14 - Training - Updating the environment",
          "Step 15 - Training - Remembering new experience and training the AI",
          "Step 16 - Training - Updating the score and current state",
          "Step 17 - Training - Updating the epsilon and saving the model",
          "Step 18 - Training - Displaying the results",
          "Step 19 - Testing - Importing the libraries",
          "Step 20 - Testing - Defining the parameters",
          "Step 21 - Testing - Initializing the Environment and the Brain",
          "Step 22 - Testing Restting current and next state and starting the main loop",
          "Step 23 - Testing - Resetting the game and starting to play the game",
          "Step 24 - Testing - Selecting an action to play",
          "Step 25 - Updating the environment and current state",
          "Step 26 - Running the code"
        ]
      },
      "requirements": [
        "High school maths",
        "Basic knowledge of programming, such as \"if\" conditions, \"for\" and \"while\" loops, etc."
      ],
      "description": "Ever wish you could harness the power of Deep Learning and Machine Learning to craft intelligent bots built for gaming?\n\n\nIf you’re looking for a creative way to dive into Artificial Intelligence, then ‘Artificial Intelligence for Simple Games’ is your key to building lasting knowledge.\n\n\nLearn and test your AI knowledge of fundamental DL and ML algorithms using the fun and flexible environment of simple games such as Snake, the Travelling Salesman problem, mazes and more.\n\n\n1. Whether you’re an absolute beginner or seasoned Machine Learning expert, this course provides a solid foundation of the basic and advanced concepts you need to build AI within a gaming environment and beyond.\n\n2. Key algorithms and concepts covered in this course include: Genetic Algorithms, Q-Learning, Deep Q-Learning with both Artificial Neural Networks and Convolutional Neural Networks.\n\n\n3. Dive into SuperDataScience’s much-loved, interactive learning environment designed to build knowledge and intuition gradually with practical, yet challenging case studies.\n\n4. Code flexibility means that students will be able to experiment with different game scenarios and easily apply their learning to business problems outside of the gaming industry.\n\n‘AI for Simple Games’ Curriculum\n\n\nSection #1 — Dive into Genetic Algorithms by applying the famous Travelling Salesman Problem to an intergalactic game. The challenge will be to build a spaceship that travels across all planets in the shortest time possible!\n\n\nSection #2 — Learn the foundations of the model-free reinforcement learning algorithm, Q-Learning. Develop intuition and visualization skills, and try your hand at building a custom maze and design an AI able to find its way out.\n\nSection #3 — Go deep with Deep Q-Learning. Explore the fantastic world of Neural Networks using the OpenAI Gym development environment and learn how to build AIs for many other simple games!\n\n\nSection #4 — Finish off the course by building your very own version of the classic game, Snake! Here you’ll utilize Convolutional Neural Networks by building an AI that mimics the same behavior we see when playing Snake.",
      "target_audience": [
        "Anyone interested in beginning their AI journey",
        "Anyone interested in creating an AI for games",
        "Anyone looking for flexible tools to solve many kinds of Artificial Intelligence problems",
        "A data science enthusiast looking to expand their knowledge of AI"
      ]
    },
    {
      "title": "Apache Airflow: Complete Hands-On Beginner to Advanced Class",
      "url": "https://www.udemy.com/course/apache-airflow-course/",
      "bio": "Learn Apache Airflow step-by-step. Real-Life Data Pipelines & Quizzes Included. Learn by Doing!",
      "objectives": [
        "Core and Advanced Concepts in Airflow through Real-World Examples",
        "Architecture Components of Apache Airflow",
        "How to Set Up Connections to External Resources",
        "How to Load and Analyse Data in a Data Warehouse using Airflow",
        "How to Schedule PySpark jobs using Apache Airflow",
        "How to Extend Airflow with Custom Operators and Sensors",
        "How to Test Airflow DAGs and Operators",
        "How to Deploy Airflow Instances with Different Executors",
        "How to Set Up Error Tracking and Monitoring"
      ],
      "course_content": {
        "Introduction": [
          "Your Airflow Journey",
          "What is Apache Airflow?",
          "Comparing Airflow to Other Tools",
          "Course Prerequisites",
          "Extra: Install Conda (Virtual Environment Manager)",
          "Quiz: Airflow Basics"
        ],
        "Getting Started with Apache Airflow": [
          "Components of Airflow",
          "Install Airflow on MacOS (Video)",
          "Install Airflow on MacOS (Guide)",
          "Install Airflow on Linux",
          "Install Airflow on Windows (Video)",
          "Install Airflow on Windows (Guide)",
          "Install and Run Airflow with Docker",
          "Run Airflow Locally",
          "Introduction to the Airflow UI",
          "Introduction to the Airflow CLI",
          "Quiz: Airflow Setup"
        ],
        "Core Concepts in Apache Airflow": [
          "What are DAGs?",
          "What are Default Arguments?",
          "What are Tasks and Operators?",
          "How to Define Dependencies?",
          "Quiz: Core Concepts"
        ],
        "Loading Data to a Data Warehouse": [
          "Use Case",
          "Set Up",
          "Connections",
          "Load Data from Storage to BigQuery",
          "Run SQL Query in BigQuery",
          "Use a Hook to List Storage Objects",
          "Cross-Task Communication (XComs)",
          "Jinja Templating and Macros",
          "Variables",
          "Quiz: Advanced Concepts"
        ],
        "Analysing Data using PySpark": [
          "Use Case",
          "Set Up",
          "Create Dataproc Hadoop Cluster",
          "Branching",
          "Submit a PySpark Job",
          "Subdags",
          "Trigger Rules",
          "DAG Documentation",
          "Quiz: Advanced Concepts"
        ],
        "Extending Airflow with Custom Plugins": [
          "Create a Custom Operator",
          "Create a Custom Sensor",
          "Run Custom Plugins",
          "Quiz: Custom Plugins"
        ],
        "Testing Airflow DAGs": [
          "Load Test DAGs",
          "Unit Test DAGs and Operators",
          "Unit Test Custom Operators",
          "Quiz: Testing"
        ],
        "Airflow in Production": [
          "Executors",
          "Configure Local Executor",
          "Configure Celery Executor",
          "Service Level Agreements (SLAs)",
          "Security: Authentication, Roles, Encryption",
          "Write Logs to a Remote Location",
          "Monitor Airflow with StatsD, Prometheus and Grafana",
          "Error Tracking with Sentry",
          "Managed Airflow Services",
          "Qiuz: Airflow in Production"
        ],
        "Finale": [
          "Clean Up in Google Cloud",
          "Additional Resources",
          "What's Next?"
        ]
      },
      "requirements": [
        "Intermediate Python programming knowledge",
        "Beginner SQL knowledge",
        "Beginner Docker knowledge",
        "Having Git, Docker and Conda (or other Virtual Environment Manager) installed on your machine"
      ],
      "description": "Hi there, my name is Alexandra Abbas. I’m an Apache Airflow Contributor and a Google Cloud Certified Data Engineer & Architect with over 3 years experience as a Data Engineer.\nAre you struggling to learn Apache Airflow on your own? In this course I will teach you Airflow in a practical manner, with every lecture comes a full coding screencast. By the end of the course you will be able to use Airflow professionally and add Airflow to your CV.\nThis course includes 50 lectures and more than 4 hours of video, quizzes, coding exercises as well as 2 major real-life projects that you can add to your Github portfolio!\nYou will learn:\nHow to install and set up Airflow on your machine\nBasic and advanced Airflow concepts\nHow to develop complex real-life data pipelines\nHow to interact with Google Cloud from your Airflow instance\nHow to extend Airflow with custom operators and sensors\nHow to test Airflow pipelines and operators\nHow to monitor your Airflow instance using Prometheus and Grafana\nHow to track errors with Sentry\nHow to set up and run Airflow in production\nThis course is for beginners. You do not need any previous knowledge of Apache Airflow, Data Engineering or Google Cloud. We will start right at the beginning and work our way through step by step.\nYou will get lifetime access to over 50 lectures plus corresponding cheat sheets, datasets and code base for the lectures!",
      "target_audience": [
        "Data Engineers",
        "Data Scientists",
        "Python Developers Interested in Data Engineering",
        "Data Analysts with Python Programming Knowledge"
      ]
    },
    {
      "title": "Bayesian Machine Learning in Python: A/B Testing",
      "url": "https://www.udemy.com/course/bayesian-machine-learning-in-python-ab-testing/",
      "bio": "Data Science, Machine Learning, and Data Analytics Techniques for Marketing, Digital Media, Online Advertising, and More",
      "objectives": [
        "Use adaptive algorithms to improve A/B testing performance",
        "Understand the difference between Bayesian and frequentist statistics",
        "Apply Bayesian methods to A/B testing"
      ],
      "course_content": {
        "Introduction and Outline": [
          "What's this course all about?",
          "Where to get the code for this course",
          "How to succeed in this course"
        ],
        "The High-Level Picture": [
          "Real-World Examples of A/B Testing",
          "What is Bayesian Machine Learning?"
        ],
        "Bayes Rule and Probability Review": [
          "Review Section Introduction",
          "Probability and Bayes' Rule Review",
          "Calculating Probabilities - Practice",
          "The Gambler",
          "The Monty Hall Problem",
          "Maximum Likelihood Estimation - Bernoulli",
          "Click-Through Rates (CTR)",
          "Maximum Likelihood Estimation - Gaussian (pt 1)",
          "Maximum Likelihood Estimation - Gaussian (pt 2)",
          "CDFs and Percentiles",
          "Probability Review in Code",
          "Probability Review Section Summary",
          "Remedial: Statistics vs Machine Learning",
          "Suggestion Box"
        ],
        "Traditional A/B Testing": [
          "Confidence Intervals (pt 1) - Intuition",
          "Confidence Intervals (pt 2) - Beginner Level",
          "Confidence Intervals (pt 3) - Intermediate Level",
          "Confidence Intervals (pt 4) - Intermediate Level",
          "Confidence Intervals (pt 5) - Intermediate Level",
          "Confidence Intervals Code",
          "Hypothesis Testing - Examples",
          "Statistical Significance",
          "Hypothesis Testing - The API Approach",
          "Hypothesis Testing - Accept Or Reject?",
          "Hypothesis Testing - Further Examples",
          "Z-Test Theory (pt 1)",
          "Z-Test Theory (pt 2)",
          "Z-Test Code (pt 1)",
          "Z-Test Code (pt 2)",
          "A/B Test Exercise",
          "Classical A/B Testing Section Summary"
        ],
        "Bayesian A/B Testing": [
          "Section Introduction: The Explore-Exploit Dilemma",
          "Applications of the Explore-Exploit Dilemma",
          "Epsilon-Greedy Theory",
          "Calculating a Sample Mean (pt 1)",
          "Epsilon-Greedy Beginner's Exercise Prompt",
          "Designing Your Bandit Program",
          "Epsilon-Greedy in Code",
          "Comparing Different Epsilons",
          "Optimistic Initial Values Theory",
          "Optimistic Initial Values Beginner's Exercise Prompt",
          "Optimistic Initial Values Code",
          "UCB1 Theory",
          "UCB1 Beginner's Exercise Prompt",
          "UCB1 Code",
          "Bayesian Bandits / Thompson Sampling Theory (pt 1)",
          "Bayesian Bandits / Thompson Sampling Theory (pt 2)",
          "Thompson Sampling Beginner's Exercise Prompt",
          "Thompson Sampling Code",
          "Thompson Sampling With Gaussian Reward Theory",
          "Thompson Sampling With Gaussian Reward Code",
          "Exercise on Gaussian Rewards",
          "Why don't we just use a library?",
          "Nonstationary Bandits",
          "Bandit Summary, Real Data, and Online Learning",
          "(Optional) Alternative Bandit Designs"
        ],
        "Bayesian A/B Testing Extension": [
          "More about the Explore-Exploit Dilemma",
          "Confidence Interval Approximation vs. Beta Posterior",
          "Adaptive Ad Server Exercise"
        ],
        "Practice Makes Perfect": [
          "Intro to Exercises on Conjugate Priors",
          "Exercise: Die Roll",
          "The most important quiz of all - Obtaining an infinite amount of practice"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (FAQ by Student Request)": [
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Python 2 vs Python 3"
        ]
      },
      "requirements": [
        "Probability (joint, marginal, conditional distributions, continuous and discrete random variables, PDF, PMF, CDF)",
        "Python coding with the Numpy stack"
      ],
      "description": "This course is all about A/B testing.\nA/B testing is used everywhere. Marketing, retail, newsfeeds, online advertising, and more.\nA/B testing is all about comparing things.\nIf you’re a data scientist, and you want to tell the rest of the company, “logo A is better than logo B”, well you can’t just say that without proving it using numbers and statistics.\nTraditional A/B testing has been around for a long time, and it’s full of approximations and confusing definitions.\nIn this course, while we will do traditional A/B testing in order to appreciate its complexity, what we will eventually get to is the Bayesian machine learning way of doing things.\nFirst, we’ll see if we can improve on traditional A/B testing with adaptive methods. These all help you solve the explore-exploit dilemma.\nYou’ll learn about the epsilon-greedy algorithm, which you may have heard about in the context of reinforcement learning.\nWe’ll improve upon the epsilon-greedy algorithm with a similar algorithm called UCB1.\nFinally, we’ll improve on both of those by using a fully Bayesian approach.\nWhy is the Bayesian method interesting to us in machine learning?\nIt’s an entirely different way of thinking about probability.\nIt’s a paradigm shift.\nYou’ll probably need to come back to this course several times before it fully sinks in.\nIt’s also powerful, and many machine learning experts often make statements about how they “subscribe to the Bayesian school of thought”.\nIn sum - it’s going to give us a lot of powerful new tools that we can use in machine learning.\nThe things you’ll learn in this course are not only applicable to A/B testing, but rather, we’re using A/B testing as a concrete example of how Bayesian techniques can be applied.\nYou’ll learn these fundamental tools of the Bayesian method - through the example of A/B testing - and then you’ll be able to carry those Bayesian techniques to more advanced machine learning models in the future.\nSee you in class!\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nProbability (joint, marginal, conditional distributions, continuous and discrete random variables, PDF, PMF, CDF)\nPython coding: if/else, loops, lists, dicts, sets\nNumpy, Scipy, Matplotlib\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students and professionals with a technical background who want to learn Bayesian machine learning techniques to apply to their data science work"
      ]
    },
    {
      "title": "Data Science:Data Mining & Natural Language Processing in R",
      "url": "https://www.udemy.com/course/data-science-datamining-natural-language-processing-in-r/",
      "bio": "Harness the Power of Machine Learning in R for Data/Text Mining, & Natural Language Processing with Practical Examples",
      "objectives": [
        "Perform the most important pre-processing tasks needed prior to machine learning in R",
        "Carry out data visualization in R",
        "Use machine learning for unsupervised classification in R",
        "Carry out supervised learning by building classification and regression models in R",
        "Evaluate the accuracy of supervised machine learning algorithms and compare their performance in R",
        "Carry out sentiment analysis using text data in R"
      ],
      "course_content": {
        "INTRODUCTION TO THE COURSE: The Key Concepts and Software Tools": [
          "Introduction",
          "Data and Scripts For the Course",
          "Introduction to R and RStudio",
          "Start with Rattle",
          "Troubleshooting For Rattle",
          "Conclusion to Section 1"
        ],
        "Reading in Data from Different Sources in R": [
          "Read in Data from CSV and Excel Files",
          "Read Data from a Database",
          "Read Data from JSON",
          "Read in Data from Online CSVs",
          "Read in Data from Online HTML Tables-Part 1",
          "Read in Data from Online HTML Tables-Part 2",
          "Read Data from Other Sources",
          "Conclusions to Section 2"
        ],
        "Exploratory Data Analysis and Data Visualization in R": [
          "Remove NAs",
          "More Data Cleaning",
          "Exploratory Data Analysis(EDA): Basic Visualizations with R",
          "More Exploratory Data Analysis with xda",
          "Introduction to dplyr for Data Summarizing-Part 1",
          "Introduction to dplyr for Data Summarizing-Part 2",
          "Data Exploration & Visualization With dplyr & ggplot2",
          "Pre-Processing Dates-Part 1",
          "Pre-Processing Dates-Part 2",
          "Plotting Temporal Data in R",
          "Twist in the (Temporal) Data",
          "Associations Between Quantitative Variables- Theory",
          "Testing for Correlation",
          "Evaluate the Relation Between Nominal Variables",
          "Cramer's V for Examining the Strength of Association Between Nominal Variable",
          "Section 3 Quiz"
        ],
        "Data Mining for Patterns and Relationships": [
          "What is Data Mining?",
          "Association Mining with Apriori",
          "Apriori with Real Data",
          "Visualize the Rules",
          "Association Mining with Eclat",
          "Eclat with Real Data"
        ],
        "Machine Learning for Data Science": [
          "How is Machine Learning Different from Statistical Data Analysis?",
          "What is Machine Learning (ML) About? Some Theoretical Pointers"
        ],
        "Unsupervised Classification- R": [
          "K-means Clustering",
          "Fuzzy K-Means Clustering",
          "Weighted K-Means Clustering",
          "Hierarchical Clustering in R",
          "Expectation-Maximization (EM) in R",
          "Use Rattle for Unsupervised Clustering",
          "Conclusions to Section 6",
          "Section 6 Quiz"
        ],
        "Dimension Reduction": [
          "Dimensionality Reduction-theory",
          "PCA",
          "Removing Highly Correlated Predictor Variables",
          "Variable Selection Using LASSO Regression",
          "Variable Selection With FSelector",
          "Boruta Analysis for Feature Selection",
          "Conclusions to Section 7",
          "Section 7 Quiz"
        ],
        "Supervised Learning Theory": [
          "Some Basic Supervised Learning Concepts",
          "Pre-processing for Supervised Learning"
        ],
        "Supervised Learning: Classification": [
          "Binary Classification",
          "What are GLMs?",
          "Logistic Regression Models as Binary Classifiers",
          "Linear Discriminant Analysis (LDA)",
          "Binary Classifier with PCA",
          "Obtain Binary Classification Accuracy Metrics",
          "Multi-class Classification Models",
          "Our Multi-class Classification Problem",
          "Classification Trees",
          "More on Classification Tree Visualization",
          "Decision Trees",
          "Random Forest (RF) classification",
          "Examine Individual Variable Importance for Random Forests",
          "GBM Classification",
          "Support Vector Machines (SVM) for Classification",
          "More SVM for Classification",
          "Conclusions to Section 9",
          "Section 9 Quiz"
        ],
        "Supervised Learning: Regression": [
          "Ridge Regression in R",
          "LASSO Regression in R",
          "Generalized Additive Models (GAMs) in R",
          "Boosted GAMs",
          "MARS Regression",
          "CART-Regression Trees in R",
          "Random Forest (RF) Regression",
          "GBM Regression",
          "Compare Models",
          "Conclusions to Section 10"
        ]
      },
      "requirements": [
        "Keen interest in learning about data science and data mining",
        "Keen interest in mining and deriving insights from text data",
        "Should have prior experience of using R and RStudio",
        "Should be able to install and read in packages in R",
        "Prior exposure to the principles of statistical data analysis , data visualization and summarizing in R will be beneficial but not necessary"
      ],
      "description": "MASTER DATA SCIENCE, TEXT MINING AND NATURAL LANGUAGE PROCESSING IN R:\nLearn to carry out pre-processing, visualization and machine learning tasks such as: clustering, classification and regression in R. You will be able to mine insights from text data and Twitter to give yourself & your company a competitive edge.\nLEARN FROM AN EXPERT DATA SCIENTIST  WITH +5 YEARS OF EXPERIENCE:\nMy name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation).\nI have several years of experience in analyzing real life data from different sources using data science related techniques and producing publications for international peer reviewed journals. Over the course of my research I realized almost all the R data science courses and books out there do not account for the multidimensional nature of the topic and use data science interchangeably with machine learning.\nThis gives students an incomplete knowledge of the subject. Unlike other courses out there, we are not going to stop at machine learning. We will also cover data mining, web-scraping, text mining and natural language processing along with mining social media sites like Twitter and Facebook for text data.\nNO PRIOR R OR STATISTICS/MACHINE LEARNING KNOWLEDGE IS REQUIRED:\nYou’ll start by absorbing the most valuable R Data Science basics and techniques. I use easy-to-understand, hands-on methods to simplify and address even the most difficult concepts in R.\nMy course will help you implement the methods using real data obtained from different sources. Many courses use made-up data that does not empower students to implement R based data science in real life. After taking this course, you’ll easily use packages like caret, dplyr to work with real data in R. You will also learn to use the common NLP packages to extract insights from text data.\nI will even introduce you to some very important practical case studies - such as detecting loan repayment and tumor detection using machine learning. You will also extract tweets pertaining to trending topics and analyze their underlying sentiments and identify topics with Latent Dirichlet allocation. With this Powerful All-In-One R Data Science course, you’ll know it all: visualization, stats, machine learning, data mining, and neural networks!\nThe underlying motivation for the course is to ensure you can apply R based data science on real data into practice today. Start analyzing data for your own projects, whatever your skill level and Impress your potential employers with actual examples of your data science projects.\nHERE IS WHAT YOU WILL GET:\n(a) This course will take you from a basic level to performing some of the most common advanced data science techniques using the powerful R based tools.\n(b) Equip you to use R to perform the different exploratory and visualization tasks for data modelling.\n(c) Introduce you to some of the most important machine learning concepts in a practical manner such that you can apply these concepts for practical data analysis and interpretation.   (d) You will get a strong understanding of some of the most important data mining, text mining and natural language processing techniques.\n(e) & You will be able to decide which data science techniques are best suited to answer your research questions and applicable to your data and interpret the results.\n\n\nMore Specifically, here's what's covered in the course:\nGetting started with R, R Studio and Rattle for implementing different data science techniques\nData Structures and Reading in Pandas, including CSV, Excel, JSON, HTML data.\nHow to Pre-Process and “Wrangle” your R data by removing NAs/No data, handling conditional data, grouping by attributes..etc\nCreating data visualizations like histograms, boxplots, scatterplots, barplots, pie/line charts, and MORE\nStatistical analysis, statistical inference, and the relationships between variables.\nMachine Learning, Supervised Learning, & Unsupervised Learning in R\nNeural Networks for Classification and Regression\nWeb-Scraping using R\nExtracting text data from Twitter and Facebook using APIs\nText mining\nCommon Natural Language Processing techniques such as sentiment analysis and topic modelling\nWe will spend some time dealing with some of the theoretical concepts related to data science. However, majority of the course will focus on implementing different techniques on real data and interpret the results.\nAfter each video you will learn a new concept or technique which you may apply to your own projects.\nAll the data and code used in the course has been made available free of charge and you can use it as you like. You will also have access to additional lectures that are added in the future for FREE.\nJOIN THE COURSE NOW!",
      "target_audience": [
        "Students wishing to learn practical data science and machine learning in R",
        "Students wishing to learn the underlying theory and application of data mining in R",
        "Students interested in obtaining/mining data from sources such as Twiter",
        "Students interested in pre-processing and visualizing real life data",
        "Students wishing to analyze and derive insights from text data",
        "Students interested in learning basic text mining and Natural Language Processing (NLP) in R"
      ]
    },
    {
      "title": "Google Bard Generative AI Masterclass : Certification Course",
      "url": "https://www.udemy.com/course/google-bard-generative-ai-masterclass-certification-course/",
      "bio": "Unlock your creative potential and master the art of AI content creation with Google Bard in this step-by-step course",
      "objectives": [
        "Introduction to Generative Artificial Intelligence and to Google Bard",
        "Steps on how to write prompts and generate response",
        "More features of Google Bard : Exporting the Response, Getting the Python Scripts, managing the activity etc",
        "Tips to get better responses (Prompt Engineering)"
      ],
      "course_content": {},
      "requirements": [
        "No experience needed, we will cover everything from scratch."
      ],
      "description": "Welcome to our comprehensive course on Google Bard! Whether you're a beginner or looking to enhance your content creation skills, this course is perfect for you.\nThroughout the course, you'll dive into the core principles of content creation, AI, techniques of using Google Bard to write prompts and get responses etc. Through practical exercises and real-world examples, you'll develop a versatile skill set that will elevate your content creation abilities.\n\n\nTop Reasons why you should learn Google Bard :\n1. It is the most advanced generative AI tool currently in market\n2. It has got features unavailable in other tools like ChatGPT, and more features will roll out soon.\n3. Its applications are wide and it can be used in many disciplines including digital marketing, sales, coding etc.\n\n\nTop Reasons why you should take this course :\n1. This course covers what is generative AI\n2. It then introduces you to Google Bard and we begin with writing prompts\n3. We cover all the features available in Bard\n4. We also look at tips to improve responses from Google Bard\n5. This course will be updated regularly with new content and features as they roll out and announcements will be made via email to all the students.\n\n\nUpon completion of the course, you'll receive a certificate that validates your expertise in Google Bard. This certificate serves as a valuable addition to your professional portfolio, that you can showcase on LinkedIn.\nDon't miss this opportunity to take your content creation skills to the next level. Enroll today and start your journey towards becoming a skilled AI professional in Google Bard!",
      "target_audience": [
        "Content Creators interested in exploring new forms of writing",
        "Researchers and academics interested in AI and machine learning",
        "Developers and programmers interested in working with AI and LLMs",
        "Students and learners interested in exploring emerging technologies",
        "Anybody using ChatGPT and other generative AI tools, looking to explore the capabilities of Google Bard"
      ]
    },
    {
      "title": "Artificial General Intelligence AGI : An Introductory course",
      "url": "https://www.udemy.com/course/artificial-general-intelligence-agi-an-introductory-course/",
      "bio": "Learn the basics of Artificial General Intelligence AGI with our course, and know the next steps to be in the AI race !",
      "objectives": [
        "Foundations of AI : What is AI and its Types",
        "Introduction to AGI : Understanding the basics",
        "Latest trends in AGI and the roadmap",
        "AGI benefits, risks and challenges"
      ],
      "course_content": {
        "Introduction": [
          "Welcome",
          "What is Artificial Intelligence AI",
          "Types of AI",
          "What is Artificial General Intelligence AGI",
          "AI vs AGI"
        ],
        "Understanding AGI in Detail": [
          "What capabilities would turn AI to AGI",
          "Examples of AGI",
          "AGI Technology - Approaches",
          "Latest Developments in AGI"
        ],
        "AGI Challenges and Risks": [
          "Challenges to reach AGI",
          "Risks of AGI",
          "Why AGI is not yet achieved"
        ],
        "Steps towards AGI": [
          "What you should be doing towards AGI",
          "Emergence of an AGI Community"
        ],
        "End Notes": [
          "Congratulations and Thankyou"
        ]
      },
      "requirements": [
        "No experience needed, this is a beginner friendly course !"
      ],
      "description": "Welcome to the future of technology, where the possibilities are endless and the potential for greatness is unlimited! Are you ready to be a part of this exciting journey? Then look no further, because we have the course for you - Artificial General Intelligence!\nOur course is designed to equip you with the knowledge and skills you need to be at the forefront of the AI revolution. You will learn about the history of AI, how it works, and the various types of AI that exist. But that's just the beginning! We'll take you on a deep dive into the most cutting-edge research and developments in the advanced field of AI : that is AGI. We talk about the benefits, risks and challenges that AGI faces, and what you, as an individual, can do to keep up with this emerging trend.\nWith the rise of AI, there has never been a better time to explore the possibilities and potential of AGI. From self-driving cars to personalized medicine, AGI has the power to transform our world and revolutionize industries across the board. And with our course, you'll be at the forefront of this exciting revolution.\nBy enrolling in our course, you'll have the opportunity to learn with other bright minds in the industry, as well as earn a certification that will demonstrate your knowledge and expertise to future employers. So what are you waiting for? Enroll now and join us in shaping the future of technology!",
      "target_audience": [
        "Professionals interested in Artificial Intelligence",
        "Organizations looking to enter the AI space",
        "Students learning about AI and its advancements",
        "AI / ML / Data Science Practitioners looking to develop the skills further in the same field"
      ]
    },
    {
      "title": "Learn Machine learning & AI [Beginner to advanced content]",
      "url": "https://www.udemy.com/course/machine-learning-and-ai-with-hands-on-projects/",
      "bio": "Get well versed with Machine learning and AI by working on Hands-on Projects.",
      "objectives": [
        "Machine Learning and its applications",
        "Building End to End Machine Learning Projects",
        "Deployment of Machine & Deep learning algorithms"
      ],
      "course_content": {
        "Project 1 - AI Project Web application for Object Identification": [
          "Project Requirements & Django Project Creation",
          "Backend creation using pretrained Keras model - Resnet50",
          "Adding Form to django App & uploading image",
          "Integrating web application with deep learning backend"
        ],
        "Project 2 - Deep Learning Project : Dog Breed Prediction (Kaggle dataset)": [
          "Dog Breed Prediction"
        ],
        "AI Project : End to End Credit Card Fraud Detection": [
          "AI Project : Fraud Detection -Introduction",
          "Module 1 : Agenda",
          "Module 1 : Understanding Objective",
          "Module 1: Kafka for gathering live credit card data",
          "Module 1 : Cassandra for storing data",
          "Module 1: Solution System Design",
          "Module 2: Agenda",
          "Module 2: System Requirements",
          "Module 2: Java Installation",
          "Module 2: Kafka Installation",
          "Module 2: Anaconda Installation",
          "Module 2: Docker Installation",
          "Module 2: Cassandra Installation",
          "Module 3: Agenda",
          "Module 3: Loading & Preprocessing of Data",
          "Module 3 : Sampling Techniques",
          "Module 3: Estimators to be considered",
          "Module 3: Connecting transformers & estimators using Pipeline",
          "Module 4: Agenda",
          "Module 4: Code Flow",
          "Module 4: Class to connect Cassandra",
          "Module 4: Class code for Model Config & Training",
          "Module 4: Hyperparameter Tuning",
          "Module 4: Stitching all components",
          "Module 5: Agenda",
          "Module 5: Model Persistence & Selection",
          "Module 5: Revisiting Solution",
          "Module 5 : Model behind Kafka Consumer",
          "Module 5: Model behind REST using Flask"
        ]
      },
      "requirements": [
        "Python",
        "scikit-learn",
        "Machine learning",
        "Deep Learning",
        "Flask"
      ],
      "description": "Do you feel overwhelmed going through all the AI and Machine learning study materials?\nThese Machine learning and AI projects will get you started with the implementation of a few very interesting projects from scratch.\nThe first one, a Web application for Object Identification will teach you to deploy a simple machine learning application.\nThe second one, Dog Breed Prediction will help you building & optimizing a model for dog breed prediction among 120 breeds of dogs. This is built using Deep Learning libraries.\nLastly, Credit Card Fraud detection is one of the most commonly used applications in the Finance Industry. We talk about it from development to deployment. Each of these projects will help you to learn practically.\n\n\nWho's teaching you in this course?\nI am Professional Trainer and consultant for Languages C, C++, Python, Java, Scala, Big Data Technologies - PySpark, Spark using Scala Machine Learning & Deep Learning- sci-kit-learn, TensorFlow, TFLearn, Keras, h2o and delivered at corporates like GE, SCIO Health Analytics, Impetus, IBM Bangalore & Hyderabad, Redbus, Schnider, JP Morgan - Singapore & HongKong, CISCO, Flipkart, MindTree, DataGenic, CTS - Chennai, HappiestMinds, Mphasis, Hexaware, Kabbage. I have shared my knowledge that will guide you to understand the holistic approach towards ML.\n\n\nHere are a few reasons for you to pursue a career in Machine Learning:\n1) Machine learning is a skill of the future – Despite the exponential growth in Machine Learning, the field faces skill shortage. If you can meet the demands of large companies by gaining expertise in Machine Learning, you will have a secure career in a technology that is on the rise.\n2) Work on real challenges – Businesses in this digital age face a lot of issues that Machine learning promises to solve. As a Machine Learning Engineer, you will work on real-life challenges and develop solutions that have a deep impact on how businesses and people thrive. Needless to say, a job that allows you to work and solve real-world struggles gives high satisfaction.\n3) Learn and grow – Since Machine Learning is on the boom, by entering into the field early on, you can witness trends firsthand and keep on increasing your relevance in the marketplace, thus augmenting your value to your employer.\n4) An exponential career graph – All said and done, Machine learning is still in its nascent stage. And as the technology matures and advances, you will have the experience and expertise to follow an upward career graph and approach your ideal employers.\n5) Build a lucrative career– The average salary of a Machine Learning engineer is one of the top reasons why Machine Learning seems a lucrative career to a lot of us. Since the industry is on the rise, this figure can be expected to grow further as the years pass by.\n6) Side-step into data science – Machine learning skills help you expand avenues in your career. Machine Learning skills can endow you with two hats- the other of a data scientist. Become a hot resource by gaining expertise in both fields simultaneously and embark on an exciting journey filled with challenges, opportunities, and knowledge.\n\nMachine learning is happening right now. So, you want to have an early bird advantage of toying with solutions and technologies that support it. This way, when the time comes, you will find your skills in much higher demand and will be able to secure a career path that’s always on the rise.\n\nPractical Learning !!\nProject-based learning has proven to be one of the most effective ways to engage students and provide a practical application for what they’re learning and  it provides opportunities for students to collaborate or drive their learning, but it also teaches them skills such as problem-solving and helps to develop additional skills integral to their future, such as critical thinking and time management\n\nBy pursuing this course you will able to understand the concept of Machine learning at the next level you will also get to know about Artificial intelligence and that will boost your skill set to be a successful ML engineer.\n\nEnroll now, see you in class!!\n\nHappy learning!\nTeam Edyoda",
      "target_audience": [
        "Python developers looking for a future in machine learning and Artificial Intelligence (AI).",
        "Machine learning learners looking for live projects to learn from.",
        "Python and Machine learning developers looking for hands on projects to work on Artificial Intelligence (AI)."
      ]
    },
    {
      "title": "LangChain For Generative AI: Using OpenAI LLMs in Python",
      "url": "https://www.udemy.com/course/langchain-for-developers-using-openai-llms-in-python/",
      "bio": "Learn how to connect LangChain to OpenAI to work with LLMs in Python through practical examples.",
      "objectives": [
        "Learn how to work with Langchain in Python",
        "Learn how to build Langchain Agents",
        "Learn how embeddings work and how to work with a vector store in Langchain",
        "Understand how large language models (LLMs) & embeddings work",
        "Learn how to connect Langchain to OpenAI's API suite"
      ],
      "course_content": {
        "Introduction to Langchain & LLMs": [
          "What is Langchain?",
          "Understanding LLMs",
          "Installing Langchain & Hello World Example"
        ],
        "Langchain Models": [
          "Different Types of Supported Models",
          "Working with LLMs",
          "Chat Models In Langchain",
          "What Are Embeddings?",
          "Using OpenAI Text Embeddings to Analyze Sentiment",
          "Google Colab Notebook For Langchain Models"
        ],
        "Prompting & Parsing In Langchain": [
          "Prompting Best Practices - Formatting, Few Shot Prompting, & CoT",
          "Using Langchain's Built-in Prompt Templates",
          "Output Parsers in Langchain",
          "Google Colab Notebook for Prompt Templates & Output Parsers"
        ],
        "Memory, Chaining, & Indexes": [
          "Managing Chatbot Memory in Langchain",
          "What is Chaining?",
          "How To Build Chains in Langchain",
          "Langchain Document Loaders & Vectorstores"
        ],
        "Langchain Agents": [
          "What are Langchain Agents?",
          "Working With Langchain Agents",
          "Building An Arxiv Summarizer Agent",
          "Google Colab Notebook for Langchain Agents"
        ],
        "Querying Your Data Using Chat Models": [
          "Document Loaders For Different Data Types & Sources",
          "Document Splitting: Exploring Different Methods",
          "Recap on Vector Stores",
          "Advanced Retrieval Methods",
          "Querying Your Data with Chat Models"
        ],
        "Projects For Applying Advanced Querying Methods": [
          "Scaling Up Our Arxiv Research Bot",
          "Hooking Up Our Chatbot to Wikipedia"
        ],
        "Function Calling in LangChain": [
          "LangChain Function Calling & LCEL",
          "Building Agents with Tools",
          "Building An Economics Chatbot with Tools",
          "Google Colab Notebook For This Section",
          "Bonus Lecture: Free AI Research Newsletter Offering"
        ]
      },
      "requirements": [
        "Little programming experience. This course is for beginners"
      ],
      "description": "This course is designed to empower developers, this comprehensive guide provides a practical approach to integrating LangcChain with OpenAI and effectively using Large Language Models (LLMs) in Python.\nIn the course's initial phase, you'll gain a robust understanding of what Langchain is, its functionalities and components, and how it synergizes with data sources and LLMs. We'll briefly dive into understanding LLMs, their architecture, training process, and various applications. We'll set up your environment with a hands-on installation guide and a 'Hello World' example using Google Colab.\nSubsequently, we'll explore the LangChain Models, covering different types such as LLMs, Chat Models, and Embeddings. We'll guide you through loading the OpenAI Chat Model, connecting LangChain to Huggingface Hub models, and leveraging OpenAI's Text Embeddings.\nThe course advances to the essential aspect of Prompting & Parsing in LangChain, focusing on best practices, delimiters, structured formats, and effective use of examples and Chain of Though Reasoning (CoT).\nThe following sections focus on the concepts of Memory, Chaining, and Indexes in LangChain, enabling you to handle complex interactions with ease. We will study how you can adjust the memory of a chatbot, the significance of Chaining, and the utility of Document Loaders & Vector Stores.\nFinally, you'll delve into the practical implementation of LangChain Agents, with a demonstration of a simple agent and a walkthrough of building an Arxiv Summarizer Agent.\nBy the end of this course, you'll have become proficient in using LangChain with OpenAI LLMs in Python, marking a significant leap in your developer journey. Ready to power up your LLM applications? Join us in this comprehensive course!",
      "target_audience": [
        "Beginner developers looking to advance their knowledge of LLMs and Langchain",
        "Data Scientists looking to learn how to build with Langchain & LLMs"
      ]
    },
    {
      "title": "Statistics for MBA/ Business statistics explained by example",
      "url": "https://www.udemy.com/course/statistics-by-example/",
      "bio": "Statistics Made Easy by Excel Simulations. Master most important concepts of introductory statistics through simulation",
      "objectives": [
        "By the end of this course, you should become very comfortable with popular concepts of statistics",
        "You should know the genesis of popular statistical concepts",
        "You should know how you apply it in business problem",
        "You should have the required course material for referral"
      ],
      "course_content": {
        "Probability and Expectations": [
          "Welcome Note",
          "Section Overview",
          "Relative Frequency and Probability with Excel simulation - view to understand it",
          "How to download excel files etc.",
          "Probability Example of rolling one and two dice",
          "Probability distribution function - descrete and continuous",
          "Expectation or Expected Value",
          "Expected Value of a Carnival Game",
          "Expected value of Casino Game",
          "Calculate probability, expected value etc.",
          "Section PDF"
        ],
        "Central Tendencies and Dispersion": [
          "Section Overview",
          "Arithmetic Mean",
          "Advantage n Disadvantage of Arithmatic Mean",
          "Geometric Mean and Its applicability",
          "Weighted Mean",
          "Median and Its Calculations",
          "Advantage and Applicability of Median",
          "Mode Its Advantage and Usage",
          "Dispersion: Why you shd Know",
          "Range and Its Advantage and Disadvantage",
          "Average Absolute Difference",
          "Variance and Standard deviation",
          "Note: Square Error Is Minimum around Mean",
          "Coefficient of Variance and Z statistics",
          "Exercise - caculate central tenedencies, dispersion etc.",
          "Section PDF"
        ],
        "Central Limit Theorem": [
          "Section outline",
          "Frequency Distribution",
          "Normal Distribution and its properties",
          "Real Life Example of Normal distribution",
          "Normal distribution due to aggregation",
          "CLT concepts and demo",
          "Exercise - Central limit theorem etc.",
          "Apply the concepts of CLT",
          "Validate properties of Normal Distribution",
          "Check properties of Normal Distribution",
          "Section PDF"
        ],
        "Sampling Distribution": [
          "Section outline",
          "Terms Associated with Sampling Distribution",
          "Examples of Sample Statistic",
          "Sampling distribution of Means",
          "Sampling Distribution of proportion",
          "Optional topic - Sampling distribution of means and proportions with IID series",
          "Point Estimate and Interval Estimate",
          "Intuitive Understanding and Demo of confidence Interval",
          "Formal defintions and table for confidence interval",
          "Calculation example of confidence interval for sample proportions",
          "Confidence Interval for Mean",
          "Demo of confidence Interval for Mean",
          "Example of Confidence Interval Calculation",
          "Preamble for small sample statistic",
          "Demo of T Distribution",
          "Confidence Interval Calculation Example for Small Sample",
          "Criteria of a good Estimator",
          "Exercise - Check sampling distribution concepts",
          "Section PDF"
        ],
        "Hypothesis Testing": [
          "Section Outline",
          "Business Example of Hypothesis Testing - part 01",
          "Business Example of Hypothesis Testing - part 02",
          "Introduction to Terms of Hypothesis Testing",
          "Steps of Hypothesis Testing",
          "Type I and II and Power of a test - part 01",
          "Type I and II and Power of a test - part 02",
          "Real Life Example of Type I and II error",
          "One and Tow Tail Tests",
          "P Value for I and II Tail Cases and Excel Computation",
          "Hypothesis Testing Examples 01",
          "Hypothesis Testing Examples 02",
          "Using MS Excel for Hypothesis Tests",
          "Exercise - Apply your learning of hypothesis testing",
          "Section PDF"
        ],
        "Simple Linear Regression": [
          "Section Outline",
          "Linear Relationship By Example",
          "Ordinary Least Square for Equation",
          "Understand Excel Chart Add Trendline Function",
          "Coefficient of determination",
          "Correlation Coefficient R",
          "Use of Linear Regression",
          "Linear Regression Using MS Excel Data Analysis Procedure",
          "Exercise - Apply your learning of Linear Regression",
          "Section PDF"
        ],
        "Categorical Data Analysis": [
          "Section Overview",
          "Introduction to Categorical Variable",
          "Describe Categorical data one way",
          "Describe Categorical data two way",
          "Chi Square Statistic",
          "Feel The Chi Square Statistic",
          "Degree of freedom of a cross tab",
          "Chi Square Distribution",
          "Using Excel to conduct Chi Square Test",
          "dependent and independent variable",
          "statistical technique applicability at a glance",
          "Exercise - Apply your learning of Categorical data analysis",
          "Section PDF"
        ],
        "Analysis of Variance (ANOVA)": [
          "Section Overview",
          "Scenario for ANOVA",
          "Clarity on ANOVA design",
          "Assumptions of ANOVA",
          "ANOVA Method at a Glance",
          "Calculate Between Sample Variance",
          "Calculate within Sample Variance",
          "Demo When Null Hypothesis Is True",
          "Intutive Understanding when samples are from different population",
          "F Statistics and F Distribution",
          "Conducting ANOVA using Excel",
          "Exercise - Apply your learning of ANOVA",
          "Section PDF"
        ],
        "Non Parametric Tests": [
          "Section Overview",
          "Scenario for Non Parametric Test",
          "Monotony vs Linearity",
          "Advantage n Disadvantage of Non Parametric Method",
          "Sign Test",
          "Mann Whitney U Test",
          "Kruskal Wallis H Test",
          "One Sample Run Test",
          "Spearman Rank Correlation",
          "Section PDF",
          "Closure Note"
        ]
      },
      "requirements": [
        "Familiarity with Microsoft Excel basics",
        "Students should be able to check formula used in excel after downloading"
      ],
      "description": "Most of the students of MBA (Master of business administration program) / machine learning program / computer science program hate the introductory statistics / business statistics course. The reason is that most of the instructor explain the concept in such a way that students are hardly able to relate to concept with real life situation. Hence the course becomes a nightmare for students and they look forward for just completion of semester to  get rid of the same.\n\n\nThat's why this course has been prepared through simulation and real life examples.\nThis course covers the entire syllabus of most of the business statistics / introductory statistics course of MBA (Master of Business administration) program. The explanations are so simple and intuitive that you will learn statistics for life and will love the subject.\n\n\nI recommend you to explore the course.\n\n\nWhat is the course about?\nThis course promises that students will\nLearn the statistics in a simple and interesting way\nKnow the business scenarios, where it is applied\nSee the demonstration of important concepts (simulations) in MS Excel\nPractice it in MS Excel to cement the learning\nGet confidence to answer questions on statistics\nBe ready to do more advance course like logistic regression etc.\nCourse Material\nThe course comprises of primarily video lectures.\nAll Excel file used in the course are available for download.\nThe complete content of the course is available to download in PDF format.\nHow long the course should take?\nIt should take approximately 25 hours for good grasp on the subject.\nWhy take the course\nTo understand statistics with ease\nGet crystal clear understanding of applicability\nUnderstand the subject with the context\nSee the simulation before learning the theory",
      "target_audience": [
        "MBA Students",
        "Statistics professionals",
        "Statistics students",
        "Analytics professionals",
        "Data analytics folks",
        "IT folks, Reporting Engineers who want to build their career into analytics or statistical analysis / market research"
      ]
    },
    {
      "title": "AWS Certified Machine Learning Specialty MLS-C01 [2025]",
      "url": "https://www.udemy.com/course/aws-machine-learning-a-complete-guide-with-python/",
      "bio": "Experience AWS SageMaker: A Practical Course with Hands-On Learning, Practice Tests. Deploy DeepSeek LLM & Hugging Face",
      "objectives": [
        "You will gain first-hand experience on how to train, optimize, deploy, and integrate ML in AWS cloud",
        "AWS Built-in algorithms, Bring Your Own, Ready-to-use AI capabilities",
        "Complete Guide to AWS Certified Machine Learning – Specialty (MLS-C01)",
        "Includes a high-quality Timed practice test (a lot of courses charge a separate fee for practice test)",
        "Zero Downtime Model Deployment",
        "How to Integrate and Invoke ML from your Application",
        "Automated Hyperparameter Tuning"
      ],
      "course_content": {
        "Introduction and Housekeeping": [
          "Downloadable Resources",
          "Introduction",
          "Increase the speed of learning",
          "Overview: AWS Machine Learning Specialty Exam",
          "Exam - Gap Analysis",
          "Preparation: AWS Machine Learning Specialty Exam",
          "Lab: AWS Account Setup, Free Tier Offers, Billing, and Support",
          "Lab: Set Up Billing Alerts and Delegate Access",
          "Instructions - Configure IAM Users, Setup CLI",
          "Lab: Configure IAM Users, Setup Command Line Interface (CLI)",
          "[Optional] Total Cost of Ownership between On-Premises and Cloud",
          "Benefits of Cloud Computing",
          "AWS Global Infrastructure Overview",
          "Security is Job Zero | AWS Public Sector Summit 2016 by Steve Schmidt",
          "Udemy Interview - A Deep Dive Into AWS Certifications"
        ],
        "SageMaker Housekeeping": [
          "Downloadable Resources",
          "Lab: S3 Bucket Setup",
          "Lab: Setting Up SageMaker Studio JupyterLab Instance",
          "README: Jupyter Notebook",
          "Lab: Setup SageMaker Notebook Instance",
          "Lab: Source Code Setup",
          "SageMaker Console looks different from the course videos - Why?",
          "Kaggle Data Setup"
        ],
        "Machine Learning Concepts": [
          "Introduction to Machine Learning: Key Concepts and Terminologies",
          "Data Types: Handling Mixed Data Types Effectively",
          "Lab: Setting Up a Python Notebook Environment",
          "Lab: Handling Missing Data in Python",
          "Lab: Data Visualization - Linear, Logarithmic, Quadratic, and More",
          "AWS Sample Question #2",
          "Answer to Question #2",
          "AWS Sample Question #9",
          "Answer to Sample Question #9"
        ],
        "Model Performance Evaluation": [
          "Model Performance",
          "Downloadable Resources",
          "Introduction",
          "Lab: Evaluating Regression Model Performance",
          "Lab: Evaluating Binary Classifier Performance",
          "Lab: Understanding the Confusion Matrix for Binary Classifiers",
          "Lab: Using SKLearn Confusion Matrix for Binary Classifiers",
          "Binary Classifier: Key Metrics Defined",
          "Binary Classifier: Metrics Calculation Explained",
          "Question - Why not Model 1?",
          "Binary Classifier: Understanding Area Under Curve (AUC) Metrics",
          "Lab: Evaluating Multiclass Classifiers",
          "Model Performance",
          "Model Performance Evaluation",
          "What metric is appropriate - Q&A Discussion",
          "AWS Sample Question #5",
          "Answer to Question #5"
        ],
        "SageMaker Service Overview": [
          "Downloadable Resources",
          "How is AWS SageMaker different from other ML frameworks?",
          "Introduction to SageMaker",
          "Instance Type and Pricing",
          "Save Money on SageMaker Usage",
          "DataFormat",
          "SageMaker Built-in Algorithms",
          "Popular Frameworks and Bring Your Own Algorithm",
          "Infrastructure, Pricing, Support - Review",
          "AWS Sample Question #1",
          "Answer for Sample Question #1",
          "AWS Sample Question #10",
          "Answer for Sample Question #10",
          "What does a data scientist in gaming do? By Carly Taylor"
        ],
        "SageMaker Service and SDK Changes": [
          "Overview of recent changes",
          "Model Training using Console",
          "Model Training using Python SDK",
          "Understanding Checkpointing, Incremental Training, and Transfer Learning",
          "Lab - Review the SageMaker console for Training Job",
          "SageMaker Training"
        ],
        "XGBoost - Gradient Boosted Trees": [
          "Downloadable Resources",
          "Introduction to XGBoost",
          "Lab - Data Preparation Simple Regression",
          "Lab - Training Simple Regression",
          "Lab - Data Preparation Non-linear Data set",
          "Lab - Training Non-linear Data set",
          "Exercise - Improving quality of predictions",
          "Lab - Data Preparation Bike Rental Regression",
          "Lab - Train Bike Rental Regression Model",
          "Lab - Train using Log of Count",
          "ResourceLimitExceeded Error - How to increase quota limit",
          "Lab - How to train using SageMaker's built-in XGBoost Algorithm",
          "Q&A: How does SageMaker built-in know the target variable?",
          "Lab - How to run predictions against an existing SageMaker Endpoint",
          "Q&A - XGBoost on SageMaker predicted values are not delimited consistently",
          "SageMaker Endpoint Features",
          "SageMaker Spot Instances - Save up to 90% for training jobs",
          "Lab - Multi-class Classification",
          "Lab - Binary Classification",
          "Exercise - Improve Data Quality in Diabetes dataset",
          "Question on Diabetes Data Quality Improvement",
          "Question on Diabetes model - is group mean on target the right approach?",
          "Data Leakage",
          "HyperParameter Tuning, Bias-Variance, Regularization (L1, L2)",
          "Exercise - Mushroom Classification",
          "Quiz - XGBoost",
          "Underfitting, Overfitting",
          "AWS Sample Question #8",
          "Answer to AWS Sample Question #8"
        ],
        "Invoke Model Endpoint from External Clients": [
          "Install SageMaker SDK, GIT Client, Source Code, Security Permissions",
          "IAM users for the lab",
          "Integration Overview",
          "Lab - Client to Endpoint using SageMaker SDK",
          "Lab - Client to Endpoint using Boto3 SDK",
          "Microservice - Lambda to Endpoint - Payload",
          "Lambda UI Changes",
          "Lab - Microservice - Lambda to Endpoint",
          "API Gateway - UI Changes",
          "Lab - API Gateway, Lambda, Endpoint"
        ],
        "Endpoint Changes with Zero Downtime": [
          "Downloadable Resources",
          "[Repeat] Endpoint Features, Monitoring and AutoScaling",
          "How to handle changes to production system?",
          "Lab - A/B Testing Multiple Production Variants",
          "Lab – Multi-model Endpoint",
          "Run Models at the Edge",
          "Endpoints"
        ],
        "No-Code/Low-Code Solutions in SageMaker: Canvas, Data Wrangler, and AutoPilot": [
          "Instructions: Working with the Kaggle Brazil House Rental Dataset",
          "Hands-On Lab: Analyzing Rental Amounts with SageMaker Data Wrangler",
          "Hands-On Lab: Data Transformation and Cleanup with SageMaker Data Wrangler",
          "Lab: Canvas Autopilot Model Training, Inference, and Cleanup",
          "Summary: SageMaker Data Wrangler",
          "Summary: SageMaker AutoPilot"
        ]
      },
      "requirements": [
        "Familiarity with Python",
        "AWS Account - I will walk through steps to setup one",
        "Basic knowledge of Pandas, Numpy, Matplotlib",
        "Be an active learner and use course discussion forum if you need help - Please don't put help needed items in course review"
      ],
      "description": "Learn about cloud based machine learning algorithms, how to integrate with your applications and Certification Prep\nWelcome to AWS Machine Learning Specialty Course!\nExperience AWS SageMaker:  A Practical Course with Hands-On Learning, Practice Tests  and Certification Preparation.\n***NEW:\nIn this course, you will gain practical experience with AWS SageMaker through hands-on labs that demonstrate specific concepts. We will begin by setting up your SageMaker environment. If you are new to machine learning, you will learn how to handle mixed data types, missing data, and how to verify the quality of the model. These topics are essential for machine learning practitioners and the certification exam.\nSageMaker uses containers to package algorithms and frameworks, such as Pytorch and TensorFlow. The container-based approach provides a standard interface for building and deploying your models, and it is easy to convert your model into a production application. Through a series of concise labs, you will train, deploy, and invoke your first SageMaker model.\nLike any other software project, a machine-learning solution also requires continuous improvement. We will look at how to safely incorporate new changes in a production system, perform A/B testing, and even roll back changes when necessary, all with zero downtime to your application.\nWe will also discuss emerging social trends in the fairness of machine learning and AI systems. What will you do if your users accuse your model of being racially or gender-biased? How will you handle it? In this section, we will cover the concept of fairness, how to explain a decision made by the model, different types of bias, and how to measure them.\nWe will also cover cloud security and how to protect your data and model from unauthorized use. You will learn about recommender systems and how to incorporate features such as movie and product recommendations. The algorithms you learn in the course are state-of-the-art, and tuning them for your dataset can be challenging. We will look at how to tune your model with automated tools, and you will gain experience in time series forecasting, anomaly detection, and building custom deep-learning models.\nWith the knowledge you gain in this course, and the included high-quality practice exam, you will be well-prepared to achieve the AWS Certified Machine Learning - Specialty certification. I am looking forward to meeting you and helping you succeed in this course. Thank you!",
      "target_audience": [
        "This course is designed for anyone who is interested in AWS cloud based machine learning and data science",
        "AWS Certified Machine Learning - Specialty Preparation"
      ]
    },
    {
      "title": "Time Series Analysis and Forecasting using Python",
      "url": "https://www.udemy.com/course/machine-learning-time-series-forecasting-in-python/",
      "bio": "Learn about time series analysis & forecasting models in Python |Time Data Visualization |AR|MA |ARIMA |Regression | ANN",
      "objectives": [
        "Get a solid understanding of Time Series Analysis and Forecasting",
        "Understand the business scenarios where Time Series Analysis is applicable",
        "Building 5 different Time Series Forecasting Models in Python",
        "Learn about Auto regression and Moving average Models",
        "Learn about ARIMA and SARIMA models for forecasting",
        "Use Pandas DataFrames to manipulate Time Series data and make statistical computations"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course",
          "What is Time Series Forecasting?",
          "Course Resources",
          "This is a Milestone!",
          "Quiz"
        ],
        "Time Series - Basics": [
          "Time Series Forecasting - Use cases",
          "Forecasting model creation - Steps",
          "Forecasting model creation - Steps 1 (Goal)",
          "Time Series - Basic Notations",
          "Quiz"
        ],
        "Setting up Python and Python Crash Course": [
          "Installing Python and Anaconda",
          "Course resources",
          "Opening Jupyter Notebook",
          "Introduction to Jupyter",
          "Arithmetic operators in Python: Python Basics",
          "Quick coding exercise on arithmetic operators",
          "Strings in Python: Python Basics",
          "Quick coding exercise on String operations",
          "Lists, Tuples and Directories: Python Basics",
          "Quick coding exercise on Tuples",
          "Working with Numpy Library of Python",
          "Quick coding exercise on NumPy Library",
          "Working with Pandas Library of Python",
          "Quick coding exercise on Pandas Library",
          "Working with Seaborn Library of Python",
          "Python file for additional practice",
          "Quiz"
        ],
        "Integrating ChatGPT with Python": [
          "Integrating ChatGPT with Jupyter notebook"
        ],
        "Time Series - Data Loading": [
          "Data Loading in Python"
        ],
        "Time Series - Feature Engineering": [
          "Time Series - Feature Engineering Basics",
          "Time Series - Feature Engineering in Python",
          "Quiz"
        ],
        "Time Series - Resampling": [
          "Time Series - Upsampling and Downsampling",
          "Time Series - Upsampling and Downsampling in Python",
          "Quiz"
        ],
        "Time Series - Visualization": [
          "Time Series - Visualization Basics",
          "Time Series - Visualization in Python"
        ],
        "Time Series - Transformation": [
          "Time Series - Power Transformation",
          "Moving Average",
          "Exponential Smoothing"
        ],
        "Time Series - Important Concepts": [
          "White Noise",
          "Random Walk",
          "Decomposing Time Series in Python",
          "Differencing",
          "Differencing in Python"
        ]
      },
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the sameStudents will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete course on Time Series Forecasting to drive business decisions involving production schedules, inventory management, manpower planning, and many other parts of the business., right?\nYou've found the right Time Series Forecasting and Time Series Analysis course using Python Time Series techniques. This course teaches you everything you need to know about different time series forecasting and time series analysis models and how to implement these models in Python time series.\nAfter completing this course you will be able to:\nImplement time series forecasting and time series analysis models such as AutoRegression, Moving Average, ARIMA, SARIMA etc.\nImplement multivariate time series forecasting models based on Linear regression and Neural Networks.\nConfidently practice, discuss and understand different time series forecasting, time series analysis models and Python time series techniques used by organizations\nHow will this course help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Time Series Forecasting course on time series analysis and Python time series applications.\nIf you are a business manager or an executive, or a student who wants to learn and apply forecasting models in real world problems of business, this course will give you a solid base by teaching you the most popular forecasting models and how to implement it. You will also learn time series forecasting models, time series analysis and Python time series techniques.\nWhy should you choose this course?\nWe believe in teaching by example. This course is no exception. Every Section’s primary focus is to teach you the concepts through how-to examples. Each section has the following components:\nTheoretical concepts and use cases of different forecasting models, time series forecasting and time series analysis\nStep-by-step instructions on implement time series forecasting models in Python\nDownloadable Code files containing data and solutions used in each lecture on time series forecasting, time series analysis and Python time series techniques\nClass notes and assignments to revise and practice the concepts on time series forecasting, time series analysis and Python time series techniques\n\n\nThe practical classes where we create the model for each of these strategies is something which differentiates this course from any other available online course on time series forecasting, time series analysis and Python time series techniques.\n.What makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using Analytics and we have used our experience to include the practical aspects of Marketing and data analytics in this course. They also have an in-depth knowledge on time series forecasting, time series analysis and Python time series techniques.\nWe are also the creators of some of the most popular online courses - with over 170,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts on time series forecasting, time series analysis and Python time series techniques.\nEach section contains a practice assignment for you to practically implement your learning on time series forecasting, time series analysis and Python time series techniques.\nWhat is covered in this course?\nUnderstanding how future sales will change is one of the key information needed by manager to take data driven decisions. In this course, we will deal with time series forecasting, time series analysis and Python time series techniques. We will also explore how one can use forecasting models to\nSee patterns in time series data\nMake forecasts based on models\nLet me give you a brief overview of the course\nSection 1 - Introduction\nIn this section we will learn about the course structure and how the concepts on time series forecasting, time series analysis and Python time series techniques will be taught in this course.\nSection 2 - Python basics\nThis section gets you started with Python.\nThis section will help you set up the python and Jupyter environment on your system and it'll teach\nyou how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nThe basics taught in this part will be fundamental in learning time series forecasting, time series analysis and Python time series techniques on later part of this course.\nSection 3 - Basics of Time Series Data\nIn this section, we will discuss about the basics of time series data, application of time series forecasting, and the standard process followed to build a forecasting model, time series forecasting, time series analysis and Python time series techniques.\nSection 4 - Pre-processing Time Series Data\nIn this section, you will learn how to visualize time series, perform feature engineering, do re-sampling of data, and various other tools to analyze and prepare the data for models and execute time series forecasting, time series analysis and implement Python time series techniques.\nSection 5 - Getting Data Ready for Regression Model\nIn this section you will learn what actions you need to take a step by step to get the data and then prepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment and missing value imputation.\nSection 6 - Forecasting using Regression Model\nThis section starts with simple linear regression and then covers multiple linear regression.We have covered the basic theory behind each concept without getting too mathematical about it so that you understand where the concept is coming from and how it is important. But even if you don't understand it, it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results.\nSection 7 - Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nSection 8 - Creating Regression and Classification ANN model in Python\nIn this part you will learn how to create ANN models in Python.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. We also solve a regression problem in which we try to predict house prices in a location. We will also cover how to create complex ANN architectures using functional API. Lastly we learn how to save and restore models.\nI am pretty confident that the course will give you the necessary knowledge and skills related to time series forecasting, time series analysis and Python time series techniques to immediately see practical benefits in your work place.\nGo ahead and click the enroll button, and I'll see you in lesson 1 of this course on time series forecasting, time series analysis and Python time series techniques!\nCheers\nStart-Tech Academy",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Machine Learning journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master Time Series Analysis using Python in short span of time"
      ]
    },
    {
      "title": "Salesforce CRM Analytics: Einstein & Tableau CRM",
      "url": "https://www.udemy.com/course/salesforce-crm-analytics-einstein-tableau-crm/",
      "bio": "Boost Salesforce CRM Results with Einstein & Tableau: Data Visualization, Analytics Techniques, and Best Practices",
      "objectives": [
        "Understanding of Salesforce CRM and how to use it for analytics.",
        "Utilization of Salesforce Einstein Analytics for data visualization and insights.",
        "Salesforce Tableau and its role in analytics.",
        "Integration of Tableau with Salesforce CRM for data visualization.",
        "Creating and managing dashboards in Salesforce Einstein Analytics and Tableau.",
        "Advanced analytics techniques such as data joins, advanced calculations, and data storytelling.",
        "Salesforce Einstein Discovery for predictive analytics.",
        "Real-world case studies and use cases of Salesforce CRM Analytics with Einstein & Tableau.",
        "Datasets, Lens and data recipes"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "What is Salesforce CRM analytics?",
          "Create Salesforce CRM Analytics Developer Org"
        ],
        "Analytics studio": [
          "Exploring Analytics studio",
          "Model manager",
          "Analytics Settings",
          "CRM Analytics Apps",
          "Create app using template",
          "Create Custom Blank App",
          "What’s the Difference Between Analytics Tab and Analytics Studio?"
        ],
        "Datasets": [
          "What is Dataset?",
          "Create Dataset from Salesforce data",
          "Data Flow",
          "Create a Dataset Using the Standard Case Object",
          "Create Dataset from Csv upload",
          "Create Dataset from External system"
        ],
        "Data Recipe": [
          "What is Data Recipe?",
          "Creating Data Recipe",
          "Joining multiple data sets",
          "Types of Joins",
          "Appending data",
          "Filter",
          "Aggregate",
          "Data Transformation",
          "Creating formulas in recipe",
          "Update node",
          "Create a data recipe in CRM Analytics"
        ],
        "Lens": [
          "What is Lens?",
          "Exploring lens options",
          "Creating Einstein Prediction Story",
          "Maps Chart",
          "Add filter",
          "Conditional formatting",
          "Showing trend",
          "Drill down data in lens",
          "Scatter Chart",
          "Add dataset to lens",
          "Rename fields and values in lens",
          "Format Lens Case Table",
          "Growth% YoY - Compare Table",
          "Pivot Table",
          "Query Mode",
          "What is SAQL?",
          "Using SAQL",
          "Group data by using rollup",
          "Show Total using SAQL",
          "Assignment - SAQL"
        ],
        "Get Started with CRM Analytics Dashboards": [
          "Meet CRM Analytics Dashboard",
          "Dashboard editor",
          "Creating Dashboard",
          "Column and Donut charts",
          "Sales Rep Performance Chart",
          "Adding Time and Funnel charts",
          "Table chart",
          "Dashboard filters",
          "Adding Date and Toggle as filters",
          "Create support dashboard"
        ],
        "Dashboard Pages": [
          "Animation to dashboard with pages",
          "Updating Existing Data Set",
          "Creating case dashboard page",
          "Range Widget",
          "Navigation widget",
          "Dark theme"
        ],
        "Dashboard Templates": [
          "Smart templates"
        ],
        "Quiz": [
          "Quiz"
        ],
        "Conclusion": [
          "Conclusion"
        ]
      },
      "requirements": [
        "None"
      ],
      "description": "This course is designed for individuals who want to take their Salesforce CRM skills to the next level. With Salesforce CRM Analytics: Einstein & Tableau CRM, you'll be able to unlock the full potential of your Salesforce data and drive results for your organization.\nThe course covers everything from basics of Einstein Analytics and Tableau, to advanced techniques such as data visualization, predictive analytics and best practices. By the end of this course, you'll have a solid understanding of how to use Einstein Analytics and Tableau with Salesforce CRM to deliver powerful insights and drive business results. Whether you're a Salesforce Administrator, Business Analyst, or IT Professional, this course will equip you with the skills you need to succeed with Salesforce CRM Analytics.\nIn this course you will learn:\n\nIntroduction to Salesforce CRM and Einstein Analytics\nSetting up CRM Analytics in Salesforce\nData Visualization and dashboards in CRM Analytics\nAdvanced analytics techniques in Einstein Analytics, including predictive modeling and data joining\nDatasets, lens, and data recipes.\nCreating and managing dashboards in Tableau CRM\nData visualization best practices in CRM Analytics\nAdvanced data analysis techniques in einstein, including advanced calculations and data storytelling\nReal-world case studies and use cases of Salesforce CRM Analytics with Einstein & Tableau\nWith a focus on hands-on learning, you'll get to work with real-world examples and use cases to build a deep understanding of the key concepts and techniques.\nWhether you're looking to advance your career, increase your business acumen, or simply learn new skills, this Salesforce CRM Analytics course is the perfect place to start. With a friendly and approachable teaching style and a commitment to student success, you're sure to get the support and guidance you need to succeed. So if you're ready to unlock the power of Salesforce CRM data, enroll today and start your journey to success!",
      "target_audience": [
        "Professionals who want to increase their knowledge and skills in Salesforce CRM Analytics",
        "This course is designed for individuals who have a basic understanding of Salesforce and want to learn how to effectively use Einstein Analytics and Tableau for data visualization and analytics."
      ]
    },
    {
      "title": "Data Integration Guide",
      "url": "https://www.udemy.com/course/data-integration-guide/",
      "bio": "Learn how data can be integrated : following which principles and for what business outcomes",
      "objectives": [
        "Data Integration concepts, principles, and main features",
        "What business use cases can Data Integration address and for what business value?",
        "How to setup a new Data Integration Solution ?",
        "How to make the right choices for your project, solution and approach, ensuring successful business outcomes ?",
        "Data Integration Patterns : ESB (Enterprise Service Bus), ETL (Extract Transform Load), EDI (Electronic Data Interchange) and API",
        "Connectors and Integration Mechanisms : with Webservices, Databases, Files and Enterprise Solutions",
        "How Data Integration can empower and enable Digital Transformation",
        "Multiple examples from several industries to help you project an efficient usage of Data Integration"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "What is Data Integration ?",
          "Intro",
          "Disclaimers"
        ],
        "Benefits, Business Value, Concepts & Terminology": [
          "Benefits & Business Value",
          "Main Concepts & Terminology",
          "Main Concepts and Terminology"
        ],
        "Scope of Data Integration": [
          "Main Features",
          "Orchestration and Service Composition",
          "Data Transformation",
          "Scope of Data Integration"
        ],
        "Data Integration Patterns": [
          "Point to Point Integration Pattern",
          "ETL - Extract Transform Load Integration Pattern",
          "ESB - Enterprise Service Bus & WebServices",
          "EDI - Electronic Data Interchange",
          "Data Integration Patterns"
        ],
        "Connectors and Integration Mechanisms": [
          "Connectors : Introduction",
          "DataBase Connectors",
          "Webservices",
          "File Connectors",
          "Data Integration Connectors"
        ],
        "Security & Technical Architecture": [
          "Security",
          "Technical Architecture",
          "Security & Technical Architecture"
        ],
        "Data Integration Projects": [
          "Introduction to DIPF : Data Integration Project Framework",
          "Step 1 : Ask the right questions",
          "Step 2 : Identify business use cases and their value & feasibility",
          "Step 3 : Solution Panel",
          "Step 4 : The MVP",
          "Step 5 : The industrialization plan",
          "Step 6 : Solution & Project Methodology choices",
          "Step 7 : Project Execution",
          "Step 8 : Transition to \"Run\"",
          "DIPF Wrap Up!",
          "Data Integration Projects"
        ],
        "Data Integration Operations Management": [
          "Managing Data Integration Operations - Introduction",
          "Software Vendor Support",
          "Monitoring and Alerts",
          "Incidents Management",
          "Environment Maintenance",
          "Operations Management - Example",
          "Operations Management"
        ],
        "Quick Overview of market solutions": [
          "OpenSource vs Proprietary",
          "Solution components",
          "Licensing and Pricing Models",
          "Overview of market solutions",
          "Demo - Building a simple Data Flow with Make, Airtable and Public API"
        ],
        "Data Integration for Digital Transformation": [
          "Digital Transformation"
        ]
      },
      "requirements": [
        "Basic knowledge on IT Domain",
        "Willingness to learn new things about Data and Data Integration",
        "No programming or Database experience needed!"
      ],
      "description": "According to the World Economic Forum, at the beginning of 2020, the number of bytes in the digital universe was 40 times bigger than the number of stars in the observable universe.\nWith data volume and usages growing, the need for Data Integration is becoming more and more central topic.\nData Integration is mainly about exchanging data across multiple systems and tools. Aligned with their business strategy, organizations need data to circulate timely and accurately through their information system and the external world (internet applications, trading partners ..).  This allows organizations to answer market needs, be competitive, reduce time to market, and become data driven by easing decision making processes.\nIn this course, we are presenting a complete guide on how to identify your need of data integration, how you can architecture your solutions, execute successfully your projects and manage data integration overtime, all of this in order to bring tangible business value and to support your business.\nIn more details we will address the following topics around Data Integration :\nWhat is Data Integration ?\nData Integration Benefits & Business Value\nMain Concepts & Features\nData Integration Paradigms & Patterns, including,\nESB, Enterprise Service Bus\nETL, Extract Transform Load\nEDI, Electronic Data Interchange\nAPI, Application Programming Interface\nConnectors for Data Integration\nWith Databases\nWith Files\nWith WebServices: SOAP, REST\nWith Enterprise Applications like SAP\nSecurity and technical architecture\nHigh availability\nData Encryption\nCloud Deployments\nData Integration Projects\nData Integration Run Operations\nQuick Overview of market solutions\nProprietary vs OpenSource\nSolution components\nLicencing and pricing models\nData Integration as Enabler for Digital Transformation\nThis course is intended to be a complete practical guide to help you understand all the aspects around Data Integration. It can help you in your career and your current activities, by bringing a complete 360° overview on Data Integration topic.\nThis course is intended to help :\nChief Information Officers\nChief Data Officers\nChief Digital Officers\nChief Analytics Officer\nHead of Data\nHead of Analytics\nIT Managers\nBusiness managers who work with Data\nData Managers\nEnterprise Architects\nData Project Managers\nDigital Projects Managers\nData Analysts\nData Specialists\nData Engineers\nData Scientists\nData Architects\nData Modelers\nIT Auditors\nInformation System Performance Analysts\nAnd also, all students and professionals who want to benefit from the big market demand in Data and this important skill!\nNo prior experience in Programming or Data Bases is needed to follow this course.\nThis course is also vendor agnostic (and independent), whether you will work with solutions like Informatica, Talend, Boomi, OpenESB, Tibco ActiveMatrix, Mulesoft, IBM Websphere, Microsoft BizTalk or other, this course is generic enough to help you in your journey regardless of the solution you use or intend to use! It will even help you make the right choice based on your requirements and constraints.\nThroughout the course, you can easily contact the instructor for any questions you have to sharpen your knowledge and have tailored made learning experience!",
      "target_audience": [
        "Data Professionals",
        "Students and Professionals passionate about Data",
        "Professionals looking for different perspective around Data and Data Integration"
      ]
    },
    {
      "title": "Object Detection Web App with TensorFlow, OpenCV and Flask",
      "url": "https://www.udemy.com/course/object-detection-web-app-with-tensorflow-opencv-and-flask/",
      "bio": "Build an Object Detection Model from Scratch using Deep Learning and Transfer Learning",
      "objectives": [
        "Object Detection",
        "Computer Vision with OpenCV",
        "Deploying Object Detection Model as Flask Web app",
        "Using Pre-trained Machine Learning Models",
        "Python Project Development",
        "Training using Tensorflow"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "About the Instructor"
        ],
        "Getting Started": [
          "Resources and Folder Structure",
          "Installation"
        ],
        "Object Detection Project": [
          "Importing Packages and Data",
          "Detection",
          "Routing and Image Upload",
          "Object Detection and Binding",
          "Implementation and Output",
          "Youtube (Bonus)"
        ]
      },
      "requirements": [
        "Beginner Knowledge of Python"
      ],
      "description": "Object detection is one of the most powerful applications of computer vision, forming the foundation for technologies like self-driving cars, intelligent surveillance, image captioning, and robotics. If you’ve ever wondered how machines can identify and label objects in real time, this course will guide you through building your very own object detection web application from scratch.\nYou will begin by exploring the fundamentals of object detection and how it differs from traditional image classification. Using Python, TensorFlow, and OpenCV, you’ll work with the pre-trained COCO dataset to detect everyday objects efficiently. The course guides you step by step through the integration of pre-trained models with OpenCV to process images, highlight detected objects, and label them with confidence scores.\nOnce the core detection system is ready, you’ll take it to the next level by deploying it as a Flask-based web application. This will allow users to upload images via a web interface and instantly see detected objects marked on their pictures. Through this process, you’ll not only strengthen your machine learning and deep learning knowledge but also gain valuable experience in web deployment — a skill in high demand today.\nBy the end of the course, you will:\nUnderstand object detection concepts and real-world use cases\nWork with TensorFlow and OpenCV for computer vision tasks\nUse transfer learning and pre-trained models effectively\nDeploy your model as a Flask web app for practical use\nNo prior deep learning expertise is required — just Python basics and curiosity. Start your journey into computer vision today!",
      "target_audience": [
        "Beginners who are curious about Object Detection",
        "Beginners who are curious about Computer Vision",
        "Python Developers",
        "Computer Vision enthusiasts"
      ]
    },
    {
      "title": "Practical Data Literacy for Leaders",
      "url": "https://www.udemy.com/course/practical-data-literacy-masterclass/",
      "bio": "Make better data-driven business-decisions. Present your data like a PRO. Learn the key data and analytics terminology",
      "objectives": [
        "How to effectively read and understand data",
        "How to analyze data like a PRO",
        "How to effectively argue with data",
        "How to present data",
        "How to make data-drive business-decisions",
        "How to use statistical and analytical methods within your analysis"
      ],
      "course_content": {},
      "requirements": [
        "Basic excel knowledge will be helpful",
        "Some of the practical exercises will require Excel to be performed"
      ],
      "description": "Learn quickly with my Practical Data Literacy for Leaders course that covers the latest best practices from the Data Industry\n\nThis is a practical course! There is a course project that we will follow as we learn all the below topics.\nIn this course you will learn:\n1. What is Data Literacy and why it is important\n2. How to ask the correct data questions\n3. How to make sure you are using the correct data\n4. Analyze the quality of data that you receive\n5. How to properly summarize data\n6. How to drill-down on big datasets without missing important information\n7. What is analysis bias and how to avoid it\n8. How to derive the correct KPIs and data points\n9. How to engage SMEs with the correct data questions for best results\n10. How to make sound data-driven business-decisions\n11. How to effectively argue with data\n12. How to choose the right format for a presentation with focus on data\n13. What charts/visualizations to choose for your presentation\n14. How to use data to tell a story to your audience\n15. Learn the most important and popular data and analytics terminology so you can undetrstand and engage in any  meeting/email communication\n16. Learn key data statistics and analytics concepts\nand a lot of tips and tricks from 10+ years of experience!\n\n\nEnroll today and enjoy:\nLifetime access to the course\n5 hours of high quality, up to date video lectures\nPractical Data Literacy course with step by step instructions on how to implement the different techniques\nThanks again for checking out my course and I look forward to seeing you in the classroom!",
      "target_audience": [
        "Executives",
        "Business Leaders",
        "Managers at all levels in the organization",
        "Experienced individual contributors"
      ]
    },
    {
      "title": "Python for Data Science and Machine Learning beginners",
      "url": "https://www.udemy.com/course/python-for-data-science-and-machine-learning-beginners/",
      "bio": "A Complete Machine learning Bootcamp learn Numpy, Pandas, Matplotlib, Stats, Plotly , EDA , Scikit-learn and more!",
      "objectives": [
        "Implement Machine Learning Algorithms",
        "Use python for Data science and Machine Learning",
        "Use Numpy and multidimensional array operations",
        "Do exploratory Data analysis with pandas profiling",
        "Create complex visualization with matplotlib and plotly",
        "Use Scikit-learn for Machine Learning Task",
        "Linear Regression",
        "Random Forest and Decision Tree",
        "Statistics For Data Science and Machine Learning"
      ],
      "course_content": {},
      "requirements": [
        "No Programming background required.",
        "Basic Mathematics is added advantage"
      ],
      "description": "Hi all Its Jay I am a data scientist by profession and Instructor by passion I have around 4 years of experience as data scientist,  I started my career as analyst as gradually moved to data scientist hence  I can understand what are programming prerequisites for data scientist. This course is created for absolute beginners of data science and machine learning.  It covers all aspect of python languages required in data science machine learning and deep learning.",
      "target_audience": [
        "beginner python developer curious about data science",
        "Beginners in programming",
        "Beginners in data science",
        "Beginners in machine learning"
      ]
    },
    {
      "title": "Looker and LookML - The Complete Course for Beginners",
      "url": "https://www.udemy.com/course/looker-learning-tutorial-for-beginners/",
      "bio": "Practical Looker course for beginners that want to quickly get up to speed with Looker and LookML",
      "objectives": [
        "Learn the Looker platform and interface",
        "Learn LookML from the basics",
        "Learn best practices for using Looker and LookML",
        "Become confident in using Looker for any project",
        "Be able to create analysis using Looker"
      ],
      "course_content": {},
      "requirements": [
        "Basic understanding of SQL will help but not mandatory"
      ],
      "description": "Welcome! I am here to help you learn quickly how to use Looker and LookML\nBeginners welcome: no need to know anything about Looker and LookML!\nLooker can be challenging to learn on your own without guidance from an experienced trainer. In this course, I will walk you through every step of using Looker and LookML in an easy to undertand way for the absolute beginner.\nThis course will give you a deep understanding of Looker's functionality by using hands-on, contextual examples designed to showcase why Looker is awesome and how how to use it for any project.\n\n\nIn this Looker course you will learn:\n· How to setup a free training account with Looker\n· Get familiar with the Looker interface\n· How to build Looks in Looker\n· How to edit a Look\n· How to build a dashboard\n· The different elements of LookML\n· How to create a custom dimension\n· How to create a view from table\n· Understand the different dimension types\n· How to structure your project\n· How to use Explores\n· How to utilize the formal Looker documentation\n· How to create a model in the Development environment\n· The interface of the Development environment\nand much, much more!\n\n\nEnroll today and enjoy:\nLifetime access to the course\n9 hours of high quality, up to date video lectures\nPractical Looker course with step by step instructions on how to implement the different features\nThanks again for checking out my course and I look forward to seeing you in the classroom",
      "target_audience": [
        "Users with no or little experience with Looker and LookML"
      ]
    },
    {
      "title": "OpenAI Python API Bootcamp (2023): Learn AI, GPT, and more!",
      "url": "https://www.udemy.com/course/openai-python-api-bootcamp-learn-to-use-ai-gpt3-and-more/",
      "bio": "Discover the power of Artificial Intelligence with OpenAI's powerful API to generate text and images in projects",
      "objectives": [
        "Understand the basics of the OpenAI API",
        "Set-up an OpenAI Account for API use",
        "Learn to use OpenAI API with Python for various tasks",
        "Create potential start-up ideas through 10+ projects",
        "Integrate OpenAI API into existing Python applications for added AI capabilities",
        "Learn how to use Text Embedding with OpenAI",
        "Understand how to apply OpenAI's GPT models",
        "Discover how to use the DALLE-2 API to generate images",
        "Learn to create content and images automatically for users"
      ],
      "course_content": {
        "Welcome to the course!": [
          "FAQs, Course Files, How to Get Help",
          "Post Dev-Day Updated Zip File",
          "Let's make sure you're ready to go!",
          "Course Curriculum Overview",
          "OpenAI Overview",
          "Crash Course: How does GPT work?",
          "Crash Course: How does DALLE work?",
          "Note on changes to Free Tier",
          "OpenAI Account Set-up",
          "AI Safety and Alignment",
          "OpenAI Updates!",
          "OpenAI Pricing"
        ],
        "Natural Language to SQL": [
          "Updates on code-davinci Models",
          "Check-in On OpenAI Updates",
          "NLP to SQL - Project Overview",
          "Tabular Data Set-up",
          "Natural Language Request",
          "Text Completion API - Parameter Overview",
          "OpenAI API Call and Request Handling"
        ],
        "Automatic Exam Creator": [
          "Automatic Exam Generation - Project Overview",
          "Prompt Design",
          "Generating Exam from Prompt",
          "Q&A Extraction",
          "Exam Simulation",
          "Python Script Walkthrough"
        ],
        "Automatic Recipe and Dish Image Creator": [
          "Introduction to Recipe Project",
          "Generating Recipe Text via Completion API",
          "Generating Dish Image via Image API (DALLE)",
          "Full Python Script Walkthrough"
        ],
        "Automatic Blog Post Creator": [
          "Introduction to Automatic Blog Post Project",
          "GitHub Page Setup",
          "GitPython - Automatic Update Functions - Part One",
          "GitPython - Automatic Update Functions - Part Two",
          "OpenAI API Calls",
          "Python Script Walkthrough"
        ],
        "GPT Sentiment Analysis": [
          "Introduction to Reddit Sentiment Analysis",
          "Reddit API Set-up",
          "Reddit Comments and Titles",
          "Perform Sentiment Analysis via OpenAI"
        ],
        "Automatic Code Explainer - Docstrings": [
          "Introduction to Code Explainer",
          "OpenAI API Call - Docstring",
          "Merge Python function and Docstring",
          "Python Script Walkthrough"
        ],
        "Translation Project": [
          "Introduction to Translation News Summary Project",
          "Scraping International Newspaper Sites",
          "OpenAI Translation and Summary"
        ],
        "Fine-Tuning and ChatBots": [
          "Introduction to Fine-Tuning and Chat Bots",
          "Data Set-Up",
          "Fine-Tuning Process and Price Estimation",
          "Using the Fine-Tuned Model"
        ],
        "Text Embedding for Question and Answering": [
          "Introduction to Text Embedding",
          "Model Hallucination",
          "Document Data",
          "Token Count and Pricing",
          "Document Similarity and Context Injection"
        ]
      },
      "requirements": [
        "Python programming experience required",
        "Python Data Ecosystem (Pandas, Numpy, etc.) useful but not required",
        "Ability to sign up to OpenAI API with a credit card",
        "You should understand how to install libraries with \"pip install\" commands"
      ],
      "description": "Welcome to the best online course for learning how to leverage the power of OpenAI's Python API for AI!\nIn this comprehensive course, you will learn how to harness the power of OpenAI to build intelligent applications and solutions using Python. The OpenAI API is one of the most advanced artificial intelligence platforms available, providing a range of capabilities for natural language processing, computer vision, and more. With this API, you can create AI applications that can understand and respond to human language, generate text, perform sentiment analysis, and much more.\nThis course is project-oriented, with every section has a unique project designed to be the basis of a start-up idea! You'll gradually learn new skills as you proceed from project to project throughout the sections of the course\nBy the end of this course, you will have a solid understanding of how to use the OpenAI API with Python and will be able to integrate AI into your own projects. You will learn how to authenticate with the API, how to make API calls, and how to process and analyze the results. You will also learn how to perform NLP tasks, such as text generation and question answering, and how to use the API for building AI-powered solutions.\nDiscover the Power of AI with Real World Projects:\nNatural Language to SQL Queries\nAutomatic Blog Post Generation\nAutomated Recipe from Ingredients\nTranslation and Summarize from International Newspaper\nAutomatic Code Docstrings for Python\nFine-Tune Custom Chatbots\nPerform Sentiment Analysis on Reddit Posts\nVector Text Embed Company Documents\nThroughout the course, you will be working on real-world examples and hands-on exercises, allowing you to gain practical experience and put your new knowledge into action. You will also learn best practices for using the OpenAI API effectively, including error handling and performance optimization.\nWhether you're a software developer, data scientist, or simply interested in learning about artificial intelligence, this course is designed for you. By the end of this course, you will have a strong foundation in using the OpenAI API and will be able to apply this knowledge to build your own AI solutions.\nSo if you're ready to take your skills to the next level and explore the world of AI, enroll in this course today and start building intelligent applications with the OpenAI API and Python!",
      "target_audience": [
        "Python developers looking to understand the full potential of the latest in OpenAI Python API"
      ]
    },
    {
      "title": "Complete ArcGIS Pro Mastery: A Hands-On, Practical Course",
      "url": "https://www.udemy.com/course/complete-arcgis-pro-mastery-a-hands-on-practical-course/",
      "bio": "Mastering ArcGIS Pro: A Hands-On Journey Through Practical Applications",
      "objectives": [
        "Mastering ArcGIS Pro Interface and Tools",
        "Creating Feature Class and Geo Databases",
        "Geospatial Analysis and Visualization",
        "Digitization and Publishing Spatial Data"
      ],
      "course_content": {
        "ArcGIS Pro Configuration": [
          "Course Overview",
          "Announcement for Mac Users",
          "Announcement Regarding ArcGIS Pro Student Licensing",
          "Learn about the tool, download and install it properly",
          "ArcGIS Pro Floating License Configuration",
          "Install ArcGIS Pro (Floating License)"
        ],
        "Hands-on the Tool": [
          "Announcement",
          "Getting Started With the Tool",
          "Creating Map, Local, Global & Catalog Scenes",
          "Linking Three Map Types",
          "Project Menu",
          "Map Menu Part 1",
          "Map Menu Part 2",
          "Insert, Analysis & Edit Menus",
          "View Menu - Catalog Option (Part One)",
          "View Menu - Catalog Option (Part Two)",
          "View Menu - Catalog Option (Part Three)",
          "Project (Part 1)",
          "Project (Part 2)",
          "Project (Part 3)",
          "Project (Part 4)",
          "Project (Part 5)",
          "Selection Pane",
          "Tools Options (Part 1)",
          "Tools Options (Part 2)",
          "Tools Options (Part 3)",
          "Symbology",
          "Creating Map Layout (Part 1)",
          "Creating Map Layout (Part 2)",
          "Creating Map Layout (Part 3)",
          "Labeling Menu (Part 1)",
          "Labeling Menu (Part 2)",
          "Labeling Menu (Part 3)",
          "Analysis Menu (Tools Part 1)",
          "Analysis Menu (Tools Part 2)",
          "Analysis Menu (Tools Part 3)",
          "Analysis Menu (Tools Part 4)",
          "Analysis Menu (Tools Part 5)",
          "Publishing the map into the arcgis online and creating web map application",
          "KML2Feature + Feature2Feature + Removing ZM Values + Alter Field",
          "FieldView Saving & Labeling visibility Problems + Generate Points Along Feat",
          "Points Along Line + Show attributes of Selected features + Data Integrity",
          "Boundry Creation plus a little bit of Arcade Programming for Labels",
          "Georeferencing Technique",
          "Creating 3D Map (Part 1)",
          "Creating 3D Map (Part 2)",
          "Creating 3D Map (Part 3) Plus Map Animations",
          "Course Conclusion",
          "QUICK QUIZZ",
          "For Updates"
        ]
      },
      "requirements": [
        "No prerequisites required! All you need is a stable internet connection and a PC or laptop running the Windows operating system."
      ],
      "description": "Throughout this course, we will dive into the key features of ArcGIS Pro, guiding you through the step-by-step process of creating maps, local and global scenes, and catalog scenes. You will learn how to link different map types seamlessly and make the most of the Project, Insert, Analysis, Edit, and View menus.\nAs you progress, you will gain hands-on experience with ArcGIS Pro through an exciting project that will challenge your mapping skills. You'll explore the Selection Pane, delve into various Tool Options, and master the art of symbology to craft visually appealing and informative maps.\nUnderstanding the importance of presenting data effectively, we will cover the creation of compelling map layouts and explore the Labeling Menu to ensure your maps convey information accurately.\nBeyond basic mapping, this course will expand your capabilities by introducing you to the Analysis Menu, empowering you to perform advanced geospatial analysis. You will also learn how to publish your maps to ArcGIS Online and create engaging web map applications for sharing your work with a wider audience.\nThroughout the course, we will cover essential data processing techniques, such as KML2Feature, Feature2Feature, Removing ZM Values, and Alter Field operations. Additionally, you will discover the power of FieldView, saving, labeling visibility, generating points along features, XY coordinates, and much more.\nTo enable you to take your maps to new heights, we will explore the creation of 3D maps and delve into building captivating map animations to enhance visualization.\nAs we progress, we will tackle more advanced topics like Georeferencing Techniques and Boundary Creation. Moreover, we will touch upon Arcade Programming for Labels, giving you the skills to customize labels and expressions to meet your specific needs.\nIn the final sections of the course, we will focus on data integrity, ensuring high-quality maps, and addressing common issues faced during mapping and analysis.\nBy the end of this course, you will have gained a deep understanding of ArcGIS Pro's capabilities, enabling you to create stunning maps, perform sophisticated geospatial analysis, and present your findings effectively.\nJoin us on this exciting journey to Master ArcGIS Pro, and unlock the full potential of geospatial technology in your professional and academic pursuits. Get ready to visualize the world in new ways and make a real impact with your mapping and analysis skills!",
      "target_audience": [
        "GIS Enthusiasts: Individuals passionate about Geographic Information Systems who want to enhance their skills and knowledge in using ArcGIS Pro.",
        "Beginners: Those new to GIS and ArcGIS Pro, seeking a comprehensive introduction to geospatial data management, analysis, and visualization.",
        "GIS Professionals: Experienced GIS practitioners looking to advance their proficiency with ArcGIS Pro's advanced features and functionalities.",
        "Students and Researchers: Those pursuing studies or research related to geography, environmental sciences, urban planning, and various other fields that utilize geospatial data analysis.",
        "Urban Planners: Professionals involved in city planning, infrastructure development, and urban design, who aim to leverage ArcGIS Pro for spatial decision-making.",
        "Environmentalists: Those dedicated to environmental conservation and resource management, using ArcGIS Pro to analyze and address ecological challenges.",
        "Government and Non-Profit Organizations: Representatives seeking to utilize GIS technology for better policy-making, resource allocation, and community development.",
        "Data Analysts: Professionals who wish to expand their skillset by incorporating geospatial data analysis into their data-driven approaches.",
        "Cartographers and Visualizers: Artists and designers interested in creating captivating maps and visual representations using ArcGIS Pro's styling capabilities.",
        "Anyone Curious about GIS: Anyone with a curiosity about Geographic Information Systems and a desire to explore the power and potential of ArcGIS Pro."
      ]
    },
    {
      "title": "Practical Guide to AI & ML: Mastering Future Tech Skills",
      "url": "https://www.udemy.com/course/practical-guide-to-ai-ml/",
      "bio": "Artificial Intelligence & Machine Learning: Practical Training for Real-World Applications & Skills Development",
      "objectives": [
        "Demonstrate a solid understanding of the difference between AI, Machine Learning and Deep Learning.",
        "Clearly articulate why Large Language Models like ChatGPT and Bard are NOT intelligent.",
        "Articulate the difference between Supervised, Unsupervised, and Reinforcement Machine Learning.",
        "Explain the concept of machine learning and its relation to AI.",
        "Define artificial intelligence (AI) and differentiate it from human intelligence.",
        "Describe what Artificial Intelligence is, and what it is not.",
        "Explain what types of sophisticated software systems are not AI systems.",
        "Describe how Machine Learning is different to the classical software development approach.",
        "Compare and contrast supervised, unsupervised, and reinforcement learning.",
        "Explain Supervised and Unsupervised Machine Learning terms such as algorithms, models, labels and features.",
        "Explain Function Approximators and the role of Neural Networks as Universal Function Approximators.",
        "Explain Encoding and Decoding when using machine learning models to work with non-numeric, categorical type data.",
        "Demonstrate an intuitive understanding of Reinforcement Learning concepts such as agents, environments, rewards and goals.",
        "Identify examples of AI in everyday life and discuss their impact.",
        "Evaluate the effectiveness of different AI applications in real-world scenarios.",
        "Apply basic principles of neural networks to a hypothetical problem.",
        "Discuss the role of data in training AI models",
        "Construct a neural network model for a specified task",
        "Assess the impact of AI on job markets and skill requirements",
        "See an end-to-end, supervised machine learning process to tackle a regression problem, using Microsoft's Model Builder and ML .Net.",
        "Understand the tasks and activities that take place behind the scenes. From data preparation all the way to model training and evaluation.",
        "Understand data transformation, feature scaling, iterating through algorithms, evaluation metrics, overfitting, cross-validation and regularization.",
        "Understanding the impact of evaluation metrics on model performance, and how to check for overfitting.",
        "Understand the lasting fundamentals of machine learning that are independent of the tools or platforms one can use.",
        "Gain a deep understanding of machine learning concepts by seeing them in action, during a practical machine learning demonstration.",
        "Understand the importance of Exploratory Data Analysis (EDA) and the impact that the statistical distribution of the data has on model performance.",
        "Learn how to set up Visual Studio and to configure it to enable Model Builder, the graphical tool that will be used to demonstrate the machine learning process.",
        "Learn how to use Model Builder to train models without having to code."
      ],
      "course_content": {
        "Introducing the first half of this course: AI and Machine Learning for Beginners": [
          "Introduction and Course Outline"
        ],
        "What is Artificial Intelligence?": [
          "What is Artificial Intelligence? How intelligent is AI and ChatGPT really?",
          "Traditional Software Programmes vs AI systems vs?"
        ],
        "What is Machine Learning?": [
          "Math and Data Science replaces Traditional Programming. A regression example.",
          "Introducing Function Approximation, Neural Networks, Encoding and Decoding",
          "Supervised, Unsupervised and Reinforcement Machine Learning Models & Algorithms"
        ],
        "Deep Learning and Neural Networks": [
          "The Basics of Deep Learning and Neural Networks"
        ],
        "Artificial Intelligence Insights and Fundamentals": [
          "Machine Learning vs Traditional Programming",
          "Supervised Machine Learning Models",
          "Unsupervised Machine Learning Models",
          "Introduction to Reinforcement Learning"
        ],
        "AI Applied to Personalized Recommenders in eCommerce, Loyalty and other settings": [
          "Introduction to AI-Powered Recommender Systems",
          "From Collaborative Filtering to Next-Generation Personalized Recommendations",
          "Introducing Reinforcement Learning in Recommender Systems",
          "Real-World Case Studies of RL Applied to Recommenders"
        ],
        "Bonus: Ecosystem.ai & X-idian Webinar- Personalized Rewards & Loyalty Programmes": [
          "Introductions and downloadable resources.",
          "Q&A 1: How have customer expectations about rewards changed in recent years",
          "Q&A 2: How has the definition of “loyalty” shifted — and the role of AI?",
          "Q&A 3: What does a healthy and engaging rewards ecosystem look like?",
          "Q&A 4: How does real-time intelligence reshape rewards experiences?",
          "Q&A 5: How would real-time hyperpersonalization change member experiences?",
          "Q&A 6: How do behavioural science principles impact the way you implement AI?",
          "Real-world challenges & benefits of implementing AI in live rewards environments",
          "Pain points encountered when shifting from static to intelligent reward systems",
          "Thoughts on loyalty/rewards going forward - Part 1",
          "Thoughts on loyalty/rewards going forward - Part 2"
        ],
        "Test your knowledge now to achieve your goals!": [
          "You can do it! Maximise your score and boost your learning!"
        ],
        "Introducing the next part of this course: Practical AI with Model Builder.": [
          "Introduction, Prerequisites and Learning Outcomes",
          "Introducing Model Builder and the Approach for this Course"
        ],
        "Visual Studio and Model Builder": [
          "Download, Install and Configure Visual Studio",
          "Launch Visual Studio and Start a Coding Project"
        ]
      },
      "requirements": [
        "There are no requirements or prerequisites for this course, but the items listed below are a guide to useful background knowledge that will increase the value and benefits of this course:",
        "High school Math and a deep interest in machine learning would be highly beneficial for this series of lessons. There is no coding or complex mathematics involved in this course. If you can't remember high-school math, it will not prevent you from learning the concepts in this course.",
        "An appreciation for, but not a deep knowledge of, the importance of Mathematics and Statistics in Machine Learning.",
        "Basic computer literacy, including familiarity with operating a computer.",
        "A basic understanding of supervised machine learning is required. The student would at the very least need to understand what regression is, what features are, and what it means for a model to be trained to fit a function to input features in order to predict labels.",
        "The student needs to have a Windows machine with a few GB of free disk space to install Visual Studio, in order to replicate the machine learning process I will demonstrate. However, this is not essential.",
        "A Windows machine is ideal, but a student with a Mac will still be able to follow along. The course content is visual enough to demonstrate the concepts, without the student having to physically do the machine learning exercise."
      ],
      "description": "Unlock the Future: Dive into the World of AI and ML!\nWelcome to an extraordinary journey into the realms of Artificial Intelligence and Machine Learning. Led by AI and Technology expert Irlon Terblanche, this course is not just an educational experience; it's an adventure into the technologies shaping our future. Whether you're a curious beginner, a business leader, or an aspiring tech guru, this course promises to transform your understanding of some of the most cutting-edge topics in tech.\nWhy This Course?\nDesigned for Curiosity and Career: Tailored for both personal and professional growth, this course demystifies AI and ML, making them accessible to everyone. It's perfect for busy professionals, entrepreneurs, and anyone with a thirst for knowledge.\nNo Math Fears: We've designed the course to be inclusive, requiring no prior expertise in math or coding. It's all about understanding concepts in a friendly, approachable manner.\nLifetime Access and Flexible Learning: Learn at your pace with full lifetime access to all resources, including videos, articles, and downloadable materials.\nWhat You'll Achieve:\nGrasp the Core Concepts: Understand the difference between AI, ML, and Deep Learning. Learn what sets them apart and how they're revolutionizing industries.\nDebunk Myths: Discover why systems like ChatGPT aren't truly intelligent and explore the limitations of current AI technologies.\nPractical Skills: Gain hands-on experience with tools like Microsoft's Model Builder and ML .Net. Understand the complete machine learning process, from data preparation to model evaluation.\nReal-World Applications: See how AI and ML are being applied in various sectors. Discuss their impact on job markets and skill requirements.\nCourse Highlights:\nEngaging Video Lectures: Over 4 hours of high-quality, engaging video content that breaks down complex ideas into digestible segments.\nComprehensive Topics: From the basics of neural networks to the intricacies of supervised and unsupervised learning.\nPractical Demonstrations: Learn by doing with practical exercises and demonstrations.\nDynamic Learning Resources: An article and a downloadable resource to complement your learning journey.\nMobile and PC Access: Learn on the go or from the comfort of your living room.\nCourse Structure:\nThe course is divided into 9 comprehensive sections, each designed to build upon the last, ensuring a smooth learning curve. Starting with an introduction to AI and ML, it moves through various topics like function approximation, neural networks, and deep learning, concluding with practical demonstrations of machine learning in action.\nEnroll Now and Transform Your Understanding of AI and ML!\nJoin us on this captivating journey into AI and ML. With Irlon Terblanche's expert guidance, engaging content, and practical insights, you're not just learning; you're preparing for the future. Enroll today and be part of the AI revolution!",
      "target_audience": [
        "Business Executives and Managers: Professionals in leadership roles who are looking to understand how AI can be leveraged for strategic advantage in their organizations.",
        "Busy professionals who need a short, easy but solid understanding of AI fundamentals.",
        "Entrepreneurs and Startup Founders: Individuals who are building or planning to build businesses where AI could play a transformative role.",
        "Technology Consultants and Advisors: Professionals who provide strategic advice on technology adoption and integration.",
        "Absolute beginners who are aspiring to become Data Scientists or Machine Learning Engineers, and who are looking for the best fundamentals of artificial intelligence and machine learning.",
        "Product Managers and Developers: Those who are involved in product development and are interested in incorporating AI into new or existing products.",
        "Non-technical Professionals: Including, but not limite to Business Analysts or Marketers. Yhis course can give you all the skills you need to be able to interact with Data Scientists, Machine Learning Engineers or other AI specialists.",
        "AI and machine learning enthusiasts: This course will still be valuable because it covers extremely important fundamental concepts that are often misunderstood.",
        "This course is not for you if you have an aversion or intense dislike for Mathematics.",
        "Also, if you are looking for coding tips, technical detail about the different machine learning algorithms, back-propagation in Neural Networks, loss functions, gradient descent, policy gradient methods, etc., then these series of lessons are definitely not for you.",
        "This course is for entry-level machine learning enthusiasts, who have had some kind of theoretical introduction to machine learning, but who wants to put the theory into practice.",
        "Machine learning enthusiasts who do not have a background in Statistics, Data Science or programming, but who want to see the complexities of machine learning in practice.",
        "Machine learning enthusiasts who want to learn about complex concepts by seeing them in action, rather than by seeing a presentation.",
        "Technical beginners who want to learn solid machine learning fundamentals before progressing onto more advanced courses where a detailed knowledge of statistics, calculus and programming may be required."
      ]
    },
    {
      "title": "PyTorch for Deep Learning with Python Bootcamp",
      "url": "https://www.udemy.com/course/pytorch-for-deep-learning-with-python-bootcamp/",
      "bio": "Learn how to create state of the art neural networks for deep learning with Facebook's PyTorch Deep Learning library!",
      "objectives": [
        "Learn how to use NumPy to format data into arrays",
        "Use pandas for data manipulation and cleaning",
        "Learn classic machine learning theory principals",
        "Use PyTorch Deep Learning Library for image classification",
        "Use PyTorch with Recurrent Neural Networks for Sequence Time Series Data",
        "Create state of the art Deep Learning models to work with tabular data"
      ],
      "course_content": {
        "Course Overview, Installs, and Setup": [
          "COURSE OVERVIEW LECTURE - PLEASE DO NOT SKIP!",
          "Installation and Environment Setup"
        ],
        "COURSE OVERVIEW CONFIRMATION CHECK": [
          "DID YOU WATCH THE COURSE OVERVIEW LECTURE?"
        ],
        "Crash Course: NumPy": [
          "Introduction to NumPy",
          "NumPy Arrays",
          "NumPy Arrays Part Two",
          "Numpy Index Selection",
          "NumPy Operations",
          "Numpy Exercises",
          "Numpy Exercises - Solutions"
        ],
        "Crash Course: Pandas": [
          "Pandas Overview",
          "Pandas Series",
          "Pandas DataFrames - Part One",
          "Pandas DataFrames - Part Two",
          "GroupBy Operations",
          "Pandas Operations",
          "Data Input and Output",
          "Pandas Exercises",
          "Pandas Exercises - Solutions"
        ],
        "PyTorch Basics": [
          "PyTorch Basics Introduction",
          "Tensor Basics",
          "Tensor Basics - Part Two",
          "Tensor Operations",
          "Tensor Operations - Part Two",
          "PyTorch Basics - Exercise",
          "PyTorch Basics - Exercise Solutions"
        ],
        "Machine Learning Concepts Overview": [
          "What is Machine Learning?",
          "Supervised Learning",
          "Overfitting",
          "Evaluating Performance - Classification Error Metrics",
          "Evaluating Performance - Regression Error Metrics",
          "Unsupervised Learning"
        ],
        "ANN - Artificial Neural Networks": [
          "Introduction to ANN Section",
          "Theory - Perceptron Model",
          "Theory - Neural Network",
          "Theory - Activation Functions",
          "Multi-Class Classification",
          "Theory - Cost Functions and Gradient Descent",
          "Theory - BackPropagation",
          "PyTorch Gradients",
          "Linear Regression with PyTorch",
          "Linear Regression with PyTorch - Part Two",
          "DataSets with PyTorch",
          "Basic Pytorch ANN - Part One",
          "Basic PyTorch ANN - Part Two",
          "Basic PyTorch ANN - Part Three",
          "Introduction to Full ANN with PyTorch",
          "Full ANN Code Along - Regression - Part One - Feature Engineering",
          "Full ANN Code Along - Regression - Part 2 - Categorical and Continuous Features",
          "Full ANN Code Along - Regression - Part Three - Tabular Model",
          "Full ANN Code Along - Regression - Part Four - Training and Evaluation",
          "Full ANN Code Along - Classification Example",
          "ANN - Exercise Overview",
          "ANN - Exercise Solutions"
        ],
        "CNN - Convolutional Neural Networks": [
          "Introduction to CNNs",
          "Understanding the MNIST data set",
          "ANN with MNIST - Part One - Data",
          "ANN with MNIST - Part Two - Creating the Network",
          "ANN with MNIST - Part Three - Training",
          "ANN with MNIST - Part Four - Evaluation",
          "Image Filters and Kernels",
          "Convolutional Layers",
          "Pooling Layers",
          "MNIST Data Revisited",
          "MNIST with CNN - Code Along - Part One",
          "MNIST with CNN - Code Along - Part Two",
          "MNIST with CNN - Code Along - Part Three",
          "CIFAR-10 DataSet with CNN - Code Along - Part One",
          "CIFAR-10 DataSet with CNN - Code Along - Part Two",
          "Loading Real Image Data - Part One",
          "Loading Real Image Data - Part Two",
          "CNN on Custom Images - Part One - Loading Data",
          "CNN on Custom Images - Part Two - Training and Evaluating Model",
          "CNN on Custom Images - Part Three - PreTrained Networks",
          "CNN Exercise",
          "CNN Exercise Solutions"
        ],
        "Recurrent Neural Networks": [
          "Introduction to Recurrent Neural Networks",
          "RNN Basic Theory",
          "Vanishing Gradients",
          "LSTMS and GRU",
          "RNN Batches Theory",
          "RNN - Creating Batches with Data",
          "Basic RNN - Creating the LSTM Model",
          "Basic RNN - Training and Forecasting",
          "RNN on a Time Series - Part One",
          "RNN on a Time Series - Part Two",
          "RNN Exercise",
          "RNN Exercise - Solutions"
        ],
        "Using a GPU with PyTorch and CUDA": [
          "Why do we need GPUs?",
          "Using GPU for PyTorch"
        ]
      },
      "requirements": [
        "Understanding of Python Basic Topics (data types,loops,functions) also Python OOP recommended",
        "Be able to work through basic derivative calculations",
        "Admin Permissions on your computer (ability to download our files)"
      ],
      "description": "Welcome to the best online course for learning about Deep Learning with Python and PyTorch!\nPyTorch is an open source deep learning platform that provides a seamless path from research prototyping to production deployment. It is rapidly becoming one of the most popular deep learning frameworks for Python. Deep integration into Python allows popular libraries and packages to be used for easily writing neural network layers in Python. A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more.\nThis course focuses on balancing important theory concepts with practical hands-on exercises and projects that let you learn how to apply the concepts in the course to your own data sets! When you enroll in this course you will get access to carefully laid out notebooks that explain concepts in an easy to understand manner, including both code and explanations side by side. You will also get access to our slides that explain theory through easy to understand visualizations.\nIn this course we will teach you everything you need to know to get started with Deep Learning with Pytorch, including:\nNumPy\nPandas\nMachine Learning Theory\nTest/Train/Validation Data Splits\nModel Evaluation - Regression and Classification Tasks\nUnsupervised Learning Tasks\nTensors with PyTorch\nNeural Network Theory\nPerceptrons\nNetworks\nActivation Functions\nCost/Loss Functions\nBackpropagation\nGradients\nArtificial Neural Networks\nConvolutional Neural Networks\nRecurrent Neural Networks\nand much more!\nBy the end of this course you will be able to create a wide variety of deep learning models to solve your own problems with your own data sets.\nSo what are you waiting for? Enroll today and experience the true capabilities of Deep Learning with PyTorch! I'll see you inside the course!\n-Jose",
      "target_audience": [
        "Intermediate to Advanced Python Developers wanting to learn about Deep Learning with PyTorch"
      ]
    },
    {
      "title": "Complete Machine Learning 2025 A-Z™: 10 Real World Projects",
      "url": "https://www.udemy.com/course/complete-machine-learning-2021-with-10-real-world-projects/",
      "bio": "Complete Beginner to Expert Guide-Data Visualization,EDA,Numpy,Pandas,Math,Statistics,Matplotlib,Seaborn,Scikit,NLP-NLTK",
      "objectives": [
        "Python",
        "Machine Learning",
        "Statistics and Math",
        "Data Science",
        "Natural Language Processing",
        "Data Analysis",
        "Data Visualization"
      ],
      "course_content": {
        "Introduction to Data Science and Machine Learning": [
          "Introduction to Data Science and ML",
          "ML Process and Types"
        ],
        "Python Basics, Decision Making and Loops": [
          "Python_Installation",
          "Python Practice Guidelines",
          "Python_Numbers",
          "Practice Numbers",
          "String Operations",
          "String Slicing",
          "Practice Strings",
          "Practice String Functions",
          "Lists",
          "Boolean Operations",
          "If Else Conditions",
          "For and While Loops",
          "Functions"
        ],
        "Python Data Structures": [
          "List comprehension",
          "Dictionaries",
          "Sets",
          "Tuples",
          "Dynamic Function Arguments",
          "Lambda functions, Map, Reduce, Filter"
        ],
        "Python Practice Questions": [
          "Practice Sets and Dictionary",
          "List Comprehension Practice",
          "Functions Practice",
          "Functions Practice 2",
          "Practice String and List Comprehension",
          "Practice Functions"
        ],
        "OOPS": [
          "Intro to OOPS",
          "OOPS : Without vs with OOPS",
          "OOPS: classes objects attributes",
          "OOPS: Methods",
          "OOPS: Inheritance",
          "OOPS: Polymorphism",
          "OOPS: Encapsulation",
          "Practice OOPS",
          "Python Assignment"
        ],
        "Descriptive Statistics": [
          "Introduction to Statistics_Population & Sampling",
          "Measure Of Central Tendencies Mean Median Mode",
          "Measure Of Variability - Variance Standard Deviation IQR",
          "Data Diatributions Correlation & Covariance",
          "Descriptive statistics Practice questions"
        ],
        "Inferential Statistics: Intro, Central Limit Theorem,Z-Score,CI": [
          "Intro to Inferential Statistics",
          "Variable Types",
          "Central_Limit_Theorem",
          "Z-Score",
          "Confidence Interval",
          "CI examples"
        ],
        "Hypothesis Testing": [
          "Hypothesis Testing Introduction",
          "Hypothesis Testing Theory Explained",
          "Type of Errors and Significant Difference"
        ],
        "T-Test, chi-Square , AnOVa Test and more": [
          "T-Tests",
          "Chi Square test of Goodness of Fit",
          "Chi Square test of Independance",
          "Anova",
          "Which test to pick",
          "Statistics Using Graphpad"
        ],
        "Case Study: Statistics on House Pricing Data Set": [
          "Inferential Statistics Case Study"
        ]
      },
      "requirements": [
        "Motivation to Learn"
      ],
      "description": "Data Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world’s most interesting problems!\nThis course is designed for both beginners with some programming experience or experienced developers looking to make the jump to Data Science!\nThis course is made to give you all the required knowledge at the beginning of your journey, so that you don’t have to go back and look at the topics again at any other place. This course is the ultimate destination with all the knowledge, tips and trick you would require to start your career.\nIt gives detailed guide on the Data science process involved and Machine Learning algorithms. All the algorithms are covered in detail so that the learner gains good understanding of the concepts. Although Machine Learning involves use of pre-developed algorithms one needs to have a clear understanding of what goes behind the scene to actually convert a good model to a great model.\nOur exotic journey will include the concepts of:\nComparison between Artificial intelligence, Machine Learning, Deep Learning and Neural Network.\nWhat is data science and its need.\nThe need for machine Learning and introduction to NLP (Natural Language Processing).\nThe different types of Machine Learning – Supervised and Unsupervised Learning.\nHands-on learning of Python from beginner level so that even a non-programmer can begin the journey of Data science with ease.\nAll the important libraries you would need to work on Machine learning lifecycle.\nFull-fledged course on Statistics so that you don’t have to take another course for statistics, we cover it all.\nData cleaning and exploratory Data analysis with all the real life tips and tricks to give you an edge from someone who has just the introductory knowledge which is usually not provided in a beginner course.\nAll the mathematics behind the complex Machine learning algorithms provided in a simple language to make it easy to understand and work on in future.\nHands-on practice on more than 20 different Datasets to give you a quick start and learning advantage of working on different datasets and problems.\nMore that 20 assignments and assessments allow you to evaluate and improve yourself on the go.\nTotal 10 beginner to Advance level projects so that you can test your skills.",
      "target_audience": [
        "Beginner",
        "Intermediate",
        "Advanced"
      ]
    },
    {
      "title": "Complete Agentic AI Bootcamp With LangGraph and Langchain",
      "url": "https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/",
      "bio": "Learn to build real-world AI agents, multi-agent workflows, and autonomous apps with LangGraph and LangChain",
      "objectives": [
        "Understand the core principles of Agentic AI and how to design intelligent, autonomous agents for real-world tasks.",
        "Master building AI agents using LangGraph, including creating workflows, managing agent state, memory, and event-driven behavior.",
        "Develop and deploy multi-agent collaborative systems that can communicate, reason, and solve complex problems together.",
        "mplement hands-on projects to create powerful agentic applications like autonomous research agents, task automation systems, and knowledge retrieval assistants."
      ],
      "course_content": {
        "Introduction To the Course": [
          "Welcome"
        ],
        "Installation Of Anaconda And VS Code IDE": [
          "Installation Of Anaconda And VS Code Editor",
          "Creating Virtual Environments Using Conda",
          "Creating Virtual Environments Using UV Package Manager"
        ],
        "Python Prerequisites": [
          "Getting Started With VS Code",
          "Python Basics- Syntax And Semantics",
          "Variables In Python",
          "Basic Datatypes In Python",
          "Operators In Python",
          "Conditional Statements(if,elif,else)",
          "Loops In Python",
          "List And List Comprehension In Python",
          "Practical Exmaples Of List",
          "Sets In Python",
          "Dictionaries In Python",
          "Tuples In Python",
          "Getting Started With Functions",
          "More Coding Examples With Functions",
          "Python Lambda Funbction",
          "Maps Functions Python",
          "Filter Function In Python",
          "Import Modules And Package In Python",
          "Standard Library Overview",
          "File Operation In Python",
          "Working With File Paths",
          "Exception Handling",
          "Classes And Objects In Python",
          "Inheritance In OOPS",
          "Polymorphism In OOPS",
          "Encapsulations In OOPS",
          "Abstraction In OOPS",
          "Magic Methods In Python",
          "Operative Overloading In Python",
          "Custom Exception Handling",
          "Iterators In Python",
          "Generators In Python",
          "Fucntion Copy.Closures and Decorators",
          "Numpy In Python",
          "Pandas-DataFrame And Series",
          "Data Manipulation With Pandas And Numpy",
          "Reading Data From Various Data Source Using Pandas",
          "Logging Practical Implementation In Python",
          "Logging With Multiple Loggers",
          "Logging With A Real World Examples"
        ],
        "Getting Started With Pydantic In Python": [
          "Introduction To Pydantic",
          "Pydantic Practical Implementation"
        ],
        "Langchain Hands On": [
          "Getting Started With Langchain And Open AI",
          "Creating Virtual Environment",
          "Important Components Of LangChain",
          "Data Ingestion With Documents Loaders",
          "Recursive Character Text Splitter",
          "Character Text Splitter With Langchain",
          "HTML Header Text Splitter",
          "Recursive Json Text Splitter",
          "Introduction To OPENAI Embeddings",
          "Ollama Embeddings",
          "HuggingFace Embeddings",
          "Vector Stores-FAISS",
          "Vector Store And Retriever- Chroma DB"
        ],
        "Getting Started With OpenAI And Ollama": [
          "Building Important Components Of Langchain",
          "Building GENAI Apps",
          "Understanding Retrievers And Chains",
          "Introduction To Ollama And Set Up",
          "Simple GenAI App Using Ollama",
          "Tracking GENAI App Using Langsmith"
        ],
        "Building Basic LLM Application Using LCEL": [
          "Getting Started With Open Source Models Uing Groq API",
          "Building LLM Prompt And StrOutput Parser Chain With LCEL",
          "Deploy Langserve Runnable And Chains As API"
        ],
        "Building AI agents With Conversation History Using Langchain": [
          "Building Chatbot With Message History Using Langchain",
          "Working With Prompt Template And Message ChatHistory Using LAngchain",
          "Managing the Chat Conversation History Using Langchain",
          "Working With VectorStore And Retriever"
        ],
        "AI Agents Vs Agentic AI": [
          "What is Ai Agent Vs Agentic AI",
          "Some More Examples"
        ],
        "Getting Started With LangGraph": [
          "Introduction To LangGraph",
          "Getting Started LangGraph Application- Creating The Environment",
          "Setting Up OpenAI API Key",
          "Setting Up GROQ API KEY",
          "Setting Up LangSmith API Key",
          "Developing A Simple Graph or Workflow Using LangGraph- Building Nodes And Edges",
          "Building Simple Graph StateGraph And Graph Compiling",
          "Developing LLM Powered Simple Chatbot Using LangGraph"
        ]
      },
      "requirements": [
        "Basic knowledge of Python programming (variables, functions, classes).",
        "Understanding of APIs and RESTful services (basic level).",
        "Familiarity with Large Language Models (LLMs) concepts (like OpenAI, Hugging Face models, etc.).",
        "Curiosity and willingness to build real-world AI applications — no prior experience with LangGraph needed!"
      ],
      "description": "Are you excited about the future of AI where intelligent agents can think, act, and collaborate to solve complex tasks autonomously? Welcome to the Complete Agentic AI Bootcamp with LangGraph and LangChain — your one-stop course to master the art of building agentic AI applications from scratch!\nThis course is designed to teach you everything you need to know about Agentic AI, LangGraph, and LangChain — two of the most powerful frameworks for building intelligent AI agents and multi-agent systems.\nYou will start by understanding the fundamentals of Agentic AI — how it differs from traditional AI models, the key components of agents (memory, tools, decision-making), and real-world use cases.\nWe will then dive deep into LangGraph, a cutting-edge framework that helps you design complex agent workflows using graphs, events, and state transitions. You’ll also learn how to combine LangChain's power with LangGraph to build production-ready agent applications.\nThroughout the course, you will build real-world projects step-by-step, including:\nCreating single intelligent agents with memory and tool-usage capabilities.\nDesigning multi-agent collaboration systems with message passing and shared goals.\nImplementing autonomous research assistants, task automation bots, and retrieval-augmented generation (RAG) agents.\nYou will not just learn theory — you will build and deploy multiple end-to-end agentic applications, gaining real-world experience in constructing powerful AI systems.\nBy the end of this course, you will have the skills and confidence to create your own AI agents and deploy complex agentic applications for various domains like search, research, task planning, customer support, and beyond.\nWhat You Will Learn:\nCore concepts behind Agentic AI and how intelligent agents operate.\nHands-on mastery of LangGraph and LangChain for building agent systems.\nBuilding autonomous, event-driven AI workflows with memory, reasoning, and tools.\nDeploying and optimizing single-agent and multi-agent applications.\nReal-world project experience with RAG agents, auto-research agents, and more.\nWhy Take This Course?\nHands-on, Project-Based Learning: Build actual AI agent applications, not just toy examples.\nComplete and Beginner-Friendly: Designed to take you from beginner to advanced agent builder.\nReal-World Skills: Learn techniques that companies are starting to use for next-generation AI products.\nCutting-Edge Technologies: Master the latest innovations in AI agent orchestration with LangGraph and LangChain.\nIf you are a developer, data scientist, AI/ML engineer, or tech enthusiast looking to future-proof your skills and build cutting-edge AI applications, this is the course for you!\nEnroll now and start building the future with intelligent AI agents today!",
      "target_audience": [
        "AI/ML Engineers and Developers who want to build advanced AI agent workflows and autonomous applications.",
        "Data Scientists and Researchers looking to integrate agentic behavior into their data-driven projects.",
        "Tech Enthusiasts and Students eager to explore the next generation of AI application development with practical hands-on projects.",
        "Software Engineers interested in learning how to orchestrate multi-agent systems using modern frameworks like LangGraph."
      ]
    },
    {
      "title": "Regression Analysis for Statistics & Machine Learning in R",
      "url": "https://www.udemy.com/course/regression-analysis-for-statistics-machine-learning-in-r/",
      "bio": "Learn Complete Hands-On Regression Analysis for Practical Statistical Modelling and Machine Learning in R",
      "objectives": [
        "Implement and infer Ordinary Least Square (OLS) regression using R",
        "Apply statistical and machine learning based regression models to deals with problems such as multicollinearity",
        "Carry out variable selection and assess model accuracy using techniques like cross-validation",
        "Implement and infer Generalized Linear Models (GLMS), including using logistic regression as a binary classifier",
        "Build machine learning based regression models and test their robustness in R",
        "Learn when and how machine learning models should be applied",
        "Compare different different machine learning algorithms for regression modelling"
      ],
      "course_content": {
        "Get Started with Practical Regression Analysis in R": [
          "INTRODUCTION TO THE COURSE: The Key Concepts and Software Tools",
          "Data For the Course",
          "Difference Between Statistical Analysis & Machine Learning",
          "Getting Started with R and R Studio",
          "Reading in Data with R",
          "Data Cleaning with R",
          "Some More Data Cleaning with R",
          "Basic Exploratory Data Analysis in R",
          "Conclusion to Section 1"
        ],
        "Ordinary Least Square Regression Modelling": [
          "OLS Regression- Theory",
          "OLS-Implementation",
          "More on Result Interpretations",
          "Confidence Interval-Theory",
          "Calculate the Confidence Interval in R",
          "Confidence Interval and OLS Regressions",
          "Linear Regression without Intercept",
          "Implement ANOVA on OLS Regression",
          "Multiple Linear Regression",
          "Multiple Linear regression with Interaction and Dummy Variables",
          "Some Basic Conditions that OLS Models Have to Fulfill",
          "Conclusions to Section 2"
        ],
        "Deal with Multicollinearity in OLS Regression Models": [
          "Identify Multicollinearity",
          "Doing Regression Analyses with Correlated Predictor Variables",
          "Principal Component Regression in R",
          "Partial Least Square Regression in R",
          "Ridge Regression in R",
          "LASSO Regression",
          "Conclusion to Section 3"
        ],
        "Variable & Model Selection": [
          "Why Do Any Kind of Selection?",
          "Select the Most Suitable OLS Regression Model",
          "Select Model Subsets",
          "Machine Learning Perspective on Evaluate Regression Model Accuracy",
          "Evaluate Regression Model Performance",
          "LASSO Regression for Variable Selection",
          "Identify the Contribution of Predictors in Explaining the Variation in Y",
          "Conclusions to Section 4"
        ],
        "Dealing With Other Violations of the OLS Regression Models": [
          "Data Transformations",
          "Robust Regression-Deal with Outliers",
          "Dealing with Heteroscedasticity",
          "Conclusions to Section 5"
        ],
        "Generalized Linear Models(GLMs)": [
          "What are GLMs?",
          "Logistic regression",
          "Logistic Regression for Binary Response Variable",
          "Multinomial Logistic Regression",
          "Regression for Count Data",
          "Goodness of fit testing",
          "Conclusions to Section 6"
        ],
        "Working with Non-Parametric and Non-Linear Data": [
          "Work With Non-Parametric and Non-Linear Data",
          "Polynomial and Non-linear regression",
          "Generalized Additive Models (GAMs) in R",
          "Boosted GAM Regression",
          "Multivariate Adaptive Regression Splines (MARS)",
          "Machine Learning Regression-Tree Based Methods",
          "CART-Regression Trees in R",
          "Conditional Inference Trees",
          "Random Forest(RF)",
          "Gradient Boosting Regression",
          "ML Model Selection",
          "Conclusions to Section 7"
        ],
        "Miscellaneous Lectures": [
          "Read in DTA Extension File",
          "Getting Acquainted with Github Desktop",
          "Using R Colab",
          "Group By Time",
          "POSIT"
        ]
      },
      "requirements": [
        "Should have prior experience of working with R and RStudio",
        "Should have basic knowledge of statistics",
        "Should have prior experience of using simple linear regression modelling",
        "Should have interest in building on the previous concepts to learn which regression models are applicable under different circumstances",
        "Should have an interest in learning the machine learning based regression models in R"
      ],
      "description": "With so many R Statistics & Machine Learning courses around, why enrol for this?\n\nRegression analysis is one of the central aspects of both statistical and machine learning based analysis. This course will teach you regression analysis for both statistical data analysis and machine learning in R in a practical hands-on manner. It explores the relevant concepts  in a practical manner from basic to expert level. This course can help you achieve better grades, give you new analysis tools for your academic career, implement your knowledge in a work setting or make business forecasting related decisions. All of this while exploring the wisdom of an Oxford and Cambridge educated researcher.\nMy name is MINERVA SINGH and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation). I have several years of experience in analyzing real life data from different sources using data science related techniques and producing publications for international peer reviewed journals. This course is based on my years of regression modelling experience and implementing different regression models on real life data.  Most statistics and machine learning courses and books only touch upon the basic aspects of regression analysis. This does not teach the students about all the different regression analysis techniques they can apply to their own data in both academic and business setting, resulting in inaccurate modelling. My course will change this. You will go all the way from implementing and inferring simple OLS (ordinary least square) regression models to dealing with issues of multicollinearity in regression to machine learning based regression models.\n\nBecome a Regression Analysis Expert and Harness the Power of R for Your Analysis\nGet started with R and RStudio. Install these on your system, learn to load packages and read in different types of data in R\nCarry out data cleaning and data visualization using R\nImplement ordinary least square (OLS) regression in R and learn how to interpret the results.\nLearn how to deal with multicollinearity both through variable selection and regularization techniques such as ridge regression\nCarry out variable and regression model selection using both statistical and machine learning techniques, including using cross-validation methods.\nEvaluate regression model accuracy\nImplement generalized linear models (GLMs) such as logistic regression and Poisson regression. Use logistic regression as a binary classifier to distinguish between male and female voices.\nUse non-parametric techniques such as Generalized Additive Models (GAMs) to work with non-linear and non-parametric data.\nWork with tree-based machine learning models\nImplement machine learning methods such as random forest regression and gradient boosting machine regression for improved regression prediction accuracy.\nCarry out model selection\nBecome a Regression Analysis Pro and Apply Your Knowledge on Real-Life Data\nThis course is your one shot way of acquiring the knowledge of statistical and machine learning analysis that I acquired from the rigorous training received at two of the best universities in the world, the perusal of numerous books and publishing statistically rich papers in a renowned international journal like PLOS One. Specifically, the course will:\n(a) Take the students with a basic level of statistical knowledge to perform some of the most common advanced regression analysis based techniques\n(b) Equip students to use R for performing the different statistical and machine learning data analysis and visualization tasks\n(c) Introduce some of the most important statistical and machine learning concepts to students in a practical manner such that the students can apply these concepts for practical data analysis and interpretation\n(d) Students will get a strong background in some of the most important statistical and machine learning concepts for regression analysis.\n(e) Students will be able to decide which regression analysis techniques are best suited to answer their research questions and applicable to their data and interpret the results\nIt is a practical, hands-on course, i.e. we will spend some time dealing with some of the theoretical concepts related to both statistical and machine learning regression analysis. However, the majority of the course will focus on implementing different techniques on real data and interpreting the results. After each video, you will learn a new concept or technique which you may apply to your own projects.\nTAKE ACTION TODAY! I will personally support you and ensure your experience with this course is a success.",
      "target_audience": [
        "People who have completed my course on Statistical Modeling for Data Analysis in R (or equivalent experience)",
        "People with basic knowledge of R based statistical modelling",
        "People with knowledge of linear regression modelling",
        "People wanting to extend their knowledge of regression modelling for solving real world problems.",
        "People wanting to learn how to apply machine learning based regression models using R",
        "Undergraduates and postgraduates seeking to deepen their knowledge of statistical and machine learning analysis",
        "Academic researchers seeking to learn new techniques for data analysis",
        "Business data analysts who wish to use regression modelling for predictive analysis"
      ]
    },
    {
      "title": "CDMP (Certified Data Management Professional) in 30 days",
      "url": "https://www.udemy.com/course/cdmp-certified-data-management-professional-in-30-days/",
      "bio": "The Practical Fast-Track CDMP Guide to Mastering Data Management Concepts and passing the Exam!",
      "objectives": [
        "Get full understanding of the 14 essential topics needed to pass the CDMP exam",
        "Learn as per the DAMA-DMBOK2 Revised Edition best practices",
        "Understand the Data Management fundamentals",
        "Get to practice for the exam with multiple practice tests",
        "Learn the tips and tricks of passing the exam"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Day 1 - The Basics of CDMP (Certified Data Management Professional)": [
          "What is the CDMP?",
          "Why the CDMP is a great certification to have",
          "Study smart - topics weight",
          "Time commitment needed to pass",
          "Tips for effective learning to pass the exam"
        ],
        "Day 2 - The Data Management Process": [
          "What is Data Management",
          "Data Management Essential Concepts",
          "Data Management Principles",
          "Data Management Frameworks",
          "Homework for Day 2",
          "Data Management Process - Practice Test"
        ],
        "Day 3 - Data Ethics": [
          "What is Data Ethics",
          "Data Ethics Essential Concepts",
          "Ethical Principles for Data",
          "Data Privacy Law",
          "Data Ethics for Online Data",
          "Data Ethics Risks",
          "Data Ethics culture",
          "Homework for Day 3",
          "Data Ethics - Practice Test"
        ],
        "Days 4, 5 - Data Governance": [
          "What is Data Governance",
          "Why Data Governance is important",
          "Data Governance Goals",
          "Data Governance Principles",
          "Data Governance Essential Concepts",
          "Data-centric Organization",
          "Data Governance organization",
          "Data Governance Operating Model Types",
          "Data Stewardship",
          "Data Policies",
          "Data Asset Valuation",
          "Data Governance Activities",
          "Data Governance Tools and Techniques",
          "Data Governance Metrics",
          "Homework for Day 4 and 5",
          "Data Governance - Practice Test"
        ],
        "Days 6, 7 - Data Architecture": [
          "Data Architecture intro",
          "Data Architecture Diagram example",
          "Data Architecture Goals",
          "Key Principles of Modern Data Architecture",
          "Data Architecture Activities",
          "Data Architecture Tools",
          "Key Roles in Data Architecture",
          "Homework for Day 6 and 7",
          "Data Architecture - Practice Test"
        ],
        "Days 8, 9 - Data Modeling and Design": [
          "Data Modeling and Design intro",
          "Why is Data Modeling important",
          "What is a Data Model",
          "The 4 types of data that can be modeled",
          "Data Model Components",
          "Data Model Levels intro",
          "Conceptual Data Model",
          "Logical Data Model",
          "Physical Data Model",
          "Data Modeling Activities",
          "Tools",
          "Homework for Day 8 and 9",
          "Data Modeling and Design - Practice Test"
        ],
        "Days 10, 11 - Data Storage and Operations": [
          "Data Storage and Operations intro",
          "They key Goals of Data Storage",
          "Database Terminology",
          "Activities for managing Database Technology",
          "Activities for managing databases",
          "Tools for Data Storage",
          "Important Techniques related to Data Storage",
          "Homework for Day 10 and 11",
          "Data Storage and Operations - Practice Test"
        ],
        "Days 12, 13 - Data Security": [
          "What is Data Security",
          "Goals of Data Security",
          "What is a Vulnerability, Threat, Risk",
          "Data Types Risk Classification",
          "Data Security Processes",
          "What is Data Obfuscation/masking and data encryption",
          "Network Security Terms",
          "Types of Data Security",
          "Data Security activities",
          "Data Security tools",
          "Homework for Day 12 and 13",
          "Data Security - Practice Test"
        ],
        "Days 14, 15 - Data Integration and Interoperability": [
          "Data Integration and Interoperability intro",
          "Data Integration principles",
          "Extract, Transform and Load (ETL)",
          "Extract, Load, Transform (ELT)",
          "What is the difference between ETL and ELT",
          "Data Integration processing types",
          "Data Replication",
          "Data Archiving",
          "Data Integration activities intro",
          "Plan and Analyze Activities",
          "Design Data Integration Solutions Activities",
          "Develop Data Integration Solutions Activities",
          "Implement and Monitor Activities",
          "Data integration tools",
          "Homework for Day 14 and 15",
          "Data Integration and Interoperability - Practice Test"
        ]
      },
      "requirements": [
        "If you want to pass the CDMP exam. You need to have the DAMA-DMBOK2 Revised Edition. The exam is open book, so you will be able to use the book during the exam."
      ],
      "description": "Unlock your potential in the field of data management with our intensive course, CDMP (Certified Data Management Professional) in 30 Days.\nDesigned for aspiring data professionals, this course provides a comprehensive roadmap to mastering the essential concepts and practices outlined in the Data Management Body of Knowledge (DMBoK) version 2 revised, ensuring you are fully prepared to pass the CDMP exam.\nOver the span of 30 days, you will engage in a structured learning experience and practice tests. Each week focuses on key domains of data management, including data governance, data quality, metadata management, and more. You will gain insights into industry best practices and develop a robust understanding of how to apply these principles in real-world scenarios.\nKey Features:\nCover all 14 essential topics required for the CDMP exam.\nLearn from an experienced data management professional who share their knowledge and insights.\nPractice Exams. Test your knowledge as you progress to make sure you are ready to take the exam.\nAccess study materials, tips for effective exam strategies, and mock exams to build your confidence.\nBy the end of this course, you will not only be equipped with the knowledge needed to pass the CDMP exam but also gain valuable skills that will enhance your career in data management. Join me on this journey to certification and take the next step toward becoming a recognized expert in the field!",
      "target_audience": [
        "Data Management Professionals: Individuals currently working as data analysts, data architects, data engineers, etc who want to formalize their expertise with a recognized certification.",
        "Data Governance Managers: Professionals responsible for overseeing data governance initiatives within their organizations.",
        "Chief Data Officers (CDOs) and Chief Information Officers (CIOs): Executives looking to enhance their credentials and understanding of data management practices.",
        "Data Scientists and Analysts: Those seeking to deepen their knowledge of data management frameworks and improve their career prospects.",
        "Anyone Interested in Data Management: Individuals looking to transition into the field of data management or enhance their current skill set."
      ]
    },
    {
      "title": "Python Data Science: Data Prep & EDA with Python",
      "url": "https://www.udemy.com/course/data-science-in-python-data-prep-eda/",
      "bio": "Learn Python + Pandas for data cleaning, profiling & EDA, and prep data for machine learning & data science with Python",
      "objectives": [
        "Master the core building blocks of Python for data science BEFORE applying machine learning algorithms",
        "Scope data science projects by clearly defining the goals, techniques, and data sources needed for your analysis",
        "Import and export flat files, Excel workbooks, and SQL database tables using Pandas",
        "Clean data by converting data types, handling common data issues, and creating new columns for analysis",
        "Perform exploratory data analysis (EDA) by sorting, filtering, grouping, and visualizing data to discover patterns and insights",
        "Prepare data for machine learning models by joining tables, aggregating rows, and applying feature engineering techniques"
      ],
      "course_content": {},
      "requirements": [
        "Jupyter Notebooks (free download, we'll walk through the install)",
        "Familiarity with base Python and Pandas is recommended, but not required"
      ],
      "description": "This is a hands-on, project-based course designed to help you master the core building blocks of Python for data science and machine learning.\n\n\nWe'll start by introducing the fields of data science and machine learning, discussing the difference between supervised and unsupervised learning, and reviewing the Python data science workflow we'll be using throughout the course.\n\n\nFrom there we'll do a deep dive into the data prep & EDA steps of the workflow. You'll learn how to scope a data science project, use Python and Pandas to gather data from multiple sources and handle common data cleaning issues, and perform exploratory data analysis (EDA) using techniques like filtering, grouping, and visualizing data.\n\n\nThroughout the course, you'll play the role of a Jr. Data Scientist for Maven Music, a streaming service that’s been struggling with customer churn. Using the skills you learn throughout the course, you'll use Python to gather, clean, and explore the data to provide insights about their customers.\n\n\nLast but not least, you'll practice preparing data for data science and machine learning models by joining multiple tables, adjusting row granularity, and engineering useful fields and features.\n\n\nCOURSE OUTLINE:\n\n\nIntro to Data Science & Machine Learning\nIntroduce the field of data science, review essential skills, and introduce each phase of the data science workflow\n\n\nScoping a Project\nReview the process of scoping a data science project, including brainstorming problems and solutions, choosing techniques, and setting clear goals\n\n\nGathering Data\nRead flat files into a Pandas DataFrame in Python, and review common data sources & formats, including Excel spreadsheets and SQL databases\n\n\nCleaning Data\nIdentify and convert data types, find and fix common data quality issues like missing values, duplicates, and outliers, and create new columns for analysis\n\n\nExploratory Data Analysis (EDA)\nExplore datasets to discover insights by sorting, filtering, and grouping data, then visualize it using common chart types like scatterplots & histograms\n\n\nMID-COURSE PROJECT\nPut your skills to the test by cleaning, exploring, and visualizing data from a brand-new data set containing Rotten Tomatoes movie ratings\n\n\nPreparing for Modeling\nStructure your data so that it’s ready for machine learning models by creating a numeric, non-null table and engineering new features\n\n\nFINAL COURSE PROJECT\nApply all the skills learned throughout the course by gathering, cleaning, exploring, and preparing multiple data sets for Maven Music\n\n\n__________\n\n\nReady to dive in? Join today and get immediate, LIFETIME access to the following:\n\n\n8.5 hours of high-quality video\n16 homework assignments\n7 quizzes\n2 projects (1 mid-course, 1 final)\nData Science in Python: Data Prep & EDA ebook (190+ pages)\nDownloadable project files & solutions\nExpert support and Q&A forum\n30-day Udemy satisfaction guarantee\n\n\nIf you're an aspiring data scientist or business intelligence professional looking for an introduction to the world of machine learning and data science with Python and Pandas, this is the course for you.\n\n\nHappy learning!\n-Alice Zhao (Python Expert & Data Science Instructor, Maven Analytics)\n\n\n__________\nLooking for our full business intelligence stack? Search for \"Maven Analytics\" to browse our full course library, including Excel, Power BI, MySQL, Tableau and Machine Learning courses!\n\n\nSee why our courses are among the TOP-RATED on Udemy:\n\n\n\"Some of the BEST courses I've ever taken. I've studied several programming languages, Excel, VBA and web dev, and Maven is among the very best I've seen!\" Russ C.\n\n\n\"This is my fourth course from Maven Analytics and my fourth 5-star review, so I'm running out of things to say. I wish Maven was in my life earlier!\" Tatsiana M.\n\n\n\"Maven Analytics should become the new standard for all courses taught on Udemy!\" Jonah M.",
      "target_audience": [
        "Data scientists looking to learn core techniques and best practices for data prep and exploratory data analysis",
        "Python users who want to build the core skills required before applying AI and machine learning models",
        "Data analysts or BI experts looking to transition into a data science role",
        "Anyone interested in learning one of the most popular open source programming languages in the world"
      ]
    },
    {
      "title": "neural networks for sentiment and stock price prediction",
      "url": "https://www.udemy.com/course/neural-networks-for-stock-price-prediction-and-sentiment/",
      "bio": "How to predict stock prices with neural networks and sentiment with neural networks. Machine learning hands on data scie",
      "objectives": [
        "You can create an LSTM neural network and do a basic stock price prediction",
        "You know how to do sentiment analysis with LSTM neural networks",
        "You increase your knowledge and understanding of the deep learning library keras and pyhton",
        "You might open up new career opportunities for you which are not only highly rewarding but also offer more job satisfaction"
      ],
      "course_content": {
        "LSTM neural networks for stock price prediction": [
          "Stock market prediction with LSTM neural networks intro",
          "Download the resources",
          "One important thing before you start",
          "1 Install Dependencies and load the data",
          "2 Creating the dataset",
          "3 Preprocessing the Dataset",
          "4 Training the network",
          "5 Visualizing the Network results"
        ],
        "Sentiment prediction with LSTM neural networks": [
          "Sentiment Analysis Intro and load the dataset",
          "Visualizing the data",
          "Data preprocessing for sentiment analysis with neural networks",
          "Creating the model and train the neural network",
          "Testing and evaluating the LSTM model on sentiment",
          "Final words - Congratulation"
        ],
        "NEW BONUS - Predicting the next X days in the future with LSTM Models (multiple)": [
          "The dataset we use (you can also use your own later on)",
          "1 Predicting future prices for the next X days Introduction",
          "2 Predicting future prices - dataprepreparation and visualization",
          "3 Predicting future prices - defining and training the model",
          "4 Predicting future prices - visualizing training and prediction results",
          "5 Predicting future prices - next 10 day prediction and visualization",
          "More learning"
        ],
        "Googles TimesFM A GPT inspired TimeSeries Forecasting Model": [
          "Introduction to Googles TimesFM A GPT inspired TimeSeries Forecasting Model"
        ]
      },
      "requirements": [
        "Your personal interest in the topic and a hands on mentality",
        "Basic knowledge in Python",
        "Tools are free - no additional costs required",
        "This course is hands on - instead of theory we implement the networks in code and I explain what we do and why we do it",
        "You should be familiar with neural networks - I do not explain how a LSTM Network works from scratch"
      ],
      "description": "Let's dive into data science with python and predict stock prices  and customer sentiment.\nmachine learning / ai ? How to learn machine learning in python? And what is transfer learning ? How to use it ? How to create a sentiment classification algorithm in python? How to train a neural network  for stock price prediction?\nGood questions here is a point to start searching for answers\nIn the world of today and especially tomorrow machine learning and artificial intelligence will be the driving force of the economy. Data science  No matter who you are, an entrepreneur or an employee, and in which industry you are working in, machine learning (especially deep learning neural networks) will be on your agenda.\n\n\n\"From my personal experience I can tell you that companies will actively searching for you if you aquire some skills in the data science field. Diving into this topic can not only immensly improve your career opportunities but also your job satisfaction!\"\n\n\nIt's time to get your hands dirty and dive into one of the hottest topics on this planet.\nTo me the best way to get exposure is to do it \"Hands on\". And that's exactly what we do. Together we will go through the whole process of data import, preprocess the data , creating an long short term neural network in keras (LSTM), training the neural network and test it (= make predictions)\nThe course consists of 2 parts. In the first part we will create a neural network for stock price prediction. In the second part we create a neural network for sentiment analysis on twitter tweets.\nLet's get into it. See you in the first lecture",
      "target_audience": [
        "It's a hands on course so Your committment to code along with me",
        "beginners to intermediate students in neural networks and machine learning who already know the basics",
        "students who are eager to learn and dive into one of the hottest topics currently out there",
        "students who ask how to do stock market predictions with neural networks",
        "students who ask how to do sentiment analysis with neural networks"
      ]
    },
    {
      "title": "Complete Data Science BootCamp",
      "url": "https://www.udemy.com/course/complete-data-science-bootcamp/",
      "bio": "Learn about Data Science, Machine Learning and Deep Learning and build 5 different projects.",
      "objectives": [
        "Learn about Libraries like Pandas and Numpy which are heavily used in Data Science.",
        "Build Impactful visualizations and charts using Matplotlib and Seaborn.",
        "Learn about Machine Learning LifeCycle and different ML algorithms and their implementation in sklearn.",
        "Learn about Deep Learning and Neural Networks with TensorFlow and Keras",
        "Build 5 complete projects based on the concepts covered in the course."
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Numpy": [
          "Introduction to Numpy",
          "Numpy Arrays",
          "Shape and Reshape",
          "Numpy Array Indexing",
          "Iterating Numpy Arrays",
          "Slicing",
          "Numpy Array Searching and Sorting"
        ],
        "Pandas": [
          "Pandas Introduction",
          "Series in Pandas",
          "Pandas DataFrame",
          "Read CSV",
          "Analyzing Data Frames in Pandas"
        ],
        "Data Visualization": [
          "Introduction to Matplotlib",
          "Different type of plots in Matplotlib",
          "Seaborn"
        ],
        "Data Preprocessing": [
          "Handling Missing Values",
          "Feature Encoding",
          "Feature Scaling"
        ],
        "Machine Learning": [
          "Introduction to Machine Learning",
          "Supervised Machine Learning",
          "Unsupervised Machine Learning",
          "Machine Learning Life Cycle",
          "Train Test Split",
          "Regression Analysis",
          "Linear Regression",
          "Logistic Regression",
          "KNN",
          "SVM",
          "Decision Tree",
          "Random Forest",
          "K Means Clustering",
          "Hyper Parameter Optimization with GridSearchCV",
          "Machine Learning Pipeline",
          "Machine Learning Model Evaluation Metrics"
        ],
        "Cloud Computing for Machine Learning": [
          "Cloud Computing Introduction",
          "Introduction to AWS",
          "Different AWS Services",
          "Introduction to AWS SageMaker",
          "First Machine Learning Practical on AWS SageMaker",
          "Built in ML Algorithms in AWS SageMaker",
          "Linear Learner Algorithm Practical Implementation",
          "No Code ML using AWS SageMaker Canvas",
          "AWS SageMaker MarketPlace"
        ],
        "Deep Learning": [
          "Artificial Neural Network (ANN)",
          "Activation Functions in Neural Networks",
          "Optimizers in Neural Networks",
          "Convolutional Neural Network (CNN)",
          "Recurrent Neural Network (RNN)"
        ],
        "Projects": [
          "Diabetes Prediction",
          "Medical Insurance Cost Prediction",
          "Gold Price Prediction using ANN",
          "Implementation of CNN using keras and tensor flow",
          "Stock Price Prediction using LSTM"
        ]
      },
      "requirements": [
        "Basic understanding of Python Programming Language."
      ],
      "description": "Data science is the field that encompasses the various techniques and methods used to extract insights and knowledge from data. Machine learning (ML) and deep learning (DL) are both subsets of data science, and they are often used together to analyze and understand data.\nIn data science, ML algorithms are often used to build predictive models that can make predictions based on historical data. These models can be used for tasks such as classification, regression, and clustering. ML algorithms include linear regression, decision trees, and k-means.\nDL, on the other hand, is a subset of ML that is based on artificial neural networks with multiple layers, which allows the system to learn and improve through experience. DL is particularly well-suited for tasks such as image recognition, speech recognition, and natural language processing. DL algorithms include convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\nIn a data science project, DL models are often used in combination with other techniques such as feature engineering, data cleaning, and visualization, to extract insights and knowledge from data. For instance, DL models can be used to automatically extract features from images, and then these features can be used in a traditional ML model.\nIn summary, Data science is the field that encompasses various techniques and methods to extract insights and knowledge from data, ML and DL are subsets of data science that are used to analyze and understand data, ML is used to build predictive models and DL is used to model complex patterns and relationships in data. Both ML and DL are often used together in data science projects to extract insights and knowledge from data.\n\n\nIN THIS COURSE YOU WILL LEARN ABOUT :\n\n\nLife Cycle of a Data Science Project.\nPython libraries like Pandas and Numpy used extensively in Data Science.\nMatplotlib and Seaborn for Data Visualization.\nData Preprocessing steps like Feature Encoding, Feature Scaling etc...\nMachine Learning Fundamentals and different algorithms\nCloud Computing for Machine Learning\nDeep Learning\n5 projects like Diabetes Prediction, Stock Price Prediction etc...\n\n\nALL THE BEST !!!",
      "target_audience": [
        "People who want to start their Data Science Journey in Python.",
        "Someone who is looking for a complete course that covers all important topics of Data Science, Machine Learning and Deep Learning."
      ]
    },
    {
      "title": "Machine Learning Real World projects in Python",
      "url": "https://www.udemy.com/course/machine-learning-real-world-projects-in-python/",
      "bio": "Build a Portfolio/Resume of Machine Learning projects in Python and get a job of Data Scientist/ ML Engineer",
      "objectives": [
        "Machine Learning Engineers earn on average $164,000 - become Job Ready ML Engineer with this course!",
        "Go from zero to hero in Entire Pipeline of Machine learning from Data Collection to building a Machine Learning Model",
        "Solve any problem in your business, job or in real-time with powerful Machine Learning algorithms",
        "Mathematics behind All Machine Learning algos ( Linear Regression , logistic , Decision Tree , Ensemble algos , KNN , Naive Bayes & many more !",
        "Various Feature selection Techniques & how to apply it in Real-World",
        "How to Approach a problem in Real-world..",
        "Case studies"
      ],
      "course_content": {
        "Intro to this course": [
          "Introduction & Course Benefits",
          "Utilize QnA of the course ( Golden Oppurtunity ) !",
          "How to follow this course-Must Watch",
          "Installation of Anaconda Navigator",
          "Quick Summary of Jupyter Notebook"
        ],
        "Introduction to Machine Learning": [
          "Data Science vs AI vs Machine Learning vs Deep Learning vs Generative AI !",
          "What is Machine Learning ?"
        ],
        "Introduction to Life-Cycle of Machine Learning Project": [
          "Part 1 : Business Understanding in Real World",
          "Part 2 : Data Collection & Cleaning !",
          "Part 3 : What is EDA(Exploratory Data Analysis) !",
          "Part 4 : What is Feature Engineering !",
          "Part 5 : Model Building & Deployment of Model !",
          "Part 6 : Model Testing & Optimisation of Model !"
        ],
        "Project 1-->> Predict the cancellation of Hotel Booking": [
          "Introduction to Business Problem & Dataset",
          "Datasets & Resources",
          "How to read data",
          "Lets Perform data cleaning..",
          "Analysing Demand Of hotels",
          "Analysing Prices of Hotels across year",
          "Analysing Demand Of hotels",
          "Lets Analyse which month has highest avg. daily rate ?",
          "Lets perform Advance Data Analysis ..",
          "How to create useful features for Machine Learning model ..",
          "how to apply Feature encoding on Categorical data .",
          "How to Handle Outliers .",
          "Select important Features using Co-relation & univariate analysis",
          "Applying Techniques of Feature Importance .",
          "Intuition behind Logistic Regression --part 1",
          "Intuition behind Logistic Regression --part 2",
          "Building Machine Learning model .",
          "Idea Behind Cross Validation- Part 1",
          "Idea Behind Cross Validation- Part 2",
          "How to cross-validate model .",
          "Intuition Behind Decision Tree- Part 1",
          "Intuition Behind Decision Tree- Part 2",
          "Intuition Behind Decision Tree- Part 3",
          "Intuition Behind Decision Tree- Part 4",
          "Intuition Behind Decision Tree- Part 5",
          "Intuition Behind Decision Tree- Part 6",
          "Intuition Behind Random Forest Part-1",
          "Intuition Behind Random Forest Part-2",
          "Intuition Behind KNN- Part 1",
          "Intuition Behind KNN- Part 2",
          "Intuition Behind KNN- Part 3",
          "Intuition Behind KNN- Part 4",
          "Intuition Behind Naive Bayes-Part 1",
          "Intuition Behind Naive Bayes-Part 2",
          "Applying Multiple algorithms on data"
        ],
        "Project 2-->> Predict status of Chronic kidney disease (Health care Case-study )": [
          "Datasets & Resources",
          "Prepare your data for Analysis & Modelling",
          "How to clean your data",
          "Analysing Distributions of your data",
          "How to check co-relation in data",
          "How to Automate your Analysis",
          "Perform Exploratory Data Analysis on data..",
          "How to come across with missing values in data",
          "Clean your missing values using Random Value Imputation",
          "Applying feature Encoding on data",
          "How to Select best features for your model",
          "How to handle Imbalance data in Machine Learning !",
          "Building a Cross-validated Model & checking its accuracy"
        ],
        "Project 3-->> Predict Prices of Flights Tickets ( Airline Case-study )": [
          "Introduction to Business Problem & Dataset",
          "Datasets & Resources",
          "Lets read data",
          "Lets deal with missing values (Data Cleaning)",
          "Perform data-preprocessing & extract derived Features .",
          "Perform data Cleaning & extract Derived attributes ..",
          "Lets Perform Data Analysis",
          "Perform Pre-processing on Duration feature ..",
          "Lets Analyse whether Duration impacts Price or not",
          "Lets Perform Bi-variate Analysis !",
          "Applying one-hot Encoding on data ( feature Encoding)",
          "Applying target guided encoding on data ( Feature Encoding )",
          "Lets Perform Label Encoding manually ! (Feature Encoding)",
          "How to handle Outliers in data.",
          "Select Best features using Feature Selection !",
          "Applying Random Forest(ML) algorithm on data..",
          "How to automate Machine Learning pipeline",
          "How to hypertune Machine Learning model.."
        ],
        "Bonus Section": [
          "Bonus Session"
        ]
      },
      "requirements": [
        "Basic knowledge of Python programming is recommended."
      ],
      "description": "This is the first course that gives hands-on Machine Learning Projects using Python..\n\n\nStudent Testimonials:\n\n\nAnother great course! Real world data, various data manipulations techniques, practical visualizations and insights, useful ways of automation. I've learnt a lot in a short period of time. Solid 5 stars! Thanks - Sebastian Suminski\n\n\nVery clear and understood explanation using case study. It was very clear showing the beauty of the function and the visualization when he used according to the case study. Thanks - Sadik\n\n\nThe course is really good and practical. The explanation right to the point. Definitely recommended for who learned the theory and want to do hands- on - Deepthi kiran Chebrolu\n\n\nExcellent Course and amazing work @shan singh sir I am glad to be your student one small suggestion is try to show the deployment phase like showing our end project for others using flask, streamlit, gradio etc It will help lot of us to learn about the UI and deployment as well. - Veluturi Sunil Tagore\n\n\nSir this is amazing i have completed a little bit remaining this is so much adorable thank you to completing my project in ml - Jay Kumar Vagairya\n\n\n\n\nMachine Learning is one of the hottest technology field in the world right now! This field is exploding with opportunities and career prospects. Machine Learning techniques are widely used in several sectors now a days such as banking, healthcare, finance, education transportation and technology.\n\n\nThis course covers several technique in a practical manner, the projects include coding sessions as well as Algorithm Intuition:\nSo, if you’ve ever wanted to play a role in the future of technology development, then here’s your chance to get started with Machine Learning. Because in a practical life, machine learning seems to be complex and tough,thats why we’ve designed a course to help break it down into real world use-cases that are easier to understand.\n\n1.. Task #1 @Predicting the Hotel booking  : Predict Whether Hotel booking  is going to cancel or not\n3.. Task #2 @Predict Whether Person has a Chronic Disease or not : Develop a Machine learning Model that predicts whether person has Chronic kidney disease or not\n2.. Task #3 @Predict the Prices of Flight : Predict the prices of Flight using Regression & Ensemble Algorithms..\n\n\n\n\nThe course covers a number of different machine learning algorithms such as Regression and Classification algorithms. From there you will learn how to incorporate these algorithms into actual projects so you can see how they work in action! But, that’s not all. In addition to quizzes that you’ll find at the end of each section, the course also includes a 3 brand new projects that can help you experience the power of Machine Learning using real-world examples!",
      "target_audience": [
        "Data Scientists who want to apply their knowledge on Real World Case Studies",
        "Machine Learning Enthusiasts who look to add more projects to their Portfolio"
      ]
    },
    {
      "title": "Linear Algebra and Feature Selection in Python",
      "url": "https://www.udemy.com/course/linear-algebra-and-feature-selection-in-python/",
      "bio": "Gain the Theoretical and Practical Foundations to Learn Machine Learning and AI with Understanding",
      "objectives": [
        "Understand the math behind machine learning and AI models",
        "Become familiar with basic and advanced linear algebra notions",
        "Be able to solve linear equations",
        "Determine independency of a set of vectors and explore what it means for ML and AI",
        "Calculate eigenvalues and eigenvectors",
        "Perform Linear Discriminant Analysis (LDA)",
        "Perform Dimensionality Reduction in Python",
        "Carry out Principal Components Analysis (PCA)",
        "Compare the performance of PCA and LDA for classification with SVMs"
      ],
      "course_content": {
        "Linear Algebra Essentials": [
          "What Does The Course Cover",
          "Why Linear Algebra?",
          "The Case of the Mysterious Model Failure",
          "Solving Quadratic Equations",
          "Why Quadratic Equations Are More Than Just a Formula",
          "Vectors",
          "Vector Addition: Geometric View",
          "Matrices",
          "Matrices",
          "Matrix Transformations: Geometric View",
          "Vectors and Matrices: The Language of Intelligence",
          "The Transpose of Vectors and Matrices, the Identity Matrix",
          "Why the Identity Matrix Matters in AI",
          "Linear Independence and Linear Span of Vectors",
          "Smart Features, Smarter Models: Why Linear Independence Fuels AI",
          "Basis of a Vector Space, Determinant of a Matrix, Inverse of a Matrix",
          "Basis, Determinant, and Inverse — The Backbone of Machine Learning Math",
          "Solving Equations of the Form Ax=b",
          "The Gauss Method",
          "The Gauss Method",
          "Other Solutions to the Equation Ax=b",
          "Determining Linear Independence of a Random Set of Vectors",
          "Eigenvalues and Eigenvectors",
          "Eigenvalues & Eigenvectors — The DNA of Transformations in AI",
          "Explain the role of linear algebra to your colleague",
          "Calculating Eigenvalues",
          "Calculating Eigenvectors",
          "Linear Algebra Essentials"
        ],
        "Dimensionality Reduction Motivation": [
          "Feature Selection, Feature Extraction, and Dimensionality Reduction",
          "The Curse of Dimensionality",
          "The Curse of Dimensionality"
        ],
        "Principal Component Analysis (PCA)": [
          "An Overview of PCA",
          "A Step-by-Step Explanation of PCA on California Estates – Example",
          "The Theory Behind PCA",
          "PCA Covariance Matrix in Jupyter – Analysis and Interpretation"
        ],
        "Linear Discriminant Analysis (LDA)": [
          "Overall Mean and Class Means",
          "An Overview of LDA",
          "LDA: Calculating the Within- and Between-Class Scatter Matrices",
          "A Step-by-Step Еxplanation of LDA on a Wine Quality Dataset – Example",
          "Calculating the Within- and Between-Class Scatter Matrices",
          "Calculating Eigenvectors and Eigenvalues for the LDA",
          "Analysis of LDA"
        ],
        "LDA vs PCA": [
          "LDA vs. PCA",
          "Setting Up the Classifier to Compare LDA and PCA",
          "Coding the Classifier for LDA and PCA",
          "Analysis of the Training and Testing Times for the Classifier and Its Accuracy",
          "Choosing Between LDA and PCA"
        ]
      },
      "requirements": [
        "Suitable for beginners. Some understanding of Python basics and math would be an advantage."
      ],
      "description": "Do you want to learn linear algebra?\nYou have come to the right place!\nFirst and foremost, we want to congratulate you because you have realized the importance of obtaining this skill. Whether you want to pursue a career in data science, AI engineering, machine learning, data analysis, software engineering, or statistics, you will need to know how to apply linear algebra.\nThis course will allow you to become a professional who understands the math on which algorithms are built, rather than someone who applies them blindly without knowing what happens behind the scenes.\nBut let’s answer a pressing question you probably have at this point:\n“What can I expect from this course and how it will help my professional development?”\nIn brief, we will provide you with the theoretical and practical foundations for two fundamental parts of data science and statistical analysis – linear algebra and dimensionality reduction.\nLinear algebra is often overlooked in data science and AI courses, despite being of paramount importance. Most instructors tend to focus on the practical application of specific frameworks rather than starting with the fundamentals, which leaves you with knowledge gaps and a lack of full understanding. In this course, we give you an opportunity to build a strong foundation that would allow you to grasp complex ML and AI topics.\nThe course starts by introducing basic algebra notions such as vectors, matrices, identity matrices, the linear span of vectors, and more. We’ll use them to solve practical linear equations, determine linear independence of a random set of vectors, and calculate eigenvectors and eigenvalues, all preparing you for the second part of our learning journey - dimensionality reduction.\nThe concept of dimensionality reduction is crucial in data science, statistical analysis, and machine learning. This isn’t surprising, as the ability to determine the important features in a dataset is essential - especially in today’s data-driven age when one must be able to work with very large datasets.\nImagine you have hundreds or even thousands of attributes in your data. Working with such complex information could lead to a variety of problems – slow training time, the possibility of multicollinearity, the curse of dimensionality, or even overfitting the training data.\nDimensionality reduction can help you avoid all these issues, by selecting the parts of the data which actually carry important information and disregarding the less impactful ones.\nIn this course, we’ll discuss two staple techniques for dimensionality reduction – Principal Components Analysis (PCA), and Linear Discriminant Analysis (LDA). These methods transform the data you work with and create new features that carry most of the variance related to a given dataset. First, you will learn the theory behind PCA and LDA. Then, going through two complete examples in Python, you will see how data transformation occurs in practice. For this purpose, you will get one step-by-step application of PCA and one of LDA. Finally, we will compare the two algorithms in terms of speed and accuracy.\nWe’ve put a lot of effort into making this course the perfect foundational training for anyone who wants to become a data analyst, data scientist, machine learning engineer, or AI engineer.",
      "target_audience": [
        "Ideal for beginner data science and machine learning students",
        "Aspiring data analysts",
        "Aspiring data scientists",
        "Aspiring machine learning engineers",
        "People who want to level-up their career and add value to their company",
        "Anyone who wants to start a career in data science or machine learning"
      ]
    },
    {
      "title": "Apache Beam | Google Data Flow (Python) | Hands on course",
      "url": "https://www.udemy.com/course/apache-beam/",
      "bio": "Data Engineering with complete Hands on course for Apache Beam | Google Data Flow to build Big Data Pipelines",
      "objectives": [
        "Apache Beam"
      ],
      "course_content": {
        "Apache Beam Introduction": [
          "Intro",
          "Apache Beam Intro",
          "Architecture",
          "Apache Beam Flow",
          "Pcollection",
          "Installation",
          "Getting Started",
          "Learn BigData",
          "Enroll into Big Data Course"
        ],
        "Apache Beam Transformations": [
          "Read Input",
          "Write Output",
          "Map/FlatMap",
          "CoGroupByKey",
          "Partition",
          "Flatten",
          "Composite Transformation"
        ],
        "Pardo with Side Inputs/Outputs": [
          "Pardo Transformation",
          "Side Inputs",
          "Side Outputs"
        ],
        "Streaming with Apache Beam": [
          "Streaming intro",
          "Google PubSub Intro",
          "Google PubSub Example"
        ],
        "Windows in Apache Beam": [
          "Windows Intro",
          "Time Notions",
          "Tumbling/Sliding WIndows",
          "Tumbling Code",
          "Sliding Code",
          "Session Windows"
        ],
        "Handling Late Elements": [
          "Allow Lateness"
        ],
        "Using Triggers": [
          "Triggers",
          "Composite Triggers",
          "Composition with Default Trigger"
        ],
        "Google Cloud Dataflow": [
          "Dataflow Intro",
          "Google Dataflow Templates",
          "Google Dataflow Notebooks"
        ],
        "Beam SQL": [
          "Beam SQL Intro",
          "Beam SQL on Google Cloud"
        ]
      },
      "requirements": [
        "Basic Python"
      ],
      "description": "Apache Beam is future of Big Data technology  and is used to build big data pipelines. This course is designed for beginners  who want to learn how to use Apache Beam using python language . It also covers  google cloud dataflow which is hottest way to build big data pipelines nowadays using Google cloud.\n\n\nThis course consist of various hands on to get you comfortable with various topics in Apache Beam.This course will introduce various topics:\n\n\nArchitecture\nTransformations\nSide Inputs/Outputs\nStreaming with Google PubSub\nWindows in Streaming\nHandling Late elements\nUsing Triggers\nGoogle Cloud Dataflow\nBeam SQL / Beam SQL on GCP\nBy the end of this course, you will find yourself ready to start using Apache Beam in real work environment.\n\n\nWhat make this course unique - it's concise that's in only 3 hours you will be able to complete it, covers all relevant topics and slides and presentations are really very exciting and easy to understand.\n\n\nWhy Apache beam is future of Big Data?\n1. It runs on top of popular big data engine like spark, flink, Google data flow.\n2. It is used by big giant like Google.\n3. It solves the industry biggest problem of migration and unification from one processing engine to another.\n\n\nSo if you want to learn future technology , then you are right place.",
      "target_audience": [
        "Big data learners"
      ]
    },
    {
      "title": "Mastering AI Agents Bootcamp: Build Smart Chatbots & Tools",
      "url": "https://www.udemy.com/course/mastering-ai-agents-bootcamp-build-smart-chatbots-tools/",
      "bio": "Build AI agents, automation bots, chat assistants, task managers, and smart workflows using local AI models—no APIs req",
      "objectives": [
        "Build AI-powered agents for task automation, chatbots, and intelligent assistants, enhancing workflow efficiency without relying on external APIs.",
        "Develop AI models that understand, process, and generate human-like responses, enabling interactive and dynamic conversation systems.",
        "Implement AI-driven automation bots that perform repetitive tasks, manage schedules, and optimize workflows without manual intervention.",
        "Utilize local vector databases like FAISS to store, retrieve, and process knowledge for AI assistants, eliminating reliance on cloud-based APIs.",
        "Integrate speech-to-text and text-to-speech capabilities into AI agents, enabling hands-free interaction for enhanced accessibility.",
        "Design AI chatbots with long-term memory using local storage, allowing intelligent agents to retain context across multiple conversations.",
        "Develop web-based AI assistants with Streamlit, providing interactive user interfaces for real-time AI-powered automation and assistance.",
        "Create AI-powered document readers that extract, summarize, and answer questions from PDFs without requiring cloud-based AI services.",
        "Build AI-driven personal finance trackers that analyze expenses, provide budgeting advice, and generate financial insights locally.",
        "Enhance AI models with prompt engineering techniques, enabling better responses, improved task execution, and more personalized interactions."
      ],
      "course_content": {
        "Day 1: Introduction to AI Agents & Tools - Hands-On AI Agents Course": [
          "Goals for Day 1: Introduction to AI Agents & Tools",
          "What are AI Agents?",
          "Why Use Ollama for AI Agents?",
          "Setting Up Ollama",
          "Download a Model for AI Agents",
          "Python Basics for AI (Optional)",
          "Build a Simple AI Agent",
          "Adding Memory to the AI Agent",
          "Building a Web UI for the AI Agent"
        ],
        "Day 2: Building a Personal AI Assistant - Hands-On AI Agents Course": [
          "Goals for Day 2: Building a Personal AI Assistant",
          "Install Dependencies",
          "Build the AI Voice Assistant",
          "How the AI Assistant Works",
          "Running the AI Assistant",
          "Create a Web-Based AI Voice Assistant"
        ],
        "Day 3: AI-Powered Web Scraper Agent - Hands-On AI Agents Course": [
          "Goals for Day 3: AI-Powered Web Scraper Agent",
          "Install Dependencies",
          "Build the AI Web Scraper",
          "How the AI Web Scraper Works",
          "Running the AI Web Scraper",
          "Store Scraped Data in a Vector Database"
        ],
        "Day 4: AI-Powered Document Reader & Q&A Bot - Hands-On AI Agents Course": [
          "Goals for Day 4: AI-Powered Document Reader & Q&A Bot",
          "Install Dependencies",
          "Build the AI Document Reader",
          "How the AI Document Reader Works",
          "Running the AI Document Reader",
          "Enable File Download of AI-Summarized Reports"
        ]
      },
      "requirements": [
        "No prior AI experience needed! Basic Python programming knowledge is helpful but not required, as fundamental coding concepts will be covered along the way.",
        "A computer (Windows, macOS, or Linux) with internet access to install required tools, run AI models locally, and develop interactive AI-powered applications.",
        "Familiarity with command-line tools like Terminal or Command Prompt is useful but not mandatory, as step-by-step guidance will be provided for all setups.",
        "Basic understanding of logic, problem-solving, and structured thinking will help in designing AI workflows and automating tasks efficiently.",
        "Some exposure to machine learning concepts is beneficial but not required; AI fundamentals and practical applications will be introduced progressively.",
        "A willingness to experiment, debug, and iterate on AI projects to gain hands-on experience in building, testing, and refining intelligent automation tools.",
        "Patience and curiosity to explore AI technologies, learn prompt engineering techniques, and build functional AI agents with real-world use cases.",
        "An open mindset to adopt AI-driven solutions, automate daily tasks, and improve efficiency using AI-powered assistants and automation bots.",
        "Basic knowledge of JSON, text processing, or web scraping is useful but not necessary; all relevant concepts will be explained with hands-on projects.",
        "No expensive software or cloud services needed! All AI agents and automation bots will be built using open-source tools and local execution methods."
      ],
      "description": "Artificial intelligence is transforming the way we work, automate tasks, and interact with technology. This course is designed to help learners build AI-powered agents, automation bots, chat assistants, and task management systems using open-source tools without relying on external APIs or cloud-based services. Whether you are a beginner exploring artificial intelligence or a developer looking to integrate AI into real-world applications, this course provides a hands-on approach to building AI-driven automation solutions.(AI)\nThroughout this course, learners will gain practical experience in developing intelligent assistants that can process text, respond to user queries, automate repetitive tasks, and manage workflows efficiently. The focus will be on implementing AI-powered chatbots, smart task managers, document readers, web scrapers, and personal productivity assistants. By leveraging local AI models, vector databases, and natural language processing techniques, students will learn how to create AI solutions that function entirely on their machines without any reliance on cloud APIs.\nThe course starts with an introduction to AI agents, covering the fundamental concepts of natural language processing, automation workflows, and task execution. Learners will build chatbots capable of carrying on meaningful conversations while maintaining memory of past interactions. By integrating AI models with local vector databases such as FAISS, students will store and retrieve information efficiently, allowing their AI agents to answer complex queries based on stored knowledge. As the course progresses, students will develop AI-powered task automation bots capable of scheduling, organizing, and prioritizing tasks using machine intelligence.\nOne of the key aspects of this course is building AI-driven document readers that extract, summarize, and provide answers from PDF files. Learners will implement an AI system that processes and retrieves relevant information, enabling intelligent document search and Q&A functionalities. Additionally, students will create an AI-powered web scraper that extracts text from websites, summarizes content, and stores valuable insights in a searchable vector database for later use. These AI automation techniques can be applied in various domains, including research, business intelligence, and content generation.\nAs learners progress through the course, they will work on projects that integrate AI into daily productivity tools. They will develop personal AI assistants that help with scheduling, reminders, and workflow management. The course also covers AI-powered task prioritization, where students will train models to analyze deadlines and assign importance to different activities. By the end of the course, students will have a strong understanding of how to build AI agents capable of automating complex tasks, enhancing productivity, and managing data-driven workflows.\nThis course is designed for software developers, data analysts, AI enthusiasts, and anyone interested in building AI automation solutions. No prior experience in artificial intelligence is required, as all concepts are introduced progressively with step-by-step implementations. Learners will gain hands-on experience with AI tools, machine learning models, and automation frameworks, making this course ideal for those who want to integrate AI into real-world applications. All projects are built using open-source software and executed locally, ensuring privacy, security, and full control over AI-driven automation systems.\nBy the end of this course, students will have the knowledge and practical skills to create AI-powered chatbots, automation bots, document readers, web scrapers, and intelligent personal assistants. They will be equipped to develop AI solutions that streamline workflows, enhance productivity, and automate repetitive tasks efficiently. This course provides a solid foundation in AI-driven automation and equips learners with the ability to design, build, and deploy AI agents for various use cases.",
      "target_audience": [
        "Beginners and tech enthusiasts who want to explore AI and build intelligent assistants without prior experience in machine learning or programming.",
        "Software developers and engineers looking to integrate AI-powered automation into applications, optimize workflows, and build custom AI assistants.",
        "Entrepreneurs and business owners who want to leverage AI automation for customer service, task management, and business efficiency.",
        "Freelancers and professionals seeking to enhance their productivity by developing AI-driven task automation bots and personal assistants.",
        "Data analysts and researchers interested in using AI to automate data extraction, analysis, and summarization for better insights.",
        "Students and learners passionate about artificial intelligence, eager to gain hands-on experience building AI agents and automation tools.",
        "Content creators and marketers looking to automate content generation, social media management, and audience engagement using AI.",
        "AI hobbyists and innovators who enjoy experimenting with open-source AI tools to develop custom assistants and automation systems.",
        "Anyone curious about AI and automation who wants to learn practical applications without relying on complex APIs or cloud services.",
        "Developers transitioning into AI and automation, seeking hands-on experience with building smart AI agents using local execution methods."
      ]
    },
    {
      "title": "The Complete Self-Driving Car Course - Applied Deep Learning",
      "url": "https://www.udemy.com/course/applied-deep-learningtm-the-complete-self-driving-car-course/",
      "bio": "Learn to use Deep Learning, Computer Vision and Machine Learning techniques to Build an Autonomous Car with Python",
      "objectives": [
        "Learn to apply Computer Vision and Deep Learning techniques to build automotive-related algorithms",
        "Understand, build and train Convolutional Neural Networks with Keras",
        "Simulate a fully functional Self-Driving Car with Convolutional Neural Networks and Computer Vision",
        "Train a Deep Learning Model that can identify between 43 different Traffic Signs",
        "Learn to use essential Computer Vision techniques to identify lane lines on a road",
        "Learn to build and train powerful Neural Networks with Keras",
        "Understand Neural Networks at the most fundamental perceptron-based level"
      ],
      "course_content": {
        "Introduction": [
          "Why This Course?",
          "Your Instructor"
        ],
        "Installation": [
          "Overview",
          "Anaconda Distribution - Mac",
          "Anaconda Distribution - Windows",
          "Download Atom Text Editor",
          "Text Editor (Read Previous Article First)",
          "Outro"
        ],
        "Python Crash Course (Optional)": [
          "Python Crash Course Part 1 - Data Types",
          "Jupyter Notebooks",
          "Arithmetic Operations",
          "Variables",
          "Numeric Data Types",
          "String Data Types",
          "Booleans",
          "Methods",
          "Lists",
          "Slicing",
          "Membership Operators",
          "Mutability",
          "Mutability II",
          "Common Functions & Methods",
          "Tuples",
          "Sets",
          "Dictionaries",
          "Compound Data Structures",
          "Part 1 - Outro",
          "Part 2 - Control Flow",
          "If, else",
          "elif",
          "Complex Comparisons",
          "For Loops",
          "For Loops II",
          "While Loops",
          "Break",
          "Part 2 - Outro",
          "Part 3 - Functions",
          "Functions",
          "Scope",
          "Doc Strings",
          "Lambda & Higher Order Functions",
          "Part 3 - Outro"
        ],
        "NumPy Crash Course (Optional)": [
          "Overview",
          "Vector Addition - Arrays vs Lists",
          "Multidimensional Arrays",
          "One Dimensional Slicing",
          "Reshaping",
          "Multidimensional Slicing",
          "Manipulating Array Shapes",
          "Matrix Multiplication",
          "Stacking",
          "Part 4 - Outro"
        ],
        "Computer Vision: Finding Lane Lines": [
          "Overview",
          "Image needed for the next lesson",
          "Loading Image",
          "Save your file before running!",
          "Grayscale Conversion",
          "Smoothening Image",
          "Simple Edge Detection",
          "Region of Interest",
          "Binary Numbers & Bitwise_and",
          "Line Detection - Hough Transform",
          "Hough Transform II",
          "Optimizing",
          "Resource for upcoming video",
          "Finding Lanes on Video",
          "Numpy.float64 Error (Quick Fix)",
          "Source Code",
          "Part 5 - Conclusion"
        ],
        "The Perceptron": [
          "Overview",
          "Machine Learning",
          "Supervised Learning - Friendly Example",
          "Classification",
          "Linear Model",
          "Perceptrons",
          "Weights",
          "Project - Initial Stages",
          "Sample Code for Initial Stages",
          "Error Function",
          "Sigmoid",
          "Sigmoid Implementation (Code)",
          "Source code",
          "Cross Entropy",
          "Cross Entropy (Code)",
          "Source Code",
          "Gradient Descent",
          "Gradient Descent (Code)",
          "Recap",
          "Source Code",
          "Part 6 - Conclusion"
        ],
        "Keras": [
          "Overview",
          "Intro to Keras (See next article for installation fix)",
          "Stop Using Jupyter - Use Colab instead",
          "How to Import Keras",
          "Starter Code",
          "Fix for next video (update)",
          "Keras Models",
          "Keras - Predictions",
          "Source Code",
          "Part 7 - Outro"
        ],
        "Deep Neural Networks": [
          "Overview",
          "Non-Linear Boundaries",
          "Architecture",
          "Feedforward Process",
          "Error Function",
          "Backpropagation",
          "Fix for next video (update)",
          "Code Implementation",
          "Source Code",
          "Section 8 - Conclusion"
        ],
        "Multiclass Classification": [
          "Overview",
          "Softmax",
          "Cross Entropy",
          "Implementation",
          "Source Code",
          "Section 9 - Outro"
        ],
        "MNIST Image Recognition": [
          "Overview",
          "MNIST Dataset",
          "Train & Test",
          "Hyperparameters",
          "Implementation Part 1",
          "Fix for upcoming video",
          "Implementation Part 2",
          "Resource for upcoming video",
          "Implementation Part 3",
          "Final Source Code",
          "Section 10 - Outro"
        ]
      },
      "requirements": [
        "A working computer",
        "No experience required!"
      ],
      "description": "Self-driving cars have rapidly become one of the most transformative technologies to emerge. Fuelled by Deep Learning algorithms, they are continuously driving our society forward and creating new opportunities in the mobility sector.\nDeep Learning jobs command some of the highest salaries in the development world. This is the first, and only course which makes practical use of Deep Learning, and applies it to building a self-driving car, one of the most disruptive technologies in the world today.\nLearn & Master Deep Learning in this fun and exciting course with top instructor Rayan Slim. With over 28000 students, Rayan is a highly rated and experienced instructor who has followed a \"learn by doing\" style to create this amazing course.\nYou'll go from beginner to Deep Learning expert and your instructor will complete each task with you step by step on screen.\nBy the end of the course, you will have built a fully functional self-driving car fuelled entirely by Deep Learning. This powerful simulation will impress even the most senior developers and ensure you have hands on skills in neural networks that you can bring to any project or company.\n\n\nThis course will show you how to:\nUse Computer Vision techniques via OpenCV to identify lane lines for a self-driving car.\nLearn to train a Perceptron-based Neural Network to classify between binary classes.\nLearn to train Convolutional Neural Networks to identify between various traffic signs.\nTrain Deep Neural Networks to fit complex datasets.\nMaster Keras, a power Neural Network library written in Python.\nBuild and train a fully functional self driving car to drive on its own!\nNo experience required. This course is designed to take students with no programming/mathematics experience to accomplished Deep Learning developers.\nThis course also comes with all the source code and friendly support in the Q&A area.",
      "target_audience": [
        "Anyone with an interest in Deep Learning and Self Driving Cars",
        "Anyone (no matter the skill level) who wants to transition into the field of Artificial Intelligence",
        "Entrepreneurs with an interest in working on some of the most cutting edge technologies",
        "All skill levels are welcome!"
      ]
    },
    {
      "title": "The Complete Machine Learning Course with Python",
      "url": "https://www.udemy.com/course/machine-learning-course-with-python/",
      "bio": "Build a Portfolio of 12 Machine Learning Projects with Python, SVM, Regression, Unsupervised Machine Learning & More!",
      "objectives": [
        "Machine Learning Engineers earn on average $166,000 - become an ideal candidate with this course!",
        "Solve any problem in your business, job or personal life with powerful Machine Learning models",
        "Train machine learning algorithms to predict house prices, identify handwriting, detect cancer cells & more",
        "Go from zero to hero in Python, Seaborn, Matplotlib, Scikit-Learn, SVM, unsupervised Machine Learning etc"
      ],
      "course_content": {
        "Introduction": [
          "What Does the Course Cover?",
          "How to Succeed in This Course",
          "Project Files and Resources"
        ],
        "Getting Started with Anaconda": [
          "Installing Applications and Creating Environment",
          "Hello World",
          "Iris Project 1: Working with Error Messages",
          "Iris Project 2: Reading CSV Data into Memory",
          "Iris Project 3: Loading data from Seaborn",
          "Iris Project 4: Visualization"
        ],
        "Regression": [
          "Scikit-Learn",
          "EDA",
          "Correlation Analysis and Feature Selection",
          "Correlation Analysis and Feature Selection",
          "Linear Regression with Scikit-Learn",
          "Five Steps Machine Learning Process",
          "Robust Regression",
          "Evaluate Regression Model Performance",
          "Multiple Regression 1",
          "Multiple Regression 2",
          "Regularized Regression",
          "Polynomial Regression",
          "Dealing with Non-linear Relationships",
          "Feature Importance",
          "Data Preprocessing",
          "Variance-Bias Trade Off",
          "Learning Curve",
          "Cross Validation",
          "CV Illustration"
        ],
        "Classification": [
          "Logistic Regression",
          "Introduction to Classification",
          "Understanding MNIST",
          "SGD",
          "Performance Measure and Stratified k-Fold",
          "Confusion Matrix",
          "Precision",
          "Recall",
          "f1",
          "Precision Recall Tradeoff",
          "Altering the Precision Recall Tradeoff",
          "ROC"
        ],
        "Support Vector Machine (SVM)": [
          "Support Vector Machine (SVM) Concepts",
          "Linear SVM Classification",
          "Polynomial Kernel",
          "Radial Basis Function",
          "Support Vector Regression"
        ],
        "Tree": [
          "Introduction to Decision Tree",
          "Training and Visualizing a Decision Tree",
          "Visualizing Boundary",
          "Tree Regression, Regularization and Over Fitting",
          "End to End Modeling",
          "Project HR",
          "Project HR with Google Colab"
        ],
        "Ensemble Machine Learning": [
          "Ensemble Learning Methods Introduction",
          "Bagging",
          "Random Forests and Extra-Trees",
          "AdaBoost",
          "Gradient Boosting Machine",
          "XGBoost Installation",
          "XGBoost",
          "Project HR - Human Resources Analytics",
          "Ensemble of Ensembles Part 1",
          "Ensemble of ensembles Part 2"
        ],
        "k-Nearest Neighbours (kNN)": [
          "kNN Introduction",
          "Project Cancer Detection",
          "Addition Materials",
          "Project Cancer Detection Part 1"
        ],
        "Unsupervised Learning: Dimensionality Reduction": [
          "Dimensionality Reduction Concept",
          "PCA Introduction",
          "Project Wine",
          "Kernel PCA",
          "Kernel PCA Demo",
          "LDA vs PCA",
          "Project Abalone"
        ],
        "Unsupervised Learning: Clustering": [
          "Clustering",
          "k_Means Clustering"
        ]
      },
      "requirements": [
        "Basic Python programming knowledge is necessary",
        "Good understanding of linear algebra"
      ],
      "description": "The Complete Machine Learning Course in Python has been FULLY UPDATED for November 2019!\nWith brand new sections as well as updated and improved content, you get everything you need to master Machine Learning in one course! The machine learning field is constantly evolving, and we want to make sure students have the most up-to-date information and practices available to them:\nBrand new sections include:\nFoundations of Deep Learning covering topics such as the difference between classical programming and machine learning, differentiate between machine and deep learning, the building blocks of neural networks, descriptions of tensor and tensor operations, categories of machine learning and advanced concepts such as over- and underfitting, regularization, dropout, validation and testing and much more.\nComputer Vision in the form of Convolutional Neural Networks covering building the layers, understanding filters / kernels, to advanced topics such as transfer learning, and feature extractions.\nAnd the following sections have all been improved and added to:\nAll the codes have been updated to work with Python 3.6 and 3.7\nThe codes have been refactored to work with Google Colab\nDeep Learning and NLP\nBinary and multi-class classifications with deep learning\nGet the most up to date machine learning information possible, and get it in a single course!\n\n\n*         *         *\n\n\nThe average salary of a Machine Learning Engineer in the US is $166,000! By the end of this course, you will have a Portfolio of 12 Machine Learning projects that will help you land your dream job or enable you to solve real life problems in your business, job or personal life with Machine Learning algorithms.\nCome learn Machine Learning with Python this exciting course with Anthony NG, a Senior Lecturer in Singapore who has followed Rob Percival’s “project based\" teaching style to bring you this hands-on course.\nWith over 18 hours of content and more than fifty 5 star ratings, it's already the longest and best rated Machine Learning course on Udemy!\nBuild Powerful Machine Learning Models to Solve Any Problem\nYou'll go from beginner to extremely high-level and your instructor will build each algorithm with you step by step on screen.\nBy the end of the course, you will have trained machine learning algorithms to classify flowers, predict house price, identify handwritings or digits, identify staff that is most likely to leave prematurely, detect cancer cells and much more!\nInside the course, you'll learn how to:\nGain complete machine learning tool sets to tackle most real world problems\nUnderstand the various regression, classification and other ml algorithms performance metrics such as R-squared, MSE, accuracy, confusion matrix, prevision, recall, etc. and when to use them.\nCombine multiple models with by bagging, boosting or stacking\nMake use to unsupervised Machine Learning (ML) algorithms such as Hierarchical clustering, k-means clustering etc. to understand your data\nDevelop in Jupyter (IPython) notebook, Spyder and various IDE\nCommunicate visually and effectively with Matplotlib and Seaborn\nEngineer new features to improve algorithm predictions\nMake use of train/test, K-fold and Stratified K-fold cross validation to select correct model and predict model perform with unseen data\nUse SVM for handwriting recognition, and classification problems in general\nUse decision trees to predict staff attrition\nApply the association rule to retail shopping datasets\nAnd much much more!\nNo Machine Learning required. Although having some basic Python experience would be helpful, no prior Python knowledge is necessary as all the codes will be provided and the instructor will be going through them line-by-line and you get friendly support in the Q&A area.\nMake This Investment in Yourself\n\nIf you want to ride the machine learning wave and enjoy the salaries that data scientists make, then this is the course for you!\nTake this course and become a machine learning engineer!",
      "target_audience": [
        "Anyone willing and interested to learn machine learning algorithm with Python",
        "Any one who has a deep interest in the practical application of machine learning to real world problems",
        "Anyone wishes to move beyond the basics and develop an understanding of the whole range of machine learning algorithms",
        "Any intermediate to advanced EXCEL users who is unable to work with large datasets",
        "Anyone interested to present their findings in a professional and convincing manner",
        "Anyone who wishes to start or transit into a career as a data scientist",
        "Anyone who wants to apply machine learning to their domain"
      ]
    },
    {
      "title": "Python for Machine Learning & Data Science Masterclass",
      "url": "https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/",
      "bio": "Learn about Data Science and Machine Learning with Python! Including Numpy, Pandas, Matplotlib, Scikit-Learn and more!",
      "objectives": [
        "You will learn how to use data science and machine learning with Python.",
        "You will create data pipeline workflows to analyze, visualize, and gain insights from data.",
        "You will build a portfolio of data science projects with real world data.",
        "You will be able to analyze your own data sets and gain insights through data science.",
        "Master critical data science skills.",
        "Understand Machine Learning from top to bottom.",
        "Replicate real-world situations and data reports.",
        "Learn NumPy for numerical processing with Python.",
        "Conduct feature engineering on real world case studies.",
        "Learn Pandas for data manipulation with Python.",
        "Create supervised machine learning algorithms to predict classes.",
        "Learn Matplotlib to create fully customized data visualizations with Python.",
        "Create regression machine learning algorithms for predicting continuous values.",
        "Learn Seaborn to create beautiful statistical plots with Python.",
        "Construct a modern portfolio of data science and machine learning resume projects.",
        "Learn how to use Scikit-learn to apply powerful machine learning algorithms.",
        "Get set-up quickly with the Anaconda data science stack environment.",
        "Learn best practices for real-world data sets.",
        "Understand the full product workflow for the machine learning lifecycle.",
        "Explore how to deploy your machine learning models as interactive APIs."
      ],
      "course_content": {
        "Introduction to Course": [
          "Welcome to the Course!",
          "COURSE OVERVIEW LECTURE - PLEASE DO NOT SKIP!",
          "Anaconda Python and Jupyter Install and Setup",
          "Note on Environment Setup - Please read me!",
          "Environment Setup"
        ],
        "OPTIONAL: Python Crash Course": [
          "OPTIONAL: Python Crash Course",
          "Python Crash Course - Part One",
          "Python Crash Course - Part Two",
          "Python Crash Course - Part Three",
          "Python Crash Course - Exercise Questions",
          "Python Crash Course - Exercise Solutions"
        ],
        "Machine Learning Pathway Overview": [
          "Machine Learning Pathway"
        ],
        "NumPy": [
          "Introduction to NumPy",
          "NumPy Arrays",
          "Coding Exercise Check-in: Creating NumPy Arrays",
          "NumPy Indexing and Selection",
          "Coding Exercise Check-in: Selecting Data from Numpy Array",
          "NumPy Operations",
          "Check-In: Operations on NumPy Array",
          "NumPy Exercises",
          "Numpy Exercises - Solutions"
        ],
        "Pandas": [
          "Introduction to Pandas",
          "Series - Part One",
          "Check-in: Labeled Index in Pandas Series",
          "Series - Part Two",
          "DataFrames - Part One - Creating a DataFrame",
          "DataFrames - Part Two - Basic Properties",
          "DataFrames - Part Three - Working with Columns",
          "DataFrames - Part Four - Working with Rows",
          "Pandas - Conditional Filtering",
          "Pandas - Useful Methods - Apply on Single Column",
          "Pandas - Useful Methods - Apply on Multiple Columns",
          "Pandas - Useful Methods - Statistical Information and Sorting",
          "Missing Data - Overview",
          "Missing Data - Pandas Operations",
          "GroupBy Operations - Part One",
          "GroupBy Operations - Part Two - MultiIndex",
          "Combining DataFrames - Concatenation",
          "Combining DataFrames - Inner Merge",
          "Combining DataFrames - Left and Right Merge",
          "Combining DataFrames - Outer Merge",
          "Pandas - Text Methods for String Data",
          "Pandas - Time Methods for Date and Time Data",
          "Pandas Input and Output - CSV Files",
          "Pandas Input and Output - HTML Tables",
          "Pandas Input and Output - Excel Files",
          "Pandas Input and Output - SQL Databases",
          "Pandas Pivot Tables",
          "Pandas Project Exercise Overview",
          "Pandas Project Exercise Solutions"
        ],
        "Matplotlib": [
          "Introduction to Matplotlib",
          "Matplotlib Basics",
          "Matplotlib - Understanding the Figure Object",
          "Matplotlib - Implementing Figures and Axes",
          "Matplotlib - Figure Parameters",
          "Matplotlib - Subplots Functionality",
          "Matplotlib Styling - Legends",
          "Matplotlib Styling - Colors and Styles",
          "Advanced Matplotlib Commands (Optional)",
          "Matplotlib Exercise Questions Overview",
          "Matplotlib Exercise Questions - Solutions"
        ],
        "Seaborn Data Visualizations": [
          "Introduction to Seaborn",
          "Scatterplots with Seaborn",
          "Distribution Plots - Part One - Understanding Plot Types",
          "Distribution Plots - Part Two - Coding with Seaborn",
          "Categorical Plots - Statistics within Categories - Understanding Plot Types",
          "Categorical Plots - Statistics within Categories - Coding with Seaborn",
          "Categorical Plots - Distributions within Categories - Understanding Plot Types",
          "Categorical Plots - Distributions within Categories - Coding with Seaborn",
          "Seaborn - Comparison Plots - Understanding the Plot Types",
          "Seaborn - Comparison Plots - Coding with Seaborn",
          "Seaborn Grid Plots",
          "Seaborn - Matrix Plots",
          "Seaborn Plot Exercises Overview",
          "Seaborn Plot Exercises Solutions"
        ],
        "Data Analysis and Visualization Capstone Project Exercise": [
          "Capstone Project Overview",
          "Capstone Project Solutions - Part One",
          "Capstone Project Solutions - Part Two",
          "Capstone Project Solutions - Part Three"
        ],
        "Machine Learning Concepts Overview": [
          "Introduction to Machine Learning Overview Section",
          "Why Machine Learning?",
          "Types of Machine Learning Algorithms",
          "Supervised Machine Learning Process",
          "Companion Book - Introduction to Statistical Learning"
        ],
        "Linear Regression": [
          "Introduction to Linear Regression Section",
          "Linear Regression - Algorithm History",
          "Linear Regression - Understanding Ordinary Least Squares",
          "Linear Regression - Cost Functions",
          "Linear Regression - Gradient Descent",
          "Python coding Simple Linear Regression",
          "Overview of Scikit-Learn and Python",
          "Linear Regression - Scikit-Learn Train Test Split",
          "Linear Regression - Scikit-Learn Performance Evaluation - Regression",
          "Linear Regression - Residual Plots",
          "Linear Regression - Model Deployment and Coefficient Interpretation",
          "Polynomial Regression - Theory and Motivation",
          "Polynomial Regression - Creating Polynomial Features",
          "Polynomial Regression - Training and Evaluation",
          "Bias Variance Trade-Off",
          "Polynomial Regression - Choosing Degree of Polynomial",
          "Polynomial Regression - Model Deployment",
          "Regularization Overview",
          "Feature Scaling",
          "Introduction to Cross Validation",
          "Regularization Data Setup",
          "L2 Regularization - Ridge Regression Theory",
          "L2 Regularization - Ridge Regression - Python Implementation",
          "L1 Regularization - Lasso Regression - Background and Implementation",
          "L1 and L2 Regularization - Elastic Net",
          "Linear Regression Project - Data Overview"
        ]
      },
      "requirements": [
        "Basic Python Knowledge (capable of functions)"
      ],
      "description": "This is the most complete course online for learning about Python, Data Science, and Machine Learning. Join Jose Portilla's over 3 million students to learn about the future today!\nWhat is in the course?\nWelcome to the most complete course on learning Data Science and Machine Learning on the internet! After teaching over 2 million students I've worked for over a year to put together what I believe to be the best way to go from zero to hero for data science and machine learning in Python!\nThis course is designed for the student who already knows some Python and is ready to dive deeper into using those Python skills for Data Science and Machine Learning. The typical starting salary for a data scientists can be over $150,000 dollars, and we've created this course to help guide students to learning a set of skills to make them extremely hirable in today's workplace environment.\nWe'll cover everything you need to know for the full data science and machine learning tech stack required at the world's top companies. Our students have gotten jobs at McKinsey, Facebook, Amazon, Google, Apple, Asana, and other top tech companies! We've structured the course using our experience teaching both online and in-person to deliver a clear and structured approach that will guide you through understanding not just how to use data science and machine learning libraries, but why we use them. This course is balanced between practical real world case studies and mathematical theory behind the machine learning algorithms.\nWe cover advanced machine learning algorithms that most other courses don't! Including advanced regularization methods and state of the art unsupervised learning methods, such as DBSCAN.\nThis comprehensive course is designed to be on par with Bootcamps that usually cost thousands of dollars and includes the following topics:\nProgramming with Python\nNumPy with Python\nDeep dive into Pandas for Data Analysis\nFull understanding of Matplotlib Programming Library\nDeep dive into seaborn for data visualizations\nMachine Learning with SciKit Learn, including:\nLinear Regression\nRegularization\nLasso Regression\nRidge Regression\nElastic Net\nK Nearest Neighbors\nK Means Clustering\nDecision Trees\nRandom Forests\nNatural Language Processing\nSupport Vector Machines\nHierarchal Clustering\nDBSCAN\nPCA\nModel Deployment\nand much, much more!\n\n\nAs always, we're grateful for the chance to teach you data science, machine learning, and python and hope you will join us inside the course to boost your skillset!\n\n\n-Jose and Pierian Data Inc. Team",
      "target_audience": [
        "Beginner Python developers curious about Machine Learning and Data Science with Python"
      ]
    },
    {
      "title": "Complete Data Analysis with Pandas : Hands-on Pandas Python",
      "url": "https://www.udemy.com/course/data-analysis-with-pandas-python/",
      "bio": "Learn in demand skill Pandas, Sci-kit Learn, Numpy For Data Science & Machine Learning : Seaborn | MatplotLib | Python",
      "objectives": [
        "Update your resume with one of the in demand skill : Data analysis Pandas",
        "Setting up Python in anaconda environment",
        "Refresh Python basics with crash course",
        "Learn Most demanded python data analysis library : Pandas",
        "Three important data structure of pandas : Series, Data Frame, Panel",
        "Learn how to analyse one, two and three dimensional data",
        "How to group Data for analysis",
        "How to deal with Text Data with Pandas Functions",
        "Analyse data having multiple level index.",
        "Array and Matrix manipulation Library NumPy",
        "Master pandas with quizzes.",
        "Data Visualization Matplotlib and Seaborn Library",
        "Importing data from various different kinds of sources",
        "Complete Machine Learning work flow implementation with Scikit-learn"
      ],
      "course_content": {
        "Introduction": [
          "What is Data analysis",
          "Claim your Free Gift",
          "Course FAQ",
          "Join Online Classroom",
          "Introduction to Pandas",
          "Course FAQ",
          "How to get Certificate",
          "Pandas"
        ],
        "Installation and IDE": [
          "Different ways of installation",
          "Download and Install anaconda + Pandas",
          "Troubleshooting : 'conda' is not recognized as an internal or external command",
          "Anaconda + Conda Command",
          "Conda Cheatsheet",
          "anaconda, conda & pandas Update",
          "Getting started with Jupyter Lab",
          "Jupyter Notebook cheatsheet",
          "Import Library",
          "Pandas Version",
          "Installation"
        ],
        "Code Download": [
          "Python Code"
        ],
        "Python Crash Course [Optional]": [
          "Introduction",
          "Python Basics - I",
          "Data types, Numbers, String",
          "Data types",
          "Correct error in string variable",
          "Python Basics - II",
          "Loops & Decision making",
          "Lists and tuples",
          "Lists and tuples",
          "Dictionary and set",
          "Dictionary and set",
          "Functions",
          "Python - 1",
          "Python - 2"
        ],
        "Python Exercises": [
          "Exercise Overview",
          "Solutions"
        ],
        "Numpy": [
          "Creating NumPy array",
          "Numpy indexing and selection, Functions",
          "Some more Numpy Functions",
          "Linear algebra with NumPy",
          "List vs NumPy Array",
          "Views vs Copy - Numpy Array",
          "Insert, Append and Delete NumPy array",
          "Split, Concatenate, Tile and Repeat array",
          "NumPy"
        ],
        "Series : Pandas": [
          "Series",
          "Introduction to Series",
          "Create Series from Python Object",
          "Create Series from CSV file",
          "Create Series Object",
          "Series attributes & methods",
          "Series attributes & methods",
          "Label indexing",
          "Label indexing",
          "inplace parameter, sort_values & sort_index",
          "inplace parameter, sort_values & sort_index",
          "Apply Python built in function on Series",
          "Extract Value from Series",
          "Extract Value from Series",
          ".value_counts() Method",
          ".apply() and .map() method",
          ".apply() and .map() method",
          "Series"
        ],
        "Data Frame : Pandas": [
          "Introduction to Data Frame",
          "Create Data Frame - random data + from File",
          "Data frame attributes and methods",
          "Adding new column",
          "Select one or more than one column",
          "Broadcasting operation",
          "Drop missing row or column",
          "Filtering Data with one condition",
          "Filtering Data with multiple condition",
          "Filtering Data with .isin() method",
          "Filtering Data with .between() method",
          "unique() & nunique() method",
          "sorting values",
          "sort index and inplace parameter",
          ".loc() and .iloc() method",
          ".ix() method",
          ".astype() method - optimize memory requirement",
          "set_index() : change index column",
          ".apply() method on single column",
          ".apply() method on multiple column",
          "Fetch random sample"
        ],
        "Pandas Exercise": [
          "Exercise Overview : Google App store dataset",
          "Pandas Exercise Solution - I",
          "Pandas Exercise Solution - II"
        ],
        "Panel : Pandas": [
          "Warning - Panel Data type"
        ]
      },
      "requirements": [
        "Windows/Linux/MAC machine",
        "Basic idea about Programming concepts"
      ],
      "description": "JOIN OTHER 40,000 SUCCESSFUL STUDENTS WHO HAVE ALREADY ENROLLED & MASTERED PYTHON & PANDAS SKILLS (DATA ANALYSIS LIBRARY) WITH ONE OF MY BEST SELLING, TOP RATED COURSE.\nStudent Testimonial :\nGreat going, ankit is good at explanation of data processing stuff. i bought many of his course related to python and machine learning. - Jay\nEvery concept is clearly explained and the tutor of this course replies to every question asked in Q&A section. - Mukka Akshay\nIt was very good session. The instructor has enough knowledge and able to make me understand clearly. Thank you Ankit! - Bibek Baniya\nThis is an amazing course if you want to understand the extent of the power of Pandas. - Venkat Raj\nIt's one of the best course !!! Most of the topics has been covered and explained up to the expectation - Ankur SIngh\nit is a good match with what i was looking for, the instructor is quite knowledgeable. - Shivi Dhir\nThis class is not too fast or too slow, the way he teaches is perfect. - Frankie Y\nIt is excellent -  Rakhshee Misbah\ngood experience - Weiting\n-----------------------------------------------------------------------------------------------------------\nUpdate : New section on Data visualization library  Matplotlib and Seaborn added.\nUpdate : New section on Numpy Library get added.\n-----------------------------------------------------------------------------------------------------------\nIf you want to master most in-demand data analysis library pandas, carry on reading.\nHi, I am Ankit, one of the Best Selling author on Udemy, taught various courses on Data Science, Python, Pandas, PySpark, Model Deployment.\nBy the end of this course, you will able to apply all majority of Data analysis function on various different datasets with built in function available in pandas. Analysis techniques like exploratory data analysis, data transformation, data wrangling, time series data analysis, analysis through visualization and many more. Carry on reading to know more about course.\nThe era of Microsoft Excel is going to be over, so would you like to learn the next generation one of the most powerful data processing tool and in demand skill required for data analyst, data scientist and data engineer.\nThen this course is for you, welcome to the course on data analysis with python's most powerful data processing library Pandas.\n\n\nWhy this course?\nData scientist is one of the hottest skill of 21st century and many organisation are switching their project from Excel to Pandas the advanced Data analysis tool .\nThis course is basically design to get you started with Pandas library  at beginner level,  covering majority of important concepts of data processing data analysis and a Pandas library and make you feel confident about data processing task with Pandas at advanced level.\nWhat is this course?\nThis course covers\nBasics of Pandas library\nPython crash course for any of you want refresh basic concept of python\nPython anaconda and Pandas installation\nDetail understanding about two important data structure available in a Pandas library\nSeries data type\nData frame data type\nHow you can group the data for better analysis\nHow to use Pandas for text processing\nHow to visualize the data with Pandas inbuilt visualization tool\nMultilevel index in Pandas.\nTime series analysis\nNumerical Python : NumPy Library\nMatplotlib and Seaborn for Data visualization\nMachine Learning Theoretical background\nComplete end to end Machine Learning Model implementation with Scikit-learn API\n(from Importing Data to Splitting data, Fitting data and Evaluating Data) & How to Improve Machine Learning Model\nImporting Data from various different kind of file\nYou will get following after enrolling in this course.\n150+ HD quality video lecture\n16+ hours of content\nDiscussion forum to resolve your query.\nquizzes to to test your understanding\nThis course is still in a draft mode. I am still adding more and more content, quiz, projects related to data processing with different functionalities of Pandas. So stay tuned and enroll now.\nRegards\nAnkit Mistry",
      "target_audience": [
        "Beginner Python developer who is curious about Data Science, Not for experienced Data Scientist",
        "Anyone who want to make career in Data Science, Data analytics",
        "Anyone wants to learn data analysis with python language",
        "Excel user who wants to enhance data analysis skills."
      ]
    },
    {
      "title": "Fundamental Data Analysis and Visualization Tools in Python",
      "url": "https://www.udemy.com/course/data-analysis-and-visualization-tools/",
      "bio": "Learn how to analyze and visualize data by using Python libraries such as Plotly, Seaborn, Matplotlib, Pandas, and NumPy",
      "objectives": [
        "Create presentable data visualizations with Python",
        "Learn how to analyze data with Python",
        "Make interactive data visualizations using the Plotly module",
        "Learn how to create plots from your data using Matplotlib, and Seaborn",
        "Analyze data using the Pandas library to create and structure data",
        "Analyze data using the NumPy library to create and manipulate arrays",
        "Use the Jupyter Notebook Environment"
      ],
      "course_content": {
        "Introduction and Setup": [
          "Intro",
          "Setting Up"
        ],
        "Data Analysis - NumPy": [
          "Basic Array Operations with NumPy"
        ],
        "Data Analysis - Pandas": [
          "Structuring Data with Pandas"
        ],
        "Data Visualization - Matplotlib": [
          "Basic Data Visualization with Matplotlib"
        ],
        "Data Visualization - Seaborn": [
          "Making Attractive Plots with Seaborn"
        ],
        "Data Visualization - Plotly": [
          "Making Interactive Plots with Plotly"
        ],
        "Exercises": [
          "Titanic Dataset Visualization"
        ]
      },
      "requirements": [
        "Basic to Intermediate Python Skills",
        "Basic math skills",
        "A Desire to learn!"
      ],
      "description": "This course will provide an introduction to the fundamental Python tools for effectively analyzing and visualizing data. You will have a strong foundation in the field of Data Science!\nYou will gain an understanding of how to utilize Python in conjunction with scientific computing and graphing libraries to analyze data, and make presentable data visualizations.\nThis course is designed for both beginners with some basic programming experience or experienced developers looking to explore the world of Data Science!\n\n\nIn this course you will:\n- Learn how to create and analyze data arrays using the NumPy package\n- Learn how to use the Pandas library to create and analyze data sets\n- Learn how to use Matplotlib, and Seaborn to create professional, eye-catching data visualizations\n- Learn how to use Plotly to create interactive charts and plots\n\n\nYou will also get lifetime access to all the video lectures, detailed code notebooks for every lecture, as well as the ability to reach out to me anytime for directed inquiries and discussions.",
      "target_audience": [
        "Anyone interested in learning more about Python, Data Science, or Data Visualizations",
        "Developers interested in learning how to Analyze and Visualize data with Python",
        "Anyone interested about the rapidly expanding world of Data Science!"
      ]
    },
    {
      "title": "Math 0-1: Calculus for Data Science & Machine Learning",
      "url": "https://www.udemy.com/course/calculus-data-science/",
      "bio": "A Casual Guide for Artificial Intelligence, Deep Learning, and Python Programmers",
      "objectives": [
        "Limits, limit definition of derivative, derivatives from first principles",
        "Derivative rules (chain rule, product rule, quotient rule, implicit differentiation)",
        "Integration, area under curve, fundamental theorem of calculus",
        "Vector calculus, partial derivatives, gradient, Jacobian, Hessian, steepest ascent",
        "Optimize (maximize or minimize) a function",
        "l'Hopital's Rule",
        "Newton's Method"
      ],
      "course_content": {
        "Introduction and Outline": [
          "Introduction",
          "Outline",
          "How to Succeed in this Course",
          "Where to Get the Code"
        ],
        "Review": [
          "Functions Review",
          "Functions Review in Python"
        ],
        "Limits": [
          "What Are Limits?",
          "Precise Definition of Limit (Optional)",
          "Limit Laws",
          "Infinities and Asymptotes",
          "Indeterminate Forms",
          "Limits in Python",
          "Limits with Plotting in Python",
          "Limits Section Summary"
        ],
        "Derivatives From First Principles": [
          "Slopes, Tangent Lines, and Derivatives",
          "More On Tangent Lines, Derivative Checking",
          "Exercise: Quadratic",
          "Exercise: Cubic",
          "Exercise: Reciprocal",
          "Exercise: Root",
          "Alternate Notations & Higher Order Derivatives",
          "Derivative Checking in Python",
          "Derivatives Section Summary"
        ],
        "Derivative Rules": [
          "Power Rule",
          "Constant Multiple, Addition, Subtraction Rules",
          "Exponent Rule",
          "Exponent Rule (continued)",
          "Chain Rule",
          "Exercises: Chain Rule",
          "Product and Quotient Rules",
          "Exercises: Product and Quotient Rules",
          "Implicit Differentiation",
          "Logarithm Rule",
          "Implicit Differentiation Applications",
          "Logarithmic Differentiation",
          "Exercise: Derivatives of Hyperbolic Functions",
          "Exercise: Sum of Polynomials",
          "Exercise: Gaussian Variance",
          "Exercise: Entropy",
          "Trigonometric Functions (Optional)",
          "Inverse Trigonometric Functions (Optional)",
          "Derivative Rules Section Summary"
        ],
        "Applications of Differentiation": [
          "Finding the Minimum / Maximum",
          "Minimum / Maximum Clarifications and Examples",
          "Second Derivative Test",
          "Exercise: Minimums and Maximums",
          "Exercise: Entropy",
          "Exercise: Gaussian 1",
          "Exercise: Gaussian 2",
          "l'Hopital's Rule",
          "Newton's Method",
          "Newton's Method in Python",
          "Applications Section Summary"
        ],
        "Integration (Calculus 2)": [
          "Integrals: Section Introduction",
          "Area Under Curve",
          "Fundamental Theorem of Calculus (pt 1)",
          "Fundamental Theorem of Calculus (pt 2)",
          "Definite and Indefinite Integrals",
          "Exercises: Definite Integrals",
          "Exercises: Indefinite Integrals",
          "Exercises: Improper Integrals",
          "Numerical Integration in Python",
          "Integration Section Summary"
        ],
        "Vector Calculus in Multiple Dimensions (Calculus 3)": [
          "Functions of Multiple Variables",
          "Partial Differentiation",
          "The Gradient",
          "The Jacobian and Hessian",
          "Differentials and Chain Rule in Multiple Dimensions",
          "Why is the Gradient the Direction of Steepest Ascent?",
          "Steepest Ascent in Python",
          "Optimization and Lagrange Multipliers (pt 1)",
          "Optimization and Lagrange Multipliers (pt 2)",
          "Vector Calculus Section Summary"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (Appendix/FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow",
          "Where To Get the Code Troubleshooting",
          "How to use Github & Extra Coding Tips (Optional)"
        ]
      },
      "requirements": [
        "Firm understanding of high school math"
      ],
      "description": "Common scenario: You try to get into machine learning and data science, but there's SO MUCH MATH.\nEither you never studied this math, or you studied it so long ago you've forgotten it all.\nWhat do you do?\nWell my friends, that is why I created this course.\nCalculus is one of the most important math prerequisites for machine learning. It's required to understand probability and statistics, which form the foundation of data science. Backpropagation, the learning algorithm behind deep learning and neural networks, is really just calculus with a fancy name.\nIf you want to do machine learning beyond just copying library code from blogs and tutorials, you must know calculus.\nNormally, calculus is split into 3 courses, which takes about 1.5 years to complete.\nLuckily, I've refined these teachings into just the essentials, so that you can learn everything you need to know on the scale of hours instead of years.\nThis course will cover Calculus 1 (limits, derivatives, and the most important derivative rules), Calculus 2 (integration), and Calculus 3 (vector calculus). It will even include machine learning-focused material you wouldn't normally see in a regular college course. We will even demonstrate many of the concepts in this course using the Python programming language (don't worry, you don't need to know Python for this course). In other words, instead of the dry old college version of calculus, this course takes just the most practical and impactful topics, and provides you with skills directly applicable to machine learning and data science, so you can start applying them today.\nAre you ready?\nLet's go!\n\n\nSuggested prerequisites:\nFirm understanding of high school math (functions, algebra, trigonometry)",
      "target_audience": [
        "Anyone who wants to learn calculus quickly",
        "Students and professionals interested in machine learning and data science but who've gotten stuck on the math"
      ]
    },
    {
      "title": "The Ultimate Beginners Guide to Natural Language Processing",
      "url": "https://www.udemy.com/course/the-ultimate-beginners-guide-to-natural-language-processing/",
      "bio": "Learn step-by-step the main concepts of natural language processing in Python! Build a sentiment classifier!",
      "objectives": [
        "Understand the basic concepts of natural language processing, such as: part-of-speech, lemmatization, stemming, named entity recognition, and stop words",
        "Understand more advanced concepts, such as: dependency parsing, tokenization, word and sentence similarity",
        "Load texts from the Internet to apply natural language processing techniques",
        "How to visualize the most frequent terms using wordcloud",
        "Implement text summarization and keyword search",
        "Learn how to represent texts using Bag of Words and TF-IDF",
        "Implement sentiment analysis using NLTK library (natural language toolkit), TF-IDF and spaCy library"
      ],
      "course_content": {},
      "requirements": [
        "Programming logic",
        "Basic Python programming"
      ],
      "description": "The area of Natural Language Processing (NLP) is a subarea of Artificial Intelligence that aims to make computers capable of understanding human language, both written and spoken. Some examples of practical applications are: translators between languages, translation from text to speech or speech to text, chatbots, automatic question and answer systems (Q&A), automatic generation of descriptions for images, generation of subtitles in videos, classification of sentiments in sentences, among many others! Learning this area can be the key to bringing real solutions to present and future needs!\nBased on that, this course was designed for those who want to grow or start a new career in Natural Language Processing, using the spaCy and NLTK (Natural Language Toolkit) libraries and the Python programming language! SpaCy was developed with the focus on use in production and real environments, so it is possible to create applications that process a lot of data. It can be used to extract information, understand natural language and even preprocess texts for later use in deep learning models.\nThe course is divided into three parts:\nIn the first one, you will learn the most basic natural language processing concepts, such as: part-of-speech, lemmatization, stemming, named entity recognition, stop words, dependency parsing, word and sentence similarity and tokenization\nIn the second part, you will learn more advanced topics, such as: preprocessing function, word cloud, text summarization, keyword search, bag of words, TF-IDF (Term Frequency - Inverse Document Frequency), and cosine similarity. We will also simulate a chatbot that can answer questions about any subject you want!\nFinally, in the third and last part of the course, we will create a sentiment classifier using a real Twitter dataset! We will implement the classifier using NLTK, TF-IDF and also the spaCy library\nThis can be considered the first course in natural language processing, and after completing it, you can move on to more advanced materials. If you have never heard about natural language processing, this course is for you! At the end you will have the practical background to develop some simple projects and take more advanced courses. During the lectures, the code will be implemented step by step using Google Colab, which will ensure that you will have no problems with installations or configurations of software on your local machine.",
      "target_audience": [
        "People interested in natural language processing",
        "People interested in the spaCy and NLTK libraries",
        "Students who are studying subjects related to Artificial Intelligence",
        "Data Scientists who want to increase their knowledge in natural language processing"
      ]
    },
    {
      "title": "Professional Certificate in SQL and SQL for Data Analysis",
      "url": "https://www.udemy.com/course/professional-certificate-in-sql-and-sql-for-data-analysis/",
      "bio": "Structured Query Language, SQL, SQLite, Retrieving and Manipulating Data, Commands, Queries, Data Aggregation, Features",
      "objectives": [
        "SQL Mastery: From Beginner to Pro",
        "Introduction to SQL and SQLite",
        "Basic SQL Commands – The Foundation",
        "Retrieving and Manipulating Data",
        "Advanced Queries and Data Aggregation",
        "Working with Joins",
        "Subqueries and Nested Queries",
        "Modifying Data in SQL",
        "Optimising and Indexing Your Queries",
        "Advanced SQL Features"
      ],
      "course_content": {},
      "requirements": [
        "For a better learning experience, we suggest you to use a laptop / mobile phone / pen and paper for taking notes, highlighting important points, and making summaries to reinforce your learning."
      ],
      "description": "Welcome to Program: Professional Certificate in SQL and SQL for Data Analysis by MTF Institute\n\n\nCourse provided by MTF Institute of Management, Technology and Finance\nMTF is the global educational and research institute with HQ at Lisbon, Portugal, focused on business & professional hybrid (on-campus and online) education at areas: Business & Administration, Science & Technology, Banking & Finance.\nMTF R&D center focused on research activities at areas: Artificial Intelligence, Machine Learning, Data Science, Big Data, WEB3, Blockchain, Cryptocurrency & Digital Assets, Metaverses, Digital Transformation, Fintech, Electronic Commerce, Internet of Things.\nMTF is the official partner of: IBM, Intel, Microsoft, member of the Portuguese Chamber of Commerce and Industry.\nMTF is present in 215 countries and has been chosen by more than 660 000 students.\n\n\nCourse Author:\nDr. Alex Amoroso is a seasoned professional with a rich background in academia and industry, specializing in research methodologies, strategy formulation, and product development. With a Doctorate Degree from the School of Social Sciences and Politics in Lisbon, Portugal, where she was awarded distinction and honour for her exemplary research, Alex Amoroso brings a wealth of knowledge and expertise to the table.\nIn addition to her doctoral studies, Ms. Amoroso has served as an invited teacher, delivering courses on to wide range of students from undergraduate level to business students of professional and executives courses. Currently, at EIMT in Zurich, Switzerland, she lectures for doctoral students, offering advanced instruction in research design and methodologies, and in MTF Institute Ms. Amoroso is leading Product Development academical domain.\nIn synergy between academical and business experience, Ms. Amoroso achieved high results in business career, leading R&D activities, product development, strategic development, market analysis activities in wide range of companies. She implemented the best market practices in industries from Banking and Finance, to PropTech, Consulting and Research, and Innovative Startups.\nAlex Amoroso's extensive scientific production includes numerous published articles in reputable journals, as well as oral presentations and posters at international conferences. Her research findings have been presented at esteemed institutions such as the School of Political and Social Sciences and the Stressed Out Conference at UCL, among others.\nWith a passion for interdisciplinary collaboration and a commitment to driving positive change, Alex Amoroso is dedicated to empowering learners and professionals for usage of cutting edge methodologies for achieving of excellence in global business world.\n\n\nCourse overview\n● Introduction to SQL and SQLite\n● Basic SQL Commands – The Foundation\n● Retrieving and Manipulating Data\n● Advanced Queries and Data Aggregation\n● Working with Joins\n● Subqueries and Nested Queries\n● Modifying Data in SQL\n● Optimizing and Indexing Your Queries\n● Advanced SQL Features\n\n\nWhat is SQL\n● SQL stands for Structured Query Language\n● It’s a domain-specific language for managing relational databases\n● SQL is used to:\n○ Create and modify databases\n○ Query data\n○ Insert, update, and delete records\n● Examples of databases: MySQL, PostgreSQL, Oracle, SQLite\n● SQL is universal across relational database systems\n\n\nWhy SQL is Important for Business\n● Marketing: SQL helps analyse customer behaviours and optimise campaigns.\n● Finance: SQL is essential for tracking financial transactions and generating reports.\n● Data Analytics: Extracts valuable business insights from large datasets.\n● Operations: SQL can help streamline and automate workflows.\n● Data-driven Decision Making: Businesses can make more informed decisions by analysing their data with SQL.\n\n\nWhat is SQLite\n● SQLite is a self-contained, serverless SQL database engine\n● Open-source and free to use\n● Designed for embedded database applications\n● Easy to install and use\n● Ideal for development, testing, and small-scale applications\n● SQLite works on multiple platforms: Windows, Mac, Linux, and mobile devices",
      "target_audience": [
        "No special requirements. A course for anyone who wants to build career in business and Data Analysis"
      ]
    },
    {
      "title": "LangChain 101 for Beginners (OpenAI / ChatGPT / LLMOps)",
      "url": "https://www.udemy.com/course/langchain-python/",
      "bio": "Learn all the basics of LangChain by building LLM-powered Python applications with OpenAI, HuggingFace and Chroma!",
      "objectives": [
        "Master the basics of LangChain and the fundamentals of Large Language Models (LLMs) from industry leaders such as OpenAI and HuggingFace.",
        "Gain proficiency in creating, calling, and chaining prompts for effective and interactive applications.",
        "Develop an understanding of conversational chatbots within LangChain, along with exploring memory functionalities for sophisticated responses.",
        "Learn to apply LLM techniques to personal documents and projects, paving the way for real-world application of course knowledge."
      ],
      "course_content": {
        "Introduction": [
          "Welcome to this course!",
          "Read this before you begin!"
        ],
        "LangChain 101": [
          "Getting Started with LangChain and OpenAI",
          "Calling Prompts with LLMs",
          "Using different Large Language Models",
          "Prompt Templating and Chaining",
          "Using Simple Sequential Chains",
          "Action Agents",
          "Human as a Tool",
          "Plan and Execute Agents",
          "Memory and Chat Bots",
          "Storing and Retrieving Chat History",
          "Document Loading and Retrieval Chains"
        ]
      },
      "requirements": [
        "Basic programming experience with Python!"
      ],
      "description": "Ready for an electrifying plunge into the universe of language technology? Prepare to enter the thrilling realm of LangChain with \"LangChain 101 for Beginners (OpenAI / ChatGPT / LLMOps)\", where you'll be taught how to harness the power of LangChain and Large Language Models (LLMs) to build your very own Python applications.\nOur aim for this course is simple - to equip you with everything you need to embark on your LangChain adventure. You'll be walked through using different LLMs from industry giants OpenAI and HuggingFace, understand the magic of calling prompts, creating templates, and chaining these prompts together to create a robust, interactive system.\nBut that's not all! We’ll dive into the heart of conversational chatbots and explore how memory works within LangChain. We'll wrap things up with a detailed tutorial on how you can apply these impressive LLMs to your own documents.\nThis course isn’t just informative—it’s also seriously fun. Through the use of memes, real-world analogies, and an engaging, down-to-earth approach, we've designed this course to be an enjoyable journey into the world of LangChain.\nSay goodbye to those long, never-ending courses that are all fluff and no substance. This course is compact, to-the-point, and perfect for Python developers looking for a fast-track introduction to LangChain and LLMs. We know your time is precious, so we've packed all the essential information into one power-packed hour.\n\"LangChain 101 for Beginners\" is your golden ticket to understanding and implementing LangChain. By the end of this course, you'll not only have a comprehensive understanding of LangChain, but also be ready to dive headfirst into your next project with a newfound arsenal of skills and knowledge.\nDon't wait—let's start scripting the future, together. Let’s dive into the incredible world of LangChain and Large Language Models, and have some fun along the way!",
      "target_audience": [
        "Python Developers of any skill level interested in building LLM-powered Python applications with LangChain"
      ]
    },
    {
      "title": "Unsupervised Deep Learning in Python",
      "url": "https://www.udemy.com/course/unsupervised-deep-learning-in-python/",
      "bio": "Autoencoders, Restricted Boltzmann Machines, Deep Neural Networks, t-SNE and PCA",
      "objectives": [
        "Understand the theory behind principal components analysis (PCA)",
        "Know why PCA is useful for dimensionality reduction, visualization, de-correlation, and denoising",
        "Derive the PCA algorithm by hand",
        "Write the code for PCA",
        "Understand the theory behind t-SNE",
        "Use t-SNE in code",
        "Understand the limitations of PCA and t-SNE",
        "Understand the theory behind autoencoders",
        "Write an autoencoder in Theano and Tensorflow",
        "Understand how stacked autoencoders are used in deep learning",
        "Write a stacked denoising autoencoder in Theano and Tensorflow",
        "Understand the theory behind restricted Boltzmann machines (RBMs)",
        "Understand why RBMs are hard to train",
        "Understand the contrastive divergence algorithm to train RBMs",
        "Write your own RBM and deep belief network (DBN) in Theano and Tensorflow",
        "Visualize and interpret the features learned by autoencoders and RBMs",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Introduction and Outline": [
          "Introduction and Outline",
          "How to Succeed in this Course",
          "Where to get the code and data",
          "Tensorflow or Theano - Your Choice!",
          "What are the practical applications of unsupervised deep learning?",
          "Where does this course fit into your deep learning studies?"
        ],
        "Principal Components Analysis": [
          "What does PCA do?",
          "How does PCA work?",
          "Why does PCA work? (PCA derivation)",
          "PCA only rotates",
          "MNIST visualization, finding the optimal number of principal components",
          "PCA implementation",
          "PCA for NLP",
          "PCA objective function",
          "PCA Application: Naive Bayes",
          "SVD (Singular Value Decomposition)",
          "Suggestion Box"
        ],
        "t-SNE (t-distributed Stochastic Neighbor Embedding)": [
          "t-SNE Theory",
          "t-SNE Visualization",
          "t-SNE on the Donut",
          "t-SNE on XOR",
          "t-SNE on MNIST"
        ],
        "Autoencoders": [
          "Autoencoders",
          "Denoising Autoencoders",
          "Stacked Autoencoders",
          "Writing the autoencoder class in code (Theano)",
          "Testing our Autoencoder (Theano)",
          "Writing the deep neural network class in code (Theano)",
          "Autoencoder in Code (Tensorflow)",
          "Testing greedy layer-wise autoencoder training vs. pure backpropagation",
          "Cross Entropy vs. KL Divergence",
          "Deep Autoencoder Visualization Description",
          "Deep Autoencoder Visualization in Code",
          "An Autoencoder in 1 Line of Code"
        ],
        "Restricted Boltzmann Machines": [
          "Basic Outline for RBMs",
          "Introduction to RBMs",
          "Motivation Behind RBMs",
          "Intractability",
          "Neural Network Equations",
          "Training an RBM (part 1)",
          "Training an RBM (part 2)",
          "Training an RBM (part 3) - Free Energy",
          "RBM Greedy Layer-Wise Pretraining",
          "RBM in Code (Theano) with Greedy Layer-Wise Training on MNIST",
          "RBM in Code (Tensorflow)"
        ],
        "The Vanishing Gradient Problem": [
          "The Vanishing Gradient Problem Description",
          "The Vanishing Gradient Problem Demo in Code"
        ],
        "Applications to NLP (Natural Language Processing)": [
          "Application of PCA and SVD to NLP (Natural Language Processing)",
          "Latent Semantic Analysis in Code",
          "Application of t-SNE + K-Means: Finding Clusters of Related Words"
        ],
        "Applications to Recommender Systems": [
          "Recommender Systems Section Introduction",
          "Why Autoencoders and RBMs work",
          "Data Preparation and Logistics",
          "Data Preprocessing Code",
          "AutoRec",
          "AutoRec in Code",
          "Categorical RBM for Recommender System Ratings",
          "Recommender RBM Code pt 1",
          "Recommender RBM Code pt 2",
          "Recommender RBM Code pt 3",
          "Recommender RBM Code Speedup"
        ],
        "Theano and Tensorflow Basics Review": [
          "(Review) Theano Basics",
          "(Review) Theano Neural Network in Code",
          "(Review) Tensorflow Basics",
          "(Review) Tensorflow Neural Network in Code"
        ],
        "Appendix / FAQ Finale": [
          "What is the Appendix?"
        ]
      },
      "requirements": [
        "Knowledge of calculus and linear algebra",
        "Python coding skills",
        "Some experience with Numpy, Theano, and Tensorflow",
        "Know how gradient descent is used to train machine learning models",
        "Install Python, Numpy, and Theano",
        "Some probability and statistics knowledge",
        "Code a feedforward neural network in Theano or Tensorflow"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis course is the next logical step in my deep learning, data science, and machine learning series. I’ve done a lot of courses about deep learning, and I just released a course about unsupervised learning, where I talked about clustering and density estimation. So what do you get when you put these 2 together? Unsupervised deep learning!\nIn these course we’ll start with some very basic stuff - principal components analysis (PCA), and a popular nonlinear dimensionality reduction technique known as t-SNE (t-distributed stochastic neighbor embedding).\nNext, we’ll look at a special type of unsupervised neural network called the autoencoder. After describing how an autoencoder works, I’ll show you how you can link a bunch of them together to form a deep stack of autoencoders, that leads to better performance of a supervised deep neural network. Autoencoders are like a non-linear form of PCA.\nLast, we’ll look at restricted Boltzmann machines (RBMs). These are yet another popular unsupervised neural network, that you can use in the same way as autoencoders to pretrain your supervised deep neural network. I’ll show you an interesting way of training restricted Boltzmann machines, known as Gibbs sampling, a special case of Markov Chain Monte Carlo, and I’ll demonstrate how even though this method is only a rough approximation, it still ends up reducing other cost functions, such as the one used for autoencoders. This method is also known as Contrastive Divergence or CD-k. As in physical systems, we define a concept called free energy and attempt to minimize this quantity.\nFinally, we’ll bring all these concepts together and I’ll show you visually what happens when you use PCA and t-SNE on the features that the autoencoders and RBMs have learned, and we’ll see that even without labels the results suggest that a pattern has been found.\nAll the materials used in this course are FREE. Since this course is the 4th in the deep learning series, I will assume you already know calculus, linear algebra, and Python coding. You'll want to install Numpy, Theano, and Tensorflow for this course. These are essential items in your data analytics toolbox.\nIf you are interested in deep learning and you want to learn about modern deep learning developments beyond just plain backpropagation, including using unsupervised neural networks to interpret what features can be automatically and hierarchically learned in a deep learning system, this course is for you.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus\nlinear algebra\nprobability\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\ncan write a feedforward neural network in Theano or Tensorflow\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "Students and professionals looking to enhance their deep learning repertoire",
        "Students and professionals who want to improve the training capabilities of deep neural networks",
        "Students and professionals who want to learn about the more modern developments in deep learning"
      ]
    },
    {
      "title": "Data Engineer/Data Scientist - Power BI/ Python/ ETL/SSIS",
      "url": "https://www.udemy.com/course/data-engineerdata-scientist-power-bi-python-etlssis/",
      "bio": "Hands-on Data Interaction and Manipulation.",
      "objectives": [
        "Connecting to various data sources",
        "Extracting Data",
        "Cleaning Data",
        "Transforming Data",
        "Loading Data",
        "Data Visualization",
        "Exploring Pandas DataFrame",
        "Creating SSIS Package",
        "Debugging SSIS Package",
        "Creating and publishing reports",
        "Data Modeling"
      ],
      "course_content": {
        "Power BI Setup": [
          "Introduction",
          "What is Power BI",
          "What is Power BI Desktop",
          "Office 365 Setup",
          "Activating Office 365 Apps",
          "Logging into Office 365",
          "Installing Power BI Desktop",
          "Power BI Desktop Tour"
        ],
        "Overview of Power BI": [
          "Power BI Overview- Part 1",
          "Power BI Overview- Part 2",
          "Power BI Overview- Part 3",
          "Components of Power BI",
          "Building Blocks of Power BI",
          "Exploring Power BI Desktop Interface",
          "Exploring Power BI Service",
          "Power BI Apps"
        ],
        "Power BI : Connect to Web Data | Transform Web Data": [
          "Connecting to Web Based Data",
          "Clean and transform data: Part 1",
          "Clean and transform data: Part 2",
          "Combining Data Sources",
          "Creating visuals - Part 1",
          "Creating visuals - Part 2",
          "Publishing Report to Power BI Service"
        ],
        "Database Server Setups": [
          "What is SQL Server",
          "Minimum SQL Server Installation Requirements",
          "SQL Server Editions",
          "SQL Server Download",
          "SQL Server Installation",
          "Install SSMS",
          "Connecting to SQL Server with SSMS",
          "Install Sample Database in SQL Server",
          "What is PostgreSQL",
          "PostgreSQL Minimum Installation Requirements",
          "Installing PostgreSQL",
          "Installing PostgreSQL on MacOS",
          "Installing PostgreSQL on Linux",
          "Connecting to PostgreSQL Server with pgAdmin4",
          "Installing PgAmin on MacOS",
          "Installing PgAmin on Linux",
          "Install Sample Database in PostgreSQL"
        ],
        "Power BI: Connecting to Databases Data Sources": [
          "Connecting to SQL Sever with Power BI",
          "Connecting to PostgreSQL Server with Power BI",
          "Connecting to Access Database File"
        ],
        "Power BI: Transforming Data": [
          "Changing Locale",
          "Connecting to Access Database File",
          "Power Query Editor and Queries",
          "Creating and Managing Query Groups",
          "Renaming Queries",
          "Splitting Columns",
          "Changing Data Types",
          "Removing and Reordering Columns",
          "Duplicating and Adding Columns",
          "Creating Conditional Columns",
          "Connecting to files in a folder",
          "Appending Queries",
          "Merge Queries",
          "Query Dependency View",
          "Transforming Less Structured Data: Part 1",
          "Transforming Less Structured Data: Part 2",
          "Creating Tables",
          "Query Parameters"
        ],
        "Power BI: Data Modelling": [
          "What is Data Modelling",
          "Creating and managing data relationships",
          "Creating Calculated Columns",
          "Connecting to PostgreSQL Database Server - Part 2",
          "Optimizing Models for reporting",
          "Time Intelligence",
          "Applying Filters on Visuals"
        ],
        "Microsoft Visual Studio Setup": [
          "What is Visual Studio",
          "Minimum installation requirements for visual studio",
          "Installing Visual Studio",
          "Visual studio workloads",
          "Installing SQL Server Data Tools- SSDT",
          "Installing SSDT Designer Templates"
        ],
        "Implementing ETL Using SSIS": [
          "What is ETL",
          "ETL Process Illustration",
          "What is SSIS",
          "Creating SSIS Package in Visual Studio 2019",
          "Creating SSIS Package in Visual Studio 2015",
          "Exploring data source: Part 1",
          "Exploring data source: Part 2",
          "Control flow: Part 1",
          "Control flow: Part 2",
          "Data flow: part 1",
          "Data flow: part 2",
          "Debugging SSIS Package : Part 1",
          "Debugging SSIS Package : Part 2",
          "Handling SSIS Package Errors : part 1",
          "Handling SSIS Package Errors : part 2"
        ],
        "Data Science | Python Setup": [
          "What is Jupyter Notebook",
          "Installing Jupyter Notebook Server",
          "Running Jupyter Server",
          "Jupyter Notebook Commands",
          "Jupyter Notebook Components",
          "Jupyter Notebook Interface",
          "Jupyter Notebook Dashboard",
          "Creating a new Jupyter Notebook"
        ]
      },
      "requirements": [
        "A Computer with Internet access is required."
      ],
      "description": "A common problem that organizations face is how to gathering data from multiple sources, in multiple formats, and move it to one or more data stores. The destination may not be the same type of data store as the source, and often the format is different, or the data needs to be shaped or cleaned before loading it into its final destination.\nExtract, transform, and load (ETL) is a data pipeline used to collect data from various sources, transform the data according to business rules, and load it into a destination data store.\nSQL Server Integration Services (SSIS) is a useful  and powerful Business Intelligence Tool . It is best suited to work with SQL Server Database . It is added to SQL Server Database when you install SQL Server Data Tools  (SSDT)which adds the Business Intelligence Templates to Visual studio that is used to create Integration projects.\nSSIS can be used for:\nData Integration\nData Transformation\nProviding solutions to complex Business problems\nUpdating data warehouses\nCleaning data\nMining data\nManaging SQL Server objects and data\nExtracting data from a variety of sources\nLoading data into one or several destinations\nPower BI is a business analytics solution that lets you visualize your data and share insights across your organization, or embed them in your app or website. Connect to hundreds of data sources and bring your data to life with live dashboards and reports.\nDiscover how to quickly glean insights from your data using Power BI. This formidable set of business analytics tools—which includes the Power BI service, Power BI Desktop, and Power BI Mobile—can help you more effectively create and share impactful visualizations with others in your organization.\nIn this beginners course you will learn how to  get started with this powerful toolset.  We will  cover topics like  connecting to and transforming web based data sources.  You will learn how to publish and share your reports and visuals on the Power BI service.\nData science is the study of data. It involves developing methods of recording, storing, and analyzing data to effectively extract useful information\nData is a fundamental part of our everyday work, whether it be in the form of valuable insights about our customers, or information to guide product,policy or systems development.   Big business, social media, finance and the public sector all rely on data scientists to analyse their data and draw out business-boosting insights.\nPython is a dynamic modern object -oriented programming language that is easy to learn and can be used to do a lot of things both big and small. Python is what is referred to as a high level language. That means it is a language that is closer to humans than computer.It is also known as a general purpose programming language due to it's flexibility. Python is used a lot in data science.\nThis course is a beginners course that will introduce you to some basics of data science  using Python.\n\n\nWhat You Will Learn\nHow to set up environment to explore using Jupyter Notebook\nHow to import Python Libraries into your environment\nHow to work with Tabular data\nHow to explore a Pandas DataFrame\nHow to explore a Pandas Series\nHow to Manipulate a Pandas  DataFrame\nHow to clean data\nHow to visualize data",
      "target_audience": [
        "Beginners to Data Science",
        "Beginners to Data Engineering",
        "Beginner Data Analyst",
        "Beginner Data Engineer"
      ]
    },
    {
      "title": "The Local LLM Crash Course - Build an AI Chatbot in 2 hours!",
      "url": "https://www.udemy.com/course/the-local-llm-crash-course-build-a-hugging-face-ai-chatbot/",
      "bio": "The only course you need to build a ChatGPT-like App in Python. Learn LLMs, Prompts, Hugging Face & LangChain Hands On!",
      "objectives": [
        "Learn to use Large Language Models like Llama2 and Orca in Python locally",
        "Understand Prompt Engineering techniques for making LLMs ready to Chat",
        "Use Hugging Face models in Python",
        "Teach an LLM to keep Conversational Memory using Prompt Engineering",
        "Learn to display model responses real-time, ChatGPT-style",
        "Integrate your LLM into Modern Python Chat Interfaces like Chainlit"
      ],
      "course_content": {
        "Introduction and Environment Setup": [
          "Introduction",
          "What will you learn?",
          "Prerequisites and Requirements",
          "Course Blueprint",
          "DO NOT SKIP - How to take the most out of this course",
          "BOOKMARK THESE - Course Resources",
          "Setting up the Coding Environment",
          "A Quiz on Codespaces"
        ],
        "Getting a Local LLM to Work": [
          "Introduction to Huggingface",
          "Finding the best the LLM for our use case",
          "Further Resources on LLMs",
          "Our Huggingface Model",
          "Getting the LLM to Work - Completing Sentences",
          "Can you make the model say the capital of India?",
          "A Prompt Engineering Trick you could use in the Exercise",
          "Speeding up our LLM - Streaming the LLM Response",
          "The Local LLM Quiz"
        ],
        "Adding Chat Capabilities to the LLM": [
          "Understanding Prompts for Chat and Instruction",
          "Python Type Hints",
          "Implementing the LLM Chat Prompt Logic",
          "Using LLama2 instead Orca",
          "The LLM Chat Logic Quiz"
        ],
        "Implementing Conversational Memory": [
          "Prompt Engineering for Achieving Conversational Memory"
        ],
        "Building the Chat UI": [
          "Introduction to Chainlit and Chainlit Setup",
          "Message Processing in Chainlit",
          "Customizing Chainlit",
          "Integrating our Model to the Chat UI",
          "Real-time response streaming to the UI",
          "Conversational Memory in Chainlit",
          "The Chat UI Quiz"
        ],
        "Capstone Exercise and Wrapping it Up": [
          "Capstone Exercise: Multiple LLMs and History Management in Chainlit"
        ],
        "EXTRA: Use LangChain to Make our Chatbot Production-Ready!": [
          "LangChain Introduction",
          "Integrating a Hugging Face Model into LangChain and Managing Response Length",
          "The LangChain Prompt Template",
          "Conversational Memory in LangChain",
          "Chainlit's LangChain Integration & Token Streaming",
          "Congrats and Wrapping Up",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic Python Programming Knowledge",
        "A working internet connection with unrestricted access to GitHub"
      ],
      "description": "Unlock the Power of AI Chatbots on Your Own Computer!\nAre you intrigued by the potential of AI but concerned about privacy and data security? Dive into the world of local large language models (LLMs) with our hands-on crash course, designed to empower you with the skills to build your very own ChatGPT-like chatbot using pure Python and later LangChain. This course cuts through the complexity, offering a direct path to deploying your LLM securely on your own devices.  We set up the environment for you so you can head start in a minute or choose to use your own computer. No Installation Needed!\n\n\nWhy Learn From Me?\nHello, my name is Zoltan, your guide on this exciting journey. With years of experience in data engineering, machine learning, and software development, I've designed this course to bridge the gap between theoretical knowledge and practical skills. My expertise in leveraging these technologies ensures you learn from a seasoned professional. Join me to gain control over your AI applications, ensuring they're both powerful and private.\nCourse Highlights\nRapid Learning Path: Zero to a working local LLM in just two hours.\nModern Technologies: You will use Hugging Face, Chainlit and top-of-the-line LLM models\nHands-On Learning: Engage in a bootcamp-style, practical coding environment from the start.\nClear Instructions: I'll ensure you understand every step on your way to building a fully functional AI chatbot.\n\n\nWhy This Course Matters\nIn today's digital age, AI and chatbots are revolutionizing how we interact with technology. However, the true potential is unlocked when these tools are customized and controlled locally. Whether you're looking to enhance your personal projects, secure proprietary data, or simply curious about the inner workings of AI chatbots, this course provides valuable insights and skills. Big-name tech employers and innovative startups alike are searching for professionals who can navigate the complexities of AI while prioritizing privacy and security.\n\n\nWho Should Enroll?\nThis course is perfect for:\nDevelopers seeking to integrate AI into their projects without compromising on privacy.\nHobbyists interested in AI and looking for a secure, hands-on approach to learning.\nProfessionals aiming to enhance their resume with cutting-edge skills.\nPre-requisites: A basic understanding of programming concepts is recommended to get the most out of this course.\nNot For:\nThose seeking a deep theoretical overview without hands-on practice.\nIndividuals not comfortable with basic coding.\nYour Takeaway\nBy the end of this course, you'll have a fully functional, locally-run AI chatbot, similar to ChatGPT, built by you. More importantly, you'll have the knowledge and confidence to apply these skills in a variety of scenarios, ensuring your digital creations remain secure and private.\n\n\nWhy Wait? Enroll Now!\nEmbrace the future of AI with confidence. This course isn't just about building a chatbot; it's about unlocking a new realm of possibilities, secure in the knowledge that you're in complete control. The course comes with a 30-day money-back guarantee, so Enroll today and take the first step toward becoming a local LLM expert!",
      "target_audience": [
        "You want to understand how to use Large Language Models can be used",
        "You want to build your own AI Chat using Huggingface",
        "You are interested how to implement chat functionalities for LLMs",
        "You want to be hands-on creating your own AI chatbots"
      ]
    },
    {
      "title": "Complete Machine Learning with R Studio - ML for 2025",
      "url": "https://www.udemy.com/course/machine-learning-with-r-studio/",
      "bio": "Linear & Logistic Regression, Decision Trees, XGBoost, SVM & other ML models in R programming language - R studio",
      "objectives": [
        "Learn how to solve real life problem using the Machine learning techniques",
        "Machine Learning models such as Linear Regression, Logistic Regression, KNN etc.",
        "Advanced Machine Learning models such as Decision trees, XGBoost, Random Forest, SVM etc.",
        "Understanding of basics of statistics and concepts of Machine Learning",
        "How to do basic statistical operations and run ML models in R",
        "Indepth knowledge of data collection and data preprocessing for Machine Learning problem",
        "How to convert business problem into a Machine learning problem"
      ],
      "course_content": {
        "Welcome to the course": [
          "Introduction",
          "Course Resources"
        ],
        "Setting up R Studio and R crash course": [
          "Installing R and R studio",
          "This is a Milestone!",
          "Basics of R and R studio",
          "Packages in R",
          "Inputting data part 1: Inbuilt datasets of R",
          "Inputting data part 2: Manual data entry",
          "Inputting data part 3: Importing from CSV or Text files",
          "Creating Barplots in R",
          "Creating Histograms in R",
          "Quiz"
        ],
        "Basics of Statistics": [
          "Types of Data",
          "Types of Statistics",
          "Describing the data graphically",
          "Measures of Centers",
          "Measures of Dispersion",
          "Quiz"
        ],
        "Intorduction to Machine Learning": [
          "Introduction to Machine Learning",
          "Building a Machine Learning Model",
          "Quiz: Introduction to Machine Learning"
        ],
        "Data Preprocessing for Regression Analysis": [
          "Gathering Business Knowledge",
          "Data Exploration",
          "The Data and the Data Dictionary",
          "Importing the dataset into R",
          "Univariate Analysis and EDD",
          "EDD in R",
          "Outlier Treatment",
          "Outlier Treatment in R",
          "Missing Value imputation",
          "Missing Value imputation in R",
          "Seasonality in Data",
          "Bi-variate Analysis and Variable Transformation",
          "Variable transformation in R",
          "Non Usable Variables",
          "Dummy variable creation: Handling qualitative data",
          "Dummy variable creation in R",
          "Correlation Matrix and cause-effect relationship",
          "Correlation Matrix in R",
          "Quiz"
        ],
        "Linear Regression Model": [
          "The problem statement",
          "Basic equations and Ordinary Least Squared (OLS) method",
          "Assessing Accuracy of predicted coefficients",
          "Assessing Model Accuracy - RSE and R squared",
          "Simple Linear Regression in R",
          "Multiple Linear Regression",
          "The F - statistic",
          "Interpreting result for categorical Variable",
          "Multiple Linear Regression in R",
          "Quiz",
          "Test-Train split",
          "Bias Variance trade-off",
          "More about test-train split",
          "Test-Train Split in R",
          "Practice Assignment"
        ],
        "Regression models other than OLS": [
          "Linear models other than OLS",
          "Subset Selection techniques",
          "Subset selection in R",
          "Shrinkage methods - Ridge Regression and The Lasso",
          "Ridge regression and Lasso in R",
          "Quiz",
          "Assignment 1: Regression Analysis"
        ],
        "Introduction to the classification Models": [
          "Three classification models and Data set",
          "Importing the data into R",
          "The problem statements",
          "Why can't we use Linear Regression?"
        ],
        "Logistic Regression": [
          "Logistic Regression",
          "Training a Simple Logistic model in R",
          "Results of Simple Logistic Regression",
          "Logistic with multiple predictors",
          "Training multiple predictor Logistic model in R",
          "Confusion Matrix",
          "Evaluating Model performance",
          "Predicting probabilities, assigning classes and making Confusion Matrix in R",
          "Quiz"
        ],
        "Linear Discriminant Analysis": [
          "Linear Discriminant Analysis",
          "Linear Discriminant Analysis in R"
        ]
      },
      "requirements": [
        "Students will need to install R and R studio software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Machine Learning course that can help you launch a flourishing career in the field of Data Science, Machine Learning, R and Predictive Modeling, right?\nYou've found the right Machine Learning course!\nAfter completing this course, you will be able to:\n· Confidently build predictive Machine Learning models using R to solve business problems and create business strategy\n· Answer Machine Learning related interview questions\n· Participate and perform in online Data Analytics competitions such as Kaggle competitions\nCheck out the table of contents below to see what all Machine Learning models you are going to learn.\nHow will this course help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning, R and predictive modelling in Real world problems of business, this course will give you a solid base for that by teaching you the most popular techniques of machine learning, R and predictive modelling.\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear regression. This course will give you an in-depth understanding of machine learning and predictive modelling techniques using R.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques using R, Python, and we have used our experience to include the practical aspects of data analysis in this course.\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, machine learning, R, predictive modelling, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts of machine learning, R and predictive modelling. Each section contains a practice assignment for you to practically implement your learning on machine learning, R and predictive modelling.\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 3 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of models - Fifth and sixth section cover Classification models and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.\nWhy use R for Machine Learning?\nUnderstanding R is one of the valuable skills needed for a career in Machine Learning. Below are some reasons why you should learn Machine learning in R\n1. It’s a popular language for Machine Learning at top tech firms. Almost all of them hire data scientists who use R. Facebook, for example, uses R to do behavioral analysis with user post data. Google uses R to assess ad effectiveness and make economic forecasts. And by the way, it’s not just tech firms: R is in use at analysis and consulting firms, banks and other financial institutions, academic institutions and research labs, and pretty much everywhere else data needs analyzing and visualizing.\n2. Learning the data science basics is arguably easier in R than Python. R has a big advantage: it was designed specifically with data manipulation and analysis in mind.\n3. Amazing packages that make your life easier. As compared to Python, R was designed with statistical analysis in mind, it has a fantastic ecosystem of packages and other resources that are great for data science.\n4. Robust, growing community of data scientists and statisticians. As the field of data science has exploded, usage of R and Python has exploded with it, becoming one of the fastest-growing languages in the world (as measured by StackOverflow). That means it’s easy to find answers to questions and community guidance as you work your way through projects in R.\n5. Put another tool in your toolkit. No one language is going to be the right tool for every job. Like Python, adding R to your repertoire will make some projects easier – and of course, it’ll also make you a more flexible and marketable employee when you’re looking for jobs in data science.\nWhat are the major advantages of using R over Python?\nAs compared to Python, R has a higher user base and the biggest number of statistical packages and libraries available. Although, Python has almost all features that analysts need, R triumphs over Python.\nR is a function-based language, whereas Python is object-oriented. If you are coming from a purely statistical background and are not looking to take over major software engineering tasks when productizing your models, R is an easier option, than Python.\nR has more data analysis functionality built-in than Python, whereas Python relies on Packages\nPython has main packages for data analysis tasks, R has a larger ecosystem of small packages\nGraphics capabilities are generally considered better in R than in Python\nR has more statistical support in general than Python\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience"
      ]
    },
    {
      "title": "Machine Learning: Build neural networks in 77 lines of code",
      "url": "https://www.udemy.com/course/machine-learning-build-a-neural-network-in-77-lines-of-code/",
      "bio": "Machine Learning and Artificial Intelligence for beginners. How to build a neural network in 77 lines of Python code.",
      "objectives": [
        "Neural Networks",
        "Machine Learning",
        "Artificial Intelligence",
        "Supervised Learning"
      ],
      "course_content": {
        "Theory": [
          "Course Structure",
          "What is a neural network?",
          "The challenge",
          "Designing our architecture",
          "Weights",
          "Activation Function",
          "Training process",
          "Error Cost Function",
          "Adjusting the Weights"
        ],
        "Putting it into practise": [
          "Development Environment",
          "Coding Part 1",
          "Coding Part 2",
          "Coding Part 3",
          "Coding Part 4",
          "Using the Terminal",
          "Running our code",
          "Debugging",
          "Conclusion"
        ]
      },
      "requirements": [
        "Basic Python knowledge"
      ],
      "description": "From Google Translate to Netflix recommendations, neural networks are increasingly being used in our everyday lives. One day neural networks may operate self driving cars or even reach the level of artificial consciousness. As the machine learning revolution grows, demand for machine learning engineers grows with it. Machine learning is a lucrative field to develop your career.\n\n\nIn this course, I will teach you how to build a neural network from scratch in 77 lines of Python code. Unlike other courses, we won't be using machine learning libraries, which means you will gain a unique level of insight into how neural networks actually work. This course is designed for beginners. I don't use complex mathematics and I explain the Python code line by line, so the concepts are explained clearly and simply.\n\n\nThis is the expanded and improved video version of my blog post \"How to build a neural network in 9 lines of Python code\" which has been read by over 500,0000 students.\n\nEnroll today to start building your neural network.",
      "target_audience": [
        "Anyone interested in machine learning"
      ]
    },
    {
      "title": "Artificial Intelligence 2.0: AI, Python, DRL + ChatGPT Prize",
      "url": "https://www.udemy.com/course/deep-reinforcement-learning/",
      "bio": "Artificial Intelligence 2.0: The smartest combination of Double Deep Q-Learning, Policy Gradient, Actor Critic, DDPG",
      "objectives": [
        "Q-Learning",
        "Deep Q-Learning",
        "Policy Gradient",
        "Actor Critic",
        "Deep Deterministic Policy Gradient (DDPG)",
        "Twin-Delayed DDPG (TD3)",
        "The Foundation Techniques of Deep Reinforcement Learning",
        "How to implement a state of the art AI model that is over performing the most challenging virtual applications"
      ],
      "course_content": {
        "Part 1 - Fundamentals": [
          "Welcome",
          "Some resources before we start",
          "EXTRA: Learning Path",
          "Q-Learning",
          "Deep Q-Learning",
          "Policy Gradient",
          "Actor-Critic",
          "Taxonomy of AI models",
          "EXTRA: 5 Advantages of DRL",
          "EXTRA: RL Algorithms Map",
          "Get the materials",
          "Prizes $$ for Learning"
        ],
        "Part 2 - Twin Delayed DDPG Theory": [
          "Introduction and Initialization",
          "The Q-Learning part",
          "The Policy Learning part",
          "The whole training process"
        ],
        "Part 3 - Twin Delayed DDPG Implementation": [
          "The whole code folder of the course with all the implementations",
          "Beginning",
          "Implementation - Step 1",
          "Implementation - Step 2",
          "Implementation - Step 3",
          "Implementation - Step 4",
          "Implementation - Step 5",
          "Implementation - Step 6",
          "Implementation - Step 7",
          "Implementation - Step 8",
          "Implementation - Step 9",
          "Implementation - Step 10",
          "Implementation - Step 11",
          "Implementation - Step 12",
          "Implementation - Step 13",
          "Implementation - Step 14",
          "Implementation - Step 15",
          "Implementation - Step 16",
          "Implementation - Step 17",
          "Implementation - Step 18",
          "Implementation - Step 19",
          "Implementation - Step 20"
        ],
        "The Final Demo!": [
          "Demo - Training",
          "Demo - Inference"
        ],
        "Annex 1 - Artificial Neural Networks": [
          "Plan of Attack",
          "The Neuron",
          "The Activation Function",
          "How do Neural Networks Work?",
          "How do Neural Networks Learn?",
          "Gradient Descent",
          "Stochastic Gradient Descent",
          "Backpropagation"
        ],
        "Annex 2 - Q-Learning": [
          "Plan of Attack",
          "What is Reinforcement Learning?",
          "The Bellman Equation",
          "The Plan",
          "Markov Decision Process",
          "Policy vs Plan",
          "Living Penalty",
          "Q-Learning Intuition",
          "Temporal Difference",
          "Q-Learning Visualization"
        ],
        "Annex 3 - Deep Q-Learning": [
          "Plan of Attack",
          "Deep Q-Learning Intuition - Step 1",
          "Deep Q-Learning Intuition - Step 2",
          "Experience Replay",
          "Action Selection Policies"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Huge Congrats for completing the challenge!",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Some maths basics like knowing what is a differentiation or a gradient",
        "A bit of programming knowledge (classes and objects)"
      ],
      "description": "Welcome to Artificial Intelligence 2.0!\nIn this course, we will learn and implement a new incredibly smart AI model, called the Twin-Delayed DDPG or TD3, which combines state of the art techniques in Artificial Intelligence including continuous Double Deep Q-Learning, Policy Gradient, and Actor Critic. The model is so strong that for the first time in our courses, we are able to solve the most challenging virtual AI applications (training an ant/spider and a half humanoid to walk and run across a field).\nTo approach this model the right way, we structured the course in three parts:\n\n\nPart 1: Fundamentals\nIn this part we will study all the fundamentals of Artificial Intelligence which will allow you to understand and master the AI of this course. These include Q-Learning, Deep Q-Learning, Policy Gradient, Actor-Critic and more.\n\nPart 2: The Twin-Delayed DDPG Theory\nWe will study in depth the whole theory behind the model. You will clearly see the whole construction and training process of the AI through a series of clear visualization slides. Not only will you learn the theory in details, but also you will shape up a strong intuition of how the AI learns and works. The fundamentals in Part 1, combined to the very detailed theory of Part 2, will make this highly advanced model accessible to you, and you will eventually be one of the very few people who can master this model.\n\nPart 3: The Twin-Delayed DDPG Implementation\nWe will implement the model from scratch, step by step, and through interactive sessions, a new feature of this course which will have you practice on many coding exercises while we implement the model. By doing them you will not follow passively the course but very actively, therefore allowing you to effectively improve your skills. And last but not least, we will do the whole implementation on Colaboratory, or Google Colab, which is a totally free and open source AI platform allowing you to code and train some AIs without having any packages to install on your machine. In other words, you can be 100% confident that you press the execute button, the AI will start to train and you will get the videos of the spider and humanoid running in the end.\nSo are you ready to embrace AI at full power?\nCome join us, never stop learning, and enjoy AI!",
      "target_audience": [
        "Data Scientists who want to take their AI Skills to the next level",
        "AI experts who want to expand on the field of applications",
        "Engineers who work in technology and automation",
        "Businessmen and companies who want to get ahead of the game",
        "Students in tech-related programs who want to pursue a career in Data Science, Machine Learning, or Artificial Intelligence",
        "Anyone passionate about Artificial Intelligence"
      ]
    },
    {
      "title": "Build Autonomous AI Agents From Scratch With Python",
      "url": "https://www.udemy.com/course/build-autonomous-ai-agents-from-scratch-with-python/",
      "bio": "Step-by-step guide to develop Autonomous AI Agents from Scratch with Python and ReAct Prompting.",
      "objectives": [
        "Understand what an AI Agent is and how it works.",
        "Understand The ReAct Prompt",
        "Build a Basic AI Agent (Step-by-Step)",
        "Simplify AI Agents With SimplerLLM",
        "Build an SEO Auditor AI Agent (Real Example)",
        "Get all the Codes & Templates"
      ],
      "course_content": {
        "Introduction": [
          "What is an AI Agent?",
          "Getting Started",
          "Basic Setup",
          "LLM Limitation in Action",
          "Hardcoded Agent!",
          "Understanding ReAct Prompt",
          "Adding Functions",
          "The Loop in Action!",
          "Our Final AI Agent",
          "The Prompt",
          "Using SimplerLLM",
          "AI SEO Auditor Agent",
          "What's Next?"
        ]
      },
      "requirements": [
        "Basic to Intermediate Python Programming",
        "Core concepts of prompt engineering"
      ],
      "description": "Welcome to \"Build AI Agents From Scratch with Python,\" a course designed for individuals eager to dive into the world of autonomous AI agents from scratch.\nThis course will guide you through the fundamentals of AI agents and introduce the innovative ReAct Prompting, which enables large language models (LLMs) to think and take actionable steps.\nYou'll learn how to build basic to real-world AI agents totally from scratch without using any third parties.\nWe will be building Agents by integrating custom functions that allow these models to respond intelligently to user queries and bypass limitations like accessing live data, APIs, and external functions.\nWhether you're aiming to create a simple AI Agent or a real-world AI Agent, this course provides all the necessary codes and templates to get you started!\n\n\nWhat's Inside The Course?\n1- AI Agents Structure\nYou will understand what an AI Agent is and how it works.\n2- Understand The ReAct Prompt\nThe ReAct Prompt is the heart of the AI Agent. We are going to see how it works within the AI Agent workflow in action.\n3- Build a Basic AI Agent (Step-by-Step)\nWe will start by developing a fundamental AI agent that can use external functions to answer user queries.\n4- Simplify AI Agents With SimplerLLM\nWe will build the same AI Agent with the Help of SimplerLLM Library and see how it simplifies our code.\n5- Build an SEO Auditor AI Agent (Real Example)\nAfter you understand How AI Agents work and how to build one, we will go with an Advanced Real-World Example and Build an SEO Auditor AI Agent.\n6- Get all the Codes & Templates\nSave time and effort with access to all my codes and templates\n\n\nRequirements\nThis intermediate-level course requires attendees to have a foundational understanding of Python programming and core concepts of prompt engineering.\nIt is ideal for those who are comfortable with Python coding basics and want to expand their skills in practical AI applications.",
      "target_audience": [
        "Aspiring AI Developers: Individuals who want to expand their skills in practical AI applications.",
        "Python Programmers with Intermediate Skills willing to dive into AI Projects and Applications.",
        "Beginner prompt engineers eager to advance their skills."
      ]
    },
    {
      "title": "Learn BERT - essential NLP algorithm by Google",
      "url": "https://www.udemy.com/course/bert-nlp-algorithm/",
      "bio": "Understand and apply Google's game-changing NLP algorithm to real-world tasks. Build 2 NLP applications.",
      "objectives": [
        "Understand the history about BERT and why it changed NLP more than any algorithm in the recent years",
        "Understand how BERT is different from other standard algorithm and is closer to how humans process languages",
        "Use the tokenizing tools provided with BERT to preprocess text data efficiently",
        "Use the BERT layer as a embedding to plug it to your own NLP model",
        "Use BERT as a pre-trained model and then fine tune it to get the most out of it",
        "Explore the Github project from the Google research team to get the tools we need",
        "Get models available on Tensorflow Hub, the platform where you can get already trained models",
        "Clean text data",
        "Create datasets for AI from those data",
        "Use Google Colab and Tensorflow 2.0 for your AI implementations",
        "Create customs layers and models in TF 2.0 for specific NLP tasks"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course",
          "Course curriculum, Colab toolkit and data links",
          "EXTRA: Learning Path"
        ],
        "BERT - Intuition": [
          "What is BERT?",
          "Quiz: BERT definition",
          "Embedding",
          "Quiz: From text to numbers",
          "General Idea",
          "Quiz: General Idea",
          "Old fashioned seq2seq",
          "Transformer general understanding",
          "Attention",
          "Quiz: Transformers and attention",
          "Architecture",
          "Quiz: BERT's utility",
          "Pre-training",
          "Quiz: BERT's applications"
        ],
        "Application: using BERT's tokenizer": [
          "CNN explanation",
          "Intro",
          "Dependencies",
          "Loading Files",
          "Cleaning Data",
          "Tokenization",
          "Dataset Creation",
          "Model Building",
          "Training",
          "Evaluation"
        ],
        "Application: using BERT as an embedder": [
          "Important: correction for next lecture",
          "Inputs",
          "Model Results"
        ],
        "Application: fine-tuning BERT to create a question answering system": [
          "Intro",
          "Correction of \"Dependencies\" with new package version",
          "Dependencies",
          "Data Preprocessing",
          "Squad Layer",
          "Correction of \"Whole Model\" with new package version",
          "Whole Model",
          "Compile AI",
          "Training",
          "Evaluation Preparation",
          "Evaluation Creation",
          "Evaluation Result",
          "Home-made prediction"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic Python and Tensorflow knowledge",
        "Basic Mathematics knowledge (high school level)"
      ],
      "description": "Master BERT: The Breakthrough NLP Algorithm\nCourse Overview:\nWelcome to the ultimate guide to BERT! This comprehensive course is designed to take you on a journey from the basics to mastery of BERT (Bidirectional Encoder Representations from Transformers), a state-of-the-art algorithm transforming the field of natural language processing (NLP).\nWhy Choose This Course?\nAccessible for Everyone: Whether you're a seasoned data scientist or a newcomer to NLP, this course is crafted to be inclusive and comprehensive. We begin with the origins and history of BERT, carefully explaining each concept so that anyone can follow along. By the end of the course, you'll have a solid grasp of BERT, regardless of your starting point.\nRevolutionary and Versatile: BERT has fundamentally changed how we approach NLP tasks by eliminating the need for traditional models like RNNs and CNNs. Instead, BERT uses transformers to provide a more intuitive and effective way to process language. You'll learn how to apply BERT to a wide range of NLP tasks, making your projects more powerful and efficient.\nPractical: We prioritize practicality and usability in this course. Using TensorFlow 2.0 and Google Colab, you'll avoid common issues with local machine setups and software compatibility. These tools ensure that you are learning with the most current and advanced technologies available. You'll gain hands-on experience with real-world applications, reinforcing your learning and giving you the confidence to apply BERT in your own projects.\nHands-On Learning: Our course includes numerous practical exercises and projects to help you apply what you’ve learned. You'll work through real-world scenarios and datasets, allowing you to see firsthand how BERT can be used to solve complex NLP problems. This hands-on approach ensures that you're not just learning theory but also gaining the practical skills needed to implement BERT effectively.\nEnroll Now:\nIf you're ready to dive into the world of BERT and revolutionize your approach to natural language processing, this course is for you. Enroll now and start your journey towards mastering one of the most powerful tools in NLP today!",
      "target_audience": [
        "AI amateurs that are eager to learn how NLP research has evolved those last years and how BERT is changing everything",
        "AI students that need to have a deeper knowledge about the most recent NLP techniques",
        "Business driven people that are eager to know how to optimize NLP solutions to leverage any text data",
        "Anyone who wants to start a new career specialized in NLP and get a strong knowledge of the state-of-the art algorithm in this field, adding efficient cases to their portfolio"
      ]
    },
    {
      "title": "Introduction to Artificial Neural Network and Deep Learning",
      "url": "https://www.udemy.com/course/introduction-to-artificial-neural-network-and-deep-learning/",
      "bio": "The Best Machine Learning Techniques for Data Science in Java and Neuroph with Application in Image Recognition",
      "objectives": [
        "The structure of Neural Networks",
        "The learning process of Neural Networks",
        "Visualization in Neural Networks",
        "Deep learning and deep Neural Networks",
        "How to do classification using Neural Networks",
        "How to do regression and prediction using Neural Networks",
        "Implementing Neural Networks in Java",
        "Using Neuroph to design, test, and analyze Neural Networks"
      ],
      "course_content": {
        "Preliminaries and Essential Definitions in Artificial Neural Networks": [
          "Introduction: Is it a boy or girl? (No Stereotypes Here)",
          "Let's be more mathematical",
          "A model of an artificial neuron"
        ],
        "An Artificial Neuron (Perceptron)": [
          "An artificial neuron in action: live example",
          "Terminologies in the field of Machine Learning and Neural Networks",
          "A mathematical model of perceptron for problems with more than two features",
          "Time to learn the inspiration of perceptron and Neural Networks"
        ],
        "Learning: How to train a Perceptron": [
          "Learning and training in Neural Networks: minimizing a cost (error) function",
          "Different cost/error functions",
          "How to find the minimum of a cost function? Yes this is learning!",
          "Gradient Descent algorithm: Numerical example for optimizing weights",
          "Gradient Descent algorithm: Numerical example for optimizing biases",
          "The impact of the learning rate",
          "Challenges in training/learning Neural Networks",
          "Coding a simple Perceptron in Java"
        ],
        "A Perceptron Network, Deep Neural Networks, and deep learning": [
          "Different activation functions",
          "Multiple neurons: a perceptron network",
          "MLP: A Multi-layer Perceptron",
          "MLP in Action: A Live Demo",
          "Deep Neural Networks and Deep Learning"
        ],
        "BP: Backpropagation Algorithm": [
          "The theory of the Backpropagation Algorithm",
          "The impact of the momentum1"
        ],
        "Regression using Neural Networks": [
          "Linear and logistic (non-linear) regression using MLPs",
          "Regression in action: live demo",
          "Regression examples and issues",
          "Multiple regression",
          "Multiple regression in action: live demo",
          "MLP as a universal approximator"
        ],
        "Neuroph": [
          "Introduction to Neuroph",
          "Let's create an artificial neuron in neuroph",
          "Creating an MLP in Neuroph",
          "A sample project in Neuroph",
          "Visualizations in Neuroph",
          "Hand-written character recognition in Neuroph",
          "Image recognition in Neuroph"
        ],
        "Free e-book": [
          "My book on NNs"
        ]
      },
      "requirements": [
        "Have a basic programming skills",
        "Have a basic understanding of linear algebra particularly partial derivative"
      ],
      "description": "Machine learning is an extremely hot area in Artificial Intelligence and Data Science. There is no doubt that Neural Networks are the most well-regarded and widely used machine learning techniques.\nA lot of Data Scientists use Neural Networks without understanding their internal structure. However, understanding the internal structure and mechanism of such machine learning techniques will allow them to solve problems more efficiently. This also allows them to tune, tweak, and even design new Neural Networks for different projects.\nThis course is the easiest way to understand how Neural Networks work in detail. It also puts you ahead of a lot of data scientists. You will potentially have a higher chance of joining a small pool of well-paid data scientists.\n\n\nWhy learn Neural Networks as a Data Scientist?\nMachine learning is getting popular in all industries every single month with the main purpose of improving revenue and decreasing costs. Neural Networks are extremely practical machine learning techniques in different projects. You can use them to automate and optimize the process of solving challenging tasks.\n\n\nWhat does a data scientist need to learn about Neural Networks?\nThe first thing you need to learn is the mathematical models behind them. You cannot believe how easy and intuitive the mathematical models and equations are. This course starts with intuitive examples to take you through the most fundamental mathematical models of all Neural Networks. There is no equation in this course without an in-depth explanation and visual examples. If you hate math, then sit back, relax, and enjoy the videos to learn the math behind Neural Networks with minimum efforts.\nIt is also important to know what types of problems can be solved with Neural Networks. This course shows different types of problems to solve using Neural Networks including classification, regression, and prediction. There will be several examples to practice how to solve such problems as well.\n\n\nWhat does this course cover?\nAs discussed above, this course starts straight up with an intuitive example to see what a single Neuron is as the most fundamental component of Neural Networks. It also shows you the mathematical and conceptual model of a Neuron. After learning how easy and simple the mathematical models of a single Neuron are, you will see how it performs in action live.\nThe second part of this course covers terminologies in the field of machine learning, a mathematical model of a special type of neuron called Perceptron, and its inspiration. We will go through the main component of a perceptron as well.\nIn the third part, we will work with you on the process of training and learning in Neural networks. This includes learning different error/cost functions, optimizing the cost function, gradient descent algorithm, the impact of the learning rate, and challenges in this area.\nIn the first three parts of this course, you master how a single neuron works (e.g. Perceptron). This prepares you for the fourth part of this course, which is where we will learn how to make a network of these neurons. You will see how powerful even connecting two neurons are. We will learn the impact of multiple neurons and multiple layers on the outputs of a Neural Network. The main model here is a Multi-Layer Perceptron (MLP), which is the most well-regarded Neural Networks in both science and industry. This part of the course also includes Deep Neural Networks (DNN).\nIn the fifth section of this course, we will learn about the Backpropagation (BP) algorithm to train a multi-layer perceptron. The theory, mathematical model, and numerical example of this algorithm will be discussed in detail.\nAll the problems used in Sections 1-5 are classification, which is a very important task with a wide range of real-world applications. For instance, you can classify customers based on their interest in a certain product category. However, there are problems that require prediction. Such problems are solved by regression modes. Neural Networks can play the role of a regression method as well. This is exactly what we will be learning in Section 6 of this course. We start with an intuitive example of doing regression using a single neuron. There is a live demo as well to show how a neuron plays the role of a regression model. Other things that you will learn in this section are: linear regression, logistic (non-linear) regression, regression examples and issues, multiple regressions, and an MLP with three layers to solve any type of repression problems.\nThe last part of this course covers problem-solving using Neural Networks. We will be using Neuroph, which is a Java-based program, to see examples of Neural Networks in the areas and hand-character recognitions and image procession. If you have never used Neuroph before, there is nothing to worry about. There are several videos showing you the steps on how to create and run projects in Neuroph.\nBy the end of this course, you will have a comprehensive understanding of Neural Networks and able to easily use them in your project. You can analyze, tune, and improve the performance of Neural Networks based on your project too.\n\n\nDoes this course suit you?\nThis course is an introduction to Neural Networks, so you need absolutely no prior knowledge in Artificial Intelligence, Machine Learning, and AI. However, you need to have a basic understanding of programming especially in Java to easily follow the coding video. If you just want to learn the mathematical model and the problem-solving process using Neural Networks, you can then skip the coding videos.\n\n\nWho is the instructor?\nI am a leading researcher in the field of Machine Learning with expertise in Neural Networks and Optimization. I have more than 150 publications including 80 journal articles, 3 books, and 20 conference papers. These publications have been cited over 13,000 times around the world. As a leading researcher in this field with over 10 years of experience, I have prepared this course to make everything easy for those interested in Machine Learning and Neural Networks. I have been counseling big companies like Facebook and Google in my career too. I am also a star-rising Udemy instructor with more than 5000 students and 1000 5-star reviews, I have designed and developed this course to facilitate the process of learning Neural Networks for those who are interested in this area. You will have my full support throughout your Neural Networks journey in this course.\n\n\nThere is no RISK!\nI have some preview videos, so make sure to watch them to see if this course is for you.  This course comes with a full 30-day money-back guarantee, which means that if you are not happy after your purchase, you can get a 100% refund no question.\n\n\nWhat are you waiting?\nEnroll now using the “Add to Cart” button on the right and get started today.",
      "target_audience": [
        "Beginner data scientists interested in using Artificial Neural Networks and deep learning",
        "Expert data scientists interested in expanding their knowledge of how Neural Networks work internally",
        "Researchers who want to design and analyze current and new Neural Networks"
      ]
    },
    {
      "title": "Modern Reinforcement Learning: Actor-Critic Agents",
      "url": "https://www.udemy.com/course/actor-critic-methods-from-paper-to-code-with-pytorch/",
      "bio": "Implement Cutting Edge Artificial Intelligence Research Papers in the Open AI Gym Using the PyTorch & Tensorflow2",
      "objectives": [
        "How to code policy gradient methods in PyTorch",
        "How to code Deep Deterministic Policy Gradients (DDPG) in PyTorch",
        "How to code Twin Delayed Deep Deterministic Policy Gradients (TD3) in PyTorch",
        "How to code actor critic algorithms in PyTorch",
        "How to implement cutting edge artificial intelligence research papers in Python"
      ],
      "course_content": {
        "Introduction": [
          "What You Will Learn in this Course",
          "Required Background, Software, and Hardware",
          "How to Succeed in this Course"
        ],
        "Fundamentals of Reinforcement Learning": [
          "Review of Fundamental Concepts",
          "Calculating State Transition Probabilities",
          "Teaching an AI about Black Jack with Monte Carlo Prediction",
          "Teaching an AI How to Play Black Jack with Monte Carlo Control",
          "Review of Temporal Difference Learning Methods",
          "Teaching an AI about Balance with TD(0) Prediction",
          "Teaching an AI to Balance the Cart Pole with Q Learning"
        ],
        "Landing on the Moon with Policy Gradients & Actor Critic Methods": [
          "What's so Great About Policy Gradient Methods?",
          "Combining Neural Networks with Monte Carlo: REINFORCE Policy Gradient Algorithm",
          "Introducing the Lunar Lander Environment",
          "Coding the Agent's Brain: The Policy Gradient Network",
          "Coding the Policy Gradient Agent's Basic Functionality",
          "Coding the Agent's Learn Function",
          "Coding the Policy Gradient Main Loop and Watching our Agent Land on the Moon",
          "Actor Critic Learning: Combining Policy Gradients & Temporal Difference Learning",
          "Coding the Actor Critic Networks",
          "Coding the Actor Critic Agent",
          "Coding the Actor Critic Main Loop and Watching Our Agent Land on the Moon"
        ],
        "Deep Deterministic Policy Gradients (DDPG): Actor Critic with Continuous Actions": [
          "Getting up to Speed With Deep Q Learning",
          "How to Read and Understand Cutting Edge Research Papers",
          "Analyzing the DDPG Paper Abstract and Introduction",
          "Analyzing the Background Material",
          "What Algorithm Are We Going to Implement?",
          "What Results Should We Expect?",
          "What Other Solutions are Out There?",
          "What Model Architecture and Hyperparameters Do We Need?",
          "Handling the Explore-Exploit Dilemma: Coding the OU Action Noise Class",
          "Giving our Agent a Memory: Coding the Replay Memory Buffer Class",
          "Deep Q Learning for Actor Critic Methods: Coding the Critic Network Class",
          "Coding the Actor Network Class",
          "Giving our DDPG Agent Simple Autonomy: Coding the Basic Functions of Our Agent",
          "Giving our DDPG Agent a Brain: Coding the Agent's Learn Function",
          "Coding the Network Parameter Update Functionality",
          "Coding the Main Loop and Watching Our DDPG Agent Land on the Moon"
        ],
        "Twin Delayed Deep Deterministic Policy Gradients (TD3)": [
          "Some Tips on Reading this Paper",
          "Analyzing the TD3 Paper Abstract and Introduction",
          "What Other Solutions Have People Tried?",
          "Reviewing the Fundamental Concepts",
          "Is Overestimation Bias Even a Problem in Actor-Critic Methods?",
          "Why is Variance a Problem for Actor-Critic Methods?",
          "What Results Can We Expect?",
          "Coding the Brains of the TD3 Agent - The Actor and Critic Network Classes",
          "Giving our TD3 Agent Simple Autonomy - Coding the Basic Agent Functionality",
          "Giving our TD3 Agent a Brain - Coding the Learn Function",
          "Coding the Network Parameter Update Functionality",
          "Coding the Main Loop And Watching our Agent Learn to Walk"
        ],
        "Soft Actor Critic": [
          "A Quick Word on the Paper",
          "Getting Acquainted With a New Framework",
          "Checking Out What Has Been Done Before",
          "Inspecting the Foundation of this New Framework",
          "Digging Into the Mathematics of Soft Actor Critic",
          "Seeing How the New Algorithm Measures Up",
          "Coding the Neural Networks",
          "Coding the Soft Actor Critic Basic Functionality",
          "Coding the Soft Actor Critic Algorithm",
          "Coding the Main Loop and Evaluating Our Agent"
        ],
        "Tensorflow 2 Implementation": [
          "Coding the Policy Gradient Network in Tensorflow 2",
          "Coding the REINFORCE Agent in Tensorflow 2",
          "Coding the REINFORCE Main Loop and Evaluating our Agent",
          "Coding the Actor Critic Network in Tensorflow 2",
          "Coding the Actor Critic Agent in Tensorflow 2",
          "Coding the Actor Critic Main Program and Evaluating our Agent",
          "Coding the DDPG Networks in Tensorflow 2",
          "Coding the DDPG Agent in Tensorflow 2",
          "Coding the DDPG Main Program and Evaluating our Agent",
          "Coding the TD3 Agent in Tensorflow 2",
          "Coding the TD3 Main Program and Evaluating our Agent",
          "Coding the SAC Networks in Tensorflow 2",
          "Coding the SAC Agent in Tensorflow 2",
          "Coding the SAC Main Function and Evaluating our Agent"
        ],
        "Appendix": [
          "Setting Up Our Virtual Environment for the New OpenAI Gym",
          "Making our Agents Compliant With the New Gym Interface"
        ]
      },
      "requirements": [
        "Understanding of college level calculus",
        "Prior courses in reinforcement learning",
        "Able to code deep neural networks independently"
      ],
      "description": "In this advanced course on deep reinforcement learning, you will learn how to implement policy gradient, actor critic, deep deterministic policy gradient (DDPG), twin delayed deep deterministic policy gradient (TD3), and soft actor critic (SAC) algorithms in a variety of challenging environments from the Open AI gym. There will be a strong focus on dealing with environments with continuous action spaces, which is of particular interest for those looking to do research into robotic control with deep reinforcement learning.\nRather than being a course that spoon feeds the student, here you are going to learn to read deep reinforcement learning research papers on your own, and implement them from scratch. You will learn a repeatable framework for quickly implementing the algorithms in advanced research papers. Mastering the content in this course will be a quantum leap in your capabilities as an artificial intelligence engineer, and will put you in a league of your own among students who are reliant on others to break down complex ideas for them.\nFear not, if it's been a while since your last reinforcement learning course, we will begin with a briskly paced review of core topics.\nThe course begins with a practical review of the fundamentals of reinforcement learning, including topics such as:\nThe Bellman Equation\nMarkov Decision Processes\nMonte Carlo Prediction\nMonte Carlo Control\nTemporal Difference Prediction TD(0)\nTemporal Difference Control with Q Learning\nAnd moves straight into coding up our first agent: a blackjack playing artificial intelligence. From there we will progress to teaching an agent to balance the cart pole using Q learning.\nAfter mastering the fundamentals, the pace quickens, and we move straight into an introduction to policy gradient methods. We cover the REINFORCE algorithm, and use it to teach an artificial intelligence to land on the moon in the lunar lander environment from the Open AI gym. Next we progress to coding up the one step actor critic algorithm, to again beat the lunar lander.\nWith the fundamentals out of the way, we move on to our harder projects: implementing deep reinforcement learning research papers. We will start with Deep Deterministic Policy Gradients (DDPG), which is an algorithm for teaching robots to excel at a variety of continuous control tasks. DDPG combines many of the advances of Deep Q Learning with traditional actor critic methods to achieve state of the art results in environments with continuous action spaces.\nNext, we implement a state of the art artificial intelligence algorithm: Twin Delayed Deep Deterministic Policy Gradients (TD3). This algorithm sets a new benchmark for performance in continuous robotic control tasks, and we will demonstrate world class performance in the Bipedal Walker environment from the Open AI gym. TD3 is based on the DDPG algorithm, but addresses a number of approximation issues that result in poor performance in DDPG and other actor critic algorithms.\nFinally, we will implement the soft actor critic algorithm (SAC). SAC approaches deep reinforcement learning from a totally different angle: by considering entropy maximization, rather than score maximization, as a viable objective. This results in increased exploration by our agent, and world class performance in a number of important Open AI Gym environments.\nBy the end of the course, you will know the answers to the following fundamental questions in Actor-Critic methods:\nWhy should we bother with actor critic methods when deep Q learning is so successful?\nCan the advances in deep Q learning be used in other fields of reinforcement learning?\nHow can we solve the explore-exploit dilemma with a deterministic policy?\nHow do we get and deal with overestimation bias in actor-critic methods?\nHow do we deal with the inherent approximation errors in deep neural networks?\nThis course is for the highly motivated and advanced student. To succeed, you must have prior course work in all the following topics:\nCollege level calculus\nReinforcement learning\nDeep learning\nThe pace of the course is brisk and the topics are at the cutting edge of deep reinforcement learning research, but the payoff is that you will come out knowing how to read research papers and turn them into functional code as quickly as possible. You'll never have to rely on dodgy medium blog posts again.",
      "target_audience": [
        "Advanced students of artificial intelligence who want to implement state of the art academic research papers"
      ]
    },
    {
      "title": "Mastering LLMs with Ollama, LangChain, CrewAI, Hugging Face",
      "url": "https://www.udemy.com/course/mastering-local-llms-with-ollama-and-python-doing-projects/",
      "bio": "Hands-On Projects with Ollama, Langchain, CrewAI, and HuggingFace to Enhance Your AI Skills and Transform Everyday Tasks",
      "objectives": [
        "Understanding LLMs: Gain a solid foundation in Large Language Models (LLMs) and their applications.",
        "Using Ollama: Learn how to utilize the Ollama library for various NLP tasks.",
        "LangChain Integration: Master the integration of LangChain for building complex applications with LLMs.",
        "Project Development: Develop practical projects that reinforce learning.",
        "Creating a Learning Python Tool with Ollama.",
        "Building a Video Describer that summarizes video content.",
        "Implementing a Chat with PDF feature using Ollama LLM.",
        "Developing a Chat with VIDEO application using Ollama LLM and Whisper for audio transcription.",
        "Designing a system to Get Model Answers based on long stories or texts.",
        "Creating a Chat with Your Note application to interact with personal notes.",
        "Building a Chat with Your Diary project to reflect on personal experiences and insights.",
        "Whisper Integration: Understand how to integrate Whisper for audio processing and transcription tasks.",
        "Hugging Face Models: Learn how to leverage Hugging Face models for text generation and other NLP tasks.",
        "Practical Skills: Acquire hands-on experience through coding exercises and project implementations.",
        "Problem-Solving Techniques: Develop skills to tackle real-world problems using LLMs and related technologies.",
        "Additional Benefits: Continuous updates with new projects to enhance learning and keep up with advancements in AI and NLP technologies."
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Ollama": [
          "1 Download and Use OLLAMA",
          "2 Use Ollama with Python",
          "3 Use Streamlit",
          "4 Integrate LLM Model to Streamlit",
          "5 Use Exac function to run a Code in String Type",
          "6 Get the output of the Exac function as a variable",
          "7 Do a Learning Python Tool with Ollama",
          "8 Make llava describe the image",
          "9 Make a Image Describer page on Streamlit",
          "10 Describe multiple images",
          "11 Separate a video into frames",
          "12 Make a Video Describer",
          "13 Chat with PDF using Ollama LLM",
          "14 Chat with VIDEO using Ollama LLM and Whisper"
        ],
        "Langchain": [
          "15 - Use Local LLM with Langchain",
          "16 Use PromptTemplate and LLMChain",
          "17 Use 4 input variables in the template",
          "18 Learn Chunking Long Texts",
          "19 Get embedding and similarity between two texts",
          "20 Find the most similiar sentence in the text",
          "21 Get model answer questions based on long stories",
          "22 - Chat with Your Note",
          "23 Chat with Your Diary"
        ],
        "CrewAI": [
          "24 Create Virtual Environment on Power Shell",
          "25 Use Local Ollama LLM with CrewAI",
          "26 Story Writing with CrewAI and Ollama",
          "27 Recipe Generator and Nutrition Analyzer",
          "28 Travel Itinerary Planner",
          "29 Personal Finance Advisor"
        ],
        "HuggingFace": [
          "30 Introduction to HuggingFace",
          "31 Use HuggingFace Models with API For FREE",
          "32 Hugging Face Model Query App",
          "33 Use Question Answering Models of Hugging Face with API",
          "34 Summarization with Hugging Face Model via API",
          "35 Image Generation with Hugging Face For FREE",
          "36 Object Detection with HuggingFace"
        ]
      },
      "requirements": [
        "Basic Knowledge of Python"
      ],
      "description": "Welcome!\nThis comprehensive course is designed for individuals eager to dive into the world of Large Language Models (LLMs) and harness their power to create innovative applications that can simplify tasks in everyday life.\nCourse Overview\nIn this course, you will learn how to effectively utilize various libraries and frameworks, including Ollama, LangChain, CrewAI, and Hugging Face, to build practical projects that demonstrate the capabilities of LLMs. Through hands-on projects, you will gain a deep understanding of how these technologies work together to enhance productivity and creativity.\nWhat You Will Learn\nUnderstanding LLMs: Gain insights into the architecture and functioning of Large Language Models, including their applications in natural language processing (NLP).\nOllama and LangChain: Learn how to leverage Ollama for efficient model deployment and LangChain for building complex applications that integrate multiple components seamlessly.\nHugging Face Transformers: Explore the Hugging Face library to access a wide range of pre-trained models for various NLP tasks.\nPractical Applications: Implement real-world projects that showcase the power of LLMs in different contexts.\nProject Highlights\nLearning Python Tool with Ollama: Create an interactive tool that helps users learn Python programming through guided exercises and instant feedback using an LLM.\nMake a Video Describer: Develop an application that generates descriptive text for video content, enhancing accessibility and understanding for users.\nChat with PDF using Ollama LLM: Build a chat interface that allows users to ask questions about the content of PDF documents, providing instant answers powered by an LLM.\nChat with VIDEO using Ollama LLM and Whisper: Combine video processing with speech recognition to create an application where users can interact with video content through natural language queries.\nGet Model Answers Based on Long Stories: Design a system that allows users to input long narratives or stories and receive concise answers or summaries from the model.\nChat with Your Note: Create a personal note-taking application where users can interact with their notes using natural language queries, making information retrieval seamless.\nChat with Your Diary: Develop a diary application that allows users to reflect on their entries and ask questions about their past experiences, promoting self-reflection and personal growth.\nContinuous Learning\nThis course is designed to be dynamic, with new projects added regularly to keep pace with advancements in technology and user needs. You will have the opportunity to explore new ideas and implement them in your projects, ensuring you stay ahead in the rapidly evolving field of AI and NLP.\nWho Should Enroll\nThis course is ideal for:\nDevelopers looking to expand their skill set in AI and machine learning.\nData scientists interested in applying NLP techniques using state-of-the-art models.\nAnyone passionate about leveraging LLMs to create innovative applications that enhance productivity.\nJoin us on this exciting journey as we explore the potential of Large Language Models through practical projects. By the end of this course, you will have the skills and knowledge needed to build your own applications using Ollama, LangChain, CrewAI, and Hugging Face, empowering you to make your life easier through technology. Enroll today and start mastering LLMs!",
      "target_audience": [
        "Aspiring AI Developers: Individuals looking to build applications using LLMs.",
        "Data Scientists: Professionals wanting to enhance their data analysis skills with AI tools.",
        "Python Programmers: Developers interested in integrating LLMs into their Python projects.",
        "Students: Learners seeking practical experience with modern AI libraries and frameworks.",
        "AI Enthusiasts: Anyone passionate about exploring the capabilities of LLMs and their applications.",
        "Content Creators: Writers and creators wanting to leverage AI for content generation and enhancement.",
        "Educators: Teachers aiming to incorporate AI tools into their curriculum or projects."
      ]
    },
    {
      "title": "Python-Introduction to Data Science and Machine learning A-Z",
      "url": "https://www.udemy.com/course/python-introduction-to-data-science-and-machine-learning-a-z/",
      "bio": "Python basics Learn Python for Data Science Python For Machine learning and Python Tips and tricks",
      "objectives": [
        "Uderstand the basics of python programming",
        "learning all the basic mathematical concepts",
        "Understand the basics of Data science and how to perform it using Python",
        "Learn to use different python tools specialisez for data science",
        "Improve your python programming by integrating new concepts",
        "Learning the basics of Machine learning",
        "Perform various analysis with sklearn",
        "Finish the course with a complete understand of all the core concepts of Data science and all the required tools to perform it with python"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "What is Data Science",
          "Installation of Anaconda and Jupyter",
          "Introduction to Jupyter Part 1",
          "Introduction to Jupyter Part 2"
        ],
        "Basic Statstics knowledge": [
          "The Basics of Data",
          "The basics of statistics part 1",
          "The basics of statistics part 2",
          "The basics of statistics part 3",
          "The basics of statistics part 4",
          "The basics of statistics part 5",
          "The basics of statistics part 6"
        ],
        "Python library: NumPy": [
          "Introduction to Numpy",
          "Setting up NumPy",
          "Basic calculations Part 1",
          "Basic calculations Part 2",
          "Basic calculations Part 3",
          "Basic calculations Part 4",
          "Basic calculations Part 5"
        ],
        "Python library: Pandas": [
          "The Basics of Pandas",
          "Setting up Pandas",
          "Pandas operations part 1",
          "Pandas operations part 2",
          "Pandas operations part 3",
          "Pandas operations part 4",
          "Pandas operations part 5"
        ],
        "Python library: Scipy": [
          "The Basics of SciPy",
          "SciPy operations part 1",
          "SciPy operations part 2",
          "SciPy operations part 3",
          "SciPy operations part 4",
          "SciPy operations part 5"
        ],
        "Python library : Matplotlib": [
          "Introduction to Matplotlib",
          "Setting up MatPlotlib",
          "Basics of matplotlib part 1",
          "Basics of matplotlib part 2",
          "Basics of matplotlib part 3",
          "Basics of matplotlib part 4",
          "Basics of matplotlib part 5"
        ],
        "Python library: Seaborn": [
          "Introduction to Seaborn",
          "Setting up seaborn",
          "Seaborn operations part 1",
          "Seaborn operations part 2",
          "Seaborn operations part 3",
          "Seaborn operations part 4"
        ],
        "Machine Learning": [
          "Introduction to machine learning",
          "Presentation of Different algorithms",
          "Machine learning algorithms part 1",
          "Machine learning algorithms part 2",
          "Machine learning algorithms part 3"
        ],
        "Conclusion": [
          "Conclusion",
          "Thank you",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Strong motivation to learn new skills",
        "Basic python programming skills (can be helpful but not mandatory)"
      ],
      "description": "Learning how to program in Python is not always easy especially if you want to use it for Data science. Indeed, there are many of different tools that have to be learned to be able to properly use Python for Data science and machine learning and each of those tools is not always easy to learn. But, this course will give all the basics you need no matter for what objective you want to use it so if you :\n- Are a student and want to improve your programming skills and want to learn new utilities on how to use Python\n- Need to learn basics of Data science\n- Have to understand basic Data science tools to improve your career\n- Simply acquire the skills for personal use\nThen you will definitely love this course. Not only you will learn all the tools that are used for Data science but you will also improve your Python knowledge and learn to use those tools to be able to visualize your projects.\nThe structure of the course\nThis course is structured in a way that you will be able to to learn each tool separately and practice by programming in python directly with the use of those tools. Indeed, you will at first learn all the mathematics that are associated with Data science. This means that you will have a complete introduction to the majority of important statistical formulas and functions that exist. You will also learn how to set up and use Jupyter as well as Pycharm to write your Python code. After, you are going to learn different Python libraries that exist and how to use them properly. Here you will learn tools such as NumPy or SciPy and many others. Finally, you will have an introduction to machine learning and learn how a machine learning algorithm works. All this in just one course.\nAnother very interesting thing about this course it contains a lot of practice. Indeed, I build all my course on a concept of learning by practice. In other words, this course contains a lot of practice this way you will be able to be sure that you completely understand each concept by writing the code yourself.\nFor who is this course designed\nThis course is designed for beginner that are interested to have a basic understand of what exactly Data science is and be able to perform it with python programming language. Since this is an introduction to Data science, you don't have to be a specialist to understand the course. Of course having some basic prior python knowledge could be good but it's not mandatory to be able to understand this course. Also, if you are a student and wish to learn more about Data science or you simply want to improve your python programming skills by learning new tools you will definitely enjoy this course. Finally, this course is for any body that is interested to learn more about Data science and how to properly use python to be able to analyze data with different tools.\nWhy should I take this course\nIf you want to learn all the basics of Data science and Python this course has all you need. Not only you will have a complete introduction to Data science but you will also be able to practice python programming in the same course. Indeed, this course is created to help you learn new skills as well as improving your current programming skills.\nThere is no risk involved in taking this course\nThis course comes with a 100% satisfaction guarantee, this means that if your are not happy with what you have learned, you have 30 days to get a complete refund with no questions asked. Also, if there is any concept that you find complicated or you are just not able to understand, you can directly contact me and it will be my pleasure to support you in your learning.\nThis means that you can either learn amazing skills that can be very useful in your professional or everyday life or you can simply try the course and if you don't like it for any reason ask for a refund.\nYou can't lose with this type of offer !!\n\n\nENROLL NOW and start learning today :)",
      "target_audience": [
        "Beginner python users curious about Data Science"
      ]
    },
    {
      "title": "Introduction to AI Governance",
      "url": "https://www.udemy.com/course/introduction-to-ai-governance/",
      "bio": "Learn how to measure, monitor and control your AI Models",
      "objectives": [
        "It identifies the need for an AI model to meet its stated business objectives. It defines the dimensions for these business objectives.",
        "It establishes the guard rails and controls to monitor AI models during deployment and production of the associated IT system.",
        "It details the AI model measurement framework and associated control points. It uses examples to show how framework is established for real time Applications.",
        "It defines what it takes for a good AI-based solution engages and “Wows” the users?",
        "It proposes the skills, processes and organization to support AI Governance."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Key Driver for AI Governance include"
        ],
        "AI Governance - Components": [
          "AI Governance - Components",
          "AI Governance - Decision Quality and Compliance component supports"
        ],
        "Continuous Learning Process": [
          "Continuous Learning Process",
          "How does challenger beats a champion model"
        ],
        "Model Measurements": [
          "Model Measurements",
          "model precision"
        ],
        "Model Management": [
          "Model Management",
          "Role of Librarian"
        ],
        "Collaboration Strategies": [
          "Collaboration Strategies",
          "An example of Smarter Hierarchies"
        ],
        "Summary": [
          "Summary",
          "Model Assessment Report",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic knowledge of data and / or process governance would be beneficial, though not required.",
        "It is assumed the students are familiar with IT deployment processes."
      ],
      "description": "You worked hard to develop a collection of AI models and now you are ready to deploy them. Before you rush to deploy your application, let me ask you important questions.\nHow do we sense and interpret a poorly working AI system? Presumably, you have developed a high-quality decision-making engine. Have you provided a mechanism to instrument your AI system as system gets deployed and learns from its use? What is your criteria for a good system and when do you alert your management if the AI system starts performing poorly.\nYour AI system is deployed in a 24X7 environment where users are using it round the clock. What is your mechanism to fix the problems without tearing down the system. Can you develop a continuous learning module of your system which constantly learns from use, adjusts the model and provides sufficient logs to your contributing experts to monitor its learning.\nThe model starts to learn and now your experts are baffled. The learning is exactly not what they designed in the original system and now as they redesign their model, they have differences in opinion. How do you reconcile these differences and bring the best of your corporation.\nIs there a tool or a system for life cycle management and can you employ that tool to monitor your AI models like the way you developed process monitoring tools for your governed processes.\nYou may have many more questions. AI Governance is an evolving topic and not much material is available on this side.\nHowever, most of our IT and Business professionals understand Data and Process governance. This course will help them extend their knowledge to include AI governance. This course will provide you an overview of how above questions can be addressed by a combination of process, skills and tools.\nThis course has been developed for working business professionals and project executives / management teams involved in interested in getting involved in AI engagements for their enterprises\nIn this course, you will learn\nDoes an AI based solution meets its stated objectives?\nHow to put control and guard-rails for an AI based solution\nHow does your model perform while retaining its value accuracy and integrity?\nDoes a good AI-based solution engages and “Wows” the users?\nWhat skills and tools are available to address some of the governance challenges?",
      "target_audience": [
        "This course has been developed for working business professionals and project executives / management teams involved in interested in getting involved in AI engagements for their enterprises.",
        "This is not a course for a developer who is trying to learn how to develop an AI model."
      ]
    },
    {
      "title": "The Data Bootcamp: Transform your Data using dbt™",
      "url": "https://www.udemy.com/course/the-dbt-bootcamp-transform-your-data-using-data-build-tool/",
      "bio": "Learn dbt™ from scratch. Build data models, perform testing, generate documents & become an all rounded Data Engineer!",
      "objectives": [
        "Learn how to use dbt end-to-end through a realistic project, hands-on project",
        "Learn the fundamentals of dbt including connecting to a data warehouse, developing models, working with sources, creating tests, deploying and much more",
        "Understand how dbt is beneficial in a modern data stack and how the Extract, Load and Transform process works",
        "Learn how to use Macros and Packages in dbt to simplify and reuse your code",
        "Learn how dbt can work with complex SQL queries",
        "Perform Testing in dbt such as singular tests, generic tests and source testing",
        "Develop documentation for your models, understand modularity and dependancies in dbt",
        "Learn about the dbt Project Structure and how to set up your own project",
        "Understand how to use JINJA in dbt",
        "Learn how to deploy in dbt under a schedule and investigate run time errors",
        "Learn how to use Seeds and Analysis in dbt"
      ],
      "course_content": {
        "Introduction": [
          "Introduction to the Course",
          "What is dbt™?",
          "ELT vs ETL - What is better?",
          "How the Course is Structured"
        ],
        "Setting up dbt": [
          "Reading the Project Brief",
          "Setting up dbt™ Cloud",
          "Setting up Snowflake Data Warehouse",
          "Setting up Github",
          "Connecting to the Database and Github"
        ],
        "Getting Started with dbt": [
          "Initialising dbt™ Cloud",
          "Taking a Tour of dbt™ Cloud"
        ],
        "Models and Modularity in DBT": [
          "Developing Models in dbt™",
          "Understanding Modularity and REF Functions in dbt™"
        ],
        "Working with YML files": [
          "Working with dbt_project.yml",
          "Working with Sources in dbt™"
        ],
        "Testing and Documentation on dbt™": [
          "Introduction to Testing in dbt™",
          "How to Create Generic Tests in dbt™",
          "How to Create Singular Tests in dbt™",
          "How to Generate and work with dbt™ Documentation"
        ],
        "Using Macros, Packages and Seeds in DBT": [
          "How to use JINJA in dbt™",
          "How to Create Macros in dbt™",
          "How to use Packages in dbt™",
          "How to use Seeds (CSV files) in dbt™"
        ],
        "Deploying on dbt": [
          "Deploying on dbt™",
          "A Quick Goodbye Note"
        ]
      },
      "requirements": [
        "Basic SQL knowledge (optional)"
      ],
      "description": "Are you looking for a cutting-edge way to extract load and transform your data? Do you want to know more about dbt™ and how to use it? Well, this is the course for you. Welcome to The dbt™ Bootcamp: Transform your Data using dbt™.\n\n\nIn this course you are going to learn all about dbt™, from setting up dbt™ cloud, connecting it to Snowflake or a warehouse of your choice, developing models, creating sources, doing testing, working with the documentation and much more.\n\n\nThis course is for beginners, we will go through a realistic project and cover each of the steps mentioned in a practical approach.\n\n\ndbt™ is a data modelling tool that makes life much easier for analysts and engineers. It allows you to write SQL queries without having to worry about dependencies. dbt™, like traditional databases, is built on SQL, but it has additional functionality built on top of it utilizing templating engines such as JINJA.\nThis effectively lets you to retrieve, rearrange, and organize your data using additional logic in your SQL. You may then compile and run this code using dbt's™ run command to retrieve just the pieces you need in the transformations. It can also be swiftly coded, tested, and adjusted without having to wait for it to process all your data. In addition to that, it’s automated documentation is a big time saver.\nThe project we will be working on is about a fictitious company called GlobalMart. GlobalMart sells household items like furniture, office equipment, Appliances and Electronics. They are in the process of hiring a small data team and would like to try out dbt™ for their data transformations. They require some reporting tables about their profits and want to use dbt™ to transform their data to get them what they want.\n\n\nBy the end of this course, we will work through the project and end up accomplishing the following:\n\n\n1. Setting up a dbt™ Cloud Account\n2. Connecting to a Database (in this case Snowflake)\n3. Connecting dbt™ to a repository like GitHub\n4. Understanding the dbt™ cloud interface\n5. Building and Running Models in dbt™\n6. Using Modularity in dbt™\n7. Creating and Referencing Sources\n8. Performing Tests in dbt™ including Singular and Generic Tests\n9. How to Create and Generate Documentation in dbt™\n10. How to Deploy in dbt™\n11. How to use Jinja\n12. Using Macros and Packages in dbt™\n13. Using Seeds and Analyses in dbt™\n\n\nThis is a great, comprehensive which will really up-skill you not only in dbt but the extract, load and transform process as well.\n\n\nThank you so much for choosing this course and I’ll see you in the next lecture.",
      "target_audience": [
        "Anyone wanting to get more experience in dbt (Data Build Tool)",
        "Anyone wanting to know more about Extract, Load and Transform Process",
        "Anyone wanting to start a career in Data Engineering",
        "Anyone wanting to gain more experience in Data Analytics"
      ]
    },
    {
      "title": "Linear Algebra for AI - Generative AI",
      "url": "https://www.udemy.com/course/linear-algebra-for-data-science-machine-learning-ai/",
      "bio": "Master Linear Algebra: Essential Math for AI , Data Science, Machine Learning, and Deep Learning Applications",
      "objectives": [
        "Build Mathematical intuition required for Data Science and Machine Learning",
        "The linear algebra intuition required to become a Data Scientist",
        "How to take their Data Science career to the next level",
        "Hacks, tips & tricks for their Data Science career",
        "Implement Machine Learning Algorithms better",
        "Apply Linear Algebra in Data Analysis",
        "Learn core concept to Implement in Machine Learning"
      ],
      "course_content": {
        "Vectors Basics": [
          "Vectors - Basics",
          "Operation on Vectors",
          "Co- Ordination System",
          "Testing the Knowledge from section 1",
          "Ready for Next Section"
        ],
        "Vector Projections": [
          "Vector Magnitude and Direction",
          "Cosine Rule Triangle of Vectors",
          "Projection of Vector",
          "Operation of Vectors in Python",
          "Vector Norm / Magnitude"
        ],
        "Basis of Vectors": [
          "Changing Basis of Vectors",
          "Understanding the Basis, Linear combination and Span"
        ],
        "Matrix Basics from High school": [
          "Matrices Introduction",
          "Understanding the concept of Matrices",
          "Types of Matrices",
          "Operations on Matrices",
          "Matrix Multiplication",
          "Transpose of a Matrix",
          "Elementary Row Operations on Matrix",
          "Matrix Hands-on - Arithmetic Operations",
          "Types of Matrices",
          "Operations on Matrices"
        ],
        "Matrices - Setting up the stage - Transformations": [
          "Setting the stage for Matrices",
          "how Matrices Transform",
          "Types of Matrix Transformation",
          "Combination or Composition"
        ],
        "Gaussian Elimination": [
          "Gaussian Elimination - Solve Equations",
          "Vector Matrices relation",
          "Determinants"
        ],
        "Einstein Summation convention - Non Orthogonal basis - Gram Schmidt Process": [
          "Einstein Summation convention",
          "Transforming non orthogonal basis",
          "Transforming to new Basis",
          "Orthogonal Matrices",
          "Gram-Schmidt Process",
          "Gram-Schmidt in Python"
        ],
        "Eigen Problems": [
          "Vector Field",
          "Eigen Values",
          "Eigen - Mathematical sense",
          "Changing to Eigen Basis",
          "Changing Eigen Basis - Example",
          "Eigen Decomposition using Python",
          "Eigen Values Calculation - Recap",
          "Eigen values in Python"
        ],
        "Principal Component Analysis - Application of Eigen Values and Eigen Vectors": [
          "PCA with Eigen Background",
          "LDA on Dimensionality Reduction"
        ],
        "Google Pagerank Algorithm": [
          "Google Page Rank Algorithm",
          "Page Rank Algorithm - Python",
          "Damping"
        ]
      },
      "requirements": [
        "Pen and a paper to workout maths problem",
        "Computer with Python to execute the code",
        "Some programming experience"
      ],
      "description": "Master Linear Algebra for Data Science, Machine Learning, and Deep Learning - Unleash the Power of Mathematics in AI Applications\nAre you eager to enhance your skills in Machine Learning, Deep Learning, and Data Science by mastering the crucial foundation of Linear Algebra? Look no further – this comprehensive course is designed just for you.\nWith the increasing demand for expertise in Machine Learning and Deep Learning, it's crucial to avoid the common mistake of relying solely on tools without a deep understanding of their underlying mathematical principles. This course is your key to developing a solid foundation in mathematics, providing you with a profound intuition of how algorithms work, their limitations, and the assumptions they rely on.\nWhy is a strong mathematical foundation important? Understanding the machinery under the hood is the key to becoming a confident practitioner in the fields of Machine Learning, Data Science, and Deep Learning. Linear Algebra is universally acknowledged as a fundamental starting point in the learning journey of these domains.\nThe basic elements of Linear Algebra – Vectors and Matrices – serve as the backbone for storing and processing data in various applications of Machine Learning, Data Science, and Artificial Intelligence. From basic operations to complex tasks involving massive datasets, Linear Algebra plays a pivotal role.\nEven in advanced technologies like Deep Learning and Neural Networks, Matrices are employed to store inputs such as images and text, providing state-of-the-art solutions to complex problems.\nRecognizing the paramount importance of Linear Algebra in a Data Science career, we have crafted a curriculum that ensures you build a strong intuition for the concepts without getting lost in complex mathematics.\nBy the end of this course, you will not only grasp the analytical aspects of Linear Algebra but also witness its practical implementation through Python. Additionally, you will gain insights into the functioning of the renowned Google PageRank Algorithm, utilizing the concepts learned throughout the course.\nHere's what the course covers:\nVectors Basics\nVector Projections\nBasis of Vectors\nMatrices Basics\nMatrix Transformations\nGaussian Elimination\nEinstein Summation Convention\nEigen Problems\nGoogle Page Rank Algorithm\nSVD - Singular Value Decomposition\nPseudo Inverse\nMatrix Decomposition\nSolve Linear Regression using Matrix Methods\nLinear Regression from Scratch\nLinear Algebra in Natural Language Processing\nLinear Algebra for Deep Learning\nLinear Regression using PyTorch\nBonus: Python Basics & Python for Data Science\nThis hands-on course takes you on a step-by-step journey, providing the essential Linear Algebra skills required for Data Science, Machine Learning, Natural Language Processing, and Deep Learning. Enroll now and embark on your journey to master the mathematical foundations powering AI applications. Click the 'Enroll' button to start your learning experience – I look forward to seeing you in Lecture 1!",
      "target_audience": [
        "Data Scientists who wish to improve their career in Data Science.",
        "Machine Learning Practitioners",
        "Any one who wants to understand the underpinnings of Maths in Data Science, Machine Learning and Artificial intelligence",
        "Any Data Science enthusiast",
        "Any student or professional who wants to start or transition to a career in Data Science.",
        "Students who want to refresh and learn important maths concepts required for Machine Learning , Deep Learning & Data Science.",
        "Any data analysts who want to level up in Machine Learning.",
        "Any people who are not satisfied with their job and who want to become a Data Scientist."
      ]
    },
    {
      "title": "Python for Statistical Analysis",
      "url": "https://www.udemy.com/course/python-for-statistical-analysis/",
      "bio": "Master applied Statistics with Python by solving real-world problems with state-of-the-art software and libraries",
      "objectives": [
        "Gain deeper insights into data",
        "Use Python to solve common and complex statistical and Machine Learning-related projects",
        "How to interpret and visualize outcomes, integrating visual output and graphical exploration",
        "Learn hypothesis testing and how to efficiently implement tests in Python"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Setup",
          "Learning Paths",
          "Live Install and Verification",
          "Coding Editors",
          "Live Coding Editor Comparison",
          "File Management",
          "Get the Materials"
        ],
        "Exploring Data Analysis": [
          "Loading Data",
          "Loading Data - Practical Example",
          "Dataset Preparation - Practical Example",
          "Dealing with Outliers - Practical Example",
          "1D Distribution Overview",
          "1D Histograms - Practical Example",
          "1D Bee Swarm - Practical Example",
          "1D Box and Violin - Practical Example",
          "1D Empirical CDF and Pandas Describe - Practical Example",
          "Higher Dimensional Distributions Overview",
          "ND Scatter Matrix - Practical Example",
          "ND Correlation - Practical Example",
          "2D Histograms, Contours and KDE - Practical Example",
          "ND Scatter Probability - Practical Example",
          "Exploratory Data Analysis Summary"
        ],
        "Characterising": [
          "Introduction - Why bother characterising?",
          "Mean Median Mode - Practical Example",
          "Widths - Practical Example",
          "Skewness and Kurtosis - Practical Example",
          "Percentiles - Practical Example",
          "Multivariate Distributions - Practical Example",
          "Summary"
        ],
        "Probability": [
          "Probability Refresher",
          "Introduction to Probability Distributions",
          "Probability Distributions - Practical Example",
          "Probability Functions and Empirical Distributions",
          "Empirical Distributions - Practical Example",
          "Introduction to Sampling and the Central Limit Theorem",
          "Sampling Distributions - Practical Example",
          "Extra Writeup: More resources on sampling distributions",
          "Central Limit Theorem - Practical Example",
          "Summary"
        ],
        "Hypothesis Testing": [
          "Introduction to Hypothesis Testing",
          "Motivation Loaded Die - Practical Example",
          "Basic Tests",
          "Basic Tests Example - Asteroid Impacts",
          "Introduction to Proportion Testing",
          "Proportion Testing Example - Election Rigging",
          "Pearsons Chi2 Test - Practical Example",
          "Comparing Distributions - Kolmogorow-Smirnow and Anderson-Darling Tests",
          "Extra Writeup: All the ways to do A/B testing!",
          "Summary"
        ],
        "Conclusion": [
          "Conclusion",
          "Extra: Significance Hunting - What not to do!",
          "Extra: Introduction to Gaussian Processes",
          "Extra Prac - Cosmic Impact",
          "Extra Prac: Car Emission Standards",
          "Extra Prac: Diagnosing Diabetes",
          "Extra Prac: Numerical Uncertainty on Sales"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Python basics"
      ],
      "description": "Welcome to Python for Statistical Analysis!\n\n\nThis course is designed to position you for success by diving into the real-world of statistics and data science.\n\n\nLearn through real-world examples: Instead of sitting through hours of theoretical content and struggling to connect it to real-world problems, we'll focus entirely upon applied statistics. Taking theory and immediately applying it through Python onto common problems to give you the knowledge and skills you need to excel.\n\n\nPresentation-focused outcomes: Crunching the numbers is easy, and quickly becoming the domain of computers and not people. The skills people have are interpreting and visualising outcomes and so we focus heavily on this, integrating visual output and graphical exploration in our workflows. Plus, the extra content on great ways to spice up visuals for reports, articles and presentations, so that you can stand out from the crowd.\n\n\nModern tools and workflows: This isn't school, where we want to spend hours grinding through problems by hand for reinforcement learning. No, we'll solve our problems using state-of-the-art techniques and code libraries, utilising features from the very latest software releases to make us as productive and efficient as possible. Don't reinvent the wheel when the industry has moved to rockets.",
      "target_audience": [
        "Data Scientists who want to add to their skillset statistical analysis",
        "Data Scientists who want to do machine learning but want some more statistical foundations before jumping in",
        "Students wanting to learn applied statistics for research, coursework or business"
      ]
    },
    {
      "title": "RAG Agents: Build Apps & GPTs with APIs/MCP, LangChain & n8n",
      "url": "https://www.udemy.com/course/rag-agents-build-apps-gpts-with-apismcp-langchain-n8n/",
      "bio": "AI Agents & LLMs with RAG: n8n, LangChain, LangGraph, Flowise, MCP & more – with ChatGPT, Gemini, Claude, DeepSeek & Co.",
      "objectives": [
        "Introduction to RAG workflows & tools like Google’s NotebookLM with essential tips",
        "LLM fundamentals & RAG technologies: ChatGPT, Claude, Gemini, Deepseek, Llama, Mistral, xAI, Grok, Function Calling, vector databases, embeddings & chunking",
        "ChatGPT basics & model management: interface, models, settings, GPTs, OpenAI Playground & test‑time compute",
        "Building RAG chatbots with Custom GPTs: data preparation from PDFs, HTML webpages, YouTube videos, CSV data sources & writing‑style adaptation",
        "Open‑source RAG with Ollama & AnythingLLM: installation, models, optimizing chunking & embeddings & creating a local bot",
        "Agent capabilities & multi‑LLM integration: system prompts, temperature control, web search, scraping & AI‑agent features with Flowise/LangGraph",
        "OpenAI API & Flowise for RAG agents: pricing, project setup, GDPR compliance, Playground vs. Response API, Node.js installation, Marketplace & OpenAI Assistant",
        "Advanced Flowise workflows: web scraping, embeddings, vector databases, HTML splitter, JSON import/export & tool agents (email, calendar, Airtable, webhooks)",
        "Custom chatbot UI & self‑hosting: frontend development, Ollama & LangChain, hosting on Render, Replit branding, WordPress integration & Flowise configuration",
        "RAG agents with n8n: local installation, interface, triggers/actions, Pinecone automation via Google Drive, workflows & AI‑agent node",
        "Combining & marketing Flowise & n8n: RAG lead‑bots, website integration, CSS branding, sales, marketing, customer acquisition & offer strategies",
        "Special RAG strategies: n8n MCPs with Claude Desktop, webhooks, GPT Actions, cache‑augmented generation, GraphRAG, LightRAG & contextual retrieval",
        "Security, data protection & legal framework: jailbreaks, prompt injections, data poisoning, censorship, GDPR basics, EU AI Act & copyright",
        "Strategies of leading AI providers & comparison: OpenAI, Anthropic, Microsoft, Google xAI, Meta’s LlaMA, Deepseek, Mistral & others"
      ],
      "course_content": {
        "Introduction: Tips, Course Overview & the Easiest Start with RAG – NotebookLM": [
          "Introduction",
          "Course Overview",
          "Important Tips for the Course",
          "Explanation of the Links for the Course",
          "Important Links",
          "Instructor Introduction: Arnold Oberleiter (Arnie)",
          "Quick Start with RAG: Using Google’s NotebookLM"
        ],
        "Fundamentals: LLMs, RAG, Vector Databases & the ChatGPT Interface Explained": [
          "What to Expect in This Section",
          "LLMs Explained: ChatGPT, Claude, Gemini, Deepseek, Llama, Mistral & More",
          "Function Calling: How LLMs Communicate with Tools via APIs and use RAG",
          "Vector Databases, Embedding Models, Tok-K and Chunking for RAG",
          "What is a API?",
          "ChatGPT Basics: Interface, Models, Settings, GPTs and the OpenAI Playground",
          "Test Time Compute Explained: Thinking Models like Deepseek R1 & OpenAI o3 & o4",
          "Recap of RAG, Vectordatabases, Top-K, Chunking, LLMs, Function Calling & APIs"
        ],
        "Hands‑On RAG with ChatGPT and Custom GPTs": [
          "What You’ll Learn in This Section about RAG and GPTs",
          "First RAG Bot from PDFs with GPTs: Data Preparation (Markdown) & System Prompts",
          "Transforming an HTML Webpage into a RAG Chatbot",
          "Building a RAG Bot from a YouTube Video",
          "Training ChatGPT on Your Writing Style via Retrieval‑Augmented Generation",
          "Using a CSV File as Data Source for a RAG Bot",
          "Unlocking the Power of GPTs – Drive Traffic to Your Links and So Much More",
          "Recap: What You’ve Learned and What You Should Do!"
        ],
        "Implementing RAG with Open‑Source LLMs: AnythingLLM & Ollama": [
          "What You’ll Learn in This Section: RAg with Ollama & Anything LLM",
          "Ollama Fundamentals: Installation, Models, Commands, Server & Hardware",
          "AnythingLLM Basics: Integration with Ollama, Settings and Interface",
          "Chunk Size & Chunk Overlap for your Embeddings",
          "Creating a Local RAG Chatbot with AnythingLLM and Ollama",
          "Control AI Behavior: System Prompt, Top-K, Similarity Search, Memory Temperature",
          "Overview of Agent Capabilities: Web Search, Python Interpreter, Scraping & MCPs",
          "Key Takeaways for Ollama and AnythingLLM with RAG"
        ],
        "RAG Chatbots & Agents with the OpenAI API: LangChain & LangGraph in Flowise": [
          "Section Overview: How to Use APIs to Build Your Own Applications",
          "OpenAI API Explained: Pricing, Project Setup, Management & GDPR Compliance",
          "OpenAI Playground and Response API: Simplifying RAG, Images, Audio & more",
          "LangChain, LangGraph & Flowsie: Key Tools for AI Workflows in this section",
          "Setting Up Node.js for Flowise: Installation, Management & Usage Options",
          "Fixing Flowise Installation Issues: Manage Node Versions with NVM",
          "Installing Flowise with Node.js via Command Prompt (and update your instance)",
          "The Flowise Interface & Overview: LangChain & LangGraph made Easy",
          "Build a RAG Chatflow: Web Scraping, Embeddings, Vector Database & HTML Splitter",
          "Exporting and Importing Flows as JSON",
          "Creating Your Own Chatbot UI Interface & Frontend for Flowise",
          "Running a RAG Chatbot Locally with Ollama & LangChain (Data Security)",
          "Flowise Tool-Agent: One Workflow to Connect Any API & LLM (Openrouter & Claude)",
          "Tool Agent with RAG: Pinecone, Function Calling & APIs (Postgres & Supabase)",
          "Problems with Pinecone Embeddings",
          "Prompt Engineering: System Prompts for AI Agents and RAG Agents",
          "AI Agents (Multi-Agents) with Multiple LLM Experts and RAG",
          "Sequential Agents with Human‑in‑the‑Loop and self-improving Agentic RAG",
          "Recap: Key Learnings of RAG Agents with APIs, LangChain & LangGraph in Flowise"
        ],
        "Building RAG Chatbots & Agents with n8n": [
          "What You’ll Learn in This Section: n8n!",
          "Local Installation of n8n with Node.js and the Interface (+ free test version)",
          "Managing Node Versions with nvm (Fixing Errors in n8n Installation)",
          "Updating n8n Locally via Node.js",
          "n8n Basics: Triggers, Actions, Nodes, Models and More",
          "RAG Bot for Lead Generation Built with n8n, Pinecone & Google Sheets",
          "Use my workflows: Exporting, Importing & Combining n8n Workflows as JSON Format",
          "Converting a Website into a RAG Chatbot via HTML Requests & Scraping",
          "Section Wrap‑Up of n8n"
        ],
        "RAG Apps with Flowise & n8n: Hosting, Self-Hosting & Selling Made Easy": [
          "What You’ll Learn in This Section: Self-Hosting, Building & Selling RAG-Agents",
          "Hosting Flowise on Render: Step‑by‑Step",
          "Building a RAG Chatbot for a Client: Click by Click with Flowise Tool Agent",
          "Enhancing LangGraph Chatbot Branding and Style on Replit",
          "Embedding a RAG Chatbot into a WordPress Site",
          "Add Your Logo & Final Bot Finetuning for a Polished Look",
          "Additional Settings for Your LangChain / LangGraph App in Flowise",
          "Hosting n8n: Self-Hosting with Render, Hostinger & Other Options",
          "n8n RAG Lead Bot as a Standalone App with a Published URL (hostet version)",
          "Integrating n8n RAG Bots into Websites: HTML, WordPress & Custom CSS Branding",
          "Selling RAG Agents: Marketing, Offers, Sales & More",
          "Recap: Hosting, Self-Hosting and Selling RAG Agents"
        ],
        "Advanced Workflows: Webhooks, MCPs, Claude, GPTs, RAG & Chunking Strategies": [
          "What’s Ahead: Special Workflows, Advanced RAG Techniques & MCP",
          "MCP (Model Context Protocol) Explained: MCP Server, Client & API Work Together",
          "Using Clade Desktop as an AI Agent (MCP Host) with MCP Server in n8n",
          "Code for Claude Desktop and the Model Context Protocol (MCP)",
          "Native MCP Server & MCP Host in n8n: Like Calling Another Workflow",
          "Connecting ChatGPT with n8n via Webhooks for AI Automations",
          "Code for GPT Actions",
          "Connecting Flowise to n8n: Use HTTP Post Request & cURL Import",
          "Connecting n8n to Flowise: Webhooks & Google Sheets via JavaScript",
          "Cache‑Augmented Generation (CAG) instand of RAG",
          "GraphRAG from Microsoft: More Accurate RAG Results with Knowledge Graphs",
          "LightRAG: Fast & Cost‑Effective Alternative to GraphRAG",
          "[POWERFUL] Contextual Retrieva: Improving RAG via Anthropic’s Chunking Strategy",
          "Choosing the Right Strategy: GraphRAG, LightRAG, CAG, or Contextual Retrieval?",
          "Recap: Special Workflows, Advanced Strategies & MCPs (Model Context Protocol)"
        ],
        "Challenges, Security and Copyrights in RAG Agents": [
          "What We’ll Cover and Initial Challenges Overview",
          "Security Warning for Telegram in n8n",
          "Jailbreaks: Attacks on LLMs and Agents via Prompts",
          "Prompt Injections as Attacks on Agents and LLMs",
          "Data Poisoning and Backdoor Attacks",
          "Copyrights & Intellectual Property of Generated Data from AI Agents",
          "Data Privacy for Your Own and Customer Data, GDPR and EU AI Act",
          "Censorship, Alignment, & Bias in LLMs like Deepseek, ChatGPT, Claude, or Gemini",
          "Can you sell AI agents, AI Automations or the codebase from n8n?",
          "GDPR Basics",
          "EU & US Compliance: GDPR, (DSGVO) CCPA, CPRA & the EU AI Act",
          "GDPR: Key Information in a Comprehensive Article",
          "EU AI Act: Key Information in a Comprehensive Article",
          "Recap: Important Points to Remember"
        ],
        "What’s Next?": [
          "Recap: Thank You and What Comes Next?",
          "Bonus"
        ]
      },
      "requirements": [
        "No prior knowledge required—everything is demonstrated step by step."
      ],
      "description": "One of the most important concepts in the AI world is RAG – Retrieval-Augmented Generation!\nYou need to give LLMs knowledge!\nBut how do you build powerful RAG chatbots and intelligent AI agents to optimize your business processes and personal projects?\nIn this course, you’ll learn exactly that—comprehensively and clearly explained—using ChatGPT, Claude, Google Gemini, open‑source LLMs, Flowise, n8n, and more!\nFundamentals: LLMs, RAG & Vector Databases\nBuild a solid foundation for your AI projects:\nDeepen your knowledge of LLMs: ChatGPT, Claude, Gemini, Deepseek, Llama, Mistral, and many more.\nUnderstand how Function Calling and API communication work in LLMs.\nLearn why vector databases and embedding models are the heart of RAG.\nMaster the ChatGPT interface, GPT models, settings, and the OpenAI Playground.\nExplore key concepts like Test‑Time Compute (e.g. OpenAI o1, o3; Deepseek R1).\nDiscover how Google’s NotebookLM works and leverage it effectively for RAG projects.\nSimple RAG Implementations with ChatGPT & Custom GPTs\nGet your first AI applications up and running quickly and easily:\nCreate your very first RAG bot from PDFs using Custom GPTs.\nTurn HTML web pages and YouTube videos into interactive RAG chatbots.\nTrain ChatGPT on your personal writing style via RAG.\nUse CSV data to build smart chatbots and explore the full potential of Custom GPTs.\nRAG with Open‑Source LLMs: AnythingLLM & Ollama\nDive into the world of local AI:\nInstall and use Ollama: learn about models, commands, and hardware requirements.\nIntegrate AnythingLLM effectively with Ollama—optimize chunking and embeddings.\nBuild local RAG chatbots and precisely control language and behavior with system prompts and temperature settings.\nLeverage agent capabilities like web search, scraping, and more.\nFlowise: RAG with LangChain & LangGraph Made Easy\nHarness the power of the OpenAI API for professional applications:\nMaster the OpenAI API, pricing models, GDPR compliance, and project setup.\nBuild efficient RAG applications via the OpenAI Playground and response APIs.\nInstall Flowise, manage updates, and become proficient with its interface—including the Marketplace and OpenAI Assistant.\nCreate comprehensive RAG chatflows with web scraping, embeddings, HTML splitters, and vector databases.\nDevelop your own chatbot UI and handle Flowise’s technical details.\nImplement local AI security with Ollama & LangChain and use Flowise’s tool‑agent nodes (e.g. email, calendar, Airtable).\nCombine Pinecone vector databases with Supabase and Postgres.\nMaster prompt engineering and sequential agents with human‑in‑the‑loop workflows.\nn8n: Building AI Automations & RAG Agents\nUse n8n as a powerful automation platform for your AI projects:\nLearn local installation, updates, and n8n basics.\nAutomate Pinecone database updates via Google Drive.\nDevelop RAG chatbots with AI‑agent nodes, vector databases, and supplementary tools.\nCreate automated chatbots from websites using HTML requests and scraping.\nHosting, Selling & Monetizing Your RAG Agents\nTake your AI projects to market professionally:\nHost Flowise and n8n apps on platforms like Render and embed them in websites (HTML, WordPress).\nDesign branded, professional chatbots and offer them as services or standalone products.\nDevelop effective marketing and sales strategies for your AI agents.\nAdvanced Workflows & Specialized RAG Techniques\nAdopt professional, cutting‑edge technologies:\nLearn advanced techniques like webhooks, MCPs with Claude, GPT Actions, and n8n integration.\nUnderstand the Model Context Protocol (MCP) and build both MCP servers and clients in n8n and Claude Desktop.\nExplore innovative RAG strategies such as Cache‑Augmented Generation (CAG), GraphRAG (Microsoft), LightRAG, and Anthropic’s Contextual Retrieval.\nOptimize chunking, embedding, and Top‑K retrieval for your RAG apps.\nChoose the right strategy for your projects and maximize your RAG outcomes.\nSecurity, Privacy & Legal Foundations\nProtect your AI projects effectively:\nRecognize security risks (Telegram exploits, jailbreaks, prompt injections, data poisoning).\nSecure your AI against attacks and respect copyrights in generated content.\nDeepen your understanding of GDPR and the upcoming EU AI Act to ensure legal compliance.\nBecome an expert in AI automations, AI agents & RAG!\nBy the end of this course, you will be fully equipped to build, optimize, and successfully market RAG chatbots, AI agents, and automations.",
      "target_audience": [
        "Private individuals interested in AI and automation who want to build their own RAG agents",
        "Entrepreneurs looking to become more efficient, save money, or build an AI‑based business",
        "Anyone eager to learn something new and gain deep insights into RAG agents",
        "Anyone who wants to finally understand RAG and automate tasks"
      ]
    },
    {
      "title": "Statistics for Data Analysis Using Excel (Accredited)",
      "url": "https://www.udemy.com/course/statistics-using-excel/",
      "bio": "Descriptive and Inferential Statistics Theory With Excel Examples for Business / Six Sigma - 14 PMI PDUs (2025 Version)",
      "objectives": [
        "Fully Updated for 2025! New videos, 2 real-life projects, and 140+ quiz questions added. Dive in and upgrade your stats skills now!",
        "Master the essentials of descriptive and inferential statistics through straightforward and practical examples, making complex concepts easy to grasp.",
        "Harness the power of Microsoft Excel to perform a wide range of statistical calculations, eliminating the need to remember long and complex formulas.",
        "Gain a solid foundation in statistics, from basic principles to advanced applications, enabling you to solve business problems efficiently using Excel.",
        "Transform vast amounts of data into meaningful insights, empowering you to make informed, fact-based decisions that drive business success.",
        "Develop proficiency in Excel, to perform and streamline your data analysis tasks without the need for advanced software like Minitab, R, or Python.",
        "Visualize your data effectively by creating and interpreting histograms, box plots, and scatter plots, to communicate your findings clearly and effectively.",
        "Enhance your career prospects by gaining practical skills in data analysis using Excel, a highly valued competence in today's data-driven business environment.",
        "Boost your confidence in handling statistical analysis, making you a valuable asset to your organization and opening up opportunities for professional growth."
      ],
      "course_content": {
        "Basics of Statistics and Desriptive Statistics": [
          "Section 1 Introduction",
          "Quality Gurus Inc Certificate, Digital Badge, PMI PDUs, SHRM PDCs (Optional)",
          "Introduction to Statistics (Theory)",
          "Types and Scale of Data, Descriptive vs Inferential Statistics (Theory)",
          "Data Types and Measurement Scales",
          "Data Analysis Toolpack (Excel)",
          "Formula, Range and References",
          "Range and References",
          "Descriptive Statistics - Central Tendency (Theory)",
          "Different Types of Averages (Excel)",
          "Calculating Median (Excel)",
          "Single and Multiple Modes (Excel)",
          "Measurement of Dispersion (Theory)",
          "Range, Standard Deviation and Variance (Excel)",
          "Count Related Functions (Excel)",
          "Descriptive Statistics",
          "D-Functions an Alternate Approach to Descriptive Statistics (Excel)",
          "D-Functions",
          "Inter-quartile Range (IQR) - (Theory)",
          "Inter-quartile Range (IQR) - (Excel)",
          "Shape of the Distribution - Skewness and Kurtosis (Theory)",
          "Inter-quartile Range, Skewness and Kurtosis",
          "Arrays (Only for Excel 2019 or Older Version) - Skip if using Excel 365",
          "Download Section 1 Slides"
        ],
        "Basic Probability Concepts and Probability Distributions": [
          "Section Introduction - Probability and Probability Distributions",
          "Basic Statistical Terms",
          "Central Limit Theorem (Theory)",
          "Central Limit Theorem Demonstration (Excel)",
          "Basic Statistics Terms and Central Limit Theorem",
          "Basic Probability Concepts - Part 1 (Theory)",
          "Venn Diagrams (Theory)",
          "Rules of Addition and Multiplication (Theory)",
          "Conditional Probability (Theory)",
          "Probability Concepts",
          "Factorial, Permutations and Combinations (Theory)",
          "Calculate Factorial, Permutations and Combinations (Excel)",
          "Factorials, Permutations and Combinations",
          "Common Probability Distributions",
          "Binomial Probability Distribution (Theory)",
          "Binomial Distribution - Mean and Variance (Theory)",
          "Binomial Probability Distribution (Excel)",
          "Binomial Distribution",
          "Poisson Distribution (Theory)",
          "Poisson Distribution (Excel)",
          "Poisson Distribution",
          "PDF and PMF",
          "Normal Probability Distribution (Theory)",
          "Standard Normal Distribution (Theory)",
          "Normal Probability Distribution (Excel) - Part 1",
          "Normal Probability Distribution (Excel) - Part 2",
          "Normal Distribution",
          "Student's t Distribution (Theory)",
          "Student's t Distribution (Excel)",
          "Chi-square Distribution (Theory)",
          "Chi-square Distribution (Excel)",
          "F Distribution (Theory)",
          "F Distribution (Excel)",
          "Student's t, Chi-square and F Distributions",
          "Download Section 2 Slides"
        ],
        "Hypothesis Testing": [
          "Hypothesis Testing - Introduction",
          "Hypothesis Testing Terminology Part 1 (Theory)",
          "Hypothesis Testing Terminology Part 2 - P Value (Theory)",
          "Hypothesis Testing Part 3 - Types of Errors (Theory)",
          "Statistical vs Practical Significance (Theory)",
          "Basics of Hypothesis Testing",
          "Sample Size Calculation (Theory)",
          "Sample Size Calculation (Excel)",
          "Point and Interval Estimates (Theory)",
          "Point and Interval Estimate (Excel)",
          "Sample Size, Point and Interval Estimates",
          "Hypothesis Testing Steps",
          "Hypothesis Testing Steps",
          "Tests for Mean, Variance and Proportions - Introduction",
          "One Sample z Test (Theory)",
          "One Sample z Test - Introduction (Excel)",
          "One Sample t Test (Theory)",
          "One Sample t Test (Excel)",
          "One Proportion Test (Theory)",
          "One Proportion Test (Excel)",
          "One Variance Test (Theory)",
          "One Variance Test (Excel)",
          "One Sample Tests",
          "Two Sample z Test (Theory)",
          "Two Sample z Test (Excel)",
          "Two Sample t Test (Theory)",
          "Two Sample t Test (Excel)",
          "Two Sample t Test - Paired t Test (Theory)",
          "Two Sample t Test - Paired t Test (Excel)",
          "Two Proportions Test - Introduction (Theory)",
          "Two Proportions Test (Excel)",
          "Quiz 9: Two Sample Tests",
          "Two Variance Test (Theory)",
          "Two Variance Test (Excel)",
          "Two Sample Tests",
          "Analysis of Variance (ANOVA) - (Theory)",
          "ANOVA an Example Using Manual Calculations (Theory)",
          "ANOVA Table Example (Theory)",
          "One-Factor ANOVA (Excel)",
          "Two-Factor ANOVA without Replication (Excel)",
          "Two-Factor ANOVA with Replication (Excel)",
          "ANOVA",
          "Goodness of Fit Test (Theory)",
          "Goodness of Fit Test (Excel))",
          "Contingency Table (Theory)",
          "Contingency Table (Excel)",
          "Goodness of Fit and Contingency Tables",
          "Download Section 3 Slides"
        ],
        "Correlation and Linear Regression": [
          "Section Introduction - Correlation and Regression",
          "Correlation Coefficient - Manual Calculation (Theory)",
          "Coefficient of Determination (Theory)",
          "Plotting Scatter Plot and Calculating Correlation Coefficient (Excel)",
          "Correlation Related Functions (Excel)",
          "Regression Equation - Manual Calculation",
          "Regression Related Functions (Excel)",
          "Regression using Data Analysis Pack (Excel)",
          "Quiz 13 - Correlation and Regression",
          "Download Section 4 Slides"
        ],
        "Project Work": [
          "Section Introduction",
          "Project 1 Problem Statement",
          "Project 1 Solution",
          "Project 2 Problem Statement",
          "Project 2 Solution"
        ],
        "Conclusion": [
          "BONUS LECTURE"
        ]
      },
      "requirements": [
        "You should have some basic understanding of Microsoft Excel."
      ],
      "description": "Note: Students who complete this course can apply for the certification exam by Quality Gurus Inc. and achieve the Verified Certification from Quality Gurus Inc. It is optional, and there is no separate fee for it. Quality Gurus Inc. is the Authorized Training Partner (ATP # 6034) of the Project Management Institute (PMI®) and the official Recertification Partner of the Society for Human Resource Management (SHRM®)\nThe verified certification from Quality Gurus Inc. provides you with 14.0 pre-approved PMI PDUs and 14.0 SHRM PDCs at no additional cost to you.\nThis course is accredited by The CPD Group (UK). You are eligible to claim 14.0 CPDs for this course (Accreditation# 1016204)\n\n\nWelcome to \"Statistics for Data Analysis Using Microsoft Excel,\" a fully updated and enhanced course designed to teach you both descriptive and inferential statistics through practical, real-world examples.\nThis course will eliminate the need to memorize complex formulas by showing you how to leverage Microsoft Excel for statistical analysis effortlessly. By the end of this course, you will be able to apply statistical methods confidently to solve business problems and make fact-based decisions with real data insights.\n\n\nWhat’s New in the 2025 Update?\n140+ Quiz Questions across all sections for deeper understanding.\nTwo Real-World Projects to apply your skills in business scenarios.\nNew Video Content with clearer explanations of statistical concepts.\nDownloadable Slides & Resources for easy reference.\n\n\nWhat You’ll Learn\nDescriptive & Inferential Statistics: Understand and apply statistical concepts from basic to advanced levels.\nExcel Proficiency: Use Excel functions & formulas to simplify statistical calculations.\nData Interpretation: Learn to analyze and draw insights from raw data.\nData Visualization: Create histograms, box plots, and scatter plots to represent data visually.\nReal-World Decision-Making: Solve business problems using fact-based statistical analysis.\n\n\nCourse Structure & Topics Covered\nSection 1: Introduction to Statistics & Descriptive Statistics\nFundamentals of Statistics & their importance in business.\nExcel for Data Analysis: Key functions & tools.\nMeasures of Central Tendency: Mean, median, mode.\nMeasures of Spread: Standard deviation, range, quartiles, interquartile range.\nSection 2: Probability & Distributions\nBasic Probability Concepts: Permutations, combinations, & probability rules.\nProbability Distributions: Normal, binomial, Poisson, Student's t, Chi-square and F distributions.\nSection 3: Hypothesis Testing & ANOVA\nOne-Sample Tests: Z-tests, t-tests, p-tests, variance tests\nTwo-Sample Tests: Compare groups using statistical testing.\nAnalysis of Variance (ANOVA): Step-by-step procedures.\nSection 4: Correlation & Regression\nUnderstanding Relationships: How variables influence each other.\nCorrelation Analysis: Measure the strength and direction of relationships.\nSimple Linear Regression: Predict future trends using Excel.\nSection 5: Practical Applications & Projects\nReal-World Projects: Apply statistical methods to business scenarios.\nData Interpretation: Drawing actionable insights from analysis.\n\n\n\n\nNew for 2025: Two Hands-On Projects in Section 5!\nProject 1: Who buys more cosmetics? Male vs. Female—What does the data say?\nProject 2: Nurse Salaries—Do male or female nurses earn more?\nYour Mission (If You Choose to Accept It):\nDon’t assume females buy more cosmetics than males.\nDon’t assume one gender earns more in nursing.\nUse data, not assumptions! Let statistics reveal the truth.\nStart the Updated Course Now & Upgrade Your Data Analysis Skills!\nReady to apply statistics to real-world problems? Join 25,000+ students who are mastering data analysis with Excel!\n\n\n\n\nWhat are other students saying about this course?\nHe's better than all my university lecturers, very clear, concise and seamless progression of the evolution of concepts in a systematic way that makes it easy to understand...Bravo!!  (5 stars by Ayanda Peter)\nA well-planned curriculum, fantastic resources for downloads, professionally presented slides, very easy to understand, truly the best course for every financial analyst.  (5 stars by Arumugam K Chandrasekar)\nI am fan of his teachings. it is recommended to every other person who is not confident in Statistics and want to be a pro. (5 stars by Apnatav Bhatia)\nIts more than my expectations, absolutely wonderful since it start with basics. (5 stars by Boikaego Raditlatla)\nGreat course for learning business statistics or statistics in general. (5 stars by Sudesh Pandey)\nBrilliant course. Takes the difficult, sometimes even boring Statistics model and breaks them into easy bite size portions. Explains the theory behind it and then the Excel way of doing it. (5 stars by Karthikeyan Stalin)\nThe self study step by step and the excel examples are very great. I can follow the course and practice on my computer alongside. Thank you for putting this whole thing together. Not a very exciting subject to teach so I appreciate being able to put this long course together to make it easy for people like us to utilize and study. (5 stars by Dr Stanley Adjabeng)\nContinuous Professional Development (CPD) Units:\nFor the ASQ® Recertification Units (RUs), we suggest 1.40 RUs under the Professional Development > Continuing Education category.\nFor PMI®, 14.0 pre-approved PDUs can be provided after completing our optional/free certification exam. The detailed steps for taking Quality Gurus Inc. certification with preapproved PDUs are provided in the courses.\n\n\nWhat are you waiting for?\nThis course comes with Udemy's 30 days money-back guarantee. If you are not satisfied with the course, get your money back.\nI hope to see you on the course.",
      "target_audience": [
        "Business managers and data analysts who are trying make decision based on data and facts",
        "Six Sigma Green and Black Belt professionals using MS Excel to conduct statistical analysis"
      ]
    },
    {
      "title": "DP-900 Azure Data Fundamentals Exam Prep In One Day",
      "url": "https://www.udemy.com/course/dp900-azure/",
      "bio": "Learn the basics of Azure database services and get certified with this complete DP-900 course!",
      "objectives": [
        "Take and pass the Azure DP-900 Data Fundamentals exam",
        "Understand the core concepts of Azure data services like SQL Database",
        "Understand the data analytics products of Azure such as Azure Stream Analytics",
        "Understand the data reporting products of Azure such as Power BI"
      ],
      "course_content": {
        "Welcome to the DP-900 Azure Data Fundamentals course": [
          "Welcome to the course",
          "DP-900 Exam Requirements",
          "Udemy Video Player",
          "FAQs"
        ],
        "Core Concepts": [
          "Describe ways to represent data",
          "Identify options for data storage",
          "Describe common data workloads",
          "Identify roles and responsibilities for data workloads",
          "Section Summary",
          "Core Concepts Quiz"
        ],
        "Relational Database Concepts": [
          "Identify features of relational data",
          "Describe normalization and why it is used",
          "Three Normal Forms",
          "LIVE DEMO: A Sample Database",
          "Identify common structured query language (SQL) statements",
          "LIVE DEMO: Create a Database View",
          "LIVE DEMO: Create a Stored Procedure",
          "LIVE DEMO: Create an Index"
        ],
        "Describe relational Azure data services": [
          "Introduction to Relational DBs",
          "Azure Relational DB Options"
        ],
        "Manage Relational Databases": [
          "Create an Azure SQL Database",
          "Use ARM Templates to Manage SQL Databases",
          "SQL Database Security",
          "Relational Query Tools",
          "Structured Query Language (SQL)"
        ],
        "Azure Storage Options": [
          "Create a Unmanaged Storage Account",
          "Azure Blob Storage",
          "Azure File Storage",
          "Azure Table Storage",
          "Don't Forget to Delete Test Resources"
        ],
        "Non-Relational Database Concepts": [
          "Introduction to Non-Relational DBs",
          "Non-Relational Data Types",
          "Choose a NoSQL Database",
          "Azure Non-Relational DB Options"
        ],
        "Manage Non-Relational Databases": [
          "Create Cosmos DB",
          "Query Cosmos DB",
          "Use ARM Templates to Manage Cosmos DB",
          "Cosmos DB Security",
          "Cosmos DB Geo-Replication"
        ],
        "Data Analytics Workloads": [
          "Data Analytics Workloads",
          "OLTP",
          "OLAP",
          "Synapse Analytics SQL Data Warehouse",
          "Modern Data Warehouse",
          "*NEW* Microsoft Fabric"
        ],
        "Data Ingestion and Processing": [
          "Azure Data Factory"
        ]
      },
      "requirements": [
        "An interest in Azure cloud computing",
        "No background in data required but it would be helpful to be familiar with it"
      ],
      "description": "LEARN AZURE DATABASE AND DATA PROCESSING TECHNOLOGIES IN ONE DAY!\nThe course is completely up-to-date with new requirements.\nUp-to-date with the latest requirements for November 2024.\nComplete preparation for the new DP-900 Azure Data Fundamentals exam.\nThe content of this exam was updated for the January 2024 changes.\nDescribe core data concepts (25-30%)\nDescribe how to work with relational data on Azure (20-25%)\nDescribe how to work with non-relational data on Azure (15-20%)\nDescribe an analytics workload on Azure (25-30%)\nThis brand-new course completely covers the DP-900 exam from start to finish. Always updated with the latest requirements. This course goes over each requirement of the exam in detail. If you have no background in databases and want to learn about them and want to learn more about database concepts and services within Azure, or have some background in databases and want to progress eventually to an Azure Data Engineer or Data Analyst type role, this course is a great resource for you.\nMicrosoft Azure is still the fastest-growing large cloud platform. The opportunities for jobs in cloud computing are still out there, and finding well-qualified people is the #1 problem that businesses have.\nIf you're looking to change your career, this would be a good entry point into cloud computing on the data side.\nSign up today!\nAdded English, Spanish, and Portuguese closed captions.",
      "target_audience": [
        "Students who are just starting out in the topic of cloud data",
        "Students who want to take more advanced Azure data certifications but want to work on the basics first"
      ]
    },
    {
      "title": "Artificial Intelligence and Machine Learning: Complete Guide",
      "url": "https://www.udemy.com/course/artificial-intelligence-and-machine-learning-complete-guide/",
      "bio": "Do you want to study AI and don't know where to start? You will learn everything you need to know in theory and practice",
      "objectives": [
        "The theoretical and practical basis of the main Artificial Intelligence algorithms",
        "Implement Artificial Intelligence algorithms from scratch and using pre-defined libraries",
        "Learn the intuition and practice about machine learning algorithms for classification, regression, association rules, and clustering",
        "Learn Machine Learning without knowing a single line of code",
        "Use Orange visual tool to create, analyze and test algorithms",
        "Use Python programming language to create Artificial Intelligence algorithms",
        "Learn the basics of programming in Python",
        "Use greedy search and A* (A Star) algorithms to find the shortest path between cities",
        "Implement optimization algorithms for minimization and maximization problems",
        "Implement an AI to predict the amount of tip to be given in a restaurant, using fuzzy logic",
        "Use data exploration techniques applied to a COVID-19 disease database",
        "Create a reinforcement learning agent to simulate a taxi that needs to learn how to pick up and drop off passengers",
        "Implement artificial neural networks and convolutional neural networks to classify images of the characters Homer and Bart, from the Simpsons cartoon",
        "Learn natural language processing techniques and create a sentiment classifier",
        "Detect and recognize faces using computer vision techniques",
        "Track objects in video using computer vision",
        "Generate new images that do not exist in the real world using Artificial Intelligence"
      ],
      "course_content": {
        "Introduction": [
          "Course content",
          "Terminology",
          "Course materials"
        ],
        "Part 1 - Search algorithms": [
          "Introduction",
          "Search - intuition",
          "Heuristics - intuition",
          "Ordered arrays - intuition",
          "Ordered arrays - implementation",
          "Creating the city map",
          "Greedy search - intuition",
          "Greedy search - implementation",
          "A* search - intuition",
          "A* search - implementation",
          "HOMEWORK",
          "Homework solution"
        ],
        "Part 2 - Optimization algorithms": [
          "Optimization algorithms - intuition",
          "Case study - flight schedule",
          "Representing the problem",
          "Printing the solution",
          "Fitness function",
          "Hill climb - intuition",
          "Hill climb - implementation",
          "Simulated annealing - intuition",
          "Simulated annealing - implementation",
          "Genetic algorithm - intuition",
          "Genetic algorithm - implementation",
          "HOMEWORK",
          "Homework solution"
        ],
        "Part 3 - Fuzzy logic": [
          "Introduction",
          "Applications of fuzzy logic",
          "Fuzzy logic - intuition",
          "Implementation 1",
          "Implementation 2",
          "Implementation 3",
          "HOMEWORK",
          "Homework solution"
        ],
        "Part 4 - Machine learning": [
          "Introduction",
          "Machine learning and Data Science"
        ],
        "Classification": [
          "What is classification?",
          "Naïve Bayes - intuition",
          "Naïve Bayes in Orange",
          "Decision trees - intuition",
          "Decision trees in Orange",
          "Rule based learning - intuition",
          "Rule based learning in Orange",
          "kNN (k nearest neighbors) - intuition",
          "kNN (k nearest neighbors) in Orange",
          "SVM (Support Vectors Machines) - intuition",
          "SVM (Support Vectors Machines) in Orange",
          "Logistic regression - intuition",
          "Logistic regression in Orange",
          "Crossvalidation",
          "HOMEWORK",
          "Homework solution",
          "Image classification in Orange"
        ],
        "Regression": [
          "What is regression?",
          "Linear regression - intuition",
          "Linear regression in Orange",
          "HOMEWORK",
          "Homework solution"
        ],
        "Clustering": [
          "What is clustering?",
          "K-means algorithm - intuition",
          "K-means algorithm in Orange",
          "HOMEWORK",
          "Homework solution",
          "Clustering images in Orange"
        ],
        "Association rules": [
          "What are association rules?",
          "Apriori algorithm",
          "Apriori in Orange",
          "HOMEWORK",
          "Homework solution"
        ],
        "Additional topics": [
          "Missing values and normalization",
          "Discretization",
          "Feature selection",
          "Dimensionality reduction using PCA",
          "PCA and clustering",
          "Outliers detection",
          "Time series 1",
          "Time series 2",
          "Basic charts",
          "COVID dataset 1",
          "COVID dataset 2",
          "COVID dataset 3"
        ]
      },
      "requirements": [
        "Programming logic",
        "It is not necessary to know Python programming language, as at the end of the course there is an annex with basic classes if this is your first contact with it"
      ],
      "description": "The fields of Artificial Intelligence and Machine Learning are considered the most relevant areas in Information Technology. They are responsible for using intelligent algorithms to build software and hardware that simulate human capabilities. The job market for Machine Learning is on the rise in various parts of the world, and the trend is for professionals in this field to be in even higher demand. In fact, some studies suggest that knowledge in this area will soon become a prerequisite for IT professionals.\nTo guide you into this field, this course provides both theoretical and practical insights into the latest Artificial Intelligence techniques. This course is considered comprehensive because it covers everything from the basics to the most advanced techniques. By the end, you will have all the necessary tools to develop Artificial Intelligence solutions applicable to everyday business problems. The content is divided into seven parts: search algorithms, optimization algorithms, fuzzy logic, machine learning, neural networks and deep learning, natural language processing, and computer vision. You will learn the basic intuition of each of these topics and implement practical examples step by step. Below are some of the projects/topics that will be covered:\n\n\nFinding optimal routes on city maps using greedy search and A* (star) search algorithms\nSelection of the cheapest airline tickets and profit maximization using the following algorithms: hill climb, simulated annealing, and genetic algorithms\nPrediction of the tip you would give to a restaurant using fuzzy logic\nClassification using algorithms such as Naïve Bayes, decision trees, rules, k-NN, logistic regression, and neural networks\nPrediction of house prices using linear regression\nClustering bank data using k-means algorithm\nGeneration of association rules with Apriori algorithm\nData preprocessing, dimensionality reduction, and outlier detection in databases\nPrediction of stock prices using time series analysis\nData visualization and exploration in the context of the COVID-19 disease database\nBuilding of a reinforcement learning agent to control a taxi for passenger transportation\nClassification of cat and dog images using convolutional neural networks\nClassification of Homer and Bart images from The Simpsons cartoon using convolutional neural networks\nPOS tagging, lemmatization, stemming, word cloud, and named entity recognition using natural language processing techniques\nImplementation of a sentiment classifier in the context of a Twitter dataset\nFace detection and recognition in images\nObject tracking in videos\nGeneration of images that do not exist in the real world using advanced Computer Vision techniques\nEach type of problem requires different techniques for its solution, so by covering all AI areas, you'll know which techniques to use in various scenarios! Throughout the course, we will use the Python programming language and the graphical tool Orange. If you are not familiar with Python, you will have access to over 5 hours of video exercises covering the basics of this programming language. This course is suitable for your first exposure to Artificial Intelligence, as it covers all the necessary topics in theory and practice. If you are more advanced in this field, you can use this course as a reference to learn new areas and review concepts.",
      "target_audience": [
        "People interested in starting their studies in Artificial Intelligence, Machine Learning, Data Science or Deep Learning",
        "People who want to study Artificial Intelligence, however, don't know where to start",
        "Undergraduate students studying subjects related to Artificial Intelligence",
        "Anyone interested in Artificial Intelligence",
        "Entrepreneurs who want to apply machine learning to commercial projects",
        "Entrepreneurs who want to create efficient solutions to real problems in their companies"
      ]
    },
    {
      "title": "Data science and Data preparation with KNIME",
      "url": "https://www.udemy.com/course/data-science-and-data-preparation-with-knime/",
      "bio": "KNIME - a powerful tool for data science and machine learning Data science with higher efficiency. KNIME data cleaning",
      "objectives": [
        "New job opportunities might open up for you",
        "You might be able to increase your productivity and save time in your data preparation tasks",
        "Hopefully a higher efficiency in data preparation and data science related work",
        "What kind of loops are available and how to use them in KNIME",
        "Examples of data science machine learning workflows with KNIME",
        "Enhance your basic KNIME skills already acquired ( for example in my KNIME crash course on udemy)",
        "How to use Python in KNIME (Java and R could also be used but will not be the focus here)",
        "How to do DataScience in KNIME WITH AND WITHOUT CODING",
        "You increase your productivity"
      ],
      "course_content": {},
      "requirements": [
        "No extra costs - KNIME can be downloaded for free",
        "You should have worked with KNIME before",
        "The program itself and the basics are covered in \"KNIME - a crash course for beginners\" which is also available on udemy",
        "Basic knowledge of machine learning is certainly helpful.",
        "Coding is not required but we learn how you can use python and use your python code in KNIME"
      ],
      "description": "Master Data Science, Cleaning, and Preparation with KNIME\nWelcome to the world of efficient data preparation, where tedious tasks become a breeze. In the realm of data science and analysis, one thing is certain: data cleaning, preprocessing, or whatever you choose to call it, can be a time-consuming ordeal.\nEfficiency at Your Fingertips\nHow can we expedite this process and work smarter, not harder? The answer lies in tools that not only accelerate the process but also reduce the need for excessive coding.\nMeet KNIME - Your Data Ally\nKNIME is the hero of the day. It offers a user-friendly, drag-and-drop interface that simplifies data preparation and cleaning. No coding experience required, though you have the option to unleash the power of R, Python, or Java if you wish. KNIME's flexibility knows no bounds. You can even venture into Data Science, including machine learning and AI, with or without coding.\nDid We Mention It's FREE?\nYou read that correctly. KNIME Desktop won't cost you a dime. It's a robust tool that won't dent your budget.\nElevate Your KNIME Skills\nThis course follows our introductory KNIME class, \"KNIME - A Crash Course for Beginners,\" also available on Udemy. In this second installment, we delve deeper into advanced topics.\nWhat's Inside the Course\nWe skip the basics here (like the interface, basic data import, and filter nodes). If you're new to KNIME or need a refresher, consider exploring our first class, where we cover the fundamentals in a captivating case study.\nIn this class, we explore:\nEfficient methods to import multiple files into KNIME\nThe power of loops\nWeb scraping techniques\nScripting using Python within KNIME\nHyperparameter optimization\nFeature selection\nBasic machine learning workflows and essential KNIME nodes\nIf efficiency and diving deep into data science and preparation sound appealing to you, then let's embark on this journey together!\nAre you ready to elevate your data skills?",
      "target_audience": [
        "(Aspiring) data scientists",
        "(Aspiring) data analysts",
        "data scientists / analysts who want to work smarter faster and more efficient"
      ]
    },
    {
      "title": "Deep Learning Masterclass with TensorFlow 2 Over 20 Projects",
      "url": "https://www.udemy.com/course/deep-learning-masterclass-with-tensorflow-2-over-15-projects/",
      "bio": "Master Deep Learning with TensorFlow 2 with Computer Vision,Natural Language Processing, Sound Recognition & Deployment",
      "objectives": [
        "The Basics of Tensors and Variables with Tensorflow",
        "Basics of Tensorflow and training neural networks with TensorFlow 2.",
        "Convolutional Neural Networks applied to Malaria Detection",
        "Building more advanced Tensorflow models with Functional API, Model Subclassing and Custom Layers",
        "Evaluating Classification Models using different metrics like: Precision,Recall,Accuracy and F1-score",
        "Classification Model Evaluation with Confusion Matrix and ROC Curve",
        "Tensorflow Callbacks, Learning Rate Scheduling and Model Check-pointing",
        "Mitigating Overfitting and Underfitting with Dropout, Regularization, Data augmentation",
        "Data augmentation with TensorFlow using TensorFlow image and Keras Layers",
        "Advanced augmentation strategies like Cutmix and Mixup",
        "Data augmentation with Albumentations with TensorFlow 2 and PyTorch",
        "Custom Loss and Metrics in TensorFlow 2",
        "Eager and Graph Modes in TensorFlow 2",
        "Custom Training Loops in TensorFlow 2",
        "Integrating Tensorboard with TensorFlow 2 for data logging, viewing model graphs, hyperparameter tuning and profiling",
        "Machine Learning Operations (MLOps) with Weights and Biases",
        "Experiment tracking with Wandb",
        "Hyperparameter tuning with Wandb",
        "Dataset versioning with Wandb",
        "Model versioning with Wandb",
        "Human emotions detection",
        "Modern convolutional neural networks(Alexnet, Vggnet, Resnet, Mobilenet, EfficientNet)",
        "Transfer learning",
        "Visualizing convnet intermediate layers",
        "Grad-cam method",
        "Model ensembling and class imbalance",
        "Transformers in Vision",
        "Model deployment",
        "Conversion from tensorflow to Onnx Model",
        "Quantization Aware training",
        "Building API with Fastapi",
        "Deploying API to the Cloud",
        "Object detection from scratch with YOLO",
        "Image Segmentation from scratch with UNET model",
        "People Counting from scratch with Csrnet",
        "Digit generation with Variational autoencoders (VAE)",
        "Face generation with Generative adversarial neural networks (GAN)",
        "Sentiment Analysis with Recurrent neural networks, Attention Models and Transformers from scratch",
        "Neural Machine Translation with Recurrent neural networks, Attention Models and Transformers from scratch",
        "Intent Classification with Deberta in Huggingface transformers",
        "Neural Machine Translation with T5 in Huggingface transformers",
        "Extractive Question Answering with Longformer in Huggingface transformers",
        "E-commerce search engine with Sentence transformers",
        "Lyrics Generator with GPT2 in Huggingface transformers",
        "Grammatical Error Correction with T5 in Huggingface transformers",
        "Elon Musk Bot with BlenderBot in Huggingface transformers"
      ],
      "course_content": {},
      "requirements": [
        "Basic Math",
        "Access to an internet connection, as we shall be using Google Colab (free version)",
        "Basic Knowledge of Python"
      ],
      "description": "Deep Learning is one of the most popular fields in computer science today. It has applications in many and very varied domains. With the publishing of much more efficient deep learning models in the early 2010s, we have seen a great improvement in the state of the art in domains like Computer Vision, Natural Language Processing, Image Generation, and Signal Processing.\nThe demand for Deep Learning engineers is skyrocketing and experts in this field are highly paid, because of their value. However, getting started in this field isn’t easy. There’s so much information out there, much of which is outdated and many times don't take the beginners into consideration :(\nIn this course, we shall take you on an amazing journey in which you'll master different concepts with a step-by-step and project-based approach. You shall be using Tensorflow 2 (the world's most popular library for deep learning, and built by Google) and Huggingface. We shall start by understanding how to build very simple models (like Linear regression models for car price prediction, text classifiers for movie reviews, binary classifiers for malaria prediction) using Tensorflow and Huggingface transformers, to more advanced models (like object detection models with YOLO, lyrics generator model with GPT2 and Image generation with GANs)\nAfter going through this course and carrying out the different projects, you will develop the skill sets needed to develop modern deep-learning solutions that big tech companies encounter.\n\n\nYou will learn:\nThe Basics of Tensorflow (Tensors, Model building, training, and evaluation)\nDeep Learning algorithms like Convolutional neural networks and Vision Transformers\nEvaluation of Classification Models (Precision, Recall, Accuracy, F1-score, Confusion Matrix, ROC Curve)\nMitigating overfitting with Data augmentation\nAdvanced Tensorflow concepts like Custom Losses and Metrics, Eager and Graph Modes and Custom Training Loops, Tensorboard\nMachine Learning Operations (MLOps) with Weights and Biases (Experiment Tracking, Hyperparameter Tuning, Dataset Versioning, Model Versioning)\nBinary Classification with Malaria detection\nMulti-class Classification with Human Emotions Detection\nTransfer learning with modern Convnets (Vggnet, Resnet, Mobilenet, Efficientnet) and Vision Transformers (VITs)\nObject Detection with YOLO (You Only Look Once)\nImage Segmentation with UNet\nPeople Counting with Csrnet\nModel Deployment (Distillation, Onnx format, Quantization, Fastapi, Heroku Cloud)\nDigit generation with Variational Autoencoders\nFace generation with Generative Adversarial Neural Networks\nText Preprocessing for Natural Language Processing.\nDeep Learning algorithms like Recurrent Neural Networks, Attention Models, Transformers, and Convolutional neural networks.\nSentiment analysis with RNNs, Transformers, and Huggingface Transformers (Deberta)\nTransfer learning with Word2vec and modern Transformers (GPT, Bert, ULmfit, Deberta, T5...)\nMachine translation with RNNs, attention, transformers, and Huggingface Transformers (T5)\nModel Deployment (Onnx format, Quantization, Fastapi, Heroku Cloud)\nIntent Classification with Deberta in Huggingface transformers\nNamed Entity Relation with Roberta in Huggingface transformers\nNeural Machine Translation with T5 in Huggingface transformers\nExtractive Question Answering with Longformer in Huggingface transformers\nE-commerce search engine with Sentence transformers\nLyrics Generator with GPT2 in Huggingface transformers\nGrammatical Error Correction with T5 in Huggingface transformers\nElon Musk Bot with BlenderBot in Huggingface transformers\nSpeech recognition with RNNs\nIf you are willing to move a step further in your career, this course is destined for you and we are super excited to help achieve your goals!\nThis course is offered to you by Neuralearn. And just like every other course by Neuralearn, we lay much emphasis on feedback. Your reviews and questions in the forum will help us better this course. Feel free to ask as many questions as possible on the forum. We do our very best to reply in the shortest possible time.\n\n\nEnjoy!!!",
      "target_audience": [
        "Beginner Python Developers curious about Applying Deep Learning for Computer vision and Natural Language Processing",
        "Deep Learning for Computer vision Practitioners who want gain a mastery of how things work under the hood",
        "Anyone who wants to master deep learning fundamentals and also practice deep learning for computer vision using best practices in TensorFlow.",
        "Computer Vision practitioners who want to learn how state of art computer vision models are built and trained using deep learning.",
        "Natural Language Processing practitioners who want to learn how state of art NLP models are built and trained using deep learning.",
        "Anyone wanting to deploy ML Models",
        "Learners who want a practical approach to Deep learning for Computer vision, Natural Language Processing and Sound recognition"
      ]
    },
    {
      "title": "The Data Science MicroDegree: Data Analysis & Visualization",
      "url": "https://www.udemy.com/course/datasciencemicrodegree/",
      "bio": "We start from absolute Python scratch and gradually progress into NumPy, Pandas, Matplotlib & Seaborn for data analysis",
      "objectives": [
        "Learn Intermediate Python Programming Skills",
        "Using the Jupyter Notebook Environment",
        "Using the NumPy Library To Create & Manipulate Arrays",
        "Using The Pandas Module To Create & Structure Data",
        "Create Data Visualizations Using Matplotlib & Seaborn Modules With Python",
        "Learn To Work With Various Data Formats Within Python, Including: JSON,HTML, & MS Excel Worksheets."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "What Will You Learn?"
        ],
        "Environment Setup": [
          "Setting Up Your PC",
          "Anaconda Installation",
          "Launching Jupyter Notebook",
          "Navigating Jupyter NoteBook",
          "Markdown Cells"
        ],
        "Basics Of Python (Refresher Course)": [
          "Data Types & Arithmetic Operations",
          "What Should You Do With The Attached Resource?",
          "Variables",
          "Strings & Print Function",
          "String Splicing",
          "Lists",
          "Dictionaries",
          "Tuples & Sets",
          "Relational & Logical Operators",
          "If Else",
          "For Loops",
          "While Loops",
          "In-Built Functions",
          "Creating A Function",
          "Test Yourself",
          "Feeling Stuck?"
        ],
        "NumPy - Data Analysis": [
          "Introduction To NumPy",
          "Do you know this?",
          "NumPy Arrays",
          "Generating NumPy Arrays",
          "NumPy Linspace",
          "Identity Matrix",
          "Generating Arrays With Random Values",
          "Reshape, Min and Max",
          "Shape and Dtype",
          "NumPy Indexing",
          "Index Broadcasting I",
          "Index Broadcasting II",
          "2D Indexing",
          "Extracting Submatrices",
          "Conditional Indexing",
          "NumPy Operations",
          "Universal Functions",
          "Reference - Universal Functions"
        ],
        "Pandas - Data Analysis": [
          "Pandas Series I",
          "Pandas Series II",
          "Pandas Dataframes",
          "Dataframes - Adding & Dropping columns",
          "Loc and iLoc",
          "Conditional Selection",
          "Multiple Conditions",
          "Reset Index & Set Index",
          "dropna & fillna",
          "Group By",
          "Join, Merge & Concatenate",
          "Pandas Operations",
          "File Processing"
        ],
        "MatPlotLib - Data Visualization": [
          "Introduction To Matplotlib",
          "Plotting A Simple Graph",
          "Multiple Plots Inside Same Canvas",
          "Object Oriented Plots",
          "Subplots Using OOP",
          "Modifying Figure Size & DPI",
          "Saving The Plot",
          "Creating A Legend",
          "Customization",
          "Plot Range"
        ],
        "Seaborn - Data Visualization": [
          "Introduction To Seaborn",
          "Distribution Plots - Part 1",
          "Distribution Plots - Part 2",
          "Categorical Plots - Part 1",
          "Categorical Plots - Part 2",
          "Matrix Plots",
          "Grids",
          "Size & Color"
        ]
      },
      "requirements": [
        "Desire & Interest In Data Science",
        "Basic Mathematics",
        "Basic Python Or Programming Knowledge Would Help",
        "A Computer With An Operating System (Windows, Mac or Linux)"
      ],
      "description": "There are lots of Python courses and lectures out there. However, Python has a very steep learning curve and students often get overwhelmed. This course is different! This course is truly step-by-step. In every new tutorial, we build on what had already learned and move one extra step forward. After every video, you learn a new valuable concept that you can apply right away. And the best part is that you learn through live examples.\nThis comprehensive course will be your guide to learning how to use the power of Python to analyze data and create beautiful visualizations. This course is designed for both beginners with some programming experience or experienced developers looking to make the jump to Data Science!\n\"Data Scientist\" has been ranked the Number #1 Job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems!\nIn summary, this course has been designed for all skill levels and even if you have no programming or statistical background you will still be successful in this course! I can't wait to see you in class.\n\n\nIn This Course You'll Learn:\nProgramming with Python\nNumPy with Python\nUsing pandas Data Frames to solve complex tasks\nUse pandas to handle Excel Files\nUse matplotlib and seaborn for data visualizations",
      "target_audience": [
        "Students With A Keen Interest In Data Science",
        "Job Seekers Who Want To Leverage Their Data Skills",
        "Python & Data Science Beginners Who Don't Know Where To Start"
      ]
    },
    {
      "title": "Machine Learning A-Z: Become Kaggle Master",
      "url": "https://www.udemy.com/course/machine-learning-become-kaggle-master/",
      "bio": "Master Machine Learning Algorithms Using Python From Beginner to Super Advance Level including Mathematical Insights.",
      "objectives": [
        "Master Machine Learning on Python",
        "Learn to use MatplotLib for Python Plotting",
        "Learn to use Numpy and Pandas for Data Analysis",
        "Learn to use Seaborn for Statistical Plots",
        "Learn All the Mathmatics Required to understand Machine Learning Algorithms",
        "Implement Machine Learning Algorithms along with Mathematic intutions",
        "Projects of Kaggle Level are included with Complete Solutions",
        "Learning End to End Data Science Solutions",
        "All Advanced Level Machine Learning Algorithms and Techniques like Regularisations , Boosting , Bagging and many more included",
        "Learn All Statistical concepts To Make You Ninza in Machine Learning",
        "Real World Case Studies",
        "Model Performance Metrics",
        "Deep Learning",
        "Model Selection"
      ],
      "course_content": {},
      "requirements": [
        "Any Beginner Can Start this Course",
        "2+2 knowledge is more than sufficient as we have covered almost everything from scratch."
      ],
      "description": "Want to become a good Data Scientist?  Then this is a right course for you.\nThis course has been designed by IIT professionals who have mastered in Mathematics and Data Science.  We will be covering complex theory, algorithms and coding libraries in a very simple way which can be easily grasped by any beginner as well.\nWe will walk you step-by-step into the World of Machine Learning. With every tutorial you will develop new skills and improve your understanding of this challenging yet lucrative sub-field of Data Science from beginner to advance level.\nWe have solved few Kaggle problems during this course and provided complete solutions so that students can easily compete in real world competition websites.\nWe have covered following topics in detail in this course:\n1. Python Fundamentals\n2. Numpy\n3. Pandas\n4. Some Fun with Maths\n5. Inferential Statistics\n6. Hypothesis Testing\n7. Data Visualisation\n8. EDA\n9. Simple Linear Regression\n10. Multiple Linear regression\n11. Hotstar/ Netflix: Case Study\n12. Gradient Descent\n13. KNN\n14. Model Performance Metrics\n15. Model Selection\n16. Naive Bayes\n17. Logistic Regression\n18. SVM\n19. Decision Tree\n20. Ensembles - Bagging / Boosting\n21. Unsupervised Learning\n22. Dimension Reduction\n23. Advance ML Algorithms\n24. Deep Learning",
      "target_audience": [
        "This course is meant for anyone who wants to become a Data Scientist"
      ]
    },
    {
      "title": "Statistics with R - Intermediate Level",
      "url": "https://www.udemy.com/course/statistics-with-r-intermediate-level/",
      "bio": "Statistical analyses using the R program",
      "objectives": [
        "run parametric and non-parametric correlation (Pearson, Spearman, Kendall)",
        "perform partial correlation",
        "run the chi-square test for association",
        "run the independent sample t test",
        "run the paired sample t test",
        "execute the one-way analysis of variance",
        "perform the two-way and three-way analysis of variance",
        "run the one-way multivariate analysis of variance",
        "run non-parametric tests for mean difference (Mann-Whitney, Kruskal-Wallis, Wilcoxon)",
        "execute the multiple linear regression",
        "compute the Cronbach's alpha",
        "compute other reliability indicators (Cohen's kappa, Kendall's W)"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Test of Association": [
          "Pearson Correlation",
          "Spearman and Kendall Correlation",
          "Partial Correlation",
          "Chi-Square Test For Independence",
          "R Codes File for the First Chapter",
          "Practical Exercises for the First Chapter"
        ],
        "Mean Difference Tests": [
          "Independent-Sample T Test",
          "Paired-Sample T Test",
          "Oneway ANOVA",
          "Twoway ANOVA - Basics",
          "Twoway ANOVA - Simple Main Effects",
          "Threeway ANOVA - Basics",
          "Threeway ANOVA - Simple Second Order Interaction Effects",
          "Threeway ANOVA - Simple Main Effects",
          "Oneway MANOVA",
          "Mann-Whitney Test",
          "Wilcoxon Test",
          "Kruskal-Wallis Test",
          "R Codes File for the Second Chapter",
          "Practical Exercises for the Second Chapter"
        ],
        "Predictive Techniques": [
          "Multiple Linear Regression - Basics",
          "Multiple Linear Regression - Testing Assumptions",
          "Multiple Regression with Dummy Variables",
          "Sequential Regression",
          "R Codes File for the Third Chapter",
          "Practical Exercises for the Third Chapter"
        ],
        "Reliabilty Analysis": [
          "Cronbach's Alpha",
          "Cohen's Kappa",
          "Kendall's W",
          "R Codes File for the Fourth Chapter",
          "Practical Exercises for the Fourth Chapter"
        ],
        "Course Materials": [
          "Download Links"
        ]
      },
      "requirements": [
        "R and R studio",
        "knowledge of statistics"
      ],
      "description": "If you want to learn how to perform the most useful statistical analyses in the R program, you have come to the right place.\nNow you don’t have to scour the web endlessly in order to find how to do a Pearson or Spearman correlation, an independent t test or a factorial ANOVA, how to perform a sequential regression analysis or how to compute the Cronbach’s alpha. Everything is here, in this course, explained visually, step by step.\nSo, what will you learn in this course?\nFirst of all, you will learn how to perform association tests in R, both parametric and non-parametric: the Pearson correlation, the Spearman and Kendall correlation, the partial correlation and the chi-square test for independence.\nThe test of mean differences represent a vast part of this course, because of their great importance. We will approach the t tests, the analysis of variance (both univariate and multivariate) and a few non-parametric tests. For each technique we will present the preliminary assumption, run the procedure and carefully interpret all the results.\nNext you will learn how to perform a multiple linear regression analysis. We have assign several big lectures to this topic, because we will also learn how to check the regression assumptions and how to run a sequential (or hierarchical) regression in R.\nFinally, we will enter the territory of statistical reliability – you will learn how to compute three important reliability indicators in R.\nSo after graduating this course, you will get some priceless statistical analysis knowledge and skills using the R program. Don’t wait, enroll today and get ready for an exciting journey!",
      "target_audience": [
        "students",
        "PhD candidates",
        "academic researchers",
        "business researchers",
        "University teachers",
        "anyone looking for a job in the statistical analysis field",
        "anyone who is passionate about quantitative analysis"
      ]
    },
    {
      "title": "Applied Text Mining and Sentiment Analysis with Python",
      "url": "https://www.udemy.com/course/applied-text-mining-and-sentiment-analysis-with-python/",
      "bio": "Perform Sentiment Analysis on Twitter data by combining Text Mining and NLP techniques, NLTK and Scikit-Learn",
      "objectives": [
        "How to use common Text Mining and NLP techniques",
        "How to use Regex to clean up Tweets",
        "How to use NLTK to pre-process text",
        "How to use Scikit-Learn to build a Sentiment Analysis prediction model",
        "How to predict the sentiment of any tweet"
      ],
      "course_content": {
        "Course Preview": [
          "Preview"
        ],
        "Introduction to Text Mining": [
          "Section Overview",
          "What is Text?",
          "What is Text Mining?",
          "Text Mining and NLP",
          "Sentiment Analysis",
          "Roadmap",
          "(Python Practice) Google Colab",
          "(Python Practice) Dataset Connection",
          "(Python Practice) Dataset Overview",
          "(Python Practice) Dataset Visualization"
        ],
        "Text Normalization": [
          "Section Overview",
          "What is Text Normalization?",
          "Text Cleaning (1/2) - Twitter Features",
          "(Python Practice) Cleaning Twitter Features",
          "Text Cleaning (2/2) - General Features",
          "(Python Practice) Cleaning General Features",
          "Tokenization",
          "(Python Practice) Applied Tokenization (1/3)",
          "(Python Practice) Applied Tokenization (2/3)",
          "(Python Practice) Applied Tokenization (3/3)",
          "Stemming",
          "(Python Practice) Applied Stemming",
          "Lemmatization",
          "(Python Practice) Applied Lemmatization",
          "(Python Pratice) Tweet Pre-Processing"
        ],
        "Text Vectorization": [
          "Section Overview",
          "Why Representing Text?",
          "(Python Practice) Dataset Preprocessing",
          "Positive/Negative Word Frequencies",
          "(Python Practice) Applied Positive/Negative Frequencies",
          "Bag-of-Words",
          "(Python Practice) Applied Bag-of-Words",
          "TF-IDF",
          "(Python Practice) Applied TF-IDF"
        ],
        "Sentiment Analysis": [
          "Section Overview",
          "Why a model?",
          "Logistic Regression",
          "ML Model Training",
          "(Python Practice) Train/Test split",
          "(Python Practice) ML Model Fitting",
          "Model Performance Measures",
          "(Python Practice) Applied Performance Measures",
          "(Python Practice) Prediction Pipeline"
        ],
        "BONUS SECTION: final word & coupons": [
          "Coupon codes"
        ]
      },
      "requirements": [
        "A basic Python IDE (Spyder, Pycharm, etc.) or a web-based Python IDE (Jupyter Notebook, Google Colab, etc.). Google Colab will be used by default to teach this course.",
        "General knowledge of Python, as this is a course about learning Sentiment Analysis and Text Mining, not properly about learning Python."
      ],
      "description": "\"Bitcoin (BTC) price just reached a new ALL TIME HIGH! #cryptocurrency #bitcoin #bullish\"\nFor you and me, it seems pretty obvious that this is good news about Bitcoin, isn't it? But is it that easy for a machine to understand it? ... Probably not ... Well, this is exactly what this course is about: learning how to build a Machine Learning model capable of reading and classifying all this news for us!\nSince 2006, Twitter has been a continuously growing source of information, keeping us informed about all and nothing. It is estimated that more than 6,000 tweets are exchanged on the platform every second, making it an inexhaustible mine of information that it would be a shame not to use.\nFortunately, there are different ways to process tweets in an automated way, and retrieve precise information in an instant ... Interested in learning such a solution in a quick and easy way? Take a look below ...\n_____________________________________________________\nWhat will you learn in this course?\nBy taking this course, you will learn all the steps necessary to build your own Tweet Sentiment prediction model. That said, you will learn much more as the course is separated into 4 different parts, linked together, but providing its share of knowledge in a particular field (Text Mining, NLP and Machine Learning).\nSECTION 1: Introduction to Text Mining\nIn this first section, we will go through several general elements setting up the starting problem and the different challenges to overcome with text data. This is also the section in which we will discover our Twitter dataset, using libraries such as Pandas or Matplotlib.\nSECTION 2: Text Normalization\nTwitter data are known to be very messy. This section will aim to clean up all our tweets in depth, using Text Mining techniques and some suitable libraries like NLTK. Tokenization, stemming or lemmatization will have no secret for you once you are done with this section.\nSECTION 3: Text Representation\nBefore our cleansed data can be fed to our model, we will need to learn how to represent it the right way. This section will aim to cover different methods specific to this purpose and often used in NLP (Bag-of-Words, TF-IDF, etc.). This will give us an additional opportunity to use NLTK.\nSECTION 4: ML Modelling\nFinally ... the most exciting step of all! This section will be about putting together all that we have learned, in order to build our Sentiment prediction model. Above all, it will be about having an opportunity to use one of the most used libraries in Machine Learning: Scikit-Learn (SKLEARN).\n_____________________________________________________\nWhy is this course different from the others I can find on the same subject?\nOne of the key differentiators of this course is that it's not about learning Text Mining, NLP or Machine Learning in general. The objective is to pursue a very precise goal (Sentiment Analysis) and deepen all the necessary steps in order to reach this goal, by using the appropriate tools.\nSo no, you might not yet be an unbeatable expert in Artificial Intelligence at the end of this course, sorry ... but you will know exactly how, and why, your Sentiment application works so well.\n_____________________________________________________\nAbout AIOutsider\nAIOutsider was created in 2020 with the ambition of facilitating the learning of Artificial Intelligence. Too often, the field has been seen as very opaque or requiring advanced knowledge in order to be used. At AIOutsider, we want to show that this is not the case. And while there are more difficult topics to cover, there are also topics that everyone can reach, just like the one presented in this course. If you want more, don't hesitate to visit our website!\n_____________________________________________________\nSo, if you are interested in learning AI and how it can be used in real life to solve practical issues like Sentiment Analysis, there is only one thing left for you to do ... learn with us and join this course!",
      "target_audience": [
        "Anyone having an interest in Artificial Intelligence and NLP",
        "Anyone willing to learn what is Text Mining and how it can be used",
        "Anyone willing to learn how to easily predict the sentiment of any tweet"
      ]
    },
    {
      "title": "Deep Learning Prerequisites: Linear Regression in Python",
      "url": "https://www.udemy.com/course/data-science-linear-regression-in-python/",
      "bio": "Data science, machine learning, and artificial intelligence in Python for students and professionals",
      "objectives": [
        "Derive and solve a linear regression model, and apply it appropriately to data science problems",
        "Program your own version of a linear regression model in Python",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion",
        "Understand regularization for machine learning and deep learning",
        "Understand closed-form solutions vs. numerical methods like gradient descent",
        "Apply linear regression to a wide variety of real-world problems"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "How to Succeed in this Course",
          "Statistics vs. Machine Learning"
        ],
        "1-D Linear Regression: Theory and Code": [
          "What is machine learning? How does linear regression play a role?",
          "Define the model in 1-D, derive the solution (Updated Version)",
          "Define the model in 1-D, derive the solution",
          "Coding the 1-D solution in Python",
          "Exercise: Theory vs. Code",
          "Determine how good the model is - r-squared",
          "R-squared in code",
          "Introduction to Moore's Law Problem",
          "Demonstrating Moore's Law in Code",
          "Moore's Law Derivation",
          "R-squared Quiz 1",
          "Suggestion Box"
        ],
        "Multiple linear regression and polynomial regression": [
          "Define the multi-dimensional problem and derive the solution (Updated Version)",
          "Define the multi-dimensional problem and derive the solution",
          "How to solve multiple linear regression using only matrices",
          "Coding the multi-dimensional solution in Python",
          "Polynomial regression - extending linear regression (with Python code)",
          "Predicting Systolic Blood Pressure from Age and Weight",
          "R-squared Quiz 2"
        ],
        "Practical machine learning issues": [
          "What do all these letters mean?",
          "Interpreting the Weights",
          "Generalization error, train and test sets",
          "Generalization and Overfitting Demonstration in Code",
          "Categorical inputs",
          "One-Hot Encoding Quiz",
          "Probabilistic Interpretation of Squared Error",
          "L2 Regularization - Theory",
          "L2 Regularization - Code",
          "The Dummy Variable Trap",
          "Gradient Descent Tutorial",
          "Gradient Descent for Linear Regression",
          "Bypass the Dummy Variable Trap with Gradient Descent",
          "L1 Regularization - Theory",
          "L1 Regularization - Code",
          "L1 vs L2 Regularization",
          "Why Divide by Square Root of D?"
        ],
        "Conclusion and Next Steps": [
          "Brief overview of advanced linear regression and machine learning topics",
          "Exercises, practice, and how to get good at this"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (FAQ by Student Request)": [
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Python 2 vs Python 3"
        ],
        "Effective Learning Strategies for Machine Learning (FAQ by Student Request)": [
          "How to Succeed in this Course (Long Version)",
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "Machine Learning and AI Prerequisite Roadmap (pt 1)",
          "Machine Learning and AI Prerequisite Roadmap (pt 2)"
        ],
        "Appendix / FAQ Finale": [
          "BONUS"
        ]
      },
      "requirements": [
        "How to take a derivative using calculus",
        "Basic Python programming",
        "For the advanced section of the course, you will need to know probability"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis course teaches you about one popular technique used in machine learning, data science and statistics: linear regression. We cover the theory from the ground up: derivation of the solution, and applications to real-world problems. We show you how one might code their own linear regression module in Python.\nLinear regression is the simplest machine learning model you can learn, yet there is so much depth that you'll be returning to it for years to come. That's why it's a great introductory course if you're interested in taking your first steps in the fields of:\ndeep learning\nmachine learning\ndata science\nstatistics\nIn the first section, I will show you how to use 1-D linear regression to prove that Moore's Law is true.\nWhat's that you say? Moore's Law is not linear?\nYou are correct! I will show you how linear regression can still be applied.\nIn the next section, we will extend 1-D linear regression to any-dimensional linear regression - in other words, how to create a machine learning model that can learn from multiple inputs.\nWe will apply multi-dimensional linear regression to predicting a patient's systolic blood pressure given their age and weight.\nFinally, we will discuss some practical machine learning issues that you want to be mindful of when you perform data analysis, such as generalization, overfitting, train-test splits, and so on.\nThis course does not require any external materials. Everything needed (Python, and some Python libraries) can be obtained for FREE.\nIf you are a programmer and you want to enhance your coding abilities by learning about data science, then this course is for you. If you have a technical or mathematical background, and you want to know how to apply your skills as a software engineer or \"hacker\", this course may be useful.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus (taking derivatives)\nmatrix arithmetic\nprobability\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "People who are interested in data science, machine learning, statistics and artificial intelligence",
        "People new to data science who would like an easy introduction to the topic",
        "People who wish to advance their career by getting into one of technology's trending fields, data science",
        "Self-taught programmers who want to improve their computer science theoretical skills",
        "Analytics experts who want to learn the theoretical basis behind one of statistics' most-used algorithms"
      ]
    },
    {
      "title": "Elasticsearch Masterclass: ES|QL, Vector, and More",
      "url": "https://www.udemy.com/course/elasticsearch-masterclass/",
      "bio": "Master Elasticsearch 7 to 9, Logstash, Kibana, ES|QL, Vector Search & NLP – the Complete Guide to the Elastic Stack",
      "objectives": [
        "Understand the core architecture of Elasticsearch and how it works under the hood",
        "Master indexing, searching, updating, and deleting documents",
        "Write complex and performant queries using Elasticsearch Query DSL and ES|QL",
        "Perform semantic search using NLP models and vector similarity (ANN search)",
        "Ingest and process data using Logstash from multiple sources",
        "Visualize and analyze real-time data using Kibana dashboards",
        "Apply full-text search, filters, aggregations, and scoring in real-world scenarios",
        "Understand how Lucene powers Elasticsearch and what’s new in Lucene 10",
        "Build scalable, production-grade search and analytics applications",
        "Keep up with Elasticsearch 7, 8, and 9 version-specific features and best practices"
      ],
      "course_content": {
        "Introduction": [
          "Getting Started",
          "Environment Setup",
          "Security by Default in Elasticsearch 8+: TLS, Authentication, & User Management",
          "Lucene 10 and Performance Improvements in Elasticsearch 9"
        ],
        "Dive into the functionality": [
          "Overview of Elasticsearch",
          "Interacting with data in Elasticsearch",
          "Index the document Only if it does not already exist",
          "Index vs Index vs Index",
          "Search Basics"
        ],
        "Inside Cluster": [
          "Life Inside Cluster-node-index-shards",
          "Components of an Index",
          "Analysis ( Prepare the data for index)"
        ],
        "Search in Depth": [
          "Prerequisite",
          "Search-in-depth (Part - 1)",
          "Search-in-depth (Part - 2)",
          "Searching with Relevancy",
          "Native Vector Search in Elasticsearch: High-Dimensional Similarity with ANN",
          "Semantic Search and NLP in Elasticsearch",
          "ES|QL – The New Query Language in Elasticsearch: Pipelined, Relational, and Read"
        ],
        "Aggregation - Having fun with statistics": [
          "Aggregation in Depth"
        ],
        "Processing the events in Logstash": [
          "Logstash - Event Processing"
        ]
      },
      "requirements": [
        "Basic understanding of JSON and REST APIs is helpful (but not mandatory)",
        "No prior experience with Elasticsearch is required — the course starts from scratch",
        "Familiarity with terminal/command line and basic programming concepts is a plus",
        "A computer with internet access to install Elasticsearch locally or use Elastic Cloud",
        "Curiosity to explore how search engines work and willingness to learn hands-on"
      ],
      "description": "Learn Elasticsearch the right way — from fundamentals to the latest advancements in version 9.x. This masterclass gives you everything you need to build scalable search and analytics solutions using the full Elastic Stack (Elasticsearch, Logstash, and Kibana), while also covering cutting-edge features like ES|QL, vector search, semantic search with NLP, and Lucene 10 optimizations.\n\n\nWhether you're indexing logs, powering search for an application, or working with large-scale analytics pipelines, this course is designed to give you both deep technical understanding and hands-on experience.\nThis course is fully updated and future-proof — ideal for developers, DevOps engineers, analysts, and anyone looking to master Elasticsearch from version 7 to 9 and beyond.\n\n\nHere's what makes this course different:\nCovers Elasticsearch 7, 8, and 9 — with clear distinctions between versions\nIncludes real-world examples and guided walkthroughs (no boring slides)\nIntroduces ES|QL — the powerful new piped query language in Elasticsearch 8.15+\nTeaches vector search and semantic ranking using NLP models like BERT\nExplains the impact of Lucene 10 under the hood for faster search and indexing\nUses Kibana and Logstash for visualization and data ingestion\nWhether you're building an intelligent search engine, an observability pipeline, or simply want to gain mastery over structured and unstructured data search, this course is built for you.\nUpdated regularly to keep pace with Elasticsearch releases, this is the most complete and up-to-date resource you’ll find online.\n\n\nEnroll today and start building powerful search solutions with confidence.",
      "target_audience": [
        "Developers and backend engineers who want to integrate Elasticsearch into applications",
        "DevOps engineers managing Elasticsearch clusters and pipelines",
        "Data analysts and architects working on real-time search and analytics projects",
        "Professionals exploring log analytics, monitoring, or observability (ELK Stack)",
        "Anyone preparing for Elasticsearch-related job interviews or certifications",
        "Learners who want to go beyond keyword search and build intelligent, AI-driven systems using NLP and semantic relevance"
      ]
    },
    {
      "title": "Optimization with Python: Solve Operations Research Problems",
      "url": "https://www.udemy.com/course/optimization-with-python-linear-nonlinear-and-cplex-gurobi/",
      "bio": "Solve optimization problems with CPLEX, Gurobi, Pyomo... using linear programming, nonlinear, evolutionary algorithms...",
      "objectives": [
        "Solve optimization problems using linear programming, mixed-integer linear programming, nonlinear programming, mixed-integer nonlinear programming,",
        "LP, MILP, NLP, MINLP, SCOP, NonCovex Problems",
        "Main solvers and frameworks, including CPLEX, Gurobi, and Pyomo",
        "Genetic algorithm, particle swarm, and constraint programming",
        "From the basic to advanced tools, learn how to install Python and how to use the main packages (Numpy, Pandas, Matplotlib...)",
        "How to solve problems with arrays and summations"
      ],
      "course_content": {
        "Introduction to the course": [
          "Introduction",
          "What is optimization"
        ],
        "Installing Python": [
          "Installing Python",
          "Packages",
          "Important note about Python",
          "IDE Spyder",
          "Jupyter Notebook\\Lab",
          "Exercises"
        ],
        "Starting with Python": [
          "Lists, Tuples, and Dictionary",
          "If, For, While",
          "Functions",
          "Numpy",
          "Pandas",
          "Pandas: reading Excel",
          "Graphs",
          "Exercises",
          "PDFs to learn more about Python"
        ],
        "Introduction to mathematical modelling": [
          "What is Mathematical Modelling?",
          "How do we solve optimization problems?",
          "Type of Variables",
          "Objective Function and Constraints",
          "How to model your problem?",
          "Our first formulation",
          "Example 1: investiment",
          "Example 2: investiment",
          "Example 3: production cost",
          "Example 4: route problem",
          "Example 5: construction assignment",
          "Example 6: construction assignment",
          "Example 7: job assignment",
          "Example 8: job assignment",
          "How to Learn More?",
          "Some references for you learn more (problems of VRPTW, TSP, JobShop...)"
        ],
        "Linear Programming (LP)": [
          "LP: Introduction",
          "Framework and Solvers",
          "LP: Ortools",
          "LP: SCIP",
          "LP: SCIP | errors during installation",
          "LP: Gurobi, CPLEX, and GLPK (installation)",
          "Academic License for Gurobi [Updates]",
          "LP: Pyomo (using Gurobi, CPLEX, and GLPK)",
          "LP: Pyomo | overcoming errors",
          "LP: PuLP",
          "Which solver and frameworks should we choose?",
          "LP: Exercise, solve it by yourself",
          "LP: Concepts"
        ],
        "Working with Pyomo": [
          "Pyomo: Using other solvers (CBC)",
          "Pyomo: Summations",
          "Pyomo: Double Summations and Variables with 2 or more indexes",
          "Pyomo: Pprint",
          "Pyomo: Manual"
        ],
        "Mixed-Integer Linear Programming (MILP)": [
          "MILP: Introduction",
          "MILP: Pyomo",
          "MILP: Ortools",
          "MILP: SCIP",
          "MILP: Exercise, solve it by yourself",
          "MILP: Exercise solution",
          "MILP: Concepts"
        ],
        "Nonlinear Programming (NLP)": [
          "NLP: Introduction",
          "NLP: Pyomo (IPOPT)",
          "NLP: SCIP",
          "NLP: Exercise, solve it by yourself",
          "NLP: Exercise Solution",
          "NLP: Concepts"
        ],
        "Mixed-Integer Nonlinear Programming (MINLP)": [
          "MINLP: Introduction",
          "MINLP: Pyomo (Couenne)",
          "MINLP: Pyomo (decomposition using mindtpy)",
          "MINLP: SCIP"
        ],
        "Genetic Algorithm and Particle Swarm": [
          "Genetic Algorithm: Introduction",
          "Genetic Algorithm: Base Case Example",
          "Genetic Algorithm: Routing Problem",
          "Multi-Objective Problems using NSGA-II - An introduction",
          "Particle Swarm (PSO): Base Case Example",
          "PSO: Concepts"
        ]
      },
      "requirements": [
        "Some knowledge in programming logic",
        "Why and where to use optimization",
        "It is NOT necessary to know Python"
      ],
      "description": "Operational planning and long term planning for companies are more complex in recent years. Information changes fast, and the decision making is a hard task. Therefore, optimization algorithms (operations research) are used to find optimal solutions for these problems. Professionals in this field are one of the most valued in the market.\nIn this course you will learn what is necessary to solve problems applying Mathematical Optimization and Metaheuristics:\nLinear Programming (LP)\nMixed-Integer Linear Programming (MILP)\nNonLinear Programming (NLP)\nMixed-Integer Linear Programming (MINLP)\nGenetic Algorithm (GA)\nMulti-Objective Optimization Problems with NSGA-II (an introduction)\nParticle Swarm (PSO)\nConstraint Programming (CP)\nSecond-Order Cone Programming (SCOP)\nNonConvex Quadratic Programming (QP)\n\n\nThe following solvers and frameworks will be explored:\nSolvers: CPLEX – Gurobi – GLPK – CBC – IPOPT – Couenne – SCIP\nFrameworks: Pyomo – Or-Tools – PuLP – Pymoo\nSame Packages and tools: Geneticalgorithm – Pyswarm – Numpy – Pandas – MatplotLib – Spyder – Jupyter Notebook\n\n\nMoreover, you will learn how to apply some linearization techniques when using binary variables.\n\n\nIn addition to the classes and exercises, the following problems will be solved step by step:\nOptimization on how to install a fence in a garden\nRoute optimization problem\nMaximize the revenue in a rental car store\nOptimal Power Flow: Electrical Systems\nMany other examples, some simple, some complexes, including summations and many constraints.\n\n\nThe classes use examples that are created step by step, so we will create the algorithms together.\nBesides this course is more focused in mathematical approaches, you will also learn how to solve problems using artificial intelligence (AI), genetic algorithm, and particle swarm.\nDon't worry if you do not know Python or how to code, I will teach you everything you need to start with optimization, from the installation of Python and its basics, to complex optimization problems. Also, I have created a nice introduction on mathematical modeling, so you can start solving your problems.\nI hope this course can help you in your career. Yet, you will receive a certification from Udemy.\n\n\nOperations Research | Operational Research | Mathematical Optimization\n\n\nSee you in the classes!!",
      "target_audience": [
        "Undergrad, graduation, master program, and doctorate students.",
        "Companies that wish to solve complex problems",
        "People interested in complex problems and artificial inteligence"
      ]
    },
    {
      "title": "PowerBI Zero to Hero",
      "url": "https://www.udemy.com/course/powerbi-hero/",
      "bio": "A practical guide to building Dashboards with Power BI",
      "objectives": [
        "Power bi",
        "Business Intelligence",
        "Data Visualisation",
        "Dashboarding",
        "Powerbi",
        "DAX",
        "Power Query",
        "Data Modeling",
        "BI",
        "Data vizualisation"
      ],
      "course_content": {
        "Chapter 0 (Introduction)": [
          "Welcome, Hero !",
          "What is Power BI ?",
          "More reasons to use Power BI ESPECIALLY if you use excel",
          "Resources & Course Structure",
          "Quick tour & Interface overview"
        ],
        "Chapter 01 - Extract, Load and Transform Data in Power Query": [
          "Power Query, Connecting to a Database and Data Types",
          "Filtering",
          "Removing & Ordering Columns",
          "Conditional Columns",
          "Connecting to a folder as a Data Source",
          "Combining Data",
          "Dealing with less structured data",
          "Fixing Errors",
          "Exercises Scenario & Resources",
          "Exercise 01 - Import Data from Access Data Base",
          "Exercise 02 - Import Data from a folder containing CSV files",
          "Exercise 03 - Import a less structured Data from an Excel file",
          "Exercises Solutions"
        ],
        "Chapter 02 - Data Modeling": [
          "Data Modeling ?",
          "Tables and Relationships",
          "BEST PRACTICE : Dimensional Modeling & Star Schemas",
          "BEST PRACTICE : Optimising The model for Development Time",
          "Exercise",
          "Exercise Solution"
        ],
        "Chapter 03 - DAX for Data Analysis eXpressions": [
          "DAX, the “POWER” in POWER BI",
          "Using Measures to create Calculations and KPIs",
          "Comparing this year’s Value to last year’s value",
          "Year over Year Varience, and Waterfall Charts",
          "Year to Date with DAX",
          "Exercises Scenario & Resources",
          "Exercise 1 - Last year comparison",
          "Exercise 2 - Year to Date",
          "Exercise 3 - Market Share",
          "Exercise 4 - Best Practices : Optimize the Data Model",
          "Exercises Solution"
        ],
        "Chapter 04 - Data Vizualisation": [
          "Why Data Visualisation Really Matters",
          "Pie Chart and Treemap",
          "Hierarchies",
          "Filtering and TopN",
          "Dual-Axis (Combo) Chart",
          "Advances Visual Analytics Pane - Trends, Targets, Forecasts and more",
          "Artificial Intelligence: Decomposition Tree, Q&A and Key Influencers",
          "Slicers",
          "MAPS and Dynamic Coloring !",
          "Fancy Tables",
          "Custom Formatting & Design tips",
          "App Like User Experience & Navigation",
          "Exercises Scenario & Resources",
          "Exercice 1 - Cross Tabular Report",
          "Exercise 2- Part-to-Whole Report",
          "Exercise 3- Relationship Report",
          "Exercise 4- Trend Report",
          "Exercise 5- Rank Report",
          "Exercise 6- Customise your report & Navigation",
          "Exercices Solutions"
        ],
        "Chapter 05 - Power BI Service": [
          "Sharing and Collaboration in Power BI",
          "How to Publish a report to the Power BI service",
          "Deep Dive into the Power BI service"
        ],
        "Congratulations Hero!": [
          "Why Power BI is going to be huge"
        ]
      },
      "requirements": [
        "Familiarity with Microsoft Excel or Google Sheets"
      ],
      "description": "Hello & Welcome to this Microsoft Power BI course. If you're trying to quickly master the most suitable tool for connecting, transforming, visualising, and analysing your Data, you are in the right place. We will take you from zero to hero, through our practical learning approach, and will quickly get you up running & building analytical solutions for your organisation.\nMy name is Abdelkarim M. MAHMOUD, and I am a Power BI certified Consultant. I have spent the last years empowering different type of organizations by helping them gain performance and competitivity through their Data. I had the chance and the pleasure to train dozens of teams from simple Power BI users to Chief Data Officers. I am here today to put all my expertise at your disposal in a simple and practical way.\nThis in-depth training strikes the perfect balance between theory and practice, several PowerBI use cases are covered to allow you to get the most value from your data. Here's what we're gonna dive into :\n\n\nIntroduction - What is Power BI ?\nMore reasons to use Power BI ESPECIALLY if you use excel\nResources & Course Structure\nQuick tour & Interface overview\n\n\nChapter 1 - Extract, Load, and Transform Data in Power Query\nPower Query, Connecting to a Database and Data Types\nFiltering\nRemoving & Ordering Columns\nConditional Columns\nConnecting to a folder as a Data Source\nCombining Data\nDealing with less structured data\nFixing Errors\n\n\nChapter 2 - Data Modeling ?\nTables and Relationships\nBEST PRACTICE : Dimensional Modeling & Star Schemas\nBEST PRACTICE : Optimising The model for Development Time\n\n\nChapter 3 - DAX for Data Analysis Expressions\nDAX, the “POWER” in POWER BI\nUsing Measures to create Calculations and KPIs\nComparing this year’s Value to last year’s value\nYear over Year Varience, and Waterfall Charts\nYear to Date with DAX\n\n\nChapter 4 - Data Vizualisation\nWhy Data Visualisation Really Matters\nPie Chart and Treemap\nHierarchies\nFiltering and TopN\nDual-Axis (Combo) Chart\nAdvances Visual Analytics Pane - Trends, Targets, Forecasts and more\nArtificial Intelligence: Decomposition Tree, Q&A and Key Influencers\nSlicers\nMAPS and Dynamic Coloring !\nFancy Tables\n\n\nChapter 5 - Power BI Service\nSharing and Collaboration in Power BI\nHow to Publish a report to the Power BI service\nDeep Dive into the Power BI service",
      "target_audience": [
        "Excel users wishing to switch to Power BI or interested by a more powerful tool",
        "Business and data analysts",
        "CXO",
        "Young managers, future managers and MBA students",
        "people looking to start or to switch to a data career"
      ]
    },
    {
      "title": "Explainable Al (XAI) with Python",
      "url": "https://www.udemy.com/course/xai-with-python/",
      "bio": "Simplified Way to Learn XAI",
      "objectives": [
        "Importance of XAI in modern world",
        "Differentiation of glass box, white box and black box ML models",
        "Categorization of XAI on the basis of their scope, agnosticity, data types and explanation techniques",
        "Trade-off between accuracy and interpretability",
        "Application of InterpretML package from Microsoft to generate explanations of ML models",
        "Need of counterfactual and contrastive explanations",
        "Working principles and mathematical modeling of XAI techniques like LIME, SHAP, DiCE, LRP, counterfactual and contrastive explanationss",
        "Application of XAI techniques like LIME, SHAP, DiCE, LRP to generate explanations for black-box models for tabular, textual, and image datasets.",
        "What-if tool from Google to analyze data points and to generate counterfactuals"
      ],
      "course_content": {
        "Introduction to XAI": [
          "XAI in Action",
          "Need and Importance of XAI",
          "By Design Interpretable Models: Decision Tree: Glass Box Models",
          "By Design Interpretable Models: Logistic Regression: Glass Box Models",
          "Black Box Models: Part-1",
          "Black Box Models: Part-2",
          "XAI Categorization",
          "Basics of XAI"
        ],
        "Demonstration of By Design Interpretable Models: Glass Box": [
          "Demonstration of Glass Box Models: Part-1",
          "Demonstration of Glass Box Models: Part-2",
          "Need for Train-Test Split",
          "Techniques for Balancing the Dataset",
          "Code for Balancing the Dataset",
          "Quality Metrics for Classification: Confusion Matrix, Precision, Recall, F1Score",
          "Demo of Data Exploration for Stroke Dataset",
          "InterpretML Package",
          "Demo for Logistic Regression Model Explanation",
          "Demo for Decision Tree Classifier Explanation",
          "Explainable Boosting Classifier: Working Principle",
          "Demo for Explainable Boosting Classifier Explanaation",
          "Quiz on Demonstration of By Design Interpretable models"
        ],
        "LIME (Local Interpretable Model Agnostic Explanations)": [
          "LIME Working Principle",
          "Mathematical Modelling of LIME: Part-1",
          "Mathematical Modelling of LIME: Part-2",
          "Demo of LIME for tabular Stroke Dataset",
          "LIME Demonstration for textual dataset: Part-1",
          "LIME Demonstration for textual dataset: Part-2",
          "LIME Demonstration for textual dataset: Part-3",
          "Quiz on LIME",
          "Implementing LIME over multiclass textual data",
          "Recommended Practice Tasks"
        ],
        "SHAP (SHapley Additive exPlanations)": [
          "SHAP Working Principle",
          "Mathematical Modelling of SHAP: Part-1",
          "Mathematical Modelling of SHAP: Part-2",
          "Mathematical Modelling of SHAP: Part-3",
          "SHAP Demonstration",
          "Recommended Practice Tasks"
        ],
        "Counterfactual Explanations": [
          "Working Principle of Counterfactual Explanations-1",
          "Working Principle of Counterfactual Explanations",
          "Mathematical Modelling of Counterfactual Explanations",
          "Global Counterfactuals",
          "Demo of Counterfactual Explanations on Stroke Dataset",
          "Quiz on Counterfactual Explanations",
          "Recommended Practice Tasks"
        ],
        "Google's What-if Tool (WIT) for AI fairness and Counterfactuals": [
          "Case Study-1: Demo of What-if Tool (WIT)",
          "Case Study-2: Demo of What-if Tool (WIT)",
          "Case Study-3: Demo of What-if Tool (WIT)",
          "Case Study-4: Demo of What-if Tool (WIT)",
          "Case Study-5: Demo of What-if Tool (WIT)"
        ],
        "Layer-wise Relevance Propagation (LRP)": [
          "Interaction Demos of LRP",
          "Working Principle of LRP",
          "Mathematical Modelling of LRP",
          "Demo of LRP on MRI dataset: Part-1",
          "Demo of LRP on MRI dataset: Part-2",
          "Recommended Practice Tasks"
        ],
        "Contrastive Explanations Method (CEM)": [
          "Working Principle and Applications of Contrastive Explanations Method (CEM)"
        ],
        "Useful Resources for XAI": [
          "Useful Resources for XAI"
        ],
        "Final Quiz": [
          "Quiz on all the learnings"
        ]
      },
      "requirements": [
        "No programming experience needed. You will learn everything you need to know to apply XAI for generating explanations for ML models."
      ],
      "description": "XAI with Python\nThis course provides detailed insights into the latest developments in Explainable Artificial Intelligence (XAI). Our reliance on artificial intelligence models is increasing day by day, and it's also becoming equally important to explain how and why AI makes a particular decision. Recent laws have also caused the urgency about explaining and defending the decisions made by AI systems. This course discusses tools and techniques using Python to visualize, explain, and build trustworthy AI systems.\nThis course covers the working principle and mathematical modeling of LIME (Local Interpretable Model Agnostic Explanations), SHAP (SHapley Additive exPlanations) for generating local and global explanations. It discusses the need for counterfactual and contrastive explanations, the working principle, and mathematical modeling of various techniques like Diverse Counterfactual Explanations (DiCE) for generating actionable counterfactuals.\nThe concept of AI fairness and generating visual explanations are covered through Google's What-If Tool (WIT).  This course covers the LRP (Layer-wise Relevance Propagation) technique for generating explanations for neural networks.\nIn this course, you will learn about tools and techniques using Python to visualize, explain, and build trustworthy AI systems. The course covers various case studies to emphasize the importance of explainable techniques in critical application domains.\nAll the techniques are explained through hands-on sessions so that learns can clearly understand the code and can apply it comfortably to their AI models. The dataset and code used in implementing various XAI techniques are provided to the learners for their practice.",
      "target_audience": [
        "Students taking Machine Learning Course or Artificial Intelligence Course",
        "Students who are looking to make career in AI",
        "Beginner Python programmers who already have some foundational knowledge with machine learning libraries.",
        "Researchers who already use Python for building AI models and can benefit from learning the latest explainable AI techniques to generate explanations of their models",
        "Data analysts and data scientists that want an introduction to explainable AI tools and techniques using Python for machine learning models."
      ]
    },
    {
      "title": "Data Science: Supervised Machine Learning in Python",
      "url": "https://www.udemy.com/course/data-science-supervised-machine-learning-in-python/",
      "bio": "Full Guide to Implementing Classic Machine Learning Algorithms in Python and with Scikit-Learn",
      "objectives": [
        "Understand and implement K-Nearest Neighbors in Python",
        "Understand the limitations of KNN",
        "User KNN to solve several binary and multiclass classification problems",
        "Understand and implement Naive Bayes and General Bayes Classifiers in Python",
        "Understand the limitations of Bayes Classifiers",
        "Understand and implement a Decision Tree in Python",
        "Understand and implement the Perceptron in Python",
        "Understand the limitations of the Perceptron",
        "Understand hyperparameters and how to apply cross-validation",
        "Understand the concepts of feature extraction and feature selection",
        "Understand the pros and cons between classic machine learning methods and deep learning",
        "Use Sci-Kit Learn",
        "Implement a machine learning web service"
      ],
      "course_content": {
        "Introduction and Review": [
          "Introduction and Outline",
          "How to Succeed in this Course",
          "Where to get the Code and Data",
          "Review of Important Concepts"
        ],
        "K-Nearest Neighbor": [
          "K-Nearest Neighbor Intuition",
          "K-Nearest Neighbor Concepts",
          "KNN in Code with MNIST",
          "When KNN Can Fail",
          "KNN for the XOR Problem",
          "KNN for the Donut Problem",
          "Effect of K",
          "KNN Exercise",
          "Suggestion Box"
        ],
        "Naive Bayes and Bayes Classifiers": [
          "Bayes Classifier Intuition (Continuous)",
          "Bayes Classifier Intuition (Discrete)",
          "Naive Bayes",
          "Naive Bayes Handwritten Example",
          "Naive Bayes in Code with MNIST",
          "Non-Naive Bayes",
          "Bayes Classifier in Code with MNIST",
          "Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)",
          "Generative vs Discriminative Models"
        ],
        "Decision Trees": [
          "Decision Tree Intuition",
          "Decision Tree Basics",
          "Information Entropy",
          "Maximizing Information Gain",
          "Choosing the Best Split",
          "Decision Tree in Code"
        ],
        "Perceptrons": [
          "Perceptron Concepts",
          "Perceptron in Code",
          "Perceptron for MNIST and XOR",
          "Perceptron Loss Function"
        ],
        "Practical Machine Learning": [
          "Hyperparameters and Cross-Validation",
          "Feature Extraction and Feature Selection",
          "Comparison to Deep Learning",
          "Multiclass Classification",
          "Sci-Kit Learn",
          "Regression with Sci-Kit Learn is Easy"
        ],
        "Building a Machine Learning Web Service": [
          "Building a Machine Learning Web Service Concepts",
          "Building a Machine Learning Web Service Code"
        ],
        "Conclusion": [
          "What’s Next? Support Vector Machines and Ensemble Methods (e.g. Random Forest)"
        ],
        "Appendix / FAQ Finale": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, and Sci-Kit Learn"
        ]
      },
      "requirements": [
        "Python, Numpy, and Pandas experience",
        "Probability and statistics (Gaussian distribution)",
        "Strong ability to write algorithms"
      ],
      "description": "In recent years, we've seen a resurgence in AI, or artificial intelligence, and machine learning.\nMachine learning has led to some amazing results, like being able to analyze medical images and predict diseases on-par with human experts.\nGoogle's AlphaGo program was able to beat a world champion in the strategy game go using deep reinforcement learning.\nMachine learning is even being used to program self driving cars, which is going to change the automotive industry forever. Imagine a world with drastically reduced car accidents, simply by removing the element of human error.\nGoogle famously announced that they are now \"machine learning first\", meaning that machine learning is going to get a lot more attention now, and this is what's going to drive innovation in the coming years. It's embedded into all sorts of different products.\nMachine learning is used in many industries, like finance, online advertising, medicine, and robotics.\nIt is a widely applicable tool that will benefit you no matter what industry you're in, and it will also open up a ton of career opportunities once you get good.\nMachine learning also raises some philosophical questions. Are we building a machine that can think? What does it mean to be conscious? Will computers one day take over the world?\nIn this course, we are first going to discuss the K-Nearest Neighbor algorithm. It’s extremely simple and intuitive, and it’s a great first classification algorithm to learn. After we discuss the concepts and implement it in code, we’ll look at some ways in which KNN can fail.\nIt’s important to know both the advantages and disadvantages of each algorithm we look at.\nNext we’ll look at the Naive Bayes Classifier and the General Bayes Classifier. This is a very interesting algorithm to look at because it is grounded in probability.\nWe’ll see how we can transform the Bayes Classifier into a linear and quadratic classifier to speed up our calculations.\nNext we’ll look at the famous Decision Tree algorithm. This is the most complex of the algorithms we’ll study, and most courses you’ll look at won’t implement them. We will, since I believe implementation is good practice.\nThe last algorithm we’ll look at is the Perceptron algorithm. Perceptrons are the ancestor of neural networks and deep learning, so they are important to study in the context of machine learning.\nOne we’ve studied these algorithms, we’ll move to more practical machine learning topics. Hyperparameters, cross-validation, feature extraction, feature selection, and multiclass classification.\nWe’ll do a comparison with deep learning so you understand the pros and cons of each approach.\nWe’ll discuss the Sci-Kit Learn library, because even though implementing your own algorithms is fun and educational, you should use optimized and well-tested code in your actual work.\nWe’ll cap things off with a very practical, real-world example by writing a web service that runs a machine learning model and makes predictions. This is something that real companies do and make money from.\nAll the materials for this course are FREE. You can download and install Python, Numpy, and Scipy with simple commands on Windows, Linux, or Mac.\n\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus (for some parts)\nprobability (continuous and discrete distributions, joint, marginal, conditional, PDF, PMF, CDF, Bayes rule)\nPython coding: if/else, loops, lists, dicts, sets\nNumpy, Scipy, Matplotlib\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Students and professionals who want to apply machine learning techniques to their datasets",
        "Students and professionals who want to apply machine learning techniques to real world problems",
        "Anyone who wants to learn classic data science and machine learning algorithms",
        "Anyone looking for an introduction to artificial intelligence (AI)"
      ]
    },
    {
      "title": "Data Mining with R: Go from Beginner to Advanced!",
      "url": "https://www.udemy.com/course/data-mining-with-r-go-from-beginner-to-advanced/",
      "bio": "Learn to use R software for data analysis, visualization, and to perform dozens of popular data mining techniques.",
      "objectives": [
        "Use R software for data import and export, data exploration and visualization, and for data analysis tasks, including performing a comprehensive set of data mining operations.",
        "Effectively use a number of popular, contemporary data mining methods and techniques in demand by industry including: (1) Decision, classification and regression trees (CART); (2) Random forests; (3) Linear and logistic regression; and (4) Various cluster analysis techniques.",
        "Apply the dozens of included \"hands-on\" cases and examples using real data and R scripts to new and unique data analysis and data mining problems."
      ],
      "course_content": {
        "Data Types and Structures in R": [
          "Who should take and what will you get from this course ?",
          "Installing R and RStudio",
          "Orientation to Data Types and Structures Section",
          "Materials for Data Types and Structures",
          "Vectors: The Basic Default Data Structure in R",
          "Matrices, Lists and Dataframes: Other Important R Data Structures",
          "Manipulating Vectors in R",
          "Naming Vectors in R",
          "Creating Matrices in R",
          "Creating Lists in R",
          "Creating Lists in R (continued)",
          "Creating Dataframes in R"
        ],
        "Data and File Input and Output": [
          "Orientation to Data and File Input and Output",
          "Materials for Data and File Input and Output",
          "Reading in Data using scan() Function",
          "Reading in Data with scan() Function (continued)",
          "Using readline() Function to Prompt User for Input",
          "Reading in Files with read.table() and read.csv() Functions",
          "Writing R Session Files to Disk (Outputting Data)",
          "Data Input and Output Exercise"
        ],
        "Visualizing (Getting to Know) your Data": [
          "Solution to Data Input and Output Exercise from Section 2 (1 of 2)",
          "Solution to Data Input and Output Exercise from Section 2 (2 of 2)",
          "Materials for Visualizing your Data Section 3",
          "Preprocessing and Visualizing Birth Data",
          "Preprocessing and Visualizing Birth Data (part 2)",
          "Preprocessing and Visualizing Birth Data (part 3)",
          "Visualizing Alumni Donations",
          "Visualizing Alumni Donations (part 2)",
          "Visualizing Alumni Donations (part 3)",
          "Visualizing Alumni Donations (part 4)",
          "Visualizing (Getting to Know) your Data Section Exercise"
        ],
        "Decision Trees and Random Forests": [
          "Solution to Visualizing Virginia Deaths Exercise",
          "Introduction to Decision Trees and Random Forests",
          "Training Decision Trees with party Package",
          "Training Decision Trees with party Package (part 2)",
          "Bodyfat Decision Tree example with Package rpart",
          "Bodyfat Decision Tree example with Package rpart (part 2)",
          "Bagging and Random Forests with Section Exercise"
        ],
        "Linear Modeling (Regression) and Generalized Linear Modeling (GLMs)": [
          "Begin Decision Tree and Random Forests Exercise Solution",
          "Random Forests Exercise Bagging Segment Solution",
          "Random Forests Exercise Solution (part 3)",
          "Materials for Regression and GLMs Section",
          "Begin Regression Example",
          "Continue Regression Example",
          "Finish Regression Example",
          "Begin Regression and GLM Slides",
          "Finish Generalized Linear Modeling Slides",
          "Heart Data Binomial GLM Example",
          "Epidemic Data Poisson GLM Example",
          "Regression and GLMs Exercises"
        ],
        "K-Means, K-Medoids, and Hierarchical Cluster Analysis Approaches": [
          "Materials and End-of-Section-6 Exercise",
          "Regression and GLM Exercises Solutions (part 1)",
          "Regression and GLM Exercises Solutions (part 2)",
          "Regression and GLM Exercises Solutions (part 3)",
          "K-Means Iris Flower Example",
          "K-Means Exoplanets Example",
          "K-Medoids Iris Flower Re-Analysis Example",
          "Hierarchical Clustering Iris Flower Example",
          "Hierarchical Clustering Pottery Example"
        ],
        "Density-Based and Agglomerative Hierarchical Clustering": [
          "Materials for Density-Based and Hierarchical Agglomerative Clustering Section",
          "Density-Based and Agglomerative Clustering Introduction and Previous Exercise",
          "Density-Based Clustering Example",
          "Body Measurements and Agglomerative Hierarchical Clustering Example",
          "Continue Body Measurements Agglomerative Clustering Example",
          "Clustering Jet Fighters Example"
        ],
        "More Cluster Analysis Examples, Graphics, and Detecting Outliers": [
          "Materials and End-of-Section-8 Exercise",
          "K-Means Clustering Explained in Detail",
          "Clustering Crime Rates Example",
          "Clustering Crime Rates Example (part 2)",
          "Gastroenterologist Questionnaire Model-Based Clustering Eample",
          "Graphical Approaches to Cluster Analysis Examples",
          "Detecting Outliers",
          "Detecting Outliers (part 2)"
        ],
        "K-Means TAM Residuals Cluster Analysis Software Case example": [
          "Crime Data Exercise Solution",
          "Crime Data Exercise Solution (part 2)",
          "Materials for Final Data Mining Course Section",
          "K-Means Clustering PLS-POS Capability Implementation",
          "K-Means Clustering PLS-POS Capability Implementation Concepts",
          "Implementing K-Means Clustering for TAM Residuals Continued",
          "Implementing K-Means Clustering for TAM Residuals in R Software"
        ]
      },
      "requirements": [
        "Download and install no-cost R software (complete, easy-to-follow instructions are provided).",
        "Download and install no-cost RStudio IDE software (complete, easy-to-follow instructions are provided)."
      ],
      "description": "This is a \"hands-on\" business analytics, or data analytics course teaching how to use the popular, no-cost R software to perform dozens of data mining tasks using real data and data mining cases. It teaches critical data analysis, data mining, and predictive analytics skills, including data exploration, data visualization, and data mining skills using one of the most popular business analytics software suites used in industry and government today. The course is structured as a series of dozens of demonstrations of how to perform classification and predictive data mining tasks, including building classification trees, building and training decision trees, using random forests, linear modeling, regression, generalized linear modeling, logistic regression, and many different cluster analysis techniques. The course also trains and instructs on \"best practices\" for using R software, teaching and demonstrating how to install R software and RStudio, the characteristics of the basic data types and structures in R, as well as how to input data into an R session from the keyboard, from user prompts, or by importing files stored on a computer's hard drive. All software, slides, data, and R scripts that are performed in the dozens of case-based demonstration video lessons are included in the course materials so students can \"take them home\" and apply them to their own unique data analysis and mining cases. There are also \"hands-on\" exercises to perform in each course section to reinforce the learning process. The target audience for the course includes undergraduate and graduate students seeking to acquire employable data analytics skills, as well as practicing predictive analytics professionals seeking to expand their repertoire of data analysis and data mining knowledge and capabilities.",
      "target_audience": [
        "Anyone who wants to learn more about performing data analysis using a variety of popular, contemporary data mining techniques.",
        "Data Mining beginners and professionals who wish to enhance their data mining knowledge and skill levels",
        "Individuals seeking to gain more proficiency using the popular R and RStudio software suites.",
        "Undergraduate students seeking to acquire in-demand analytics skills to enhance employment opportunities.",
        "Graduate students seeking to acquire a wider repertoire of analytics skills for research data analysis tasks."
      ]
    },
    {
      "title": "Practical AI with Python and Reinforcement Learning",
      "url": "https://www.udemy.com/course/practical-ai-with-python-and-reinforcement-learning/",
      "bio": "Learn how to use Reinforcement Learning techniques to create practical Artificial Intelligence programs!",
      "objectives": [
        "Reinforcement Learning with Python",
        "Creating Artificial Neural Networks with TensorFlow",
        "Using TensorFlow to create Convolution Neural Networks for Images",
        "Using OpenAI to work with built-in game environments",
        "Using OpenAI to create your own environments for any problem",
        "Create Artificially Intelligent Agents",
        "Tabular Q-Learning",
        "State–action–reward–state–action (SARSA)",
        "Deep Q-Learning (DQN)",
        "DQN using Convolutional Neural Networks",
        "Cross Entropy Method for Reinforcement Learning",
        "Double DQN",
        "Dueling DQN"
      ],
      "course_content": {},
      "requirements": [
        "You should be very comfortable with basic Python and installing Python libraries.",
        "This is NOT a course for beginners, we highly suggest you take our \"Data Science and Machine Learning Masterclass\" first!"
      ],
      "description": "Please note! This course is in an \"early bird\" release, and we're still updating and adding content to it, please keep in mind before enrolling that the course is not yet complete.\n\n\n“The future is already here – it’s just not very evenly distributed.“\nHave you ever wondered how Artificial Intelligence actually works? Do you want to be able to harness the power of neural networks and reinforcement learning to create intelligent agents that can solve tasks with human level complexity?\nThis is the ultimate course online for learning how to use Python to harness the power of Neural Networks to create Artificially Intelligent agents!\nThis course focuses on a practical approach that puts you in the driver's seat to actually build and create intelligent agents, instead of just showing you small toy examples like many other online courses. Here we focus on giving you the power to apply artificial intelligence to your own problems, environments, and situations, not just those included in a niche library!\n\n\nThis course covers the following topics:\nArtificial Neural Networks\nConvolution Neural Networks\nClassical Q-Learning\nDeep Q-Learning\nSARSA\nCross Entropy Methods\nDouble DQN\nand much more!\n\n\nWe've designed this course to get you to be able to create your own deep reinforcement learning agents on your own environments. It focuses on a practical approach with the right balance of theory and intuition with useable code. The course uses clear examples in slides to connect mathematical equations to practical code implementation, before showing how to manually implement the equations that conduct reinforcement learning.\nWe'll first show you how Deep Learning with Keras and TensorFlow works, before diving into Reinforcement Learning concepts, such as Q-Learning. Then we can combine these ideas to walk you through Deep Reinforcement Learning agents, such as Deep Q-Networks!\n\n\nThere is still a lot more to come, I hope you'll join us inside the course!\nJose",
      "target_audience": [
        "Python developers familiar with basics of machine learning, such as Scikit-Learn, but now want to learn how to create Artificially Intelligent Agents through Reinforcement Learning"
      ]
    },
    {
      "title": "Master the Art of Prompt Engineering for Generative AI",
      "url": "https://www.udemy.com/course/master-the-art-of-prompt-engineering-for-generative-ai/",
      "bio": "Learn the Best Framework for Building Highly Effective Prompts for ChatGPT, Google Gemini and Microsoft Copilot",
      "objectives": [
        "A Framework for Building and Optimizaing Text Prompts",
        "Prompt Engineering",
        "Chain-of-Thought Prompting",
        "Prompt Chaining",
        "Generated Knowledge Prompting",
        "Adding External Knowledge"
      ],
      "course_content": {
        "Getting Started": [
          "Welcome!",
          "A Few Recommendations"
        ],
        "A Framework for Building Prompts": [
          "Introduction",
          "Download the List of Prompts",
          "#1 - (Clearly Define the Required) Instruction",
          "#2 - (Provide just Enough) Context",
          "#3 - Examples (as Templates)",
          "#4 - (Role Playing with) Persona",
          "#5 - (Defining the Desired) Format",
          "#6 - (Setting the Right) Tone",
          "A Couple of Tips",
          "Summary"
        ],
        "Prompt Engineering Methods": [
          "Introduction",
          "#1 - Splitting Complex Tasks",
          "#2 - Sharing the Reasoning Steps",
          "#3 - Using Interactive Prompts",
          "#4 - Generating Knowledge as a Pre-Step",
          "#5 - Adding External Knowledge",
          "Summary"
        ],
        "Course Summary": [
          "Thank You!",
          "** BONUS **"
        ]
      },
      "requirements": [
        "Nothing specific is needed to start the training program."
      ],
      "description": "Unlock the Power of Generative AI\nIn the rapidly evolving world of artificial intelligence, the ability to communicate effectively with AI systems is becoming a critical skill. Prompt engineering is becoming an essential as it acts as the bridge between human intent and artificial intelligence, enabling us to effectively guide AI systems to produce meaningful, accurate, and relevant responses. With AI models like ChatGPT being capable of processing vast amounts of information, the quality of their output largely depends on how well prompts are crafted.\nThis training is a hands-on course designed to empower you with the tools and techniques to craft precise, effective prompts that harness the full potential of large language models (LLMs) like ChatGPT and Google Gemini.\nA Simple Framework\nWe will explore and use a simple yet powerful framework for building highly effective prompts. The framework is based on six building blocks: instruction, context, examples, persona, format and tone.\nMost Practical Methods\nIn the second part of the course, we will review the top practical prompt engineering methods that will be useful to handle more complex use cases and tasks.\nJoin the Gen AI Revolution\nReady to embark on this transformative journey? Join me as we explore the exciting world of Generative AI.",
      "target_audience": [
        "Anyone that is using Generative AI tools (e.g. ChatGPT, Google Gemini)"
      ]
    },
    {
      "title": "LangChain in Action: Develop LLM-Powered Applications",
      "url": "https://www.udemy.com/course/langchain-in-action-develop-llm-powered-applications/",
      "bio": "From the Basics of LLMs to Production-Grade Microservice Architecture with Kubernetes (Latest Version 0.3.0)",
      "objectives": [
        "Master LangChain from basics to advanced features",
        "Understand and implement Retrieval Augmented Generation (RAG) using VectorStores",
        "Learn about the creation and use of powerful Autonomous Agents.",
        "Grasp the functionalities and applications of the Indexing API.",
        "Explore the LangSmith Platform for production ready application",
        "Learn about Microservice architecture in the context of large language model (LLM) applications.",
        "Learn about the new LangChain Expression Language with the Runnable Interface"
      ],
      "course_content": {
        "Before we start...": [
          "What to expect from this course and how to get all ressources",
          "Why this course is different",
          "Prerequisites",
          "Essential topics and terms (theory)",
          "Why this course does not cover Open Source models like LLama2",
          "Optional: Install Visual Studio Code",
          "Get the source files with Git from Github",
          "Create OpenAI Account and create API Key",
          "LangChain 0.3.x - Whats changed?"
        ],
        "Preparation": [
          "What we have to do before delving into LangChain",
          "Setup of a virtual environment",
          "Setup OpenAI Api-Key as environment variable",
          "Exploring the vanilla OpenAI package"
        ],
        "LangChain Basics": [
          "IMPORTANT NOTE - Updates and Code changes",
          "LLM Basics",
          "Prompting Basics",
          "Theory: Prompt Engineering Basics",
          "Few Shot Prompting",
          "Chain of thought prompting",
          "Pipeline-Prompts",
          "Prompt Serialisation"
        ],
        "Chains - From basic to advanced chains": [
          "Introduction to chains",
          "Basic chains - the LLMChain",
          "Response Schemas and OutputParsers",
          "LLMChain with multiple inputs",
          "SequentialChains",
          "RouterChains"
        ],
        "Callbacks": [
          "Callbacks"
        ],
        "Memory": [
          "Memory basics - ConversationBufferMemory",
          "ConversationSummaryMemory",
          "EXERCISE: Use Memory to build a streamlit Chatbot",
          "SOLUTION: Chatbot with Streamlit"
        ],
        "OpenAI Function Calling": [
          "OpenAI Function Calling - Vanilla OpenAI Package",
          "Function Calling with LangChain [DEPRECATED]",
          "Limits and issues of the langchain Implementation [DEPRECATED]",
          "Tool/Function Calling with LangChain - The new way"
        ],
        "Retrieval Augmented Generation (RAG)": [
          "RAG - Theory and building blocks",
          "Loaders and Splitters",
          "Embeddings - Theory and practice",
          "VectorStores and Retrievers",
          "RetrievalQAChain - the new (Runnable) way",
          "RAG Service with FastAPI"
        ],
        "Agents": [
          "Agents Basics - LLMs learn to use tools",
          "Agents with a custom RAG-Tool",
          "ChatAgents"
        ],
        "Indexing API": [
          "Indexing API - keep your documents in sync",
          "PREREQUISITE: Docker Installation",
          "Setup of PgVector and RecordManager",
          "Indexing Documents in practice",
          "Document Retrieval with PgVector"
        ]
      },
      "requirements": [
        "Intermediate Python Skills (OOP, Datatypes, Functions, modules etc.)",
        "Helpful: Terminal and Docker knowledge"
      ],
      "description": "This course provides an in-depth exploration into LangChain, a framework pivotal for developing generative AI applications. Aimed at both beginners and experienced practitioners in the AI world, the course starts with the fundamentals, such as the basic usage of the OpenAI API, progressively delving into the more intricate aspects of LangChain.\nYou'll learn about the intricacies of input and output mechanisms in LangChain and how to craft effective prompt templates for OpenAI models. The course takes you through the critical components of LangChain, such as Chains, Callbacks, and Memory, teaching you to create interactive and context-aware AI systems.\nMidway, the focus shifts to advanced concepts like Retrieval Augmented Generation (RAG) and the creation of Autonomous Agents, enriching your understanding of intelligent system design. Topics like Hybrid Search, Indexing API, and LangSmith will be covered, highlighting their roles in enhancing the efficiency and functionality of AI applications.\nToward the end, the course integrates theory with practical skills, introducing Microservice Architecture in large language model (LLM) applications and the LangChain Expression Language. This ensures not only a theoretical understanding of the concepts but also their practical applications.\nThis course is tailored for individuals with a foundational knowledge of Python, aiming to build or enhance their expertise in AI. The structured curriculum ensures a comprehensive grasp of LangChain, from basic concepts to complex applications, preparing you for the future of generative AI.",
      "target_audience": [
        "Python Developers, AI Enthusiats"
      ]
    },
    {
      "title": "Artificial Intelligence I: Meta-Heuristics and Games in Java",
      "url": "https://www.udemy.com/course/artificial-intelligence-games-in-java/",
      "bio": "Graph Algorithms, Genetic Algorithms, Simulated Annealing, Swarm Intelligence, Minimax, Heuristics and Meta-Heuristics",
      "objectives": [
        "Get a good grasp of artificial intelligence",
        "Understand how AI algorithms work",
        "Understand graph search algorithms - BFS, DFS and A* search",
        "Understand meta-heuristics",
        "Understand genetic algorithms",
        "Understand simulated annealing",
        "Understand swarm intelligence and particle swarm optimization",
        "Understand game trees",
        "Understand minimax algorithm and alpha-beta pruning",
        "Tic Tac Toe game from scratch with minimax algorithm"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Why Should You Learn Artificial Intelligence?": [
          "What is AI good for?"
        ],
        "### PATHFINDING ALGORITHMS (GRAPHS) ###": [
          "Why to consider graph algorithms?"
        ],
        "Breadth-First Search (BFS)": [
          "What is breadth-first search?",
          "Breadth-first search implementation",
          "Applications of breadth-first search",
          "Breadth-First Search Quiz"
        ],
        "Depth-First Search (DFS)": [
          "What is depth-first search?",
          "Depth-first search implementation I - with stack",
          "Depth-first search implementation II - with recursion",
          "Depth-first search and stack memory visualization",
          "Memory comparison of graph traversal algorithms",
          "Applications of depth-first search",
          "Depth-First Search Quiz"
        ],
        "Course Challenge #1 - Maze Escape": [
          "Maze problem introduction",
          "Course challenge #1 - maze problem",
          "Maze problem implementation",
          "Maze problem stack memory visualization"
        ],
        "Iterative Deepening Depth-First Search (IDDFS)": [
          "Enhanced search algorithms introduction (IDDFS)",
          "Iterative deepening depth-first search (IDDFS) implementation",
          "Enhanced Search Quiz"
        ],
        "A* Search Algorithm": [
          "A* search introduction",
          "A* search illustration",
          "A* search implementation I",
          "A* search implementation II",
          "A* search implementation III",
          "Path finding algorithms comparison",
          "A* Search Quiz"
        ],
        "### OPTIMIZATION ###": [
          "Brute-force method",
          "Brute-force method implementation",
          "Hill climbing method",
          "Hill climbing method implementation",
          "Optimization Quiz"
        ],
        "### META-HEURISTICS ###": [
          "Heuristics and meta-heuristics",
          "Heuristics Quiz"
        ]
      },
      "requirements": [
        "Basic Java (SE)"
      ],
      "description": "This course is about the fundamental concepts of artificial intelligence. This topic is getting very hot nowadays because these learning algorithms can be used in several fields from software engineering to investment banking. Learning algorithms can recognize patterns which can help detecting cancer for example. We may construct algorithms that can have a very  good guess about stock price movement in the market.\n- PATHFINDING ALGORITHMS -\nSection 1 - Breadth-First Search (BFS)\nwhat is breadth-first search algorithm\nwhy to use graph algorithms in AI\nSection 2 - Depth-First Search (DFS)\nwhat is depth-first search algorithm\nimplementation with iteration and with recursion\ndepth-first search stack memory visualization\nmaze escape application\nSection 3 - Iterative Deepening Depth-First Search (IDDFS)\nwhat is iterative deepening depth-first search algorithm\nSection 4 - A* Search Algorithm\nwhat is A* search algorithm\nwhat is the difference between Dijkstra's algorithm and A* search\nwhat is a heuristic\nManhattan distance and Euclidean distance\n- OPTIMIZATION -\nSection 5 - Optimization Approaches\nbasic optimization algorithms\nbrute-force search\nhill climbing algorithm\n- META-HEURISTICS -\nSection 6 - Simulated Annealing\nwhat is simulated annealing\nhow to find the extremum of functions\nhow to solve combinatorial optimization problems\ntravelling salesman problem (TSP)\nSection 7 - Genetic Algorithms\nwhat are genetic algorithms\nartificial evolution and natural selection\ncrossover and mutation\nsolving the knapsack problem\nSection 8 - Particle Swarm Optimization (PSO)\nwhat is swarm intelligence\nwhat is the Particle Swarm Optimization algorithm\n- GAMES AND GAME TREES -\nSection 9 - Game Trees\nwhat are game trees\nhow to construct game trees\nSection 10 - Minimax Algorithm and Game Engines\nwhat is the minimax algorithm\nwhat is the problem with game trees?\nusing the alpha-beta pruning approach\nchess problem\nSection 11 - Tic Tac Toe with Minimax\nTic Tac Toe game and its implementation\nusing minimax algorithm\nIn the first chapter we are going to talk about the basic graph algorithms. Several advanced algorithms can be solved with the help of graphs, so as far as I am concerned these algorithms are the first steps.\nSecond chapter is about local search: finding minimum and maximum or global optimum in the main. These searches are used frequently when we use regression for example and want to find the parameters for the fit. We will consider basic concepts as well as the more advanced algorithms: heuristics and meta-heuristics.\nThe last topic will be about minimax algorithm and how to use this technique in games such as chess or tic-tac-toe, how to build and construct a game tree, how to analyze these kinds of tree like structures and so on. We will implement the tic-tac-toe game together in the end.\nThanks for joining the course, let's get started!",
      "target_audience": [
        "This course is meant for students or anyone who interested in algorithms and optimization approaches and have some background in basic Java"
      ]
    },
    {
      "title": "Generative AI, from GANs to CLIP, with Python and Pytorch",
      "url": "https://www.udemy.com/course/generative-creative-ai-from-gans-to-clip-with-python-and-pytorch/",
      "bio": "Learn to code with the most creative and exciting AI architectures, generative AI networks, from basic to advanced",
      "objectives": [
        "How to code generative A.I architectures from scratch using Python and Pytorch",
        "How generative architectures work, in great depth, from GANs to multimodal A.I, understanding every little detail in the process",
        "In addition to the coding, every section begins with an in-depth review of the key concepts related to these architectures",
        "Examples: We will code a generative network that produces human faces, and also combine two advanced networks to transform text prompts into amazing images.",
        "Examples: We will learn to edit the clothes of a person in a picture by combining a segmentation architecture with the Stable Diffusion generative model",
        "Special Bonus Section: Journey to the latent space of a neural network, learn in depth how the networks that power Generative AI learn their mappings",
        "Special Bonus Section: Experience a guided visualization to exercise the generative model in your head while you learn many things about neural networks"
      ],
      "course_content": {
        "The generative AI revolution": [
          "The roadmap, from basic to advanced and beyond",
          "Javier sends greetings from his spacecraft",
          "The generative revolution: coming home",
          "The present and future of AI is generative",
          "Applications of generative AI",
          "Latent spaces and representation learning",
          "Navigating latent spaces",
          "GANS: Generative Adversarial Networks",
          "Benefits and possibilities of Generative AI",
          "Coming home: generative AI and human nature",
          "Javier sings a song dedicated to generative AI"
        ],
        "Coding a basic generative architecture": [
          "Javier introduces section 2 from his spacecraft",
          "Understanding the battle between generator and discriminator",
          "Understanding Cross Entropy in depth",
          "Understanding the equation to calculate the discriminator loss",
          "Understanding the equation to calculate the generator loss",
          "(Optional) Google Colab Tutorial",
          "Coding: importing libraries and declaring a visualization function",
          "Coding: hyperparameters and the DataLoader",
          "Coding: the generator class",
          "Coding: the discriminator class",
          "Coding: the optimizer and testing the generator",
          "Coding: the loss values of generator and discriminator",
          "Coding: main training loop, discriminator part",
          "Coding: main training loop, generator and stats",
          "Coding: running the training",
          "Coding: results and conclusions"
        ],
        "Coding an advanced generative architecture": [
          "Javier introduces section 3 from his spacecraft",
          "Challenges and issues of the basic GAN",
          "The Wasserstein Loss",
          "The Gradient Penalty",
          "Coding: setting up libraries and parameters",
          "Coding: Login and setup of the Wandb stats library",
          "Coding: Beginning the generator",
          "Coding: Understanding convolutions",
          "Coding: The generator class",
          "Coding: The critic class",
          "Coding: Alternative way to initialize parameters (optional)",
          "Coding: Loading the CelebA dataset",
          "Coding: Declaring dataset, dataloader and optimizers",
          "Coding: the gradient penalty",
          "Coding: saving and loading checkpoints",
          "Coding: training loop - critic training",
          "Coding: training loop - generator training",
          "Coding: stats and fixing issues",
          "Coding: reviewing the code before running the training",
          "Coding: running the training",
          "Coding: results after a few epochs",
          "Coding: results after a few more epochs",
          "Coding: results getting better and better",
          "Coding: morphing between points in latent space",
          "Coding: more morphing"
        ],
        "Generating images from text by combining two advanced architectures": [
          "Javier introduces section 4 from his spacecraft",
          "Multimodal generation, an incredible adventure",
          "Coding: importing the libraries",
          "Coding: helper functions and hyperparameters",
          "Coding: Setting up the CLIP model",
          "Coding: Setting up the Generative transformer model",
          "Coding: Setting up the latent space parameters to be optimized",
          "Coding: encode the text prompts through CLIP",
          "Coding: creating crops from the generated image",
          "Coding: a function to display generated images and crops",
          "Coding: optimizing the latent space parameters",
          "Coding: the training loop",
          "Coding: running the training",
          "Coding: interpolating between points in the latent space",
          "Coding: creating a video of the interpolations and general review",
          "Coding: creating variations of the code",
          "Coding: Davinci Sfumato: Tweaking the code to create a new kind of texture",
          "Coding: Davinci Sfumato: reflecting about the process",
          "Final greetings from the spacecraft"
        ],
        "Editing people's clothes by combining segmentation and generative AI models": [
          "Intro: people's clothes replacement and editing using Generative AI",
          "Coding: Setting up libraries and the segmentation model",
          "Coding: Setting up the Stable Diffusion generative model",
          "Coding: Loading a picture and running the segmentation process to produce masks",
          "Coding: Visualizing the generated masks",
          "Coding: Inpainting, running and experimenting with the Stable Diffusion model",
          "Coding: Guide the segmentation process with text prompts",
          "Coding: run the generative model in this alternative setup",
          "Ending of the section"
        ],
        "Bonus: Journey to the latent space of a Neural Network": [
          "In Search of the Magical Mappings of Creativity",
          "The Search for the Perfect Mapping: datasets and dimensionality",
          "From Linearity to Complexity: Neural Networks and the Nonlinearities of Life",
          "Bending the Rules: Non-Linear transformations and the key to complexity",
          "Not Too Tight, Not Too Loose - Finding the perfect fit",
          "How increasing the dimensionality impacts the latent complexity of the network",
          "The Power of Depth: Creating Sophisticated Mappings with AI networks",
          "From high dimensional manifolds to dynamic and ever changing latent spaces",
          "Advanced digital representations of the latent complexity of neural networks",
          "Visualizing the Journey: Loss Landscapes and the Search for Optimal Weights",
          "Example of the dynamic Loss Landscape of a generative adversarial network",
          "Lucy - Real Time Visualization of the changing weights of a neural network",
          "Charting the hidden depths: a recap of our transformative latent space journey"
        ],
        "Bonus: Activating the Generative Model of your own mind": [
          "A guided visualization experience to exercise the generative model in your head",
          "Intro to the journey to the center of the neuron",
          "The container, the salty ocean and the 150000 cortical columns",
          "Visualizing the pyramidal neuron",
          "The Synapse, visualizing the input-output interface",
          "Biological vs Artificial Neurons: Inputs, Outputs, Speed, etc",
          "Learning in biological and artificial neurons",
          "Planning, decision making and world models",
          "Efficiency: sparsity in biological vs artificial networks",
          "Consciousness: within the neurons",
          "The future, towards AGI / ASI"
        ]
      },
      "requirements": [
        "Basic knowledge of python. It's enough with the very basics, as we will code every little thing together, line by line",
        "Access to an internet connection, as we will use the free online Google Colab service to code together",
        "Plenty of enthusiasm as we will go deep into every little detail, let's do it! :)"
      ],
      "description": "April 2024 Update: Two new sections have been added recently.\nNew Section 5: learn to edit the clothes of a person in a picture by programming a combination of a segmentation model with the Stable Diffusion generative model.\nNew bonus section 6: Journey to the latent space of a neural network - dive deep into the latent space of the neural networks that power Generative AI in order to understand in depth how they learn their mappings.\n____________________________\n\nGenerative A.I. is the present and future of A.I. and deep learning, and it will touch every part of our lives. It is the part of A.I that is closer to our unique human capability of creating, imagining and inventing. By doing this course, you gain advanced knowledge and practical experience in the most promising part of A.I., deep learning, data science and advanced technology.\nThe course takes you on a fascinating journey in which you learn gradually, step by step, as we code together a range of generative architectures, from basic to advanced, until we reach multimodal A.I, where text and images are connected in incredible ways to produce amazing results.\nAt the beginning of each section, I explain the key concepts in great depth and then we code together, you and me, line by line, understanding everything, conquering together the challenge of building the most promising A.I architectures of today and tomorrow. After you complete the course, you will have a deep understanding of both the key concepts and the fine details of the coding process.\nWhat a time to be alive! We are able to code and understand architectures that bring us home, home to our own human nature, capable of creating and imagining. Together, we will make it happen. Let's do it!",
      "target_audience": [
        "People interested in using A.I and deep learning to generate, imagine and create new things",
        "People interested in generative adversarial networks and other advanced A.I generative architectures",
        "People interested in how A.I can combine different modalities (text, images) to create new things (multimodal A.I.)",
        "People interested in learning to code the type of advanced A.I architectures that are the present and future of the field"
      ]
    },
    {
      "title": "Comprehensive AI & Machine Learning Bootcamp",
      "url": "https://www.udemy.com/course/zero-to-pro-python-3-fullstack-data-bootcamp-45-ai-projects/",
      "bio": "From HTML Basics to Advanced Python",
      "objectives": [
        "Create websites, build applications, create Artificial Intelligent learning programs that can recognize handwriting and learn while analyzing data.",
        "Will help you get a job as a Fullstack programmer or Artificial Intelligence data scientist.",
        "Build over 10 AI data analysis tools",
        "Image analysis, text analysis etc."
      ],
      "course_content": {},
      "requirements": [
        "have a PC or mac. Must have desire to learn programming. HD monitor is preferred."
      ],
      "description": "Led by GP, a distinguished AI researcher with 11 PubMed publications and a rich academic background from Cornell, UCSF, NIH, and Amherst College, this course spans the essentials of web development to the frontiers of AI technology. Dive into a learning experience with LIVE HELP available Monday to Friday, 9-5, plus additional online support.\nOur curriculum is in constant evolution, tailored to your feedback and the dynamic landscape of machine learning and AI. This isn't just another bootcamp; it's a bridge from foundational HTML to pioneering in Python 3, Machine Learning, TensorFlow, and beyond into Artificial Intelligence and Recurrent Neural Networks.\nDesigned for rapid learning, we break down complex concepts into manageable steps. Starting from HTML and CSS to Bootstrap and JavaScript, and advancing through Python 3 to data science, machine learning, and AI, we cover ground rapidly but solidly.\nExpect to delve into:\nFrontend web technologies: HTML, CSS, Bootstrap, JavaScript, jQuery\nPython programming essentials and advanced concepts\nData Science, including Machine Learning and AI with TensorFlow\nPractical applications with projects in sentiment analysis, regression, clustering, and neural networks\nAn exploration of both traditional statistics and machine learning techniques\nWith over 170 lectures and 30+ hours of video content, this course is your most comprehensive guide to becoming a proficient Python developer and an AI specialist. You'll get lifetime access to all materials, including lecture Notebooks.\nThis course is perfect for beginners with no prior programming experience, bootcamp graduates looking to tackle real-world projects, and intermediate Python programmers eager to master AI programming. With a 30-day money-back guarantee, there's no risk in taking the leap. Transform your career with the skills to thrive in the era of AI.",
      "target_audience": [
        "Anyone who wants to learn fullstack in Python 3 and apply it to making AI immediately. If you are a Python 3 Expert, you will still gain knowledge from the 45 projects.",
        "Python Developers who want to get started using Machine Learning in a realistic way using numerical or image data sets.",
        "People who want to use API's like Openai GPT4 will need to know python.",
        "For Data Scientists who want to use python to analyze data, instead of 3rd party tools."
      ]
    },
    {
      "title": "Artificial Intelligence Masterclass + ChatGPT Prize [2025]",
      "url": "https://www.udemy.com/course/artificial-intelligence-masterclass/",
      "bio": "Enter the new era of Hybrid AI Models optimized by Deep NeuroEvolution, with a complete toolkit of ML, DL and AI models.",
      "objectives": [
        "How to Build an AI",
        "How to Build a Hybrid Intelligent System",
        "Fully-Connected Neural Networks",
        "Convolutional Neural Networks",
        "Recurrent Neural Networks",
        "AutoEncoders",
        "Variational AutoEncoders",
        "Mixture Density Network",
        "Deep Reinforcement Learning",
        "Policy Gradient",
        "Genetic Algorithms",
        "Evolution Strategies",
        "Covariance-Matrix Adaptation Evolution Strategies (CMA-ES)",
        "Controllers",
        "Meta Learning",
        "Deep NeuroEvolution"
      ],
      "course_content": {
        "Introduction": [
          "Introduction + Course Structure + Demo",
          "Recommended Workshops before we dive in!",
          "Learning Paths",
          "Your Three Best Resources",
          "Download the Resources here",
          "Meet your instructors!",
          "Prizes $$ for Learning"
        ],
        "Step 1 - Artificial Neural Network": [
          "Welcome to Step 1 - Artificial Neural Network",
          "Plan of Attack",
          "The Neuron",
          "The Activation Function",
          "How do Neural Networks work?",
          "How do Neural Networks learn?",
          "Gradient Descent",
          "Stochastic Gradient Descent",
          "Backpropagation"
        ],
        "Step 2 - Convolutional Neural Network": [
          "Welcome to Step 2 - Convolutional Neural Network",
          "Plan of Attack",
          "What are Convolutional Neural Networks?",
          "Step 1 - The Convolution Operation",
          "Step 1 Bis - The ReLU Layer",
          "Step 2 - Pooling",
          "Step 3 - Flattening",
          "Step 4 - Full Connection",
          "Summary",
          "Softmax & Cross-Entropy"
        ],
        "Step 3 - AutoEncoder": [
          "Welcome to Step 3 - AutoEncoder",
          "Plan of Attack",
          "What are AutoEncoders?",
          "A Note on Biases",
          "Training an AutoEncoder",
          "Overcomplete Hidden Layers",
          "Sparse AutoEncoders",
          "Denoising AutoEncoders",
          "Contractive AutoEncoders",
          "Stacked AutoEncoders",
          "Deep AutoEncoders"
        ],
        "Step 4 - Variational AutoEncoder": [
          "Welcome to Step 4 - Variational AutoEncoder",
          "Introduction to the VAE",
          "Variational AutoEncoders",
          "Reparameterization Trick"
        ],
        "Step 5 - Implementing the CNN-VAE": [
          "Welcome to Step 5 - Implementing the CNN-VAE",
          "Introduction to Step 5",
          "Initializing all the parameters and variables of the CNN-VAE class",
          "Building the Encoder part of the VAE",
          "Building the \"V\" part of the VAE",
          "Building the Decoder part of the VAE",
          "Implementing the Training operations",
          "Full Code Section",
          "The Keras Implementation"
        ],
        "Step 6 - Recurrent Neural Network": [
          "Welcome to Step 6 - Recurrent Neural Network",
          "Plan of Attack",
          "What are Recurrent Neural Networks?",
          "The Vanishing Gradient Problem",
          "LSTMs",
          "LSTM Practical Intuition",
          "LSTM Variations"
        ],
        "Step 7 - Mixture Density Network": [
          "Welcome to Step 7 - Mixture Density Network",
          "Introduction to the MDN-RNN",
          "Mixture Density Networks",
          "VAE + MDN-RNN Visualization"
        ],
        "Step 8 - Implementing the MDN-RNN": [
          "Welcome to Step 8 - Implementing the MDN-RNN",
          "Initializing all the parameters and variables of the MDN-RNN class",
          "Building the RNN - Gathering the parameters",
          "Building the RNN - Creating an LSTM cell with Dropout",
          "Building the RNN - Setting up the Input, Target, and Output of the RNN",
          "Building the RNN - Getting the Deterministic Output of the RNN",
          "Building the MDN - Getting the Input, Hidden Layer and Output of the MDN",
          "Building the MDN - Getting the MDN parameters",
          "Implementing the Training operations (Part 1)",
          "Implementing the Training operations (Part 2)",
          "Full Code Section",
          "The Keras Implementation"
        ],
        "Step 9 - Reinforcement Learning": [
          "Welcome to Step 9 - Reinforcement Learning",
          "What is Reinforcement Learning?",
          "A Pseudo Implementation of Reinforcement Learning for the Full World Model",
          "Full Code Section"
        ]
      },
      "requirements": [
        "High school mathematics",
        "A bit of coding experience"
      ],
      "description": "Today, we are bringing you the king of our AI courses:\n\n\nThe Artificial Intelligence MASTERCLASS\n\n\nAre you keen on Artificial Intelligence? Do you want to learn to build the most powerful AI model developed so far and even play against it? Sounds tempting right?\n\n\nThen Artificial Intelligence Masterclass course is the right choice for you. This ultimate AI toolbox is all you need to nail it down with ease. You will get 10 hours step by step guide and the full roadmap which will help you build your own Hybrid AI Model from scratch.\n\n\nIn this course, we will teach you how to develop the most powerful Artificial intelligence model based on the most robust Hybrid Intelligent System. So far this model proves to be the best state of the art AI ever created beating its predecessors at all the AI competitions with incredibly high scores.\n\n\nThis Hybrid Model is aptly named the Full World Model, and it combines all the state of the art models of the different AI branches, including Deep Learning, Deep Reinforcement Learning, Policy Gradient, and even, Deep NeuroEvolution.\n\n\nBy enrolling in this course you will have the opportunity to learn how to combine the below models in order to achieve best performing artificial intelligence system:\n\nFully-Connected Neural Networks\nConvolutional Neural Networks\nRecurrent Neural Networks\nVariational AutoEncoders\nMixed Density Networks\nGenetic Algorithms\nEvolution Strategies\nCovariance Matrix Adaptation Evolution Strategy (CMA-ES)\nParameter-Exploring Policy Gradients\nPlus many others\n\n\nTherefore, you are not getting just another simple artificial intelligence course but all in one package combining a course and a master toolkit, of the most powerful AI models. You will be able to download this toolkit and use it to build hybrid intelligent systems. Hybrid Models are becoming the winners in the AI race, so you must learn how to handle them already.\n\n\nIn addition to all this, we will also give you the full implementations in the two AI frameworks: TensorFlow and Keras. So anytime you want to build an AI for a specific application, you can just grab those model you need in the toolkit, and reuse them for different projects!\nDon’t wait to join us on this EPIC journey in mastering the future of the AI - the hybrid AI Models.",
      "target_audience": [
        "Anyone interested in Artificial Intelligence, Deep Learning, or Machine Learning"
      ]
    },
    {
      "title": "AI Agents: Building Teams of LLM Agents that Work For You",
      "url": "https://www.udemy.com/course/ai-agents-building-teams-of-llm-agents-that-work-for-you/",
      "bio": "AutoGen, ChatGPT API, Streamlit, Google Cloud, build and deploy LLM AI Agents based apps (locally or at scale)",
      "objectives": [
        "Build teams of AI Agents that can achieve complex tasks",
        "Build LLM Agents based Apps",
        "Use ChatGPT's API",
        "Use AutoGen to enable AI Agents to communicate with one another",
        "Build a front-end to communicate with your team of AI Agents (optional)",
        "Run a AI Agent App at scale using Google Cloud (optional)",
        "Set up a payment system to charge users to use your AI Agents based App (optional)"
      ],
      "course_content": {
        "Intro to LLM Agents": [
          "Defining LLMs",
          "Building an Autocomplete",
          "From Autocomplete to LLMs",
          "Prompt Engineering",
          "Agentic Design"
        ],
        "LLM Agents Implementation (with OpenAI's ChatGPT)": [
          "LLM Agents Implementation Intro",
          "OpenAI vs Local LLM",
          "OpenAI setup - Getting API Key",
          "OpenAI Setup - Installation",
          "OpenAI Setup - Creating Python Environment",
          "Local OpenAI setup",
          "Autogen Intro",
          "Autogen Agents",
          "State Altering Agents: Stand-Up Comedy"
        ],
        "[Optional] Running Open Source LLMs Locally (Free) instead of OpenAI's ChatGPT": [
          "Why Run LLMs Locally",
          "AutoGen Agents with LM Studio Server"
        ],
        "LLM Agents Implementation Continued": [
          "Exploring Chat Results",
          "Chat Termination Conditions",
          "Homework"
        ],
        "AutoGen Chat Structures": [
          "Sequential Chats Intro",
          "Sequential Chats Coding",
          "Nested Chats Intro",
          "Single Agent Refinement",
          "Two Agents Refinement",
          "Multi Agents Refinement",
          "Agent Code Execution Environment",
          "Agent Code Execution",
          "Agents that Code with Skill - Intro",
          "Agents that Code with Skill - Execution",
          "Group Chat - Intro",
          "Group Chat - Execution"
        ],
        "Application: Using Agents for Stock Analysis": [
          "Building A Finance App: Application Overview",
          "Setting up your AI Agent Team",
          "Orchestrating the Chat",
          "Running the App"
        ],
        "Deploy Your AI Agent App": [
          "Create a Streamlit App",
          "Stock Analysis as a Streamlit app",
          "Setup a Google Cloud Ephemeral Machine",
          "Run your App from an Ephemeral Machine",
          "Run your Streamlit App",
          "Run your App from a Permanent Machine"
        ],
        "[Optional] Add Subscription - Payments to your App": [
          "st-paywall Setup",
          "Google Oauth Setup",
          "Stripe Setup",
          "Set Redirect URLs",
          "Set st-paywall",
          "Create Stripe Coupon",
          "Deploy your App in Production Mode"
        ],
        "Bonus Lecture": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic Programming Knowledge (we explain all code provided step by step)",
        "No prior knowledge required, everything is shown step by step."
      ],
      "description": "In this course you'll learn about this new way of using LLM Agents: deploying multiple agents to work together as teams to accomplish more complex tasks for you!\nEverything is taught step by step and the course is fully practical with multiple examples and one complete AI Agents-based App that we build together.\nOne of the things we use to accomplish this is ChatGPT's API so we can use ChatGPT through Python.\nWe also use AutoGen to enable our Agents to work together and communicate with one another (to accomplish tasks with no human intervention).\nWe also provide a few optional sections. One of these sections teaches to have a front-end, using Streamlit, to more easily interact with your AI Agents.\nAnother optional section is for those who want to run AI Agents at scale! Here we show you how to deploy your LLM Agents on Google Cloud, so anyone can use your product.\nLastly, one more optional section is available showing how to set up a payment system/subscription model using Stripe for those who want to monetize their AI Agents-based App!\nEverything is explained simply and in a step-by-step approach. All code shown in the course is also provided.\nPlease not that the OpenAI API is not free, you will need to fund your OpenAI developer account with about $5-10 to follow through with the class and build your own app. We clearly show and explain how to do this and minimize your OpenAI costs during this class.",
      "target_audience": [
        "Everyone ready to learn about this brand new way of using LLM Agents"
      ]
    },
    {
      "title": "Master Vector Database with Python for AI & LLM Use Cases",
      "url": "https://www.udemy.com/course/vector-db/",
      "bio": "Learn Vector Database using Python, Pinecone, LangChain, Open AI, Hugging Face and build out AI, ML , Chat applications",
      "objectives": [
        "Pinecone Vector Database, LangChain, Transformer Models for vector embedding, Generative AI, Open AI API Usage, Hugging Face Models",
        "Master the essential techniques for vector data embedding, indexing, and retrieval.",
        "A Practical Code Along with Semantic Search Use Case in Detail with Named Entity Recognition",
        "Developing an AI Chat Bot for Cognitive Search on Private Data Using LangChain",
        "Understand the fundamentals of vector databases and their role in AI, generative AI, and LLM (Language Model Models).",
        "Explore various vector database technologies, including Pinecone, and learn how to set up and configure a vector database environment.",
        "Learn how vector databases enhance AI workflows by enabling efficient similarity search and nearest neighbor retrieval.",
        "Gain practical knowledge on integrating vector databases with Python, utilizing popular libraries like NumPy, Pandas, and scikit-learn.",
        "Implement code along exercises to build and optimize vector indexing systems for real-world applications.",
        "Explore practical use cases of vector databases in AI, generative AI, and LLM, such as recommendation systems, content generation, and language translation.",
        "Understand how vector databases can handle large-scale datasets and support real-time inference.",
        "Gain insights into performance optimization techniques, scalability considerations, and best practices for vector database implementation."
      ],
      "course_content": {
        "Introduction to Vector Database": [
          "Course Overview",
          "Introduction to Vector Database",
          "Why Vector Database",
          "Vector Database Use Cases",
          "Quiz on Introduction to Vector Database"
        ],
        "Vector Database Foundations": [
          "Section Overview",
          "Fundamentals of Vector Database",
          "SQLite Database",
          "Storing and Retrieving Vector Data in SQLite",
          "Vector Similarity Search",
          "Chroma DB-Local Vector Data Base - Part 1: Setup & Data Insertion",
          "Chroma DB-Local Vector Data Base - Part 2: Query",
          "Vector database foundation knowledge check"
        ],
        "Pinecone Vector Database Environment Setup": [
          "Pinecone Account Setup",
          "Pinecone DB Console Overview",
          "Setting Up Development Environment in Windows",
          "Setting Up Development Environment in Ubuntu",
          "A note on Pinecone upgrade (v-2.2.1 to v-5.0.0)",
          "\"Hello World\" Script for Vector DB - [Pinecone v-5.0.0]",
          "[Older version] \"Hello World\" Script for Vector DB - [Pinecone v-2.2.1]",
          "Environment Setup"
        ],
        "Database Operations using Pinecone v5.0.0 [latest as of Oct 2024]": [
          "Database Operations: Create, Retrieve, Update and Deletion (CRUD)",
          "Insert Vectors",
          "Upsert: Insert and Update [updated for Pinecone v5.0.0]",
          "Query Vector Data",
          "Fetch Vectors by ID",
          "Delete Vector"
        ],
        "[Will be archived] Database Operations using Pinecone v2.2.1 (May 2023)": [
          "Database Operations: Create, Retrieve, Update and Deletion (CRUD)",
          "Insert Vectors",
          "Upsert: Insert and Update",
          "Query Vector Data",
          "Fetch Vectors by ID",
          "Delete Vector"
        ],
        "Data Base Management": [
          "Concepts of Index and Collection",
          "Index Management [pinecone 5.0]",
          "Partitioning Vectors",
          "Upsert using Namespace [pinecone v5.0.0]",
          "Vector Partitioning Using Metadata [v5.0.0]",
          "Distance Metrics",
          "Reading Assignment on \"Understanding Indexes\"",
          "Quiz on concepts of Pinecone database management",
          "Index Management [pinecone v2.2.1]"
        ],
        "Project 1: Application in Semantic Search": [
          "Introduction to Semantic Search",
          "Medium Posts Data Obtaining",
          "Data Preprocessing",
          "Preparing for Upsert",
          "Vector Query: \"Semantic Search\"",
          "Quiz on Semantic Search",
          "Reading Assignment: Read Hugging Face Documents on a sentence transformer"
        ],
        "Project 2: Semantic Search Powered by Named Entity": [
          "Concept of Named Entity Recognition (NER)",
          "NER Implementation Examples",
          "Setting up Environment for NER based Semantic Search",
          "Vector Embedding Models and Load Data",
          "Data Preparation",
          "Developing NER Helper Function",
          "Vector Embedding in Batches",
          "NER Extraction in Batches",
          "Metadata Processing",
          "Vector Upsert",
          "Vector Query: Semantic Search with NER"
        ],
        "Project 3: Building AI Chat Agent with LangChain and OpenAI": [
          "Building an Retrieval AI Agent with LangChain and OpenAI",
          "Obtaining OpenAI API",
          "Data Load",
          "Vector Embedding Function",
          "Setup Vector DB",
          "Processing for Meta Data",
          "Embedding and OpenAI Rate Limit Workaround",
          "Indexing",
          "Semantic Search with OpenAI",
          "Embedding with OpenAI and LangChain",
          "Retrieval QA Agent- an example of retrieval augmented generation (RAG)",
          "Chat Agent"
        ],
        "Project 4: Audio Similarity Search": [
          "Environment Setup",
          "Loading Audio Data",
          "Exploring Audio Data",
          "Embedding Audio Data",
          "Setting up Vector Database for Audio",
          "Vector Indexing",
          "Vector Querying",
          "Audio Search with your Own Data: Out of sample search",
          "[older code, will be archived soon] Audio Search out of sample example"
        ]
      },
      "requirements": [
        "Basic understanding of programming concepts and experience with at least one programming language (such as Python, Java).",
        "Good to have familiarity with basic data analysis, machine learning",
        "Familiarity with databases and their basic principles, including tables, queries, and data manipulation.",
        "Good to have familiarity with NumPy, Pandas for data manipulations",
        "A working environment for running code and executing machine learning algorithms, such as Jupyter Notebook, Google Colab, or a local development setup."
      ],
      "description": "In this comprehensive course on Vector Databases, you will delve into the exciting world of cutting-edge technologies that are transforming the field of artificial intelligence (AI), particularly in generative AI. With a focus on Future-Proofing Generative AI, this course will equip you with the knowledge and skills to harness the power of Vector Databases for advanced applications, including Language Model Models (LLM), Generative Pretrained Transformers (GPT) like ChatGPT, and Artificial General Intelligence (AGI) development.\nStarting from the foundations, you will learn the fundamentals of Vector Databases and their role in revolutionizing AI workflows. Through practical examples and hands-on coding exercises, you will explore techniques such as vector data indexing, storage, retrieval, and conditionality reduction. You will also gain proficiency in integrating Pinecone Vector Data Base with other tools like LangChain, OpenAI API using Python to implement real-world use cases and unleash the full potential of Vector Databases.\nThroughout the course, we will uncover the limitless possibilities of Vector Databases in generative AI. You will discover how these databases enable content generation, recommendation systems, language translation, and more. Additionally, we will discuss performance optimization, scalability considerations, and best practices for efficient implementation.\nLed by an expert instructor with a PhD in computational nano science and extensive experience as a data scientist at leading companies, you will benefit from their deep knowledge, practical insights, and passion for teaching AI and Machine Learning (ML). Join us now to embark on this transformative learning journey and position yourself at the forefront of Future-Proofing Generative AI with Vector Databases. Enroll today and unlock a world of AI innovation!",
      "target_audience": [
        "Data engineers, database administrators and data professionals curious about the emerging field of vector databases.",
        "Data scientists and analysts interested in exploring advanced AI techniques.",
        "Machine learning engineers seeking to enhance their knowledge of vector databases and their applications.",
        "AI researchers and practitioners looking to leverage vector databases for generative AI models.",
        "Software developers interested in integrating vector databases into their applications.",
        "Students and academics studying AI, machine learning, or data science who want to expand their knowledge in this specialized area.",
        "Individuals with a technical background or a strong interest in AI and databases, eager to explore cutting-edge technologies shaping the future of AI and ML."
      ]
    },
    {
      "title": "LLM & Generative AI Masterclass: Langchain, HuggingFace",
      "url": "https://www.udemy.com/course/complete-natural-language-processing-nlp-with-spacy-nltk/",
      "bio": "Generative AI, LLM, LangChain, HuggingFace, Ollama, Gradio, OpenAI, Gemini, DeepSeek R1, NotebookLM, Azure AI Services",
      "objectives": [
        "Set up environments for LLMs using Jupyter, Ollama, and Gradio, and integrate APIs like OpenAI GPT and Google Gemini for NLP solutions",
        "Explore Frontier Models like OpenAI GPT and Google's Gemini to create apps for web scraping, text summarization, and meal suggestions",
        "Master Hugging Face for tasks like sentiment analysis, text classification, tokenization, and fine-tuning models using datasets and repositories",
        "Use LangChain for NLP workflows, including prompt engineering, few-shot prompting, and parsing structured outputs like JSON, XML, and CSV",
        "Build web apps with Gradio, such as interactive tools and travel apps, to showcase NLP and AI functionalities",
        "Integrate DeepSeek APIs to develop recipe suggestion systems and explore advanced Python-based AI implementations",
        "Use NotebookLM to analyze and summarize PDFs, videos, CSVs, and audio files, extracting insights and actionable data efficiently.",
        "Master Natural Language Processing concepts, tasks, and tools like Scikit-learn, NLTK, and SpaCy",
        "Learn to build and deploy machine learning models for text classification and sentiment analysis",
        "Explore text summarization techniques to condense long documents into concise summaries",
        "Develop skills in tokenization, stemming, lemmatization, and POS tagging for NLP workflows",
        "Create word embeddings using Word2Vec, Keras, and pre-trained models like Google's",
        "Build deep learning models (ANN, CNN, RNN) for tasks like spam detection and text generation",
        "Analyze tweets by calculating sentiment scores using Twitter API integrations",
        "Work with Python libraries for processing text, PDFs, and datasets (PyPDF2, Pandas, etc.)",
        "Perform data visualization using Matplotlib and conduct analysis with Numpy and Pandas",
        "Implement NLP pipelines for Named Entity Recognition (NER) and vocabulary matching"
      ],
      "course_content": {
        "NEW: Getting Started with LLMs and Exploring Top Models": [
          "Installation and Environment Setup",
          "Getting Started with Jupyter Notebook",
          "Setting Up Ollama for Local LLM Deployment",
          "Building a Custom Model on Top of an Existing Ollama Model",
          "Integrating Ollama API: REST and Python Code Implementation",
          "Travel Application with Ollama & Gradio",
          "Course Resources"
        ],
        "NEW: Build Application using Gradio": [
          "Introduction to Gradio Part 1",
          "Introduction to Gradio Part 2 + Travel app & Gradio"
        ],
        "NEW: Getting Started with Frontier Models": [
          "Getting Started with OpenAI's GPT Model Using Python & API Key",
          "Creating Web Scraping & Text Summarization App Using OpenAI, Python & API Key",
          "Getting Started with Google's Gemini Model Using Python & API Key",
          "Creating a Meal Suggestion Application Using Gemini, Python & API Key"
        ],
        "NEW: DeepSeek": [
          "Using DeepSeek Model Locally via Ollama, on Hugging Face, and via Web Interface",
          "DeepSeek: API Key Setup and Python Integration",
          "DeepSeek: Recipe Suggestion Application",
          "DeepSeek: Temperature Parameter"
        ],
        "NEW: LangChain": [
          "Use LangChain to Interact with OpenAI Models",
          "Use LangChain to Interact with OpenAI Chat Models",
          "Cache Results of Individual LLM Calls using In Memory Cache (InMemoryCache)",
          "Use LangChain to Interact with Ollama Models",
          "Use LangChain to Interact with Google AI Chat Models",
          "LangChain Prompt Templates Part 1",
          "LangChain Prompt Templates Part 2",
          "LangChain Few-shot Prompting",
          "LangChain Save the Prompt",
          "LangChain Output Parsers (CommaSeparatedListOutputParser)",
          "LangChain Output Parsers (JsonOutputParser, XMLOutputParser)",
          "LangChain Output Parsers (DatetimeOutputParser)",
          "LangChain Document Loaders (csv, HTML, pdf)",
          "LangChain Text Splitters (split text by tokens, split by character)",
          "LangChain Text Embedding: Embed Documents (OpenAIEmbeddings)",
          "LangChain Text Embedding: Measure Similarity",
          "LangChain: Create & Query Vector Stores using Chroma Vector Database Part 1",
          "LangChain: Create & Query Vector Stores using Chroma Vector Database Part 2",
          "LangChain: Create & Query Vector Stores using Chroma Vector Database Part 3",
          "LangChain: Basic Single Chain",
          "LangChain: Simple Sequential Chain",
          "LangChain: Sequential Chain",
          "LangChain: LLM Router Chain (LLMRouterChain)",
          "LangChain: Trainform Chain (TrainformChain)",
          "LangChain: OpenAI Function API Chain",
          "LangChain: LLMMathChain Class"
        ],
        "NEW: HuggingFace": [
          "Create HuggingFace Account, Explore Models, Datasets, and Spaces",
          "HuggingFace: Create Your Own Model, Clone Repository",
          "HuggingFace: Download Hugging Face Models to Local Machine",
          "HuggingFace: Create Dataset, Clone Repository",
          "HuggingFace: Text Classification & Sentiment Analysis Models Part 1",
          "HuggingFace: Text Classification & Sentiment Analysis Models Part 2",
          "HuggingFace: Text Generation, Question-Answering Models",
          "HuggingFace: Tokenization",
          "HuggingFace: Fine Tuning",
          "HuggingFace: Text to Image Generation Part 1 (Diffusers, Transformers, Torch)",
          "HuggingFace: Text to Image Generation Part 2 (Diffusers, Transformers, Torch)",
          "HuggingFace: Text to Image Generation (Stable Diffusion Parameters)"
        ],
        "NEW: Google NotebookLM": [
          "Introduction to NotebookLM",
          "NotebookLM Case Study 1: PDF Analysis and Summarization",
          "NotebookLM Case Study 2: Analyzing and Summarizing YouTube Videos",
          "NotebookLM Case Study 3: CSV File Analysis and Interpretation",
          "NotebookLM Case Study 4: Audio Conversation Analysis"
        ],
        "NEW: Prompt Engineering Basics & Best Practices": [
          "Prompt Engineering Basics & Best Practices Part- 1",
          "Prompt Engineering Basics & Best Practices Part- 2",
          "Prompt Engineering Basics & Best Practices Part- 3"
        ],
        "NEW: Generative AI using Google Cloud Platform": [
          "Creating Virtual Environment",
          "Create Service Account",
          "Getting started with Vertex AI Code",
          "Text generation with Gemini Vertex AI Part - 1",
          "Text generation with Gemini Vertex AI Part - 2"
        ],
        "NEW: Azure AI Services (Azure OpenAI Service)": [
          "Create Azure OpenAI Service (Azure AI Services)",
          "Get Started with Azure AI Foundry | Azure OpenAI Service",
          "Chat Playground: Azure AI Foundry | Azure OpenAI Service",
          "Call OpenAI Model API via Python Code: Azure AI Foundry | Azure OpenAI Service",
          "Generate Images with DALL-E Model: Azure AI Foundry | Azure OpenAI Service",
          "Text Embedding Models: Azure AI Foundry | Azure OpenAI Service"
        ]
      },
      "requirements": [
        "Basic understanding of Python Programming"
      ],
      "description": "Update 1.1 as of  29/01/2025\n\n\nGenerative AI\nLLM\nLangChain\nHuggingFace\nOllama\nOpenAI\nGemini\nDeepSeek\nGoogle NotebookLM\nAzure AI Services (Azure OpenAI Service)\n\n\nRecent reviews:\n\"Thorough explanation, going great so far. A very simplistic and straightforward introduction to Natural Language Processing. I will recommend this class to any one looking towards Data Science\"\n\"This course so far is breaking down the content into smart bite-size pieces and the professor explains everything patiently and gives just enough background so that I do not feel lost.\"\n\"This course is really good for me. it is easy to understand and it covers a wide range of NLP topics from the basics, machine learning to Deep Learning.\nThe codes used is practical and useful.\nI definitely satisfy with the content and surely recommend to everyone who is interested in Natural Language Processing\"\n\n\nUpdate 1.0 :\nFasttext Library for Text classification section added.\n\n\nHi Data Lovers,\nDo you have idea about Which Artificial Intelligence field is going to get big in upcoming year?\nAccording to statista dot com which field of AI is predicted to reach $43 billion by 2025?\nIf  answer is 'Natural Language Processing', You are at right place.\n\n\n\n\nDo you want to know\nHow Google News classify millions of news article into hundreds of different category.\nHow Android speech recognition recognize your voice with such high accuracy.\nHow Google Translate actually translate hundreds of pairs of different languages into one another.\nIf answer is \"Yes\", You are on right track.\nand to help yourself, me and my friend Vijay have created comprehensive course  For Students and Professionals to learn Natural Language Processing from very Beginning\n\n\nNLP - \"Natural Language Processing\" has found space in every aspect of our daily life.\nCell phone internet are the integral part of our life. Any most application you will find the use of NLP methods, from search engine of Google to recommendation system of Amazon & Netflix.\nChat-bot\nGoogle Now, Apple Siri, Amazon Alexa\nMachine Translation\nSentiment analysis\nSpeech Recognition and many more.\nSo, welcome to my course on NLP.\nNatural Language Processing (NLP) in Python with 8 Projects\n\n\n\n\nThis course has 18+ Hours of HD Quality video, and following content.\nCourse Outline :\n1 : Welcome In this section we will get complete idea about what we are going to learn in the whole course and understanding related to natural language processing.\n\n\n2 :  Installation & Setup In this section we will get our online environment Google Colab setup.\n\n\n3 : Basics of Natural Language Processing In this section we will dive into all basic NLP task like Tokenization, Lemmatization, stop word removal, name entity   recognition, part of speech tagging, and see how to apply with different functions available in a  Spacy and NLTK library.\n\n\n4, 5, 6 : Spam Message Classification,  Restaurant Review Prediction (Good or bad),  IMDB, Amazon and Yelp review Classification\nIn the next 3 section we will get dive into a real world data set for text classification, spam detection, restaurant review classification, Amazon IMDb reviews. We will see how to do Pre-Processing and make your data suitable for machine learning algorithm and apply different Machine Learning estimator (Logistic Regression, SVM, Decision Tree) for classifying text.\n\n\n7, 8 : Automated Text Summarization,  Twitter sentiment Analysis In this 2 section we will work upon real world application of NLP.\nAutomatic text summarisation, Which compress your text to find the summary of big articles\nAnother one we will work is finding the sentiment from the recently posted tweet about some specific keyword with the help of Twitter API - tweepy library\n\n\n9 : Deep Learning Basics In This Section we will get a basic idea about Deep learning concept, like artificial neural network activation function and how ANN works.\n\n\n10 : Word Embedding In This Section, we will see How to implement word2vec on our custom datasets, as well as using Pretrained Google Model.\n\n\n11, 12 : Text Classification with CNN & RNN In this section we will see how to apply advanced deep learning model like convolution neural networks and recurrent neural networks for text classification.\n\n\n13 : Automatic Text Generation using TensorFlow, Keras and LSTM In this section we will apply neural network based LSTM model to automatically generate text.\n\n\n14, 15, 16, 17 : Numpy, Pandas, Matplotlib + File Processing In this section, for all of you who want refresh concept related to data analysis with Numpy and Pandas library, Data Visualization with Matplotlib library, and Text File processing and PDF File processing.\n\n\n\n\nSo, This is the one of the most comprehensive course on natural language processing,\nAnd I am expecting you to know basic knowledge of python and your curiosity to learn Different techniques in NLP world.\n\n\nYOU'LL ALSO GET:\nLifetime access to Natural Language Processing (NLP) with Python Course\nUdemy Certificate of Completion available for download\nFriendly support in the Q&A section\n\n\nSo What Are You Waiting For ? Enroll today! and Empower Your Career !\nI can't wait for you to get started on mastering NLP with Python.\nStart analyzing your text data & I will see you inside a class.\n\n\nRegards\nAnkit & Vijay",
      "target_audience": [
        "Anyone who is interested to learn Large Language Model and Generative AI"
      ]
    },
    {
      "title": "Generative AI: OpenAI API, DeepSeek, and ChatGPT in Python",
      "url": "https://www.udemy.com/course/genai-openai-chatgpt/",
      "bio": "Empower Your Business With GenAI, Artificial Intelligence, GPT-4o, o1, o3, DeepSeek, and More!",
      "objectives": [
        "How to setup and use the OpenAI API with ChatGPT",
        "How to effectively use prompt engineering",
        "RAG (retrieval-augmented generation) with the OpenAI Embeddings API",
        "FAISS (Facebook AI Similarity Search)",
        "How to Fine-Tune ChatGPT"
      ],
      "course_content": {
        "Welcome!": [
          "Introduction and Outline",
          "Where To Get the Code",
          "How to Succeed in this Course"
        ],
        "The ChatGPT, DeepSeek and OpenAI API": [
          "The ChatGPT, DeepSeek and OpenAI API Section Outline",
          "UPDATE: GPT-4o mini",
          "UPDATE: DeepSeek",
          "Signup For OpenAI & Get Your API Key",
          "Set Environment Variables (Windows)",
          "Set Environment Variables (Linux & Mac)",
          "Install the OpenAI and Tiktoken Libraries",
          "Pricing",
          "How to Pay & Add Billing Details",
          "Quick Start (Use the ChatGPT API Right Away!)",
          "What is a Token?",
          "Estimating Costs with Tiktoken",
          "Reproducibility",
          "System Prompt",
          "Incorporating History",
          "Temperature",
          "Frequency and Presence Penalties",
          "How LLMs Conquered NLP",
          "How LLMs Change the NLP & Machine Learning Workflow",
          "Usage Policies",
          "Suggestion Box"
        ],
        "Prompt Engineering & Applications": [
          "Prompt Engineering Section Outline",
          "Unstructured to Structured",
          "Structured to Unstructured",
          "JSON Mode",
          "Translation, Tone Enhancement, and Language Detection",
          "Sentiment Analysis & Stock Trading",
          "Summarization & ELI5",
          "Another ELI5 Example",
          "Question-Answering",
          "Chain of Thought Prompting"
        ],
        "Retrieval Augmented Generation (RAG)": [
          "What is Semantic Search?",
          "Facebook AI Similarity Search (FAISS)",
          "OpenAI's Embeddings Endpoint",
          "RAG with FAISS (pt 1)",
          "RAG with FAISS (pt 2)",
          "RAG with FAISS (pt 3)",
          "RAG with FAISS (pt 4)"
        ],
        "Fine-Tuning": [
          "What Is Fine-Tuning and When Should You Use It?",
          "OpenAI's Fine-Tuning and File Endpoints",
          "Fine-Tuning ChatGPT - Making the Dataset",
          "Fine-Tuning ChatGPT - Upload Data and Create Fine-Tuning Job",
          "Fine-Tuning ChatGPT - Check the Results"
        ],
        "Background Info (Optional)": [
          "The Roots of Semantic Search",
          "Who Should Take This Course?"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (Appendix/FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (Appendix/FAQ by Student Request)": [
          "Proof that using Jupyter Notebook is the same as not using it",
          "Where To Get the Code Troubleshooting",
          "How to use Github & Extra Coding Tips (Optional)"
        ],
        "Effective Learning Strategies for Machine Learning (Appendix/FAQ by Student Requ": [
          "How to Succeed in this Course (Long Version)",
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "What order should I take your courses in? (part 1)",
          "What order should I take your courses in? (part 2)"
        ]
      },
      "requirements": [
        "Python programming"
      ],
      "description": "Welcome to the forefront of artificial intelligence with our groundbreaking course on Generative AI (GenAI), the OpenAI API, DeepSeek, and ChatGPT. With ChatGPT and DeepSeek, you'll learn how to build with the world's most advanced Large Language Models (LLMs). This course is a must-have if you want to know how to use this cutting-edge technology for your business and work projects.\nThis course contains 5 main sections:\n\n\nBasic API Usage: All the fundamentals: signup for an account, get your API key, set environment variables on Windows / Linux / Mac, using the API in Python, setup billing, understand the pricing model, and OpenAI's usage policies. Of note is the chatbot tutorial, which goes over how to incorporate chat history into the model so that ChatGPT \"remembers\" what it said to you previously. A customer service chatbot will serve as a running example throughout this course.\nPrompt Engineering: ChatGPT Prompt Engineering for Developers - All about how to make ChatGPT do what you want it to do. We'll explore various example use-cases, such as getting ChatGPT to output structured data (JSON, tables), sentiment analysis, language translation, creative writing, text summarization, and question-answering. We'll explore techniques like chain-of-thought (CoT) prompting, and we'll even look at how to use ChatGPT to build a stock trading system!\nRetrieval Augmented Generation (RAG): Learn how to incorporate external data into LLMs. This powerful technique helps mitigate a common problem called \"hallucination\". It's critical if you have proprietary data (like product info for your company) that your LLM doesn't know about. You'll learn how semantic search / similarity search works, and how to implement it using FAISS (Facebook AI Similarity Search library). Learn how this will allow you to \"chat with your data\".\nFine-Tuning: Learn how to \"train\" an LLM on your own dataset so that it behaves the way you want it to. Sometimes prompt engineering and RAG won't cut it.\nGPT-4 with Vision: Everything in this course can be done with GPT-4, but what makes GPT-4 (and GPT-4 Turbo) special is its vision capabilities. That is, it can understand images. In this section, we'll explore many of the amazing applications of combined text-image understanding, some of which include automated homework grading, explaining memes and humor, handwriting transcription, web development, game development, and writing product descriptions based on images (business owners - you already know how this will skyrocket your productivity).\n\n\nThroughout this course, you'll engage in hands-on exercises, real-world applications, and expert guidance to solidify your understanding and mastery of generative AI concepts. Whether you're a seasoned developer, aspiring AI enthusiast, or industry professional, this course offers a transformative experience that will empower you to harness the true potential of AI.\nAre you ready to embark on this exhilarating journey into the future of AI? Join us and unlock the endless possibilities of Generative AI today!\n\n\nSuggested Prerequisites:\nPython coding",
      "target_audience": [
        "Any level (beginner to expert) of student who wants to use the OpenAI API",
        "Professionals who want to integrate ChatGPT automation into their business",
        "Entrepreneurs who want to start their own AI business (customer service bots, AI girlfriends / AI boyfriends, etc.)"
      ]
    },
    {
      "title": "Deep Learning :Adv. Computer Vision (object detection+more!)",
      "url": "https://www.udemy.com/course/advanced-computer-vision-transfer-learning-with-tensorflow/",
      "bio": "Transfer Learning, TensorFlow Object detection, Classification, Yolo object detection, real time projects much more..!!",
      "objectives": [
        "computer vision",
        "deep learning",
        "TensorFlow"
      ],
      "course_content": {},
      "requirements": [
        "Python"
      ],
      "description": "Latest update: I will show you both how to use a pretrained model and how to train one yourself with a custom dataset on Google Colab.\nThis course is a complete guide for setting up TensorFlow object detection api, Transfer learning and a lot more\n\n\nI think what you’ll find is that, this course is so entirely different from the previous one, you will be impressed at just how much material we have to cover.\nHere is the details about the project.\nHere we will star from colab understating because that will help to use free GPU provided by google to train up our model.\nWe’re going to bridge the gap between the basic CNN architecture you already know and love, to modern, novel architectures such as ResNet, and Inception.\nWe will understand object detection modules in detail using both tensorflow object detection api as well as YOLO algorithms.\nWe’ll be looking at a state-of-the-art algorithm called RESNET and MobileNetV2 which is both faster and more accurate than its predecessors.\nOne best thing is you will understand the core basics of CNN and how it converts to object detection slowly.\nI hope you’re excited to learn about these advanced applications of CNNs Yolo and Tensorflow, I’ll see you in class!\n\n\nAMAGING FACTS:\n· This course give’s you full hand’s on experience of training models in colab GPU.\n· Instead of focusing on the detailed inner workings of CNNs (which we've already done), we'll focus on high-level building blocks. The result? Almost zero math.\n· Another result? No complicated low-level code such as that written in Tensorflow, Theano,YOLO, or PyTorch (although some optional exercises may contain them for the very advanced students). Most of the course will be in Keras which means a lot of the tedious, repetitive stuff is written for you.\n\n\n\n\nSuggested Prerequisites:\n· Know how to build, train, and use a CNN using some library (preferably in Python)\n· Understand basic theoretical concepts behind convolution and neural networks\n· Decent Python coding skills, preferably in data science and the Numpy Stack\n\n\nWho this course is for:\n· Students and professionals who want to take their knowledge of computer vision and deep learning to the next level\n· Anyone who wants to learn about object detection algorithms like SSD and YOLO\n· Anyone who wants to learn how to write code for neural style transfer\n· Anyone who wants to use transfer learning\n· Anyone who wants to shorten training time and build state-of-the-art computer vision nets fast\n· Anyone who is starting with computer vison",
      "target_audience": [
        "Python developers curious about deep learning",
        "Developers curious about computer vision"
      ]
    },
    {
      "title": "2025 Python Data Analysis & Visualization Masterclass",
      "url": "https://www.udemy.com/course/python-data-analysis-visualization/",
      "bio": "Pandas, Matplotlib, Seaborn, & More! Analyze Dozens of Datasets & Create Stunning Visualizations",
      "objectives": [
        "Master Pandas Dataframes and Series",
        "Create beautiful visualizations with Seaborn",
        "Analyze dozens of real-world datasets",
        "Practice with tons of exercises and challenges",
        "Learn the ins and outs of Matplotlib",
        "Organize, filter, clean, aggregate, and analyze DataFrames",
        "Master Hierarchical Indexing",
        "Merge datasets together in Pandas",
        "Create line, bar, box, scatter, pie, violin, rug, swarm, strip, and other plots!",
        "Work with Jupyter Notebooks"
      ],
      "course_content": {},
      "requirements": [
        "Basic Python Knowledge (variables, conditionals, etc)"
      ],
      "description": "Welcome to (what I think is) the web's best course on Pandas, Matplotlib, Seaborn, and more! This course will level up your data skills to help you grow your career in Data Science, Machine Learning, Finance, Web Development, or any tech-adjacent field.\nThis is a tightly structured course that covers a ton, but it's all broken down into human-sized pieces rather than an overwhelming reference manual that throws everything at you at once. After each and every new topic, you'll have the chance to practice what you're learning and challenge yourself with exercises and projects. We work with dozens of fun and real-world datasets including Amazon bestsellers, Rivian stock prices, Presidential Tweets, Bitcoin historic data, and UFO sightings.\nIf you're still reading, let me tell you a little about the curriculum.. In the course, you'll learn how to:\nWork with Jupyter Notebooks\nUse Pandas to read and manipulate datasets\nWork with DataFrames and Series objects\nOrganize, filter, clean, aggregate, and analyze DataFrames\nExtract and manipulate date, time, and textual information from data\nMaster Hierarchical Indexing\nMerge datasets together in Pandas\nCreate complex visualizations with Matplotlib\nUse Seaborn to craft stunning and meaningful visualizations\nCreate line, bar, box, scatter, pie, violin, rug, swarm, strip, and other plots!\nWhat makes this course different from other courses on the same topics?  First and foremost, this course integrates visualizations as soon as possible rather than tacking it on at the end, as many other courses do.  You'll be creating your first plots within the first couple of sections!  Additionally, we start using real datasets from the get go, unlike most other courses which spend hours working with dull, fake data (colors, animals, etc) before you ever see your first real dataset.  With all of that said, I feel bad trash talking my competitors, as there are quite a few great courses on the platform :)\nI think that about wraps it up! The topics in this courses are extremely visual and immediate, which makes them a joy to teach (and hopefully for you to learn).   If you have even a passing interest in these topics, you'll likely enjoy the course and tear through it quickly.  This stuff might seem intimidating, but it's actually really approachable and fun! I'm not kidding when I say this is my favorite course I've ever made. I hope you enjoy it too.",
      "target_audience": [
        "Beginner Python devs curious about data analysis, data visualization, or data science"
      ]
    },
    {
      "title": "Statistics in R - The R Language for Statistical Analysis",
      "url": "https://www.udemy.com/course/statisticsinr/",
      "bio": "Statistics made easy with the open source R language. Learn about Regression, Hypothesis tests, R Commander ...",
      "objectives": [
        "know which statistical test to use for a given question",
        "know how to perform the most important statistical tests in R",
        "know how to perform regression modeling in R",
        "have a very good understand of statistical testing and regressions",
        "use R Commander as alternative to RStudio",
        "perform stats analysis on outliers"
      ],
      "course_content": {},
      "requirements": [
        "solid understanding of R programming - up to my \"R Level 1\" course",
        "R and R Studio ready on your computer",
        "basic understanding of statistics (descriptive, inferential, regression)",
        "high interest in data analysis"
      ],
      "description": "Do you want to learn more about statistical programming?\nAre you in a quantitative field?\nYou want to know how to perform statistical tests and regressions?\nDo you want to hack the learning curve and stay ahead of your competition?\nIf YES came to your mind to some of those points - read on!\nThis tutorial will teach you anything you need to know about descriptive and inferential statistics as well as regression modeling in R.\nWhile planing this course we were focusing on the most important inferential tests that cover the most common statistical questions.\nAfter finishing this course you will understand when to use which specific test and you will also be able to perform these tests in R.\nFurthermore you will also get a very good understanding of regression modeling in R. You will learn about multiple linear regressions as well as logistic regressions.\nAccording to the teaching principles of R Tutorials every section is enforced with exercises for a better learning experience. You can download the code pdf of every section to try the presented code on your own.\nShould you need a more basic course on R programming we would highly recommend our R Level 1 course. The Level 1 course covers all the basic coding strategies that are essential for your day to day programming.\nWhat R you waiting for?\nMartin",
      "target_audience": [
        "students who need data analysis in their work",
        "data analysts",
        "entrepreneurs with quantitative interests",
        "everybody interested in statistics"
      ]
    },
    {
      "title": "Natural Language Processing (NLP) Mastery in Python",
      "url": "https://www.udemy.com/course/nlp-in-python/",
      "bio": "Text Cleaning, Spacy, NLTK, Scikit-Learn, Deep Learning, word2vec, GloVe, LSTM for Sentiment, Emotion, Spam, CV Parsing",
      "objectives": [
        "Learn complete text processing with Python",
        "Learn how to extract text from PDF files",
        "Use Regular Expressions for search in text",
        "Use SpaCy and NLTK to extract complete text features from raw text",
        "Use Latent Dirichlet Allocation for Topic Modelling",
        "Use Scikit-Learn and Deep Learning for Text Classification",
        "Learn Multi-Class and Multi-Label Text Classification",
        "Use Spacy and NLTK for Sentiment Analysis",
        "Understand and Build word2vec and GloVe based ML models",
        "Use Gensim to obtain pretrained word vectors and compute similarities and analogies",
        "Learn Text Summarization and Text Generation using LSTM and GRU",
        "Understand the basic concepts and techniques of natural language processing and their applications.",
        "Learn how to use Python and its popular libraries such as NLTK and spaCy to perform common NLP tasks.",
        "Be able to tokenize and stem text data using Python.",
        "Understand and apply common NLP techniques such as sentiment analysis, text classification, and named entity recognition.",
        "Learn how to apply NLP techniques to real-world problems and projects.",
        "Understand the concept of topic modeling and implement it using Python.",
        "Learn the basics of text summarization and its implementation using Python.",
        "Understand the concept of text generation and implement it using Python",
        "Understand the concept of text-to-speech and speech-to-text conversion and implement them using Python.",
        "Learn how to use deep learning techniques for NLP such as RNN, LSTM, and word embedding."
      ],
      "course_content": {},
      "requirements": [
        "Have a desire to learn",
        "Elementary level math",
        "Have basic understanding of Python and Machine Learning"
      ],
      "description": "This comprehensive course will teach you Natural Language Processing (NLP) from scratch, leveraging Python for beginners. With over 38 hours of engaging content, this course is a hands-on learning journey that covers fundamental techniques and tools to process text data and deploy machine learning models. By the end of the course, you'll gain valuable skills to implement text processing, machine learning, deep learning, and text classification models.\n\n\nIntroduction:\nStart your journey with a gentle introduction to machine learning principles. You'll get a clear overview of this exciting field before jumping into installing all necessary software like Anaconda, Python, VS Code, and Git Bash. With step-by-step instructions for different operating systems (Windows, Ubuntu, and Mac), you'll be equipped to run Python code seamlessly using Jupyter Notebooks.\n\n\nPython Crash Course for Machine Learning:\nBuild a solid foundation in Python, specifically tailored for machine learning. Learn Python data types, control flow, loops, functions, and error handling. You'll master using lists, dictionaries, sets, and tuples effectively, enabling you to write clean, efficient code in no time.\n\n\nNumpy Crash Course for Machine Learning:\nGain proficiency in Numpy, the essential library for numerical computing in Python. Learn how to create, manipulate, and perform statistical operations on arrays. You’ll also understand how to work with multidimensional arrays, reshaping them, and performing advanced operations like sorting and handling NaN values, key to working with datasets in ML.\n\n\nPandas Crash Course for Machine Learning:\nIn this section, you’ll dive into Pandas, a critical tool for data manipulation and analysis. Learn how to load, filter, slice, and clean your data using advanced techniques like Groupby, Aggregation, and merging. You'll also focus on handling missing data and effectively preparing data for ML algorithms.\n\n\nWorking with Text Files:\nUnderstand how to handle a variety of file formats, from basic text files to CSV, Excel, and JSON files. You’ll explore how to write, read, and process these files to extract and prepare the information for Machine Learning tasks. Special focus will be given to cleaning and extracting data from complex files like PDFs and audio files.\n\n\nMastering Regular Expressions with Python:\nLearn the power of Regular Expressions (Regex) to clean and preprocess text data efficiently. This section covers pattern matching, extracting relevant information, and working with text data using regex functions in Python.\n\n\nSpacy Introduction for Text Processing:\nDiscover Spacy, an industry-standard library for text processing and NLP. You’ll learn how to tokenize, tag parts of speech (POS), and extract named entities like person names and locations using Spacy’s pre-built models. These tools will be crucial in processing large amounts of text data.\n\n\nNLTK for Text Processing:\nExplore the Natural Language Toolkit (NLTK) for text processing. Learn tokenization, stemming, and lemmatization. You'll also get hands-on with Named Entity Recognition (NER), chunking, and identifying collocations in text data.\n\n\nComplete Text Cleaning and Text Processing:\nGo deep into text cleaning with a full overview of common cleaning tasks, such as removing URLs, mentions, hashtags, and stopwords, as well as expanding contractions. You'll also be introduced to advanced tasks like spelling correction, word cloud visualizations, and sentiment analysis using the TextBlob library.\n\n\nMake Your Own Text Processing Python Package:\nThis section empowers you to build your own Python package. After setting up your project directory and necessary files, you'll implement methods to encapsulate your text processing workflows. Learn the significance of tools like setup[dot]py for package distribution.\n\n\nPublish Your Python Package on PyPi for Easy Installation:\nLearn the process of publishing your text processing package on PyPi, making it easy for others to install via pip. This section walks you through creating GitHub repositories, uploading your work, and sharing your package for open-source usage.\n\n\nLinear Regression and Interview Questions:\nGain insights into one of the foundational machine learning algorithms—Linear Regression. Learn how to code it for tasks like predicting housing prices and using evaluation metrics like Mean Squared Error (MSE). You’ll also explore common interview questions on regression models.\n\n\nLogistic Regression and Interview Questions:\nDelve into Logistic Regression, understanding how it works for binary classification tasks like predicting whether a tumor is malignant or benign. Get ready to answer key questions about cost functions, entropy, and overfitting.\n\n\nSVM, KNN, Decision Tree, Random Forest and Interview Questions:\nIn this section, understand some of the most common machine learning classifiers, such as Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Trees. You will train models and fine-tune them for optimal performance.\n\n\nSpam Text Classification:\nLearn how to build a spam email classifier using classic techniques like Bag of Words (BoW) and TF-IDF. You'll explore the process from feature extraction, data loading, model training, and evaluation.\n\n\nSentiment Analysis on IMDB Movie Reviews:\nExplore sentiment analysis by predicting movie reviews from IMDB. You’ll use TF-IDF and various machine learning models like Logistic Regression and SVM for analysis, gaining crucial insights into working with text sentiment classification tasks.\n\n\nML Model Deployment with Flask:\nLearn how to deploy machine learning models as a web application using Flask. This section covers setting up a Flask server, running your ML models on it, and deploying your machine learning API for real-time prediction.\n\n\nMulti-Label Text Classification for Tag Prediction:\nMaster multi-label classification, a technique where each instance can belong to more than one label. You'll apply it to the Stack Overflow dataset, focusing on predicting multiple tags for a post.\n\n\nSentiment Analysis using Word2Vec Embeddings:\nDive deeper into word embeddings like Word2Vec and GloVe to enhance your sentiment analysis models. By training machine learning algorithms using these word vectors, you'll increase the performance and accuracy of your models.\n\n\nResume Parsing with Spacy:\nLearn to implement Named Entity Recognition (NER) using Spacy for parsing Resumes (CVs). This powerful skill can automate tasks such as extracting key information from resumes, which is highly applicable in talent acquisition or HR automation.\n\n\nDeep Learning for Sentiment Analysis:\nExplore Deep Learning techniques for text sentiment analysis, including building and training an Artificial Neural Network (ANN) and a Convolutional Neural Network (CNN). Understand why deep learning models are so effective in working with complex text data.\n\n\nHate Speech Classification using Deep Learning:\nFocus on Deep Learning for classifying text, especially for applications like hate speech detection. By building a model using CNN, you will classify tweets and gain understanding of building powerful models for text categorization.\n\n\nPoetry Generation Using LSTM and TensorFlow/Keras:\nExplore how to generate text automatically with Long Short-Term Memory (LSTM) networks using TensorFlow and Keras. By training your models on poetry datasets, you’ll understand how to create creative applications in the field of text generation.\n\n\nDisaster Tweets Classification Using Deep Learning:\nLearn how to classify Disaster Tweets with deep learning and embeddings. This project helps you see how sentiment analysis can be scaled to real-world scenarios with a focus on disaster management communication analysis.\nEach section of this course will enrich your knowledge and prepare you for hands-on tasks in Natural Language Processing and Machine Learning, creating opportunities to master real-world projects and prepare for job-ready NLP tasks.\n\n\nNote:\nThis course requires you to download Anaconda and/or Docker Desktop from external websites. If you are a Udemy Business user, please check with your employer before downloading software.",
      "target_audience": [
        "Beginners in Natural Language Processing",
        "Data Scientist curious to learn NLP",
        "Individuals with a basic understanding of Python programming who want to expand their skills to include natural language processing",
        "Data scientists, data analysts, and researchers who want to add NLP to their toolkit",
        "Developers who want to build applications that involve natural language processing, such as chatbots or text-based recommender systems",
        "Students and professionals in fields such as linguistics, computer science, and artificial intelligence who want to gain a deeper understanding of NLP"
      ]
    },
    {
      "title": "Microsoft Power BI: Beginner to Advanced Intelligence",
      "url": "https://www.udemy.com/course/microsoft-power-bi-for-data-science/",
      "bio": "Become a Data Analyst, Data Scientist | DAX, Power Query | Real world projects | Fun and Engaging",
      "objectives": [
        "Learn to use the easiest Business Intelligence tool to create stunning reports and dashboards.",
        "Learn how to use the most important DAX functions",
        "Understand the concepts behind Measures & Calculated Columns in DAX",
        "Build business intelligence solutions to Explore and Analyze data like never before"
      ],
      "course_content": {
        "Welcome to the course": [
          "The Who, What, and Why?",
          "Course outline",
          "Optimize your learning",
          "Read me if you are on a Mac",
          "Install Power BI Desktop",
          "First look at Power BI",
          "Update paths in Course Material",
          "Get course material",
          "All set, let's get started"
        ],
        "Prepare Data": [
          "Overview",
          "Introduction to PowerBI Table View",
          "Enter data",
          "Read Excel",
          "Read CSV",
          "TRY IT",
          "TRY IT - solution"
        ],
        "Data Types": [
          "Introduction",
          "Numbers",
          "Texts",
          "Dates",
          "Booleans and blanks"
        ],
        "Visual Foundamentals": [
          "Overview of visuals",
          "Visual editor interface",
          "Bar chart",
          "Line chart",
          "Pie chart",
          "Understand chart data",
          "Filter chart data",
          "Card visual",
          "Table visual",
          "TRY IT: fun with visuals",
          "Solution for TRY IT: fun with visuals"
        ],
        "Mini Project: Personal Finance Dashboard (No DAX required)": [
          "Introduction to your first project",
          "(Optional) download your own spending data",
          "Visualize the expenditures",
          "Analyze insights from the data"
        ],
        "DAX Foundamentals": [
          "Overview of DAX",
          "Try your first DAX",
          "Define measures",
          "Calculation context",
          "Understand DAX in visuals",
          "Rename and delete measures and columns",
          "Aggregate values",
          "Aggregate numbers - sum, mean",
          "Aggregate texts - min, max, count",
          "Aggregate dates",
          "Conditional functions - if, else",
          "Conditional functions - switch",
          "Compare values",
          "CALCULATE syntax",
          "CALCULATE with filters",
          "Understand more about CALCULATE",
          "Implicit contexts in visuals"
        ],
        "Power Query Zero-to-Hero": [
          "Put \"power\" in your queries",
          "Overview of Power Query editor",
          "Remove rows overview",
          "Remove top rows",
          "Remove bottom alternate and empty rows",
          "Remove duplicated rows",
          "Remove & rename columns",
          "Replace values",
          "Filter rows",
          "Filter rows, but tricky",
          "TRY: Clean data with filter and remove",
          "SOLUTION: Clean data with filter and remove",
          "Convert data types",
          "Fix common errors in Power Query",
          "Best practice: do it in Power Query or DAX?",
          "TRY: Transform the perosnal finance data",
          "SOLUTION: Transform the perosnal finance data"
        ],
        "Real Life Project: CityLife Sales Dashboard": [
          "When you got a minute, could you...",
          "Let's plan the dashboard",
          "Import data",
          "Build visuals",
          "Validate data",
          "Style the visuals"
        ],
        "Data Models and Advanced Table Relationships": [
          "Why do we need data models?",
          "Relationship as filter",
          "Directional relationship",
          "One-one, one-many, many-many",
          "Active and inactive relationships",
          "Configure relationships",
          "Organize data as One Big Table",
          "Organize table as Star Schema",
          "Best Practices in using One Big Table vs .Star Schema",
          "Master relationships with RELATED",
          "Master relationships with USERELATION",
          "Master relationships with CROSSFILTER",
          "TRY IT - Connect the tables",
          "Solution for TRY IT - Connect the tables"
        ],
        "DAX Advanced": [
          "Context modifier - ALL",
          "Context modifier - ALLEXCEPT",
          "Context modifier - REMOVEFILTERS",
          "Get filter context with SELECTEDVALUE",
          "Concatenate Texts",
          "Split Texts",
          "Replace Texts",
          "Text Transformation - UPPER, LOWER, TRIM",
          "TRY IT - Mutual Fund Sales",
          "Solution for TRY IT - Mutual Fund Sales",
          "DAX variables",
          "DAX Variables in Calculated Columns"
        ]
      },
      "requirements": [
        "Power BI can be downloaded for free, no additional costs involved."
      ],
      "description": "Master Power BI and Unlock Real-World Data Insights!\nJoin the growing community of successful students:\n\n\n★★★★★ “Clear, useful, and easy to follow!”\n\"Great course so far really useful with lots of clear instruction, easy to follow along with the provided course materials.\"\n— Faith V.\n\n\n★★★★★ “Master Power BI with real-world projects!”\n“A fantastic course for anyone looking to master Power BI! The projects are real-world relevant!”\n— Itzayana W.\n\n\n★★★★☆ “Practical and impactful!”\n\"Good, the power bi workbooks are great resources for me to practice and i was able to leverage those books to build something for the my team.\"\n— Dayton P.\n\n\n★★★★★ “From beginner to confident user!”\n“I had never worked with Power BI, but this course guided me through transforming raw data into meaningful insights. It motivated me to enroll in the advanced analytics course and explore database management!”\n— Phoenix G.\n\n\nLifetime Access to Course Materials | 100% Money-Back Guarantee\n\n\nIn an era where data reigns supreme, mastering Business Intelligence tools can set you apart in the job market. Enter Microsoft Power BI, the leading tool for transforming raw data into insightful analytics, used by top analysts and data scientists worldwide.\n\n\nWhat You'll Gain from This Course:\n\n\nStep-by-Step Learning: Dive into real-world business scenarios with SQL Server Adventure Works Data Warehouse as our playground. We'll guide you through each step, ensuring you grasp how to leverage Power BI and advanced DAX calculations to unearth deep data insights.\nExclusive Content: Not only will you master Power BI's best practices, tips, tricks, and case studies unavailable in other courses, but you'll also get exclusive access to learn Microsoft's acclaimed \"Dashboard in a Day\" at your own pace.\nHands-On Experience: With over 25 hours of HD video content, you'll build 10 interactive BI reports and dashboards from scratch. Start with raw data and finish with polished, actionable reports. Plus, get both start and finished project files to test your skills as you learn.\nCareer Advancement: By the end of this course, you'll not just understand Power BI; you'll be proficient in DAX, ready to tackle complex business problems with confidence. Transition into or elevate your role as a Business Data Analyst, where you'll enjoy:\n\n\nHigh Salary Potential: Leverage one of the most sought-after skills in today's market.\nJob Stability: In a world awash with data, your skills will always be in demand.\nProfessional Growth: Engage with various departments, never stop learning, and become indispensable, opening doors to numerous career opportunities.\n\n\nWhy Choose This Course?\n\n\nPractical Application: Every lesson is designed with real-world application in mind, ensuring what you learn is directly applicable to your job or business.\nSupportive Learning Environment: We're here to support you through each scenario, making complex data analysis accessible and understandable.\nFuture-Proof Your Career: With data becoming ever more critical, your expertise in Power BI will keep you at the forefront of the Business Intelligence field.\n\n\nJoin us to not just learn Power BI but to master it. Transform data into decisions, insights into action, and become the data hero your company needs.\n\n\n#PowerBI #DataAnalytics #BusinessIntelligence #CareerGrowth #DAXMastery",
      "target_audience": [
        "Anyone looking to get a job and prepare to start data analytics career."
      ]
    },
    {
      "title": "Learn Machine Learning in 21 Days",
      "url": "https://www.udemy.com/course/learn-machine-learning-in-21-days/",
      "bio": "Learn to create Machine Learning Algorithms in Python Data Science enthusiasts. Code templates included.",
      "objectives": [
        "Master Machine Learning on Python",
        "Make accurate predictions",
        "Make robust Machine Learning models",
        "Use Machine Learning for personal purpose",
        "Have a great intuition of many Machine Learning models",
        "Know which Machine Learning model to choose for each type of problem",
        "Use SciKit-Learn for Machine Learning Tasks",
        "Make predictions using linear regression, polynomial regression, and multiple regression",
        "Classify data using K-Means clustering, Support Vector Machines (SVM), KNN, Decision Trees, Naive Bayes, etc."
      ],
      "course_content": {
        "Introduction": [
          "What is ML? Application & Types of ML"
        ],
        "Data Preprocessing Techniques": [
          "What is NumPy?",
          "Data Manipulation with Pandas"
        ],
        "Regression": [
          "Simple Linear Regression",
          "Multiple Linear Regression",
          "Polynomial Regression",
          "Support Vector Regression(SVR)",
          "Decision Tree Regression",
          "Random Forest Regression",
          "Regression Mini Project"
        ],
        "Classification": [
          "Logistic Regression",
          "K-Nearest Neighbour",
          "Support Vector Machine (SVM)",
          "Kernel SVM",
          "Naive Bayes Classification",
          "Decision Tree Classification",
          "Random Forest Classification",
          "Classification Mini Project"
        ],
        "Problems With ML": [
          "Underfitting and Overfitting"
        ],
        "Model Selection": [
          "Cross Validation And Grid Search"
        ],
        "Model Deployment": [
          "ML Model With Deployment"
        ]
      },
      "requirements": [
        "Some basic python programming experience.",
        "Basic understanding of python libraries like numpy, pasdas and matplotlib.(Optional)",
        "Some high school mathematics."
      ],
      "description": "Interested in the field of Machine Learning? Then this course is for you!\nThis course has been designed by Code Warriors the ML Enthusiasts so that we can share our knowledge and help you learn complex theories, algorithms, and coding libraries in a simple way.\nWe will walk you step-by-step into the World of Machine Learning. With every tutorial, you will develop new skills and improve your understanding of this challenging yet lucrative sub-field of Data Science.\nThis course is fun and exciting, but at the same time, we dive deep into Machine Learning. It is structured the following way:\nYou can do a lot in 21 Days. Actually, it’s the perfect number of days required to adopt a new habit!\nWhat you'll learn:-\n1.Machine Learning Overview\n2.Regression Algorithms on the real-time dataset\n3.Regression Miniproject\n4.Classification Algorithms on the real-time dataset\n5.Classification Miniproject\n6.Model Fine-Tuning\n7.Deployment of the ML model",
      "target_audience": [
        "Anyone interested in Machine Learning.",
        "Students who have at least high school knowledge in math and who want to start learning Machine Learning.",
        "Any intermediate level people who know the basics of machine learning, including the classical algorithms like linear regression or logistic regression, but who want to learn more about it and explore all the different fields of Machine Learning.",
        "Any people who are not that comfortable with coding but who are interested in Machine Learning and want to apply it easily on datasets.",
        "Any students in college who want to start a career in Data Science.",
        "Any people who want to create added value to their business by using powerful Machine Learning tools."
      ]
    },
    {
      "title": "Logistic Regression in R Studio",
      "url": "https://www.udemy.com/course/machine-learning-basics-classification-models-in-r/",
      "bio": "Logistic regression in R Studio tutorial for beginners. You can do Predictive modeling using R Studio after this course.",
      "objectives": [
        "Understand how to interpret the result of Logistic Regression model and translate them into actionable insight",
        "Learn the linear discriminant analysis and K-Nearest Neighbors technique in R studio",
        "Learn how to solve real life problem using the different classification techniques",
        "Preliminary analysis of data using Univariate analysis before running classification model",
        "Predict future outcomes basis past data by implementing Machine Learning algorithm",
        "Indepth knowledge of data collection and data preprocessing for Machine Learning logistic regression problem",
        "Course contains a end-to-end DIY project to implement your learnings from the lectures",
        "Graphically representing data in R before and after analysis",
        "How to do basic statistical operations in R"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course!",
          "Course resources"
        ],
        "Basics of Statistics": [
          "Types of Data",
          "This is a Milestone!",
          "Types of Statistics",
          "Describing data Graphically",
          "Measures of Centers",
          "Practice Exercise 1",
          "Measures of Dispersion",
          "Practice Exercise 2",
          "Quiz"
        ],
        "Getting started with R and R studio": [
          "Installing R and R studio",
          "Basics of R and R studio",
          "Packages in R",
          "Inputting data part 1: Inbuilt datasets of R",
          "Inputting data part 2: Manual data entry",
          "Inputting data part 3: Importing from CSV or Text files",
          "Creating Barplots in R",
          "Creating Histograms in R",
          "Quiz"
        ],
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning",
          "Building a Machine Learning model"
        ],
        "Data Preprocessing": [
          "Gathering Business Knowledge",
          "Data Exploration",
          "The Data and the Data Dictionary",
          "Importing the dataset into R",
          "Project Exercise 1",
          "Univariate analysis and EDD",
          "EDD in R",
          "Project Exercise 2",
          "Outlier Treatment",
          "Outlier Treatment in R",
          "Project Exercise 3",
          "Missing Value Imputation",
          "Missing Value imputation in R",
          "Project Exercise 4",
          "Seasonality in Data",
          "Variable transformation in R",
          "Project Exercise 5",
          "Dummy variable creation: Handling qualitative data",
          "Dummy variable creation in R",
          "Project Exercise 6",
          "Quiz"
        ],
        "Classification Models": [
          "Three Classifiers and the problem statement",
          "Why can't we use Linear Regression?",
          "Logistic Regression",
          "Training a Simple Logistic model in R",
          "Project Exercise 7",
          "Results of Simple Logistic Regression",
          "Logistic with multiple predictors",
          "Training multiple predictor Logistic model in R",
          "Quiz",
          "Project Exercise 8",
          "Confusion Matrix",
          "Evaluating Model performance",
          "Predicting probabilities, assigning classes and making Confusion Matrix",
          "Project Exercise 9",
          "Quiz"
        ],
        "Linear Discriminant Analysis (LDA)": [
          "Linear Discriminant Analysis",
          "Linear Discriminant Analysis in R",
          "Project Exercise 10"
        ],
        "Test-Train Split": [
          "Test-Train Split",
          "More about test-train split",
          "Test-Train Split in R",
          "Quiz",
          "Project Exercise 11"
        ],
        "K-Nearest Neighbors classifier": [
          "K-Nearest Neighbors classifier",
          "K-Nearest Neighbors in R",
          "Project Exercise 12"
        ],
        "Understanding the Results": [
          "Understanding the results of classification models",
          "Summary of the three models",
          "The Final Exercise!"
        ]
      },
      "requirements": [
        "Students will need to install R and R studio software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Classification modeling course that teaches you everything you need to create a Classification model in R, right?\nYou've found the right Classification modeling course covering logistic regression, LDA and kNN in R studio!\nAfter completing this course, you will be able to:\n· Identify the business problem which can be solved using Classification modeling techniques of Machine Learning.\n· Create different Classification modelling model in R and compare their performance.\n· Confidently practice, discuss and understand Machine Learning concepts\n\n\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you the most popular Classification techniques of machine learning, such as Logistic Regression, Linear Discriminant Analysis and KNN\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem using classification techniques.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a classification model, to solve business problems.\nBelow are the course contents of this course on Logistic Regression:\n· Section 1 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics then graphical representations to describe the data and then a lecture on measures of center like mean median and mode and lastly measures of dispersion like range and standard deviation\n· Section 2 - R basic\nThis section will help you set up the R and R studio on your system and it'll teach you how to perform some basic operations in R.\n· Section 3 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\n· Section 4 - Data Pre-processing\nIn this section you will learn what actions you need to take a step by step to get the data and then prepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment and missing value imputation.\n· Section 5 - Classification Models\nThis section starts with Logistic regression and then covers Linear Discriminant Analysis and K-Nearest Neighbors.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you understand where the concept is coming from and how it is important. But even if you don't understand it, it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models performance using confusion matrix, how categorical variables in the independent variables dataset are interpreted in the results, test-train split and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a classification model in R will soar. You'll have a thorough understanding of how to use Classification modelling to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhich all classification techniques are taught in this course?\nIn this course we learn both parametric and non-parametric classification techniques. The primary focus will be on the following three techniques:\nLogistic Regression\nLinear Discriminant Analysis\nK - Nearest Neighbors (KNN)\nHow much time does it take to learn Classification techniques of machine learning?\nClassification is easy but no one can determine the learning time it takes. It totally depends on you. The method we adopted to help you learn classification starts from the basics and takes you to advanced level within hours. You can follow the same, but remember you can learn nothing without practicing it. Practice is the only way to remember whatever you have learnt. Therefore, we have also provided you with another data set to work on as a separate project of classification.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 3 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of  models - Fifth and sixth section cover Classification models and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.\nWhy use R for Machine Learning?\nUnderstanding R is one of the valuable skills needed for a career in Machine Learning. Below are some reasons why you should learn Machine learning in R\n1. It’s a popular language for Machine Learning at top tech firms. Almost all of them hire data scientists who use R. Facebook, for example, uses R to do behavioral analysis with user post data. Google uses R to assess ad effectiveness and make economic forecasts. And by the way, it’s not just tech firms: R is in use at analysis and consulting firms, banks and other financial institutions, academic institutions and research labs, and pretty much everywhere else data needs analyzing and visualizing.\n2. Learning the data science basics is arguably easier in R. R has a big advantage: it was designed specifically with data manipulation and analysis in mind.\n3. Amazing packages that make your life easier. Because R was designed with statistical analysis in mind, it has a fantastic ecosystem of packages and other resources that are great for data science.\n4. Robust, growing community of data scientists and statisticians. As the field of data science has exploded, R has exploded with it, becoming one of the fastest-growing languages in the world (as measured by StackOverflow). That means it’s easy to find answers to questions and community guidance as you work your way through projects in R.\n5. Put another tool in your toolkit. No one language is going to be the right tool for every job. Adding R to your repertoire will make some projects easier – and of course, it’ll also make you a more flexible and marketable employee when you’re looking for jobs in data science.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience"
      ]
    },
    {
      "title": "Data Science and Machine Learning using Python - A Bootcamp",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-using-python-bootcamp-qazi/",
      "bio": "Numpy Pandas Matplotlib Seaborn Ploty Machine Learning Scikit-Learn Data Science Recommender system NLP Theory Hands-on",
      "objectives": [
        "Python to analyze data, create state of the art visualization and use of machine learning algorithms to facilitate decision making.",
        "Python for Data Science and Machine Learning",
        "NumPy for Numerical Data",
        "Pandas for Data Analysis",
        "Plotting with Matplotlib",
        "Statistical Plots with Seaborn",
        "Interactive dynamic visualizations of data using Plotly",
        "SciKit-Learn for Machine Learning",
        "K-Mean Clustering, Logistic Regression, Linear Regression",
        "Random Forest and Decision Trees",
        "Principal Component Analysis (PCA)",
        "Support Vector Machines",
        "Recommender Systems",
        "Natural Language Processing and Spam Filters",
        "and much more...................!"
      ],
      "course_content": {
        "Welcome, Course Introduction & overview, and Environment set-up": [
          "Welcome & Course Overview",
          "Please read, it's important for you to know!",
          "Download_Course_Material",
          "Set-up the Environment for the Course (lecture 1)",
          "Set-up the Environment for the Course (lecture 2)",
          "Download environment file and watch next lecture to setup -- super easy way",
          "Two other options to setup environment",
          "Important Note:",
          "Possible updates in the course."
        ],
        "Python Essentials": [
          "Python data types Part 1",
          "Python Data Types Part 2",
          "Comparisons Operators, if, else, elif statement",
          "Loops, List Comprehension, Functions, Lambda Expression, Map and Filter (Part 1)",
          "Loops, List Comprehension, Functions, Lambda Expression, Map and Filter (Part 2)",
          "Python Essentials Exercises Overview",
          "Python Essentials Exercises Solutions"
        ],
        "Python for Data Analysis using NumPy": [
          "What is Numpy? A brief introduction and installation instructions.",
          "NumPy Essentials - NumPy arrays, built-in methods, array methods and attributes.",
          "NumPy Essentials - Indexing, slicing, broadcasting & boolean masking",
          "NumPy Essentials - Arithmetic Operations & Universal Functions",
          "NumPy Essentials Exercises Overview",
          "NumPy Essentials Exercises Solutions"
        ],
        "Python for Data Analysis using Pandas": [
          "What is pandas? A brief introduction and installation instructions.",
          "Pandas Introduction.",
          "Pandas Essentials - Pandas Data Structures - Series",
          "Pandas Essentials - Pandas Data Structures - DataFrame",
          "Pandas Essentials - Hierarchical Indexing",
          "Pandas Essentials - Handling Missing Data",
          "Pandas Essentials - Data Wrangling - Combining, merging, joining",
          "Pandas Essentials - Groupby",
          "Pandas Essentials - Useful Methods and Operations",
          "Pandas Essentials - Project 1 (Overview) Customer Purchases Data",
          "Pandas Essentials - Project 1 (Solutions) Customer Purchases Data",
          "Pandas Essentials - Project 2 (Overview) Chicago Payroll Data",
          "Pandas Essentials - Project 2 (Solutions Part 1) Chicago Payroll Data",
          "Pandas Essentials - Project 2 (Solutions Part 2) Chicago Payroll Data"
        ],
        "Python for Data Visualization using matplotlib": [
          "Matplotlib Essentials (Part 1) - Basic Plotting & Object Oriented Approach",
          "Matplotlib Essentials (Part 2) - Basic Plotting & Object Oriented Approach",
          "Matplotlib Essentials (Part 3) - Basic Plotting & Object Oriented Approach",
          "Matplotlib Essentials - Exercises Overview",
          "Matplotlib Essentials - Exercises Solutions",
          "Matplotlib Essentials (Optional) - Advance"
        ],
        "Python for Data Visualization using Seaborn": [
          "Seaborn - Introduction & Installation",
          "Seaborn - Distribution Plots",
          "Seaborn - Categorical Plots (Part 1)",
          "Seaborn - Categorical Plots (Part 2)",
          "Seaborn - Axis Grids",
          "Seaborn - Matrix Plots",
          "Seaborn - Regression Plots",
          "Seaborn - Controlling Figure Aesthetics",
          "Seaborn - Exercises Overview",
          "Seaborn - Exercise Solutions"
        ],
        "Python for Data Visualization using pandas": [
          "Pandas Built-in Data Visualization",
          "Pandas Data Visualization Exercises Overview",
          "Panda Data Visualization Exercises Solutions"
        ],
        "Python for interactive & geographical plotting using Plotly and Cufflinks": [
          "Plotly & Cufflinks - Interactive & Geographical Plotting (Part 1)",
          "Plotly & Cufflinks - Interactive & Geographical Plotting (Part 2)",
          "Plotly & Cufflinks - Interactive & Geographical Plotting Exercises (Overview)",
          "Plotly & Cufflinks - Interactive & Geographical Plotting Exercises (Solutions)"
        ],
        "Capstone Project - Python for Data Analysis & Visualization": [
          "Project 1 - Oil vs Banks Stock Price during recession (Overview)",
          "Project 1 - Oil vs Banks Stock Price during recession (Solutions Part 1)",
          "Project 1 - Oil vs Banks Stock Price during recession (Solutions Part 2)",
          "Project 1 - Oil vs Banks Stock Price during recession (Solutions Part 3)",
          "Project 2 (Optional) - Emergency Calls from Montgomery County, PA (Overview)"
        ],
        "Python for Machine Learning (ML) - scikit-learn - Linear Regression Model": [
          "Introduction to ML - What, Why and Types.....",
          "Theory Lecture on Linear Regression Model, No Free Lunch, Bias Variance Tradeoff",
          "A note on student’s concerns and questions on FutureWarnings.",
          "scikit-learn - Linear Regression Model - Hands-on (Part 1)",
          "scikit-learn - Linear Regression Model Hands-on (Part 2)",
          "Good to know! How to save and load your trained Machine Learning Model!",
          "scikit-learn - Linear Regression Model (Insurance Data Project Overview)",
          "scikit-learn - Linear Regression Model (Insurance Data Project Solutions)"
        ]
      },
      "requirements": [
        "A PC and passion to be successful!",
        "Some experience in programming could be helpful but not required!"
      ],
      "description": "Greetings,\nI am so excited to learn that you have started your path to becoming a Data Scientist  with my course. Data Scientist is in-demand and most satisfying career, where you will solve the most interesting problems and challenges in the world. Not only, you will earn average salary of over $100,000 p.a., you will also see the impact of your work around your, is not is amazing?\nThis is one of the most comprehensive course on any e-learning platform (including Udemy marketplace) which uses the power of Python to learn exploratory data analysis and machine learning algorithms. You will learn the skills to dive deep into the data and present solid conclusions for decision making.\nData Science Bootcamps are costly, in thousands of dollars. However, this course is only a fraction of the cost of any such Bootcamp and includes HD lectures along with  detailed code notebooks for every lecture. The course also includes practice exercises on real data for each topic you cover, because the goal is \"Learn by Doing\"!\nFor your satisfaction, I would like to mention few topics that we will be learning in this course:\nBasis Python programming for Data Science\nData Types, Comparisons Operators, if, else, elif statement, Loops, List Comprehension, Functions, Lambda Expression, Map and Filter\nNumPy\nArrays, built-in methods, array methods and attributes, Indexing, slicing, broadcasting & boolean masking, Arithmetic Operations & Universal Functions\nPandas\nPandas Data Structures - Series, DataFrame, Hierarchical Indexing, Handling Missing Data, Data Wrangling - Combining, merging, joining, Groupby, Other Useful Methods and Operations, Pandas Built-in Data Visualization\nMatplotlib\nBasic Plotting & Object Oriented Approach\nSeaborn\nDistribution & Categorical Plots, Axis Grids, Matrix Plots, Regression Plots, Controlling Figure Aesthetics\nPlotly and Cufflinks\nInteractive & Geographical plotting\nSciKit-Learn (one of the world's best machine learning Python library) including:\nLiner Regression\nOver fitting , Under fitting Bias Variance Trade-off, saving and loading your trained Machine Learning Models\nLogistic Regression\nConfusion Matrix, True Negatives/Positives, False Negatives/Positives, Accuracy, Misclassification Rate / Error Rate, Specificity, Precision\nK Nearest Neighbour (KNN)\nCurse of Dimensionality, Model Performance\nDecision Trees\nTree Depth, Splitting at Nodes, Entropy, Information Gain\nRandom Forests\nBootstrap, Bagging (Bootstrap Aggregation)\nK Mean Clustering\nElbow Method\nPrinciple Component Analysis (PCA)\nSupport Vector Machine\nRecommender Systems\nNatural Language Processing (NLP)\nTokenization, Text Normalization, Vectorization, Bag-of-Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), Pipeline feature........and MUCH MORE..........!\nNot only the hands-on practice using tens of real data project, theory lectures are also provided to make you understand the working principle behind the Machine Learning models.\nSo, what are you waiting for, this is your opportunity to learn the real Data Science with a fraction of the cost of any of your undergraduate course.....!\n\n\nBrief overview of Data around us:\n\nAccording to IBM, we create 2.5 Quintillion bytes of data daily and 90% of the existing data in the world today, has been created in the last two years alone. Social media, transactions records, cell phones, GPS, emails, research, medical records and much more…., the data comes from everywhere which has created a big talent gap and the industry, across the globe, is experiencing shortage of experts who can answer and resolve the challenges associated with the data. Professionals are needed in the field of Data Science who are capable of handling and presenting the insights of the data to facilitate decision making. This is the time to get into this field with the knowledge and in-depth skills of data analysis and presentation.\nHave Fun and Good Luck!",
      "target_audience": [
        "For you, if you:",
        "want to learn Data Science with Python",
        "want to learn Machine Learning with Python",
        "are tired of complicated courses and \"Learn by Doing\""
      ]
    },
    {
      "title": "A Complete Guide on TensorFlow 2.0 using Keras API",
      "url": "https://www.udemy.com/course/tensorflow-2/",
      "bio": "Build Amazing Applications of Deep Learning and Artificial Intelligence in TensorFlow 2.0",
      "objectives": [
        "How to use Tensorflow 2.0 in Data Science",
        "Important differences between Tensorflow 1.x and Tensorflow 2.0",
        "How to implement Artificial Neural Networks in Tensorflow 2.0",
        "How to implement Convolutional Neural Networks in Tensorflow 2.0",
        "How to implement Recurrent Neural Networks in Tensorflow 2.0",
        "How to build your own Transfer Learning application in Tensorflow 2.0",
        "How to build a stock market trading bot using Reinforcement Learning (Deep-Q Network)",
        "How to build Machine Learning Pipeline in Tensorflow 2.0",
        "How to conduct Data Validation and Dataset Preprocessing using TensorFlow Data Validation and TensorFlow Transform.",
        "Putting a TensorFlow 2.0 model into production",
        "How to create a Fashion API with Flask and TensorFlow 2.0",
        "How to serve a TensorFlow model with RESTful API"
      ],
      "course_content": {},
      "requirements": [
        "Some maths basics like knowing what is a differentiation or a gradient",
        "Python basics"
      ],
      "description": "Welcome to Tensorflow 2.0!\n\n\nTensorFlow 2.0 has just been released, and it introduced many features that simplify the model development and maintenance processes. From the educational side, it boosts people's understanding by simplifying many complex concepts. From the industry point of view, models are much easier to understand, maintain, and develop.\n\n\nDeep Learning is one of the fastest growing areas of Artificial Intelligence. In the past few years, we have proven that Deep Learning models, even the simplest ones, can solve very hard and complex tasks. Now, that the buzz-word period of Deep Learning has, partially, passed, people are releasing its power and potential for their product improvements.\n\n\nThe course is structured in a way to cover all topics from neural network modeling and training to put it in production.\n\n\nIn Part 1 of the course, you will learn about the technology stack that we will use throughout the course (Section 1) and the TensorFlow 2.0 library basics and syntax (Section 2).\n\n\nIn Part 2 of the course, we will dig into the exciting world of deep learning. Through this part of the course, you will implement several types of neural networks (Fully Connected Neural Network (Section 3), Convolutional Neural Network (Section 4), Recurrent Neural Network (Section 5)). At the end of this part, Section 6, you will learn and build their own Transfer Learning application that achieves state of the art (SOTA) results on the Dogs vs. Cats dataset.\n\n\nAfter passing the part 2 of the course and ultimately learning how to implement neural networks, in Part 3 of the course, you will learn how to make your own Stock Market trading bot using Reinforcement Learning, specifically Deep-Q Network.\n\n\nPart 4 is all about TensorFlow Extended (TFX). In this part of the course, you will learn how to work with data and create your own data pipelines for production. In Section 8 we will check if the dataset has any anomalies using the TensorFlow Data Validation library and after learn how to check a dataset for anomalies, in Section 9, we will make our own data preprocessing pipeline using the TensorFlow Transform library.\n\n\nIn Section 10 of the course, you will learn and create your own Fashion API using the Flask Python library and a pre-trained model. Throughout this section, you will get a better picture of how to send a request to a model over the internet. However, at this stage, the architecture around the model is not scalable to millions of request. Enter the Section 11. In this section of the course, you will learn how to improve solution from the previous section by using the TensorFlow Serving library. In a very easy way, you will learn and create your own Image Classification API that can support millions of requests per day!\n\n\nThese days it is becoming more and more popular to have a Deep Learning model inside an Android or iOS application, but neural networks require a lot of power and resources! That's where the TensorFlow Lite library comes into play. In Section 12 of the course, you will learn how to optimize and convert any neural network to be suitable for a mobile device.\n\n\nTo conclude with the learning process and the Part 5 of the course, in Section 13 you will learn how to distribute the training of any Neural Network to multiple GPUs or even Servers using the TensorFlow 2.0 library.",
      "target_audience": [
        "Deep Learning Engineers who want to learn Tensorflow 2.0",
        "Artificial Intelligence Engineers who want to expand their Deep Learning stack skills",
        "Computer Scientists who want to enter the exciting area of Deep Learning and Artificial Intelligence",
        "Data Scientists who want to take their AI Skills to the next level",
        "AI experts who want to expand on the field of applications",
        "Python Developers who want to enter the exciting area of Deep Learning and Artificial Intelligence",
        "Engineers who work in technology and automation",
        "Businessmen and companies who want to get ahead of the game",
        "Students in tech-related programs who want to pursue a career in Data Science, Machine Learning, or Artificial Intelligence",
        "Anyone passionate about Artificial Intelligence"
      ]
    },
    {
      "title": "Artificial Intelligence (ARS): Build the Most Powerful AI",
      "url": "https://www.udemy.com/course/artificial-intelligence-ars/",
      "bio": "Learn, build and implement the most powerful AI model at home. Compete with multi-billion dollars companies using ARS.",
      "objectives": [
        "Build an AI",
        "Understand the theory behind augmented random search algorithm",
        "Learn how to build most powerful AI algorithm",
        "Train and implement ARS algorithm",
        "Train AI to solve same challenges as Google Deep Mind"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Learning Paths",
          "This PDF resource will help you a lot!",
          "Where to get materials",
          "Prizes $$ for Learning"
        ],
        "Part 1 - Augmented Random Search (ARS) Intuition": [
          "Plan of attack",
          "Overview of Augmented Random Search (ARS)",
          "How does a perceptron work?",
          "Maximising Rewards",
          "Method of Finite Differences",
          "Basic vs Augmented Random Search",
          "ARS vs other AI"
        ],
        "Part 2 - Augmented Random Search (ARS) Practical": [
          "ARS - Step 1",
          "ARS - Step 2",
          "ARS - Step 3",
          "Checkpoint!",
          "ARS - Step 4",
          "ARS - Step 5",
          "ARS - Step 6",
          "Checkpoint!",
          "ARS - Step 7",
          "ARS - Step 8",
          "ARS - Step 9",
          "ARS - Step 10",
          "Checkpoint!",
          "ARS - Step 11",
          "Checkpoint!",
          "ARS - Step 12",
          "ARS - Step 13",
          "ARS - Step 14",
          "ARS - Step 15",
          "ARS - Step 16",
          "ARS - Step 17",
          "ARS - Step 18",
          "ARS - Step 19",
          "Checkpoint!",
          "ARS - Step 20",
          "For Windows users only",
          "ARS - Final Results",
          "Final Checkpoint!",
          "THANK YOU Video"
        ],
        "Special Offer": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Python prior coding or scripting experience is required.",
        "High school level math skills will be required.",
        "PC (Windows, Mac or Linux), where Anaconda could be installed and run"
      ],
      "description": "Two months ago we discovered that a very new kind of AI was invented.\nThe kind of AI which is based on a genius idea and that you can build from scratch and without the need for any framework.\nWe checked that out, we built it, and... the results are absolutely insane!\nThis game-changing AI called Augmented Random Search, ARS for short.\nAnd in a very simple implementation, it is able to do an exact same thing that Google Deep Mind did in their accomplishment last year  - which is to train an AI to walk and run across a field.\nHowever, ARS is 100x times faster and 100x times more powerful.\nBe prepared for the most significant tech challenges of the 21st century\nNo need for sophisticated algorithms and frameworks\nWhat Facebook or Google spent on millions or even more - you can literally do at home!\nYou will be able to compete with multi-billion dollars companies\nChange the world on your own within months or even weeks\nBuild the most powerful AI that anyone has ever built\nGet your hands on Artificial Intelligence (ARS): Build the Most Powerful AI\nYou will learn, build and implement the most powerful AI model at home. Compete with multi-billion dollars companies using ARS.",
      "target_audience": [
        "Anyone interested in Artificial Intelligence, Machine Learning or Deep Learning"
      ]
    },
    {
      "title": "R Programming For Absolute Beginners",
      "url": "https://www.udemy.com/course/r-programming-course-for-absolute-beginners/",
      "bio": "Learn the basics of writing code in R - your first step to become a data scientist",
      "objectives": [
        "Work with vectors, matrices and lists",
        "Work with factors",
        "Manage data frames",
        "Write complex programming structures (loops and conditional statements)",
        "Build their own functions and binary operations",
        "Work with strings",
        "Create charts in base R"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Getting Started with R": [
          "Installing R and RStudio",
          "The RStudio Interface",
          "Installing and Activating R Packages",
          "Setting the Working Directory",
          "Basic Operations in R",
          "Working With Variables"
        ],
        "Vectors": [
          "Creating Vectors With the c() Function",
          "Creating Vectors Using the Colon Operator",
          "Creating Vectors With the rep() Function",
          "Creating Vectors With the seq() Function",
          "Creating Vectors of Random Numbers",
          "Creating Empty Vectors",
          "Indexing Vectors With Numeric Indices",
          "Indexing Vectors With Logical Indices",
          "Naming Vector Components",
          "Filtering Vectors",
          "The Functions all() and any()",
          "Sum and Product of Vector Components",
          "Vectorized Operations",
          "Treating Missing Values in Vectors",
          "Sorting Vectors",
          "Minimum and Maximum Values",
          "The ifelse() Function",
          "Adding and Multiplying Vectors",
          "Testing Vector Equality",
          "Vector Correlation",
          "Bonus Lecture: Learn Statistics with R",
          "Practical Exercises"
        ],
        "Matrices and Arrays": [
          "Creating Matrices With the matrix() Function",
          "Creating Matrices With the rbind() and cbind() Functions",
          "Naming Matrix Rows and Columns",
          "Indexing Matrices",
          "Filtering Matrices",
          "Editing Values in Matrices",
          "Adding and Deleting Rows and Columns",
          "Minima and Maxima in Matrices",
          "Applying Functions to Matrices (1)",
          "Applying Functions to Matrices (2)",
          "Applying Functions to Matrices (3)",
          "Adding and Multiplying Matrices",
          "Other Matrix Operations",
          "Creating Multidimensional Arrays",
          "Indexing Multidimensional Arrays",
          "Practical Exercises"
        ],
        "Lists": [
          "Create Lists With the list() Function",
          "Create Lists With the vector() Function",
          "Indexing Lists With Brackets",
          "Indexing Lists Using Objects Names",
          "Editing Values in Lists",
          "Adding and Removing List Objects",
          "Applying Functions to Lists",
          "Practical Example of List: the Regression Analysis Output",
          "Bonus Lecture: Data Analysis in R",
          "Practical Exercises"
        ],
        "Factors": [
          "Working With Factors",
          "Splitting a Vector By a Factor Levels",
          "The tapply() Function",
          "The by() Function",
          "Practical Exercises"
        ],
        "Data Frames": [
          "Creating Data Frames",
          "Loading Data Frames From External Files",
          "Writing Data Frames in External Files",
          "Indexing Data Frames As Lists",
          "Indexing Data Frames As Matrices",
          "Selecting a Random Sample of Entries",
          "Filtering Data Frames",
          "Editing Values in Data Frames",
          "Adding Rows and Columns to Data Frames",
          "Naming Rows and Columns in Data Frames",
          "Applying Functions to Data Frames",
          "Sorting Data Frames",
          "Shuffling Data Frames",
          "Merging Data Frames",
          "Practical Exercises"
        ],
        "Programming Structures": [
          "For Loops",
          "While Loops",
          "Repeat Loops",
          "Nested For Loops",
          "Conditional Statements",
          "Nested Conditional Statements",
          "Loops and Conditional Statements",
          "User Defined Functions",
          "The Return Command",
          "More Complex Functions Examples",
          "Checking Whether an Integer Is a Perfect Square",
          "A Custom Function That Solves Quadratic Equations",
          "Binary Operations",
          "Practical Exercises"
        ],
        "Working With Strings": [
          "Creating Strings",
          "Printing Strings",
          "Concatenating Strings",
          "String Manipulation (1)",
          "String Manipulation (2)",
          "String Manipulation (3)",
          "Functions for Finding Patterns in Strings",
          "Functions for Replacing Patterns in Strings",
          "Regular Expressions",
          "Practical Exercises"
        ],
        "Plotting in Base R": [
          "Building Scatterplot Charts",
          "Setting Graphical Parameters (1)",
          "Setting Graphical Parameters (2)",
          "Adding a Trend Line to a Scatterplot",
          "Building a Clustered Scatterplot",
          "Plotting a Line Chart",
          "Setting the Line Parameters",
          "Overplotting Lines and Dots",
          "Plotting Two Lines in the Same Chart",
          "Plotting Bar Charts",
          "Setting the Bar Parameters",
          "Plotting Histograms",
          "Plotting Density Lines",
          "Plotting Pie Charts",
          "Plotting Boxplot Charts",
          "Plotting Functions",
          "Exporting Charts",
          "Bonus Lecture: More Advanced Plotting",
          "Practical Exercises"
        ]
      },
      "requirements": [
        "No special prerequisite - you should only know how to use a computer"
      ],
      "description": "If you have decided to learn R as your data science programming language, you have made an excellent decision!\nR is the most widely used tool for statistical programming. It is powerful, versatile and easy to use. It is the first choice for thousands of data analysts working in both companies and academia. This course will help you master the basics of R in a short time, as a first step to become a skilled R data scientist.\nThe course is meant for absolute beginners, so you don’t have to know anything about R before starting. (You don’t even have to have the R program on your computer; I will show you how to install it.) But after graduating this course you will have the most important R programming skills – and you will be able to further develop these skills, by practicing, starting from what you will have learned in the course.\nThis course contains about 100 video lectures in nine sections.\nIn the first section of this course you will get started with R: you will install the program (in case you didn’t do it already), you will familiarize with the working interface in R Studio and you will learn some basic technical stuff like installing and activating packages or setting the working directory. Moreover, you will learn how to perform simple operations in R and how to work with variables.\nThe next five sections will be dedicated to the five types of data structures in R: vectors, matrices, lists, factors and data frames. So you’ll learn how to manipulate data structures: how to index them, how to edit data, how to filter data according to various criteria, how to create and modify objects (or variables), how to apply functions to data and much more. These are very important topics, because R is a software for statistical computing and most of the R programming is about manipulating data. So before getting to more advanced statistical analyses in R you must know the basic techniques of data handling.\nAfter finishing with the data structures we’ll get to the programming structures in R. In this section you’ll learn about loops, conditional statements and functions. You’ll learn how to combine loops and conditional statements to perform complex tasks, and how to create custom functions that you can save and reuse later. We will also study some practical examples of functions.\nThe next section is about working with strings. Here we will cover the most useful functions that allow us to manipulate strings. So you will learn how to format strings for printing, how to concatenate strings, how to extract substrings from a given string and especially how to create regular expressions that identify patterns in strings.\nIn the following section you’ll learn how to build charts in R. We are going to cover seven types of charts: dot chart (scatterplot), line chart, bar chart, pie chart, histogram, density line and boxplot. Moreover, you will learn how to plot a function of one variable and how to export the charts you create.\nEvery command and function is visually explained: you can see the output live. At the end of each section you will find a PDF file with practical exercises that allow you to apply and strengthen your knowledge.\nSo if you want to learn R from scratch, you need this course. Enroll right now and begin a fantastic R programming journey!",
      "target_audience": [
        "Wannabe data scientists",
        "Academic researchers",
        "Doctoral researchers",
        "Students",
        "Anyone who wants to master R"
      ]
    },
    {
      "title": "One Week of Data Science in Python - New 2025!",
      "url": "https://www.udemy.com/course/one-week-of-data-science/",
      "bio": "Master Data Science Fundamentals Quickly & Efficiently in one week! Course is Designed for Busy People",
      "objectives": [
        "Perform statistical analysis on real world datasets",
        "Understand feature engineering strategies and tools",
        "Perform one hot encoding and normalization",
        "Understand the difference between normalization and standardization",
        "Deal with missing data using pandas",
        "Change pandas DataFrame datatypes",
        "Define a function and apply it to a Pandas DataFrame column",
        "Perform Pandas operations and filtering",
        "Calculate and display correlation matrix heatmap",
        "Perform data visualization using Seaborn and Matplotlib libraries",
        "Plot single line plot, pie charts and multiple subplots using matplotlib",
        "Plot pairplot, countplot, and correlation heatmaps using Seaborn",
        "Plot distribution plot (distplot), Histograms and scatterplots",
        "Understand machine learning regression fundamentals",
        "Learn how to optimize model parameters using least sum of squares",
        "Split the data into training and testing using SK Learn Library",
        "Perform data visualization and basic exploratory data analysis",
        "Build, train and test our first regression model in Scikit-Learn",
        "Assess trained machine learning regression model performance",
        "Understand the theory and intuition behind boosting",
        "Train an XG-boost algorithm in Scikit-Learn to solve regression type problems",
        "Train several machine learning models classifier models such as Logistic Regression, Support Vector Machine, K-Nearest Neighbors, and Random Forest Classifier",
        "Assess trained model performance using various KPIs such as accuracy, precision, recall, F1-score, AUC and ROC.",
        "Compare the performance of the classification model using various KPIs.",
        "Apply autogluon to solve regression and classification type problems",
        "Use AutoGluon library to perform prototyping of AI/ML models using few lines of code",
        "Plot various models’ performance on model leaderboard",
        "Optimize regression and classification models hyperparameters using SK-Learn",
        "Learn the difference between various hyperparameters optimization strategies such as grid search, randomized search, and Bayesian optimization.",
        "Perform hyperparameters optimization using Scikit-Learn library.",
        "Understand bias variance trade-off and L1 and L2 regularization"
      ],
      "course_content": {
        "Course Introduction, Welcome Message & Data Science Starter Pack!": [
          "Course Welcome Message",
          "Introduction, Best Practices, & Success Tips",
          "Course Outline",
          "What is Data Science?",
          "What is a Typical Data Scientist Profile, Education, Experience & Salary?",
          "What do Data Scientists REALLY do?",
          "What do Recruiters Look for in Data Science Applicants?",
          "What Data Science Jobs are Available Out there?"
        ],
        "Day 1: Data Wrangling, Exploratory Data Analysis (EDA) & Feature Engineering": [
          "Day #1 Welcome Message",
          "Project Introduction & Data Wrangling 101",
          "Coding Task #1 - Import Data and Obtain Data Statistical Summary",
          "Exercise #1 [Optional]",
          "Coding Task #2 - Deal with Missing Values",
          "Exercise #2 [Optional]",
          "Coding Task #3 - Perform One-Hot Encoding",
          "Exercise #3 [Optional]",
          "Feature Scaling 101",
          "Coding Task #4 - Perform Feature Scaling: Normalization & Standardization",
          "Exercise #4 [Optional]",
          "Coding Task #5 - Perform Filtering Operation",
          "Exercise #5 [Optional]",
          "Coding Task #6 - Perform EDA on Both Classes",
          "Exercise #6 [Optional]",
          "Coding Task #7 - Functions With Pandas",
          "Exercise #7 [Optional]",
          "Coding Task #8 - Correlations and Histograms",
          "Exercise #8 [Optional]",
          "Final Capstone Project Overview",
          "Final Capstone Project Solution",
          "Day #1 Conclusion"
        ],
        "Day 2: Effective Data Visualization in Data Science": [
          "Day #2 Welcome Message",
          "Project Overview and Key Learning Outcomes",
          "Data Visualization 101",
          "Coding Task #1 - Plot Pie Charts with Matplotlib",
          "Exercise #1 [Optional]",
          "Coding Task #2 - Plot Single & Multiple LinePlots",
          "Exercise #2 [Optional]",
          "Coding Task #3 - Plot Scatterplots",
          "Exercise #3 [Optional]",
          "Coding Task #4 - Plot Histograms",
          "Exercise #4 [Optional]",
          "Coding Task #5 - Searborn Part 1",
          "Exercise #5 [Optional]",
          "Coding Task #6 - Searborn Part 2",
          "Exercise #6 [Optional]",
          "Final Capstone Project Overview",
          "Final Capstone Project Solution",
          "Day #2 Conclusion"
        ],
        "Day 3: Regression Analysis in Data Science": [
          "Day #3 Welcome Message",
          "Project Overview and Key Learning Outcomes",
          "What is Regression?",
          "What is XGBoost? and Why?",
          "Coding Task #1 - Import Libraries and Datasets",
          "Exercise #1 [Optional]",
          "Coding Task #2 - Exploratory Data Analysis (EDA) & Visualization",
          "Exercise #2 [Optional]",
          "Coding Task #3 - Prepare The Data Before Model Training",
          "Exercise #3 [Optional]",
          "Coding Task #4 - Train an XG-Boost Model",
          "Exercise #4 [Optional]",
          "Final Capstone Project Overview",
          "Final Capstone Project Solution",
          "Day #3 Conclusion"
        ],
        "Day 4: Classification Analysis in Data Science": [
          "Day #4 Welcome Message",
          "Introduction & Project Overview",
          "Classification Models KPIs",
          "Coding Task #1 - Import Libraries and Datasets",
          "Exercise #1 [Optional]",
          "Coding Task #2 - Perform Data Visualization",
          "Exercise #2 [Optional]",
          "Coding Task #3 - Plot Feature Importance",
          "Coding Task #4 - Train & Evaluate a Logistic Regression Classifier Model",
          "Exercise #3 [Optional]",
          "Coding Task #5 - Train & Evaluate Support Vector Machines",
          "Coding Task #6 - Train & Evaluate Random Forest Classifier Model",
          "Coding Task #7 - Train & Evaluate K-Nearest Neighbors Classifier",
          "Exercise #4 [Optional]",
          "Coding Task #8 - Train & Evaluate a Naïve Bayes Classifier Model",
          "Exercise #5 [Optional]",
          "Coding Task #9 - Compare Classifier Models",
          "Coding Task #10 - Concluding Remarks",
          "Final Capstone Project",
          "Final Capstone Project Solution Part #1",
          "Final Capstone Project Solution Part #2",
          "Final Capstone Project Solution Part #3",
          "Day #4 Conclusion"
        ],
        "Day 5: Data Science on Autopilot": [
          "Day #5 Welcome Message",
          "Introduction and Project Overview",
          "What is AutoGluon?",
          "Coding Task #1 - Import AutoGluon, Key Libraries & Datasets",
          "Exercise #1 [Optional]",
          "Coding Task #2 - Perform Exploratory Data Analysis (EDA)",
          "Exercise #2 [Optional]",
          "Coding Task #3 - Perform Data Visualization",
          "Exercise #3 [Optional]",
          "Coding Task #4 - Train Regression Models on Autopilot",
          "Coding Task #5 - Evaluate Trained Regression Models",
          "Exercise #4 [Optional]",
          "Final Capstone Project Overview",
          "Final Capstone Project Solution",
          "Day #5 Conclusion"
        ],
        "Day 6: Models Optimization": [
          "Day #6 Welcome Message",
          "Project Overview",
          "Hyperparameters 101",
          "Optimization Strategies",
          "Coding Task #1 - Import Libraries and Datasets",
          "Exercise #1 [Optional]",
          "Coding Task #2 - Perform Data Cleaning",
          "Coding Task #3 - Perform Data Visualization",
          "Exercise #2 [Optional]",
          "Coding Task #4 - Prepare the Data Before Model Training",
          "Coding Task #5 - Train an XG-Boost Algorithm (Without Optimization)",
          "Exercise #3 [Optional]",
          "Coding Task #6 - GridSearchCV Optimization",
          "Exercise #4 [Optional]",
          "Coding Task #7 - Random Search Optimization",
          "Coding Task #8 - Bayesian Search Optimization",
          "Final Capstone Project Question",
          "Final Capstone Project Solution",
          "Day #6 Conclusion"
        ],
        "Day 7: Deep Learning": [
          "Task #1. Project Overview",
          "Task #2. Upload and Explore the Data",
          "Task #3. Train Deep Neural Network Model",
          "Task #4. Explainable AI and Model Understanding"
        ],
        "Appendix & Optional Content for \"Regression in DS\" Section": [
          "Regression Math",
          "Least Sum of Squares",
          "Scikit Learn",
          "XG-Boost Fundaments",
          "XG-Boost Deep Dive",
          "What is Boosting?",
          "Ensemble Learning"
        ],
        "Appendix & Optional Content for Models Optimization": [
          "Bias Variance Tradeoff",
          "L2 regularization (Ridge)",
          "L1 regularization (Lasso)"
        ]
      },
      "requirements": [
        "Basic Programming skills in python"
      ],
      "description": "Do you want to learn Data Science and build robust applications Quickly and Efficiently?\nAre you an absolute beginner who wants to break into Data Science and look for a course that includes all the basics you need?\nAre you a busy aspiring entrepreneur who wants to maximize business revenues and reduce costs with Data Science but lacks the time to do so quickly and efficiently?\n\n\nThis course is for you if the answer is yes to any of these questions!\nData Science is one of the hottest tech fields to be in now!\nThe field is exploding with opportunities and career prospects.\nData Science is widely adopted in many sectors, such as banking, healthcare, transportation, and technology.\nIn business, Data Science is applied to optimize business processes, maximize revenue, and reduce cost.\nThis course aims to provide you with knowledge of critical aspects of data science in one week and in a practical, easy, quick, and efficient way.\nThis course is unique and exceptional in many ways. It includes several practice opportunities, quizzes, and final capstone projects.\nEvery day, we will spend 1-2 hours together and master a data science topic.\nFirst, we will start with the Data Science essential starter pack and master key Data Science Concepts, including the Data Science project lifecycle, what recruiters look for, and what jobs are available.\nNext, we will understand exploratory data analysis and visualization techniques using Pandas, matplotlib, and Seaborn libraries.\nIn the following section, we will learn about regression fundamentals. We will learn how to build, train, test, and deploy regression models using the Scikit Learn library.\nIn the following section, we will learn about hyperparameter optimization strategies such as grid search, randomized search, and Bayesian optimization.\nNext, we will learn how to train several classification algorithms such as Logistic Regression, Support Vector Machine, K-Nearest Neighbors, Random Forest Classifier, and Naïve Bayes in SageMaker and SK-Learn libraries.\nNext, we will cover Data Science on Autopilot! We will learn how to use the AutoGluon library for prototyping multiple AI/ML models and deploying the best one.\nSo who is this course for?\nThe course targets anyone wanting to gain a fundamental understanding of Data Science and solve practical, real-world business problems.\nIn this course:\nYou will have an actual practical project-based learning experience. We will build over ten projects together\nYou will have access to all the codes and slides\nYou will get a certificate of completion that you can post on your LinkedIn profile to showcase your skills in Data Science to employers.\nAll this comes with a 30-day money-back guarantee, so you can give a course a try risk-free!\n\n\nCheck out the preview videos and the outline to get an idea of the projects we will cover.\nEnroll today, and let’s harness the power of Data Science together!",
      "target_audience": [
        "The course is targeted towards anyone wanting to gain a fundamental understanding of Data Science and solve practical real world business problems",
        "Beginners Data Scientists wanting to advance their careers and build their portfolio",
        "Seasoned consultants wanting to transform businesses by leveraging Data Science",
        "Tech enthusiasts who are passionate and new to Data science & AI and want to gain practical experience"
      ]
    },
    {
      "title": "Fine Tuning LLM with Hugging Face Transformers for NLP",
      "url": "https://www.udemy.com/course/fine-tuning-llm-with-hugging-face-transformers/",
      "bio": "Master Transformer models like Phi2, LLAMA; BERT variants, and distillation for advanced NLP applications on custom data",
      "objectives": [
        "Understand transformers and their role in NLP.",
        "Gain hands-on experience with Hugging Face Transformers.",
        "Learn about relevant datasets and evaluation metrics.",
        "Fine-tune transformers for text classification, question answering, natural language inference, text summarization, and machine translation.",
        "Understand the principles of transformer fine-tuning.",
        "Apply transformer fine-tuning to real-world NLP problems.",
        "Learn about different types of transformers, such as BERT, GPT-2, and T5.",
        "Hands-on experience with the Hugging Face Transformers library"
      ],
      "course_content": {},
      "requirements": [
        "Basic understanding of natural language processing (NLP)",
        "Basic programming skills",
        "Familiarity with machine learning concepts",
        "Access to a computer with a GPU"
      ],
      "description": "Do not take this course if you are an ML beginner. This course is designed for those who are interested in pure coding and want to fine-tune LLMs instead of focusing on prompt engineering. Otherwise, you may find it difficult to understand.\n\n\nWelcome to \"Mastering Transformer Models and LLM Fine Tuning\", a comprehensive and practical course designed for all levels, from beginners to advanced practitioners in Natural Language Processing (NLP). This course delves deep into the world of Transformer models, fine-tuning techniques, and knowledge distillation, with a special focus on popular BERT variants like Phi2, LLAMA, T5, BERT, DistilBERT, MobileBERT, and TinyBERT.\nCourse Overview:\nSection 1: Introduction\nGet an overview of the course and understand the learning outcomes.\nIntroduction to the resources and code files you will need throughout the course.\nSection 2: Understanding Transformers with Hugging Face\nLearn the fundamentals of Hugging Face Transformers.\nExplore Hugging Face pipelines, checkpoints, models, and datasets.\nGain insights into Hugging Face Spaces and Auto-Classes for seamless model management.\nSection 3: Core Concepts of Transformers and LLMs\nDelve into the architectures and key concepts behind Transformers.\nUnderstand the applications of Transformers in various NLP tasks.\nIntroduction to transfer learning with Transformers.\nSection 4: BERT Architecture Deep Dive\nDetailed exploration of BERT's architecture and its importance in context understanding.\nLearn about Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) in BERT.\nUnderstand BERT fine-tuning and evaluation techniques.\nSection 5: Practical Fine-Tuning with BERT\nHands-on sessions to fine-tune BERT for sentiment classification on Twitter data.\nStep-by-step guide on data loading, tokenization, and model training.\nPractical application of fine-tuning techniques to build a BERT classifier.\nSection 6: Knowledge Distillation Techniques for BERT\nIntroduction to knowledge distillation and its significance in model optimization.\nDetailed study of DistilBERT, including loss functions and paper walkthroughs.\nExplore MobileBERT and TinyBERT, with a focus on their unique distillation techniques and practical implementations.\nSection 7: Applying Distilled BERT Models for Real-World Tasks like Fake News Detection\nUse DistilBERT, MobileBERT, and TinyBERT for fake news detection.\nPractical examples and hands-on exercises to build and evaluate models.\nBenchmarking performance of distilled models against BERT-Base.\nSection 8: Named Entity Recognition (NER) with DistilBERT\nTechniques for fine-tuning DistilBERT for NER in restaurant search applications.\nDetailed guide on data preparation, tokenization, and model training.\nHands-on sessions to build, evaluate, and deploy NER models.\nSection 9: Custom Summarization with T5 Transformer\nPractical guide to fine-tuning the T5 model for summarization tasks.\nDetailed walkthrough of dataset analysis, tokenization, and model fine-tuning.\nImplement summarization predictions on custom data.\nSection 10: Vision Transformer for Image Classification\nIntroduction to Vision Transformers (ViT) and their applications.\nStep-by-step guide to using ViT for classifying Indian foods.\nPractical exercises on image preprocessing, model training, and evaluation.\nSection 11: Fine-Tuning Large Language Models on Custom Datasets\nTheoretical insights and practical steps for fine-tuning large language models (LLMs).\nExplore various fine-tuning techniques, including PEFT, LORA, and QLORA.\nHands-on coding sessions to implement custom dataset fine-tuning for LLMs.\nSection 12: Specialized Topics in Transformer Fine-Tuning\nLearn about advanced topics such as 8-bit quantization and adapter-based fine-tuning.\nReview and implement state-of-the-art techniques for optimizing Transformer models.\nPractical sessions to generate product descriptions using fine-tuned models.\nSection 13: Building Chat and Instruction Models with LLAMA\nLearn about advanced topics such as 4-bit quantization and adapter-based fine-tuning.\nTechniques for fine-tuning the LLAMA base model for chat and instruction-based tasks.\nPractical examples and hands-on guidance to build, train, and deploy chat models.\nExplore the significance of chat format datasets and model configuration for PEFT fine-tuning.\nEnroll now in \"Mastering Transformer Models and LLM Fine Tuning on Custom Dataset\" and gain the skills to harness the power of state-of-the-art NLP models. Whether you're just starting or looking to enhance your expertise, this course offers valuable knowledge and practical experience to elevate your proficiency in the field of natural language processing.\nUnlock the full potential of Transformer models with our comprehensive course. Master fine-tuning techniques for BERT variants, explore knowledge distillation with DistilBERT, MobileBERT, and TinyBERT, and apply advanced models like RoBERTa, ALBERT, XLNet, and Vision Transformers for real-world NLP applications. Dive into practical examples using Hugging Face tools, T5 for summarization, and learn to build custom chat models with LLAMA.\nKeywords: Transformer models, fine-tuning BERT, DistilBERT, MobileBERT, TinyBERT, RoBERTa, ALBERT, XLNet, ELECTRA, ConvBERT, DeBERTa, Vision Transformer, T5, BART, Pegasus, GPT-3, DeiT, Swin Transformer, Hugging Face, NLP applications, knowledge distillation, custom chat models, LLAMA.",
      "target_audience": [
        "NLP practitioners: This course is designed for NLP practitioners who want to learn how to fine-tune pre-trained transformer models to achieve state-of-the-art results on a variety of NLP tasks.",
        "Researchers: This course is also designed for researchers who are interested in exploring the potential of transformer fine-tuning for new NLP applications.",
        "Students: This course is suitable for students who have taken an introductory NLP course and want to deepen their understanding of transformer models and their application to real-world NLP problems.",
        "Developers: This course is beneficial for developers who want to incorporate transformer fine-tuning into their NLP applications.",
        "Hobbyists: This course is accessible to hobbyists who are interested in learning about transformer fine-tuning and applying it to personal projects."
      ]
    },
    {
      "title": "Data Manipulation in Python: Master Python, Numpy & Pandas",
      "url": "https://www.udemy.com/course/master-data-science-in-python/",
      "bio": "Learn Python, NumPy & Pandas for Data Science: Master essential data manipulation for data science in python",
      "objectives": [
        "Learn to use Pandas for Data Analysis",
        "Learn to work with numerical data in Python",
        "Learn statistics and math with Python",
        "Learn how to code in Jupyter Notebook",
        "Learn how to install packages in Python"
      ],
      "course_content": {
        "Python Quick Refresher (Optional)": [
          "Welcome to the course!",
          "Introduction to Python",
          "Course Materials",
          "Setting up Python",
          "What is Jupyter?",
          "Anaconda Installation: Windows, Mac & Ubuntu",
          "How to implement Python in Jupyter?",
          "Managing Directories in Jupyter Notebook",
          "Input/Output",
          "Quiz 1",
          "Working with different datatypes",
          "Variables",
          "Quiz 2",
          "Quiz 3",
          "Arithmetic Operators",
          "Quiz 4",
          "Quiz 5",
          "Quiz 6",
          "Comparison Operators",
          "Logical Operators",
          "Quiz 7",
          "Quiz 8",
          "Quiz 9",
          "Conditional statements",
          "Loops",
          "Sequences: Lists",
          "Sequences: Dictionaries",
          "Sequences: Tuples",
          "Quiz 10",
          "Quiz 11",
          "Quiz 12",
          "Functions: Built-in Functions",
          "Functions: User-defined Functions",
          "Quiz 13",
          "Quiz 14"
        ],
        "Essential Python Libraries for Data Science": [
          "Installing Libraries",
          "Importing Libraries",
          "Pandas Library for Data Science",
          "NumPy Library for Data Science",
          "Pandas vs NumPy",
          "Matplotlib Library for Data Science",
          "Seaborn Library for Data Science"
        ],
        "Fundamental NumPy Properties": [
          "Introduction to NumPy arrays",
          "Creating NumPy arrays",
          "Quiz 15",
          "Indexing NumPy arrays",
          "Quiz 16",
          "Array shape",
          "Iterating Over NumPy Arrays"
        ],
        "Mathematics for Data Science": [
          "Basic NumPy arrays: zeros()",
          "Basic NumPy arrays: ones()",
          "Basic NumPy arrays: full()",
          "Quiz 17",
          "Adding a scalar",
          "Subtracting a scalar",
          "Multiplying by a scalar",
          "Dividing by a scalar",
          "Raise to a power",
          "Transpose",
          "Element wise addition",
          "Element wise subtraction",
          "Element wise multiplication",
          "Element wise division",
          "Matrix multiplication",
          "Quiz 18",
          "Statistics"
        ],
        "Python Pandas DataFrames & Series": [
          "What is a Python Pandas DataFrame?",
          "What is a Python Pandas Series?",
          "DataFrame vs Series",
          "Creating a DataFrame using lists",
          "Creating a DataFrame using a dictionary",
          "Loading CSV data into python",
          "Changing the Index Column",
          "Inplace",
          "Examining the DataFrame: Head & Tail",
          "Statistical summary of the DataFrame",
          "Slicing rows using bracket operators",
          "Quiz 19",
          "Indexing columns using bracket operators",
          "Boolean list",
          "Filtering Rows",
          "Filtering rows using & and | operators",
          "Filtering data using loc()",
          "Quiz 20",
          "Filtering data using iloc()",
          "Quiz 21",
          "Quiz 22",
          "Adding and deleting rows and columns",
          "Sorting Values",
          "Exporting and saving pandas DataFrames",
          "Concatenating DataFrames",
          "groupby()"
        ],
        "Data Cleaning": [
          "Introduction to Data Cleaning",
          "Quality of Data",
          "Examples of Anomalies",
          "Median-based Anomaly Detection",
          "Mean-based anomaly detection",
          "Z-score-based Anomaly Detection",
          "Interquartile Range for Anomaly Detection",
          "Dealing with missing values",
          "Regular Expressions",
          "Feature Scaling"
        ],
        "Data Visualization using Python": [
          "Introduction",
          "Setting Up Matplotlib",
          "Plotting Line Plots using Matplotlib",
          "Title, Labels & Legend",
          "Plotting Histograms",
          "Plotting Bar Charts",
          "Plotting Pie Charts",
          "Plotting Scatter Plots",
          "Plotting Log Plots",
          "Plotting Polar Plots",
          "Handling Dates",
          "Creating multiple subplots in one figure"
        ],
        "Exploratory Data Analysis": [
          "Introduction",
          "What is Exploratory Data Analysis?",
          "Univariate Analysis",
          "Univariate Analysis: Continuous Data",
          "Univariate Analysis: Categorical Data",
          "Bivariate analysis: Continuous & Continuous",
          "Bivariate analysis: Categorical & Categorical",
          "Bivariate analysis: Continuous & Categorical",
          "Detecting Outliers",
          "Categorical Variable Transformation"
        ],
        "Time Series in Python": [
          "Introduction to Time Series",
          "Getting stock data using yfinance",
          "Converting a Dataset into Time Series",
          "Working with Time Series",
          "Time Series Data Visualization with Python"
        ],
        "BONUS Section - Don't Miss Out": [
          "BONUS Section - Don't Miss Out"
        ]
      },
      "requirements": [
        "No prior data science knowledge required",
        "No programming experience needed"
      ],
      "description": "When it comes to being attractive, data scientists are already there. In a highly competitive job market, it is tough to keep them after they have been hired. People with a unique mix of scientific training, computer expertise, and analytical abilities are hard to find.\nLike the Wall Street \"quants\" of the 1980s and 1990s, modern-day data scientists are expected to have a similar skill set. People with a background in physics and mathematics flocked to investment banks and hedge funds in those days because they could come up with novel algorithms and data methods.\n\n\nThat being said, data science is becoming one of the most well-suited occupations for success in the twenty-first century. It is computerized, programming-driven, and analytical in nature. Consequently, it comes as no surprise that the need for data scientists has been increasing in the employment market over the last several years.\nThe supply, on the other hand, has been quite restricted. It is challenging to get the knowledge and abilities required to be recruited as a data scientist.\nLots of resources for learning Python are available online. Because of this, students frequently get overwhelmed by Python's high learning curve.\n\n\nIt's a whole new ball game in here! Step-by-step instruction is the hallmark of this course. Throughout each subsequent lesson, we continue to build on what we've previously learned. Our goal is to equip you with all the tools and skills you need to master Python, Numpy & Pandas.\nYou'll walk away from each video with a fresh idea that you can put to use right away!\nAll skill levels are welcome in this course, and even if you have no prior programming or statistical experience, you will be able to succeed!",
      "target_audience": [
        "No previous skills or expertise required. Only a drive to succeed!"
      ]
    },
    {
      "title": "RA: Retail Customer Analytics and Trade Area Modeling.",
      "url": "https://www.udemy.com/course/ra-retail-customer-analytics-and-trade-area-modeling/",
      "bio": "EP3: Learn Python and apply Customer analytics, Churn prediction, Customer Segmentation and Trade Area Modeling.",
      "objectives": [
        "Python.",
        "Customer analytics",
        "Learn How to work daily with Python",
        "Learn how to benefit from data to increase Customer Engagement.",
        "Use K-means for Customer Segmentation.",
        "Use Trade area modeling for Location and Competitive analysis.",
        "Use Recommendation systems to Propose Products To customers.",
        "Use Market Basket analysis to Make recommendations and Promotional Bundles to customers.",
        "Predict Customer lifetime value of customers"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Tesco and Andrew Pole",
          "False Positives",
          "Walmart",
          "Notable mentions",
          "Why Customer analytics",
          "Curriculum",
          "The retail Customer",
          "types of retail customers",
          "Types of retail customrs",
          "Why we need customer analytics",
          "types of retail Data",
          "Sales Data Vs Market basket Data",
          "Retail Data structre",
          "Customer analytics and machine learning applications",
          "Quiz on section 1",
          "Summary"
        ],
        "Installing Python": [
          "Python",
          "Downloading Anaconda",
          "Installing Anaconda",
          "Spyder Overview",
          "Jupiter Notebook Overview",
          "Python Libraries"
        ],
        "Python Programming Fundmentals": [
          "Intro",
          "Data Frames",
          "Arithmetic Calculations in Python",
          "Lists",
          "Dictionaries",
          "Arrays",
          "Importing Data in Python",
          "Subsetting DataFrames",
          "Conditions",
          "Writing Functions",
          "Mapping",
          "For Loops",
          "For looping a function",
          "Mapping on Dataframe",
          "For Looping a DataFrame",
          "Summary",
          "Assignment",
          "Assignment Answer 1",
          "Assignment Answer 2"
        ],
        "Manipulation of Retail Data": [
          "Inro",
          "Dropping Duplicates and NAs",
          "Conversions lecture",
          "Conversions",
          "Filterations",
          "Imputations",
          "Indexing Tutorial",
          "Slicing index",
          "Manipulation lecture",
          "Groupby",
          "Slicing the Groupby",
          "Dropping levels",
          "The proper form",
          "Pivot tables",
          "Aggregate function in pivot table",
          "Melting the Data",
          "Left join",
          "inner & outer join",
          "Joining in Python",
          "inner, left join and full join(outer)",
          "Summary",
          "Assignment",
          "Assignment answer 1",
          "Assignment answer 2",
          "Assignment answer 3",
          "Assignment answer 4",
          "Assignment answer 5"
        ],
        "Trade Area Modeling": [
          "Tade Area Modelling",
          "Introduction",
          "Different trade area modelling",
          "Drive time and Zip codes",
          "The huff model",
          "Some considerations about trade area modeling",
          "Summary of a Huff model",
          "Huff Model",
          "Example Demonstration",
          "Scaling attractiveness",
          "Developing Huff model",
          "The Huff model in Python",
          "Reading the data in python",
          "Getting the upper term",
          "Probability per Customer Community",
          "Where should I locate my store ?",
          "Assignment",
          "Assignment Answer",
          "Summary"
        ],
        "Customer RFM analysis": [
          "Intro",
          "RFM",
          "Customer segmentation based on RFM analysis",
          "Customer Recency in Python",
          "Frequency and Monetary Value",
          "Ranking",
          "Grouping",
          "Creating the Categories",
          "Insights",
          "RFM with Kmeans",
          "Centroids visualization",
          "Elbow Spree",
          "Assignment",
          "Assignment Kmeans"
        ],
        "Customer Lifetime Value": [
          "Intro",
          "CLV",
          "Feature Engineering",
          "Calculating lifetime value",
          "Outliers and Classification of Ltv",
          "Preparing the data for modeling",
          "Decision tree without tuning",
          "Randomized search CV",
          "Conclusion",
          "Conclusion Final",
          "Assignment",
          "Assignment Answer"
        ],
        "Churn Prediction with Logistic Regression": [
          "Churn Prediction",
          "Why is Churn Prediction important",
          "Data Orientation",
          "Odds and Odds ratio",
          "Another example",
          "Logistic Regression",
          "Importing data in notebook",
          "Feature Engineering",
          "Visualization",
          "Histograms",
          "Preparing the data for modelling",
          "Interpreting the logistic model",
          "Confusion matrix",
          "Precision and Recall",
          "Interpreting the threshold",
          "Log Odds",
          "Fitting the model manually",
          "Understanding Probability",
          "Patsy",
          "Interaction terms",
          "Fitting the interaction model",
          "Lasso Regression",
          "Conclusion",
          "Data Description for assignment",
          "Churn answer"
        ],
        "Market Basket Analysis": [
          "Market Basket",
          "Lecture",
          "Importing the data",
          "Visualizing Baskets",
          "Preparing the data for Market Basket",
          "Apriori and Association rules",
          "Slow moving items",
          "Conclusion"
        ],
        "Recommendation Systems- Collaborative Based Fiiltering": [
          "Intro",
          "Collaborative Based filtering",
          "Item to Item Vs User to User",
          "SVD Alghoritm",
          "Preparing the model",
          "Training on full dataset",
          "Prediction Customer rating",
          "Assignment",
          "Assignment answer"
        ]
      },
      "requirements": [
        "Basic Retail Knowledge",
        "Python Crash courses are included."
      ],
      "description": "\"This is one of the three courses in the Retail Series by RA, each course can be taken independently.\"\n\n\nMaster Retail management and analytics with Excel and Python\nRetailers face fierce competition every day and keeping up with the new trends and customer preferences is a guarantee for excellence in the modern retail environment. one Keyway to excel in retail management is utilizing the data that is produced every day. It is estimated that We produce an overwhelming amount of data every day, roughly 2.5 quintillion bytes. According to an IBM study, 90% of the world’s data has been created in the last two years.\n\n\nRetail analytics is the field of studying the produced retail data and making insightful data-driven decisions from it. as this is a wide field, I have split the Program into three parts. in this course, we focus on the customer analytics part of retail. Understanding the customer is key for maintaining loyalty and developing products to boost retail business and profitability.\n\n\n\n\nRA: Retail Customer Analytics and Trade Area Modeling.\n\n\n1- Understanding the importance of customer analytics in retail.\n2- Manipulation of Data with Pandas.\n3-Working with Python for analytics.\n5- Trade area modeling\n6- Recommendation systems\n7-  Customer lifetime value  prediction\n8- Market Basket analytics\n9- Churn prediction\n\n\nDon't worry If you don't know how to code, we learn step by step by applying retail analysis!\n*NOTE: Full Program includes downloadable resources and Python project files, homework and Program quizzes, lifetime access, and a 30-day money-back guarantee.\nWho this Program is for:\n· If you are an absolute beginner at coding, then take this Program.\n· If you work in Retail and want to make data-driven decisions, this Program will equip you with what you need.\n· If you are switching from Excel to a data science language. then this Program will fast-track your goal.\n· If you are tired of doing the same analysis again and again on spreadsheets and want to find ways to automate it, this Program is for you.\n\n\nProgram Design\nthe Program is designed as experiential learning Modules, the first couple of modules are for retail fundamentals followed by Python programming fundamentals, this is to level all of the takers of this Program to the same pace. and the third part is retail applications using Data science which is using the knowledge of the first two modules to apply. while the Program delivery method will be a mix of me explaining the concepts on a whiteboard, Presentations, and Python-coding sessions where you do the coding with me step by step. there will be assessments in most of the sections to strengthen your newly acquired skills. all the practice and assessments are real retail use cases.",
      "target_audience": [
        "Retail Managers",
        "Retail Analysts",
        "Data Scientists",
        "Data Analysts",
        "Retail Strategist",
        "Merchandizers"
      ]
    },
    {
      "title": "Recursion, Backtracking and Dynamic Programming in Python",
      "url": "https://www.udemy.com/course/algorithmic-problems-in-python/",
      "bio": "Learn Competitive Programming, Recursion, Backtracking, Divide and Conquer Methods and Dynamic Programming in Python",
      "objectives": [
        "Understanding recursion",
        "Understand backtracking",
        "Understand dynamic programming",
        "Understand divide and conquer methods",
        "Implement 15+ algorithmic problems from scratch",
        "Improve your problem solving skills and become a stronger developer"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Environment Setup": [
          "Installing Python",
          "Installing PyCharm"
        ],
        "Recursion": [
          "What are stack and heap memory?",
          "Stack memory and heap memory simulation",
          "Stack Memory and Heap Memory Quiz",
          "What is recursion (recursive function call)?",
          "Head and tail recursion implementation",
          "Recursion and stack memory (stack overflow)",
          "Recursion optimization in Python",
          "Recursion Quiz",
          "Factorial problem - with head recursion",
          "Factorial problem - visualizing the stack",
          "Transforming a head recursion into a tail recursion",
          "Fibonacci numbers problem - with head recursion",
          "Fibonacci numbers - visualizing the stack memory",
          "Fibonacci-numbers with tail recursion",
          "Fibonacci-numbers with tail recursion solution",
          "Reverse string exercise",
          "Reverse string solution",
          "Towers of Hanoi introduction",
          "Towers of Hanoi implementation",
          "Towers of Hanoi - visualizing the stack",
          "Solving recursion with iteration",
          "Solution - solving recursion with iteration",
          "What is the Euclidean algorithm?",
          "Euclidean algorithm implementation",
          "Recursion and iteration revisited",
          "Recursive Problems Quiz"
        ],
        "Search Algorithms": [
          "What is linear search?",
          "Linear search exercise",
          "Linear search implementation",
          "Linear search with recursion",
          "Solution - linear search with recursion",
          "What is binary (logarithmic) search?",
          "Binary search implementation",
          "Search Algorithms Quiz"
        ],
        "Selection Algorithms": [
          "Selection algorithms introduction",
          "Quickselect introduction - Hoare's algorithm",
          "Quickselect visualization",
          "Hoare's Quickselect Quiz",
          "Quickselect implementation",
          "Sorting with selection exercise",
          "Solution - sorting with selection",
          "What is the problem with pivots?",
          "Selection Algorithms Pivoting Quiz",
          "Advanced selection - median of medians, introselect",
          "Median of medians algorithm implementation",
          "Introselect algorithm - power of combining algorithms",
          "Online selection - the secretary problem",
          "Selection Algorithms Quiz"
        ],
        "Bit Manipulation Problems": [
          "Binary representation of numbers",
          "Logical operators",
          "Binary shift operators",
          "Finding the bit length of an integer",
          "Checking even and odd numbers",
          "Russian peasant problem",
          "Russian peasant multiplication with recursion",
          "Russian peasant multiplication with recursion solution"
        ],
        "Backtracking": [
          "What is backtracking?",
          "Brute-force search and backtracking",
          "Backtracking Quiz",
          "N-queens problem introduction",
          "What is the search tree?",
          "N-queens problem implementation I",
          "N-queens problem implementation II",
          "N-queens problem and stack memory visualization",
          "How to earn $1 million with N-queens problem?",
          "Hamiltonian paths (and cycles) introduction",
          "Hamiltonian cycle illustration",
          "Hamiltonian path implementation I",
          "Hamiltonian path implementation II",
          "Finding Hamiltonian cycles",
          "Solution - finding Hamiltonian cycle",
          "Coloring problem introduction",
          "Coloring problem visualization",
          "Coloring problem exercise",
          "Coloring problem implementation",
          "Knight's tour introduction",
          "Knight's tour implementation I",
          "Knight's tour implementation II",
          "Maze problem introduction",
          "Maze problem implementation",
          "Maze problem and stack memory visualization",
          "Sudoku introduction",
          "Sudoku implementation I",
          "Sudoku implementation II",
          "Sudoku implementation III",
          "What is the problem with backtracking?",
          "Backtracking Problems Quiz"
        ],
        "Dynamic Programming": [
          "Dynamic programming introduction",
          "Dynamic Programming Quiz",
          "Fibonacci numbers introduction",
          "Fibonacci numbers implementation",
          "Knapsack problem introduction",
          "Knapsack problem example",
          "Knapsack problem with recursion (recursion tree)",
          "Knapsack problem with recursion",
          "Solution - knapsack problem with recursion",
          "Knapsack problem implementation",
          "Rod cutting problem introduction",
          "Rod cutting problem example",
          "Rod cutting problem implementation",
          "Subset sum problem introduction",
          "Subset sum problem example",
          "Subset sum implementation",
          "Maximum subarray problem (Kadane's algorithm)",
          "Longest common subsequence introduction",
          "Longest common subsequence implementation",
          "Longest common subsequence with recursion (recursion tree)",
          "Dynamic Programming Quiz"
        ],
        "Optimal Packing Problem": [
          "Bin packing problem introduction",
          "Bin packing problem implementation",
          "Bin packing problem - useful article",
          "Bin Packing Quiz"
        ],
        "Divide and Conquer Algorithms": [
          "What are divide-and-conquer approaches?",
          "Divide and Conquer Quiz",
          "Binary search revisited",
          "Merge sort theory",
          "Merge sort implementation",
          "Merge sort and stack memory visualization",
          "Merge sort in descending order",
          "Solution - sort the items in descending order",
          "Closes pair of points problem introduction I",
          "Closes pair of points problem introduction II",
          "Closes pair of points problem implementation",
          "Closest pair of points - useful article",
          "Closest Pair of Points Quiz"
        ]
      },
      "requirements": [
        "Basic Python"
      ],
      "description": "This course is about the fundamental concepts of algorithmic problems focusing on recursion, backtracking, dynamic programming and divide and conquer approaches. As far as I am concerned, these techniques are very important nowadays, algorithms can be used (and have several applications) in several fields from software engineering to investment banking or R&D.\nSection 1 - RECURSION\nwhat are recursion and recursive methods\nstack memory and heap memory overview\nwhat is stack overflow?\nFibonacci numbers\nfactorial function\ntower of Hanoi problem\nSection 2 - SEARCH ALGORITHMS\nlinear search approach\nbinary search algorithm\nSection 3 - SELECTION ALGORITHMS\nwhat are selection algorithms?\nHoare's algorithm\nhow to find the k-th order statistics in O(N) linear running time?\nquickselect algorithm\nmedian of medians algorithm\nthe secretary problem\nSection 4 - BIT MANIPULATION PROBLEMS\nbinary numbers\nlogical operators and shift operators\nchecking even and odd numbers\nbit length problem\nRussian peasant multiplication\nSection 5 - BACKTRACKING\nwhat is backtracking?\nn-queens problem\nHamiltonian cycle problem\ncoloring problem\nknight's tour problem\nmaze problem\nSudoku problem\nSection 6 - DYNAMIC PROGRAMMING\nwhat is dynamic programming?\nknapsack problem\nrod cutting problem\nsubset sum problem\nKadane's algorithm\nlongest common subsequence (LCS) problem\nSection 7 - OPTIMAL PACKING\nwhat is optimal packing?\nbin packing problem\nSection 8 - DIVIDE AND CONQUER APPROACHES\nwhat is the divide and conquer approach?\ndynamic programming and divide and conquer method\nhow to achieve sorting in O(NlogN) with merge sort?\nthe closest pair of points problem\nSection 9 - Substring Search Algorithms\nsubstring search algorithms\nbrute-force substring search\nZ substring search algorithm\nRabin-Karp algorithm and hashing\nKnuth-Morris-Pratt (KMP) substring search algorithm\nSection 10 - COMMON INTERVIEW QUESTIONS\ntop interview questions (Google, Facebook and Amazon)\nanagram problem\npalindrome problem\ninteger reversion problem\ndutch national flag problem\ntrapping rain water problem\nSection 11 - Algorithms Analysis\nhow to measure the running time of algorithms\nrunning time analysis with big O (ordo), big Ω (omega) and big θ (theta) notations\ncomplexity classes\npolynomial (P) and non-deterministic polynomial (NP) algorithms\nIn each section we will talk about the theoretical background for all of these algorithms then we are going to implement these problems together from scratch in Python.\nThanks for joining the course, let's get started!",
      "target_audience": [
        "This course is meant for newbies who are not familiar with algorithmic problems in the main or students looking for some refresher",
        "Anyone preparing for programming interviews or interested in improving their problem solving skills"
      ]
    },
    {
      "title": "Deploying LLMs: A Practical Guide to LLMOps in Production",
      "url": "https://www.udemy.com/course/deploy-ai-smarter-llm-scalability-ml-ops-cost-efficiency/",
      "bio": "Llama 3, GPT, ML-Ops, Ray, MLFlow, LoRa, AWQ, GPTQ, LLMOps, Deployment, Generative AI, LLMs, Flash Paged Attention, Cost",
      "objectives": [
        "Learn to set-up, configure and deploy large language models with precision, ensuring smooth operation in production environments.",
        "Gain practical skills in ML-Ops with MLflow for effective model management and deployment.",
        "Conduct cost-benefit analyses and apply strategic planning for economical AI project management.",
        "Implement the latest LLM optimization and scaling techniques to enhance model performance."
      ],
      "course_content": {
        "Introduction": [
          "Introduction & Welcome"
        ],
        "Getting Started": [
          "Course Structure: How to get the Most out of this Course",
          "Environment Setup: Prepare and Use the Resource of this Course Right"
        ],
        "Pre-Deployment Strategies": [
          "Ensuring Model Correctness: Evaluation Techniques",
          "Performance Optimization: Exploring Key Dimensions",
          "Balancing Speed and Accuracy: Best Practices"
        ],
        "Advanced Model Management with ML-Ops": [
          "Fundamentals of ML Model Management and ML-Ops",
          "Overview of Effective ML-Ops Frameworks",
          "Machine & GPU Selection: A Guide to Hardware Platforms",
          "Setting up ML-Ops Framework: Introduction to MLflow (Practical)",
          "Getting Started with MLflow: A Practical Approach (Practical)",
          "Training Models with MLflow: A Hands-On Guide (Practical)",
          "MLflow for Model Inference: Techniques and Practices (Practical)",
          "Advanced Techniques in MLflow: Extending Functionality (Practical)",
          "Mastering Model Deployment: A Quick Practice Test"
        ],
        "Advanced Model Deployment Techniques": [
          "Efficiency through Batching and Dynamic Batches",
          "Hands-on Application of Batching Techniques (Practical)",
          "The Role of Sorting in Model Deployment (Practical)",
          "Leveraging Quantization for Model Efficiency (Practical)",
          "Inference Strategies: Parallelism, Flash Attention, GPTQ & AWQ,",
          "Next-Gen Scaling: LoRa, Paged Attention, ZeRO"
        ],
        "The Economics of Machine Learning Inference": [
          "The Broader Context of AI: A Wider Perspective",
          "Measuring Performance: Key Metrics for Large AI Projects",
          "Evaluating Deployment Strategies for Cost & Efficiency",
          "Real-World Benchmarks for Success: Case Studies and Insights",
          "Advanced LLMOps: Practice Test"
        ],
        "Effective Cluster Management for Large Scale ML Deployments": [
          "Basic Inference - First Levels of Deployment (Practical)",
          "Entering Optimisations - Advanced Levels of Deployment (Practical)",
          "Setting Up Data Access in Distributed Environments (Practical)",
          "Distributing Data Across a Cluster with RabbitMQ (Practical)",
          "Foundations of Distributed Computing with Ray (Practical)",
          "Scaling Large Language Models on a Cluster (Practical)"
        ],
        "Building Real-Time Streaming APIs for LLMs": [
          "From Batch to Real-Time: An Overview of Streaming Deployments",
          "Deploying Our First Real-Time LLM Server",
          "Testing and Scaling Our LLM Server for Real-Time Requests",
          "Real-Time Streaming API for LLMs",
          "Finalising Deployment: Service, Logging, and Security"
        ]
      },
      "requirements": [
        "Learners should only have a basic understanding of machine learning and proficiency in Python. All the other concepts are though inside the course."
      ],
      "description": "Welcome to the course where you'll learn how to effectively deploy and scale Large Language Models in production environments using LLMOps and cutting edge techniques.\nThis course is designed to equip you with the knowledge and skills required for using large, machine learning models into the real world.\n\n\nWhy This Course Is Essential for Your Career\n\n\n• Accelerate Your Professional Growth: Gain in-demand skills that set you apart in the rapidly evolving field of AI and machine learning.\n• Real-World Impact: Learn how to implement AI models that solve actual business challenges and drive innovation.\n• Optimize Performance and Costs: Master techniques to enhance model efficiency without compromising on quality, ensuring optimal resource utilisation.\n• Stay Ahead with Cutting-Edge Techniques: Dive into the latest advancements like Flash Attention, GPTQ, AWQ, and more to keep your skills current.\n\n\nWhat You’ll Learn\n\n\n1. Foundations of AI Deployment\n\n\n• LLMOps Essentials: Understand the operations behind managing large language models for seamless deployment.\n• Model Evaluation: Learn techniques to ensure your models are accurate and reliable before going live.\n• Performance Tuning: Optimize your models for speed and efficiency to meet production demands.\n\n\n2. Advanced Model Management with ML-Ops\n\n\n• MLflow Mastery: Set up and utilize MLflow for effective model tracking, versioning, and lifecycle management.\n• Operational Best Practices: Implement ML-Ops strategies for continuous integration and deployment (CI/CD) of AI models.\n• Secure Integration: Learn how to incorporate these practices securely into existing pipelines.\n\n\n3. State-of-the-Art Deployment Techniques\n\n\n• Efficiency Strategies: Implement advanced batching, dynamic batches, and quantization to accelerate model inference.\n• Cutting-Edge Optimizations: Explore innovations like Flash Attention, Paged Attention, GPTQ, AWQ, and LoRa to enhance model performance.\n• Innovative Scaling: Utilize advanced scaling techniques such as ZeRO and DeepSpeed to handle large-scale deployments.\n\n\n4. Economics of Machine Learning Inference\n\n\n• Cost Optimization: Balance performance with cost-effectiveness to maximize ROI.\n• Strategic Planning: Understand the business implications of deployment decisions to make informed choices.\n\n\n5. Cluster Management for Scalability\n\n\n• Distributed Deployments: Master techniques for deploying LLMs across clusters for high availability and scalability.\n• Distributed Dataflow with RabbitMQ: Learn how to manage large-scale data movement efficiently.\n• Scaling Compute Resources: Implement frameworks to accelerate AI workloads over multiple machines.\n\n\nWho Should Enroll\n\n\n• Machine Learning Engineers: Enhance your deployment toolkit with advanced skills in LLMOps and AI scaling.\n• Data Scientists and AI Practitioners: Transition your models from development to production with confidence.\n• Tech Professionals and Developers: Expand your expertise into AI deployment to stay competitive in the tech industry.\n• Business Leaders and Managers: Gain insights into the technical and economic aspects of AI deployment for strategic decision-making.\n\n\nCourse Benefits\n\n\n• Hands-On Experience: Engage in practical exercises and real-world projects to solidify your learning.\n• Expert Guidance: Learn from industry professionals with extensive experience in AI and machine learning deployment.\n• Career Advancement: Equip yourself with skills that are highly sought after in the tech industry.\n• Community Support: Join a network of like-minded professionals to collaborate and grow together.\n\n\nWhy Choose This Course\n\n\n• Comprehensive Curriculum: Covers everything from pre-deployment essentials to advanced scaling techniques.\n• Up-to-Date Content: Stay current with the latest trends and technologies in AI deployment.\n• Flexible Learning: Access course materials anytime, anywhere, and learn at your own pace.\n• Certification: Receive a certificate upon completion to showcase your new skills to employers.\n\n\nFrequently Asked Questions\n\n\nQ: Do I need prior experience in AI or machine learning?\n\n\nA: A basic understanding of machine learning concepts is recommended. However, the course includes introductory modules to bring you up to speed.\n\n\nQ: How will this course benefit my career?\n\n\nA: You’ll acquire practical skills in deploying AI models, making you a valuable asset to organizations seeking to implement AI solutions effectively.\n\n\nQ: Is this course suitable for teams?\n\n\nA: Absolutely! The course is designed to benefit both individuals and teams looking to enhance their AI deployment capabilities.\n\n\nTake the Next Step in Your AI Journey\n\n\nDon’t miss out on the opportunity to master the deployment of large language models and advance your career. Enroll today and start transforming how AI models are integrated into real-world applications.\n\n\nEnroll Now and Start Your Journey to AI Deployment Mastery!\n\n\nBy enrolling in this course, you’re investing in a future where you can confidently deploy AI models that make a significant impact. Join us and become a leader in the exciting field of AI deployment.",
      "target_audience": [
        "This course is tailored for AI practitioners, data scientists, software engineers, and business professionals aiming to integrate AI into their operations, offering deep insights into deploying large language models with an emphasis on scalability, cost efficiency, and ML-Ops, making it valuable for any company looking to leverage AI for a strategic advantage."
      ]
    },
    {
      "title": "Data Science Masterclass With R 8 Case Studies + 4 Projects",
      "url": "https://www.udemy.com/course/data-science-complete-course/",
      "bio": "Data Science by IITian -Data Science+R Programming ,Data analysis, Data Visualization, Data Pre-processing etc",
      "objectives": [
        "Learn what is Data Science and how it is helping the modern world!",
        "What are the benefits of Data Science and Machine Learning",
        "Able to Solve Data Science Related Problem with the Help of R Programming",
        "Why R is a Must Have for Data Science , AI and Machine Learning!",
        "Right Guidance of the Path if You want to be a Data Scientist + Data science Interview Preparation Guide",
        "How to switch career in Data Science?",
        "R Data Structure - Matrix, Array, Data Frame, Factor, List",
        "Work with R’s conditional statements, functions, and loops",
        "Systematically Explore data in R",
        "Data Science Package: Dplyr , GGPlot 2",
        "Index, slice, and Subset Data",
        "Get your data in and out of R - CSV, Excel, Database, Web, Text Data",
        "Data Visualization : plot different types of data & draw insights like: Line Chart, Bar Plot, Pie Chart, Histogram, Density Plot, Box Plot, 3D Plot, Mosaic Plot",
        "Data Manipulation - Apply function, mutate(), filter(), arrange (), summarise(), groupby(), date in R",
        "Statistics - A Must have for Data Sciecne",
        "Hypothesis Testing",
        "Have fun with real Life Data Sets"
      ],
      "course_content": {
        "Meet Your Instructor": [
          "Meet Your Instructor",
          "Course Curriculum Overview"
        ],
        "INTRODUCTION TO DATA SCIENCE": [
          "Introduction to Business Analytics",
          "Introduction to Business Analytics",
          "Introduction to Machine Learning",
          "Introduction to Machine Learning",
          "Introduction To Data Scientist",
          "Introduction To Data Scientist",
          "How to switch your career into ML",
          "How to switch your career into ML",
          "How to switch your career into ML",
          "How to switch your career into ML Part #2"
        ],
        "Course Curriculum Overview": [
          "What We are Going to Discuss Over the Course"
        ],
        "INTRODUCTION TO R": [
          "Introduction to R",
          "Introduction to R",
          "Setting up R"
        ],
        "R Programming": [
          "R Programming - R Operator",
          "R Conditional Statement & Loop",
          "R Conditional Statement & Loop Study Note",
          "R Programming - R Function",
          "R Programming - Function in R Study Note #1",
          "R Programming - R Function #2",
          "R Programming - R Function #2",
          "R Programming - R Function #3",
          "R Programming - R Function #3",
          "All Codes : R Programming Study Note"
        ],
        "R Data Structure": [
          "R Data Structure - Vector",
          "Vector Study Note",
          "All Code - Vector",
          "Matrix, Array and Data Frame",
          "Matrix, Array and Data Frame - Study Note",
          "CODES - Matrix, Array and Data Frame",
          "Code - Data Frame Part #2",
          "A Deep Drive to R Data Frame",
          "A Deep Drive to R Data Frame - Study Note",
          "R Data Structure - Factor",
          "R Data Structure - Factor Study Notes",
          "Codes - Factor",
          "R Data Structure - List",
          "List - Study Note",
          "Code - List",
          "All Code : R Data Structure"
        ],
        "Import and Export in R": [
          "Import CSV Data in R",
          "Import CSV Data in R study note",
          "Code - Import CSV Data in R",
          "Import Text Data in R",
          "Import Text Data in R STUDY NOTE",
          "CODE - Import Text Data in R",
          "Import Excel, Web Data in R",
          "Import Excel, Web Data in R STUDY NOTE",
          "Export Data in R - Text",
          "Export Data in R - CSV & Excel",
          "Export Data in R - Text,CSV & Excel Study Note",
          "All Code: Import and Export in R"
        ],
        "Data Manipulation": [
          "Data Manipulation - Apply Function",
          "Data Manipulation - Apply Function STUDY NOTE",
          "Data Manipulation - select",
          "Data Manipulation - mutate",
          "Data Manipulation - filter",
          "Data Manipulation - arrange",
          "mutate(),filter(),arrange() Function Study Note",
          "Data Manipulation - Pipe Operator",
          "Pipe operator Study Note",
          "Data Manipulation - group by",
          "Group By Function Study Note",
          "Data Manipulation - Date",
          "Data Manipulation - Date with R STUDY NOTE",
          "All Code: Data Manipulation"
        ],
        "Data Visualization": [
          "Introduction to Data Visualization & Scatter Plot",
          "Data Visualization - Scatter Plot Study Note",
          "Data Visualization - mfrow",
          "Data Visualization - mfrow Study Note",
          "Data Visualization - pch",
          "Data Visualization - pch Study Note",
          "Data Visualization - Color",
          "Data Visualization - Color Study Note",
          "Data Visualization - Line Chart",
          "Data Visualization - Line Chart Study Note",
          "Data Visualization - Bar Plot",
          "Data Visualization - Bar Plot STUDY NOTE",
          "Data Visualization - Pie Chart",
          "Data Visualization - Pie Chart STUDY NOTE",
          "Data Visualization - Histogram",
          "Data Visualization - Histogram STUDY NOTE",
          "Data Visualization - Density Plot",
          "Data Visualization - Density Plot STUDY NOTE",
          "Data Visualization - Box Plot",
          "Data Visualization - Box Plot STUDY NOTE",
          "Data Visualization - Mosaic Plot and Heat Map",
          "Data Visualization - Mosaic Plot and Heat Map SYUDY NOTE",
          "Data Visualization - 3D Plot",
          "Data Visualization - 3D Plot STUDY NOTE",
          "Correlation Plot and Word Cloud",
          "Data Visualization - Word Cloud STUDY NOTE",
          "Data Visualization - ggplot2 Part 1",
          "Data Visualization - ggplot2 Part 2",
          "PART 2 Data Visualization - ggplot2",
          "Data Visualization - ggplot2 Part 3",
          "PART 3 Data Visualization - ggplot2",
          "All Code: Data Visualization",
          "Par Function Code"
        ],
        "Introduction To Statistics": [
          "Intro To Stat - Part 1",
          "Introduction To Statistic - Part 1 STUDY NOTE",
          "Intro To Stat - Part 2",
          "Introduction To Statistic - Part 2 STUDY NOTE",
          "Intro To Stat - Part 3",
          "Introduction To Statistic - Part 3 STUDY NOTE",
          "Intro To Stat - Part 4",
          "Part 4 Introduction To Statistic STUDY NOTE",
          "Intro To Stat - Part 5",
          "Introduction To Statistic - Part 5 STUDY NOTE",
          "Intro To Stat - Part 6",
          "Intro To Stat - Part 7",
          "Introduction To Statistic - Part 7 STUDY NOTE",
          "Intro To Stat - Part 8",
          "Introduction To Statistic - Part 8 STUDY NOTE",
          "Intro To Stat - Part 9",
          "Introduction To Statistic - Part 9 STUDY NOTE",
          "Intro To Stat - Part 10",
          "Introduction To Statistic - Part 10 STUDY NOTE",
          "Intro To Stat - Part 11",
          "Introduction To Statistics - All Codes"
        ]
      },
      "requirements": [
        "No prior knowledge is required to understand for the Data Science & Machine Learning Course",
        "R Software will be used in the course. Installation and use of R will be taught in the course.",
        "All Software and data used in the course are free"
      ],
      "description": "Are you planing to build your career in Data Science in This Year?\nDo you the the Average Salary of a Data Scientist is $100,000/yr?\nDo you know over 10 Million+ New Job will be created for the Data Science Filed in Just Next 3 years??\nIf you are a Student / a Job Holder/ a Job Seeker then it is the Right time for you to go for Data Science!\nDo you Ever Wonder that Data Science is the \"Hottest\" Job Globally in 2018 - 2019!\n\n\n>> 30+ Hours Video\n>> 4 Capstone Projects\n>> 8+ Case Studies\n>> 24x7 Support\n\n>>ENROLL TODAY & GET DATA SCIENCE INTERVIEW PREPARATION COURSE FOR FREE <<\n\n\n\n\nWhat Projects We are Going to Cover In the Course?\n\nProject 1- Titanic Case Study which is based on Classification Problem.\nProject 2 - E-commerce Sale Data Analysis - based on Regression.\nProject 3 - Customer Segmentation which is based on Unsupervised learning.\nFinal Project - Market Basket Analysis - based on Association rule mining\n\nWhy Data Science is a MUST HAVE for Now A Days?\n\n\nThe Answer Why Data Science is a Must have for Now a days will take a lot of time to explain. Let's have a look into the Company name who are using Data Science and Machine Learning. Then You will get the Idea How it BOOST your Salary if you have Depth Knowledge in Data Science & Machine Learning!\n\n\nWhat Students Are Saying:\n\"A great course to kick-start journey in Machine Learning. It gives a clear contextual overview in most areas of Machine Learning . The effort in explaining the intuition of algorithms is especially useful\"\n- John Doe, Co-Founder, Impressive LLC\n\n\nI simply love this course and I definitely learned a ton of new concepts.\nNevertheless, I wish there was some real life examples at the end of the course. A few homework problems and solutions would’ve been good enough.\n- - Brain Dee, Data Scientist\n\n\n\n\nIt was amazing experience. I really liked the course. The way the trainers explained the concepts were too good. The only think which I thought was missing was more of real world datasets and application in the course. Overall it was great experience. The course will really help the beginners to gain knowledge. Cheers to the team\n- - Devon Smeeth, Software Developer\nAbove, we just give you a very few examples why you Should move into Data Science and Test the Hot Demanding Job Market Ever Created!\n\n\nThe Good News is That From this Hands On Data Science and Machine Learning in R course You will Learn All the Knowledge what you need to be a MASTER in Data Science.\n\n\nWhy Data Science is a MUST HAVE for Now A Days?\nThe Answer Why Data Science is a Must have for Now a days will take a lot of time to explain. Let's have a look into the Company name who are using Data Science and Machine Learning. Then You will get the Idea How it BOOST your Salary if you have Depth Knowledge in Data Science & Machine Learning!\nHere we list a Very Few Companies : -\nGoogle - For Advertise Serving, Advertise Targeting, Self Driving Car, Super Computer, Google Home etc. Google use Data Science + ML + AI to Take Decision\nApple: Apple Use Data Science in different places like: Siri, Face Detection etc\nFacebook: Data Science , Machine Learning and AI used in Graph Algorithm for Find a Friend, Photo Tagging, Advertising Targeting, Chat bot, Face Detection etc\nNASA: Use Data Science For different Purpose\nMicrosoft: Amplifying human ingenuity with Data Science\nSo From the List of the Companies you can Understand all Big Giant to Very Small Startups all are chessing Data Science and Artificial Intelligence and it the Opportunity for You!\n\n\nWhy Choose This Data Science with R Course?\nWe not only \"How\" to do it but also Cover \"WHY\" to do it?\nTheory explained by Hands On Example!\n30+ Hours Long Data Science Course\n100+ Study Materials on Each and Every Topic of Data Science!\nCode Templates are Ready to Download! Save a lot of Time\n\n\nWhat You Will Learn From The Data Science MASTERCLASS Course:\nLearn what is Data science and how Data Science is helping the modern world!\nWhat are the benefits of Data Science , Machine Learning and Artificial Intelligence\nAble to Solve Data Science Related Problem with the Help of R Programming\nWhy R is a Must Have for Data Science , AI and Machine Learning!\nRight Guidance of the Path if You want to be a Data Scientist + Data Science Interview Preparation Guide\nHow to switch career in Data Science?\nR Data Structure - Matrix, Array, Data Frame, Factor, List\nWork with R’s conditional statements, functions, and loops\nSystematically explore data in R\nData Science Package: Dplyr , GGPlot 2\nIndex, slice, and Subset Data\nGet your data in and out of R - CSV, Excel, Database, Web, Text Data\nData Science - Data Visualization : plot different types of data & draw insights like: Line Chart, Bar Plot, Pie Chart, Histogram, Density Plot, Box Plot, 3D Plot, Mosaic Plot\nData Science - Data Manipulation - Apply function, mutate(), filter(), arrange (), summarise(), groupby(), date in R\nStatistics - A Must have for Data Science\nData Science - Hypothesis Testing\nBusiness Use Case Understanding\nData Pre-processing\nSupervised Learning\nLogistic Regression\nK-NN\nSVM\nNaive Bayes\nDecision Tree\nRandom Forest\nK-Mean Clustering\nHierarchical Clustering\nDBScan Clustering\nPCA (Principal Component Analysis)\nAssociation Rule Mining\nModel Deployment\n\n\n>> 30+ Hours Video\n>> 4 Capstone Projects\n>> 8+ Case Studies\n>> 24x7 Support\n\n>>ENROLL TODAY & GET DATA SCIENCE INTERVIEW PREPARATION COURSE FOR FREE <<",
      "target_audience": [
        "Anyone who is interested in Data Science can take this course.",
        "Aspiring Data Scientists",
        "Anyone who wants to switch his career in Data Science/Analytics/Machine Learning should take this course.",
        "Beginners to any Programming and Interested In the Amazing world of Machine Learning , Artificial Intelligence & Data Science",
        "People interested in Statistics and Data Analysis"
      ]
    },
    {
      "title": "Math for Data Science Masterclass",
      "url": "https://www.udemy.com/course/math-for-data-science-masterclass/",
      "bio": "Learn about probability, statistics, and more using the mathematics that are foundational to the field of data science.",
      "objectives": [
        "Understand core concepts about data quality and quantity",
        "Learn about how to measure data with statistics",
        "Discover how to visualize data with a variety of plot types",
        "Use combinatorics to calculate permutations and combinations of objects",
        "Understand the key ideas in using probability to solve problems",
        "Learn how to use data distributions with real world data",
        "Discover the powerful insights from the normal distribution",
        "Use sampling and the central limit theorem",
        "Understand hypothesis testing on sample groups",
        "Cover the basics of linear regression"
      ],
      "course_content": {},
      "requirements": [
        "Only basic arithmetic skills are needed, we'll teach you the rest"
      ],
      "description": "Welcome to the best online course for learning about the Math behind the field of Data Science!\nWorking together for the first time ever, Krista King and Jose Portilla have combined forces to deliver you a best in class course experience in how to use mathematics to solve real world data science problems. This course has been specifically designed to help you understand the mathematical concepts behind the field of data science, so you can have a first principles level understanding of how to use data effectively in an organization.\nOften students entering the field of data science are confused on where to start to learn about the fundamental math behind the concepts. This course was specifically designed to help bridge that gap and provide students a clear, guided path through the complex and interesting world of math used in the field of data science. Designed to balance theory and application, this is the ultimate learning experience for anyone wanting to really understand data science.\nWhy choose this course?\nCombined together, Krista and Jose have taught over 3.2 million students about data science and mathematics and their joint expertise means you'll be able to get the best and clearest mathematical explanations from Krista with framing about real world data science applications from Jose.  At the end of each section is a set of practice problems developed from real-world company situations, where you can directly apply what you know to test your understanding.\nWhat's covered in this course?\nIn this course, we'll cover:\nUnderstanding Data Concepts\nMeasurements of Dispersion and Central Tendency\nDifferent ways to visualize data\nPermutations\nCombinatorics\nBayes' Theorem\nRandom Variables\nJoint Distributions\nCovariance and Correlation\nProbability Mass and Density Functions\nBinomial, Bernoulli, and Poisson Distributions\nNormal Distribution and Z-Scores\nSampling and Bias\nCentral Limit Theorem\nHypothesis Testing\nLinear Regression\nand much more!\nEnroll today and we'll see you inside the course!\nKrista and Jose",
      "target_audience": [
        "Anyone interested in learning more about the mathematics behind data science"
      ]
    },
    {
      "title": "Causal Data Science with Directed Acyclic Graphs",
      "url": "https://www.udemy.com/course/causal-data-science/",
      "bio": "Get to know the modern tools for causal inference from machine learning and AI, with many practical examples in R",
      "objectives": [
        "Causal inference in data science and machine learning",
        "How to work with directed acylic graphs (DAG)",
        "Newest developments in causal AI"
      ],
      "course_content": {
        "Introduction": [
          "Welcome"
        ],
        "Structural Causal Models, Interventions, and Graphs": [
          "Directed Acyclic Graphs",
          "Structural Causal Models",
          "D-Separation",
          "Interventions",
          "R Examples",
          "Appendix"
        ],
        "Causal Discovery": [
          "Testable Implications of DAGs",
          "R Interlude",
          "Causal Discovery",
          "The PC Algorithm",
          "Practical Considerations"
        ],
        "Confounding Bias and Surrogate Experiments": [
          "Confounding Bias",
          "Backdoor Adjustment",
          "Frontdoor Adjustment",
          "Do-Calculus",
          "R Examples 1",
          "Z-Identification",
          "R Examples 2"
        ],
        "Recovering from Selection Bias": [
          "Selection Bias",
          "Recovering from Selelection Bias",
          "R Examples"
        ],
        "Transportability of Causal Knowledge Across Domains": [
          "The Transportability Task",
          "S-Admissibility and Do-Calculus",
          "Mz-Transportability",
          "R Examples"
        ],
        "Outro": [
          "The Causal Data Science Process"
        ]
      },
      "requirements": [
        "Basic knowledge of probability and statistcs",
        "Basic programming skills would be an advantage"
      ],
      "description": "This course offers an introduction into causal data science with directed acyclic graphs (DAG). DAGs combine mathematical graph theory with statistical probability concepts and provide a powerful approach to causal reasoning. Originally developed in the computer science and artificial intelligence field, they recently gained increasing traction also in other scientific disciplines (such as machine learning, economics, finance, health sciences, and philosophy). DAGs allow to check the validity of causal statements based on intuitive graphical criteria, that do not require algebra. In addition, they open the possibility to completely automatize the causal inference task with the help of special identification algorithms. As an encompassing framework for causal thinking, DAGs are becoming an essential tool for everyone interested in data science and machine learning.\nThe course provides a good overview of the theoretical advances that have been made in causal data science during the last thirty year. The focus lies on practical applications of the theory and students will be put into the position to apply causal data science methods in their own work. Hands-on examples, using the statistical software R, will guide through the presented material. There are no particular prerequisites, but a good working knowledge in basic statistics and some programming skills are a benefit.",
      "target_audience": [
        "Data scientists",
        "Economists",
        "Computer Scientists",
        "People intersted in machine learning"
      ]
    },
    {
      "title": "Getting Started with Data Management",
      "url": "https://www.udemy.com/course/getting-started-with-data-management/",
      "bio": "Learn the basics of how organizations deal with data. Analyze data using Python and SQL. Explore Hadoop and Hive.",
      "objectives": [
        "Get started working with data",
        "Understand big data fundamentals",
        "Explore tabular data with Python",
        "Learn the fundamentals of SQL",
        "Use the ETL workflow for data integration"
      ],
      "course_content": {
        "The Value of Data Management": [
          "The Value of Data Management",
          "The Lifecycle of Data"
        ],
        "Understanding Data Governance": [
          "Why Do We Need Data Governance",
          "Understanding Data Stewardship",
          "Implementing Master Data Management",
          "Developing Data Definitions",
          "The Importance of Standardization"
        ],
        "Exploring Tabular Data": [
          "Understanding Tabular Data",
          "Installing Jupyter Notebook",
          "Exploring Tabular Data",
          "Most Common Names in California",
          "How Frequently Does Your Name Occur?",
          "Statistical Data Types",
          "Visualizing Data"
        ],
        "Relational Database Basics": [
          "Database Management Systems",
          "The Database Development Lifecycle"
        ],
        "Improving Data Quality": [
          "Removing Duplicate Data",
          "Removing Inconsistent Data",
          "Breaking-down Data into Smaller Components",
          "Requiring Complete Information",
          "Designating the Primary Key"
        ],
        "Database Design": [
          "One to Many Relationship",
          "Many to Many Relationship",
          "Integrity Constraints",
          "Indexing for Performance"
        ],
        "SQL": [
          "Downloading Oracle XE",
          "Installing Oracle XE",
          "Opening the Oracle XE Database",
          "Connecting to Your Brand New Database",
          "Oracle Live SQL",
          "The SELECT Statement",
          "Restricting and Sorting Data",
          "Using Single-row Functions",
          "Aggregating Data with Group Functions",
          "Displaying Data from Multiple Tables",
          "Answering Multi-step Questions Using Subqueries"
        ],
        "Up and Running with Ubuntu Linux": [
          "Downloading Ubuntu and Workstation Pro",
          "Up and Running with Ubuntu",
          "Transferring Files Using SFTP"
        ],
        "Data Integration": [
          "Setting-up Your Postgres Database",
          "Setting-up Your Postgres Data Sources",
          "Installing Python and PySpark",
          "Installing Java and JDBC for Postgres",
          "Extracting Data",
          "Transforming Data",
          "Loading Data"
        ],
        "Understanding Big Data": [
          "Big Data Overview",
          "Types of Big Data",
          "Limitations of Relational Systems",
          "Introducing Hadoop",
          "Setting-up Hadoop",
          "Connecting to Your Hadoop VM",
          "Issuing SQL Queries through Hive"
        ]
      },
      "requirements": [
        "If you know how to install software to your computer, you're good to go"
      ],
      "description": "Data Management is one of the most important competencies your company has. With Digital Transformation at the top of the strategic agenda for many large organizations, Data Governance and Data Management are vital to building a strong foundation for integration, analysis, execution, and overall business value. Business and data professionals are currently facing The Fourth Industrial Revolution's convergence of megatrends around Customer 360, Artificial Intelligence, Big Data, programmatic marketing, and globalization. To survive these unrelenting business pressures, it's more critical, and strategic, than ever to put your data to work!\nIn this course, you will learn about the various disciplines of data management. First, you will discover what Data Governance is and why you might want to implement a governance program for your organization, after which you will go through some very basic exploratory Data Analysis using the Python programming language.\nNext up, you'll cover basic Database Design, Data Quality essentials, and the fundamentals of the Structured Query Language. Then, you will get hands-on with some rudimentary Data Integration ETL, as well as Big Data with Hadoop.\nFinally, you will explore the various disciplines in the Data Management space.\nBy the end of the course, you will have a firm understanding of enterprise data management and what the various disciplines do.",
      "target_audience": [
        "Anyone seeking a career in data management",
        "Aspiring DBAs, Developers, Data Analysts, Data Scientists and Data Engineers"
      ]
    },
    {
      "title": "Practical Deep Learning with PyTorch",
      "url": "https://www.udemy.com/course/practical-deep-learning-with-pytorch/",
      "bio": "Accelerate your deep learning with PyTorch covering all the fundamentals of deep learning with a python-first framework.",
      "objectives": [
        "Effectively wield PyTorch, a Python-first framework, to build your deep learning projects",
        "Master deep learning concepts and implement them in PyTorch"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Software Requirements": [
          "CPU Software Requirements",
          "CPU Installation of PyTorch",
          "PyTorch with GPU on AWS",
          "PyTorch with GPU on Linux",
          "PyTorch with GPU on MacOSX"
        ],
        "PyTorch Fundamentals: Matrices": [
          "Matrix Basics",
          "Seed for Reproducibility",
          "Torch to NumPy Bridge",
          "NumPy to Torch Bridge",
          "GPU and CPU Toggling",
          "Basic Mathematical Tensor Operations",
          "Summary of Matrices"
        ],
        "PyTorch Fundamentals: Variables and Gradients": [
          "Variables",
          "Gradients",
          "Summary of Variables and Gradients"
        ],
        "Linear Regression with PyTorch": [
          "Linear Regression Introduction",
          "Linear Regression in PyTorch",
          "Linear Regression From CPU to GPU in PyTorch",
          "Summary of Linear Regression"
        ],
        "Logistic Regression with PyTorch": [
          "Logistic Regression Introduction",
          "Linear Regression Problems",
          "Logistic Regression In-depth",
          "Logistic Regression with PyTorch",
          "Logistic Regression From CPU to GPU in PyTorch",
          "Summary of Logistic Regression"
        ],
        "Feedforward Neural Network with PyTorch": [
          "Logistic Regression Transition to Feedforward Neural Network",
          "Non-linearity",
          "Feedforward Neural Network in PyTorch",
          "More Feedforward Neural Network Models in PyTorch",
          "Feedforward Neural Network From CPU to GPU in PyTorch",
          "Summary of Feedforward Neural Network"
        ],
        "Convolutional Neural Network (CNN) with PyTorch": [
          "Feedforward Neural Network Transition to CNN",
          "One Convolutional Layer, Input Depth of 1",
          "One Convolutional Layer, Input Depth of 3",
          "One Convolutional Layer Summary",
          "Multiple Convolutional Layers Overview",
          "Pooling Layers",
          "Padding for Convolutional Layers",
          "Output Size Calculation",
          "CNN in PyTorch",
          "More CNN Models in PyTorch",
          "CNN Models Summary",
          "Expanding Model's Capacity",
          "CNN From CPU to GPU in PyTorch",
          "Summary of CNN"
        ],
        "Recurrent Neural Networks (RNN)": [
          "Introduction to RNN",
          "RNN in PyTorch",
          "More RNN Models in PyTorch",
          "RNN From CPU to GPU in PyTorch",
          "Summary of RNN"
        ],
        "Long Short-Term Memory Networks (LSTM)": [
          "Introduction to LSTMs",
          "LSTM Equations",
          "LSTM in PyTorch",
          "More LSTM Models in PyTorch",
          "LSTM From CPU to GPU in PyTorch",
          "Summary of LSTM"
        ]
      },
      "requirements": [
        "You need to know basic python such as lists, dictionaries, loops, functions and classes",
        "You need to know basic differentiation",
        "You need to know basic algebra"
      ],
      "description": "Growing Importance of Deep Learning\n\nDeep learning underpins a lot of important and increasingly important applications today ranging from facial recognition, to self-driving cars, to medical diagnostics and more.\nMade for Anyone\nAlthough many courses are very mathematical or too practical in nature, this course strikes a careful balance between the two to provide a solid foundation in deep learning for you to explore further if you are interested in research in the field of deep learning and/or applied deep learning. It is purposefully made for anyone without a strong background in mathematics. And for those with a strong background, it would accelerate your learning in understanding the different models in deep learning.\nCode As You Learn\nThis entire course is delivered in a Python Notebook such that you can follow along the videos and replicate the results. You can practice and tweak the models until you truly understand every line of code as we go along. I highly recommend you to type every line of code when you are listening to the videos as this will help a lot in getting used to the syntax.\nGradual Learning Style\nThe thing about many guides out there is that they lack the transition from the very basics and people often get lost or miss out vital links that are critical in understanding certain models. Because of this, you can see how every single topic is closely linked with one another. In fact, at the beginning of every topic from logistic regression, I take the time to carefully explain how one model is simply a modification from the previous. That is the marvel of deep learning, we can trace back some part of it to linear regression where we will start.\nDiagram-Driven Code\nThis course uses more than 100 custom-made diagrams where I took hundreds of hours to carefully create such that you can clearly see the transition from one model to another and understand the models comprehensively. Also, the diagrams are created so you can clearly see the link between the theory that I would teach and the code you would learn.\nMentor Availability\nWhen I first started learning, I wished I had a mentor to guide me through the basics till the advanced theories where you can publish research papers and/or implement very complicated projects. And this course provides you with free access to ask any question, no matter how basic. I will be there and try my very best to answer your question. Even if the material is covered here, I will take the effort to point you to where you can learn here and more resources beyond this course.\n\n\nMath Prerequisite FAQ\nThis is not a course that emphasizes heavily on the mathematics behind deep learning. It focuses on getting you to understand how everything works first which is very important for you to easily catch up on the mathematics later on. There are mathematics involved but they are limited with the sole aim to enhance your understanding and provide a gentle learning curve for future courses that would dive much deeper into it.\n\n\nLatest Python Notebooks Compatible with PyTorch 0.4 and 1.0\nThere are very small changes from PyTorch 0.3 for this deep learning series where you will find it is extremely easy to transit over!",
      "target_audience": [
        "Anyone who wants to learn deep learning",
        "Deep learning researchers using other frameworks like TensorFlow, Keras, Torch, and Caffe",
        "Any python programmer"
      ]
    },
    {
      "title": "Master Langchain and Ollama - Chatbot, RAG and Agents",
      "url": "https://www.udemy.com/course/ollama-and-langchain/",
      "bio": "Master Langchain v0.3, Local LLM Projects, Ollama, DeepSeek, LLAMA 3.2, Ollama Chatbot, Ollama and Langchain Tutorial",
      "objectives": [
        "Set up and Integrate Ollama with Langchain: Students will learn how to install, configure, and operate Ollama alongside Langchain.",
        "Build Custom Chatbots: Learners will develop skills to create chat applications with memory, history, advanced chatbot features using Streamlit and Langchain.",
        "Use Prompt Templates, Chains, and Output Parsers: Students will master prompt templates and chaining methods (Sequential, Parallel, and Router Chains).",
        "Deploy Real-World Applications: The course will guide students through deploying applications on AWS EC2"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Code Files and Install Requirements.txt"
        ],
        "Latest LLM Updates": [
          "Run Deep Seek R1 Models Locally with Ollama"
        ],
        "Ollama Setup": [
          "Install Ollama",
          "Touch Base with Ollama",
          "Inspecting LLAMA 3.2 Model",
          "LLAMA 3.2 Benchmarking Overview",
          "What Type of Models are Available on Ollama",
          "Ollama Commands - ollama server, ollama show",
          "Ollama Commands - ollama pull, ollama list, ollama rm",
          "Ollama Commands - ollama cp, ollama run, ollama ps, ollama stop",
          "Create and Run Ollama Model with Predefined Settings",
          "Ollama Model Commands - /show",
          "Ollama Model Commands - /set, /clear, /save_model and /load_model",
          "Ollama Raw API Requests",
          "Load Uncesored Models for Banned Content Generation [Only Educational Purpose]"
        ],
        "Getting Started with Langchain": [
          "Langchain Introduction | LangChain (Lang Chain) Intro",
          "Lanchain Installation | LangChain (Lang Chain) Intro",
          "Langsmith Setup of LLM Observability | LangChain (Lang Chain) Intro",
          "Calling Your First Langchain Ollama API | LangChain (Lang Chain) Intro",
          "Generating Uncensored Content in Langchain [Educational Purpose]",
          "Trace LLM Input Output at Langsmith | LangChain (Lang Chain) Intro",
          "Going a lot Deeper in the Langchain | LangChain (Lang Chain) Intro"
        ],
        "Chat Prompt Templates": [
          "Why We Need Prompt Template",
          "Type of Messages Needed for LLM",
          "Circle Back to ChatOllama",
          "Use Langchain Message Types with ChatOllama",
          "Langchain Prompt Templates",
          "Prompt Templates with ChatOllama"
        ],
        "Chains": [
          "Introduction to LCEL",
          "Create Your First LCEL Chain",
          "Adding StrOutputParser with Your Chain",
          "Chaining Runnables (Chain Multiple Runnables)",
          "Run Chains in Parallel Part 1",
          "Run Chains in Parallel Part 2",
          "How Chain Router Works",
          "Creating Independent Chains for Positive and Negative Reviews",
          "Route Your Answer Generation to Correct Chain",
          "What is RunnableLambda and RunnablePassthrough",
          "Make Your Custom Runnable Chain",
          "Create Custom Chain with chain Decorator"
        ],
        "Output Parsing": [
          "What is Output Parsing",
          "What is Pydantic Parser",
          "Get Pydantic Parser Instruction",
          "Parse LLM Output Using Pydantic Parser",
          "Parsing with `.with_structured_output()` method",
          "JSON Output Parser",
          "CSV Output Parsing - CommaSeparatedListOutputParser",
          "Datetime Output Parsing"
        ],
        "Chat Message Memory | How to Keep Chat History": [
          "How to Save and Load Chat Message History (Concept)",
          "Simple Chain Setup",
          "Chat Message with History Part 1",
          "Chat Message with History Part 2",
          "Chat Message with History using MessagesPlaceholder"
        ],
        "Make Your Own Chatbot Application": [
          "Introduction",
          "Introduction To Streamlit and Our Chat Application",
          "Chat Bot Basic Code Setup",
          "Create Chat History in Streamlit Session State",
          "Create LLM Chat Input Area with Streamlit",
          "Update Historical Chat on Streamlit UI",
          "Complete Your Own Chat Bot Application",
          "Stream Output of Your Chat Bot like ChatGPT"
        ],
        "Document Loaders | Projects on PDF Documents": [
          "Introduction to PDF Document Loaders",
          "Load Single PDF Document with PyMuPDFLoader",
          "Load All PDFs from a Directory",
          "Combine All PDFs Data as Context Text",
          "How Many Tokens are There in Contex Data.",
          "Make Question Answer Prompt Templates and Chain",
          "Project 1 - Ask Questions from Your PDF Documents",
          "Project 2 - Summarize Your PDF Documents",
          "Project 3 - Generate Detailed Structured Report from the PDF Documents"
        ]
      },
      "requirements": [
        "Basic Python programming knowledge",
        "Familiarity with APIs and web requests",
        "Basic understanding of machine learning concepts",
        "Access to a computer with internet for installations and setups"
      ],
      "description": "This course is a practical guide to integrating Langchain and Ollama to build, automate, and deploy AI applications. Learn to set up these tools, create prompt templates, automate workflows, manage data retrieval, and deploy real-world applications on AWS. Each section is designed to provide you with hands-on skills and experience.\n\n\nWhat You Will Learn\nOllama & Langchain Setup\nComplete setup and installation of Ollama and Langchain.\nConfigure base URLs and handle direct API calls.\nEstablish the environment for efficient integration.\nPrompt Engineering\nUnderstand AI, human, and system message prompts.\nUse AIPromptTemplate, Human, System, and ChatMessagePromptTemplate to shape responses.\nExplore the invoke method to control the model's behavior.\nChains for Workflow Automation\nLearn Sequential, Parallel, and Router Chains to build flexible workflows.\nWork with custom chains and explore Chain Runnables for added automation.\nImplement real-world workflows using Langchain's chaining capabilities.\nOutput Parsing\nFormat data with parsers like JSON, CSV, Markdown, and Pydantic.\nParse structured output and use date-time output handling for organized data.\nChat Message Memory\nUse BaseChatMessageHistory and InMemoryChatMessageHistory for managing chat sessions.\nCreate chat applications with memory to improve user experience.\nBuild and Deploy Chatbots\nBuild a chatbot application using Streamlit.\nMaintain chat history and handle user inputs efficiently.\nDocument Loaders and Retrievals\nWork with loaders for web pages, PDFs, HTML data.\nRetrieve and summarize documents, convert text data, and use vector stores.\nVector Stores and Retrievals\nIntegrate vector stores for document retrieval using FAISS and Chroma.\nReload retrievers, index documents, and enhance retrieval accuracy.\nTool Calling and Custom Agents\nSet up tools for Tavily Search, PubMed, Wikipedia, and more.\nDesign custom tools that can be used with the Agents and execute step-by-step instructions.\nReal-World Integrations\nExecute text-based queries on MySQL.\nParse LinkedIn Profile with LLM\nParse Job Resume with LLM\nDeploy LLAMA (LAMA) with OLLAMA on AWS\nWho This Course Is For\nDevelopers and data scientists who want to use Langchain and Ollama for AI applications.\nAI enthusiasts looking to automate workflows and create document retrieval systems.\nProfessionals needing to build end-to-end chatbots or deploy applications on AWS.\nLearners with basic Python knowledge who want practical experience with real-world AI tools.\n\n\nBy the end of this course, you’ll have the skills to build, deploy, and manage AI-powered applications, from chatbots to document retrievers, ready for production.",
      "target_audience": [
        "Developers aiming to integrate language models into applications.",
        "Data scientists interested in automating workflows and leveraging document retrieval.",
        "AI enthusiasts eager to build custom chatbots and conversational tools.",
        "Professionals seeking skills in deploying applications on AWS and other platforms.",
        "Learners with basic Python and API knowledge who want to create end-to-end AI solutions."
      ]
    },
    {
      "title": "LangChain Mastery: Build GenAI Apps with LangChain &Pinecone",
      "url": "https://www.udemy.com/course/master-langchain-pinecone-openai-build-llm-applications/",
      "bio": "Step-by-Step Approach to LangChain and Pinecone for GenAI with LLMs. Develop Real-World LLM-Powered Apps with Python",
      "objectives": [
        "How to Use LangChain, Pinecone, and OpenAI to Build LLM-Powered Applications.",
        "Learn about LangChain components, including LLM wrappers, prompt templates, chains, and agents.",
        "Learn about using multimodal Google's Gemini Pro Vision",
        "How to integrate Google's Gemini Pro and Pro Vision AI models with LangChain",
        "Learn about the different types of chains available in LangChain, such as stuff, map_reduce, refine, and LangChain agents.",
        "Acquire a solid understanding of embeddings and vector data stores.",
        "Learn how to use embeddings and vector data stores to improve the performance of your LangChain applications.",
        "Deep Dive into Pinecone.",
        "Learn about Pinecone Indexes and Similarity Search.",
        "Project: Build an LLM-powered question-answering app with a modern web-based front-end for custom or private documents.",
        "Project: Build a summarization system for large documents using various methods and chains: stuff, map_reduce, refine, or LangChain Agents.",
        "This will be a Learning-by-Doing Experience. We'll Build Together, Step-by-Step, Line-by-Line, Real-World Applications (including front-ends using Streamlit).",
        "You'll learn how to create web interfaces (front-ends) for your LLM and generative AI apps using Streamlit.",
        "Streamlit: main concepts, widgets, session state, callbacks.",
        "Learn how to use Jupyter AI efficiently."
      ],
      "course_content": {
        "Getting Started": [
          "How to Get the Most Out of This Course",
          "Join My Private Community!",
          "Course Resources"
        ],
        "Deep Dive into LangChain": [
          "LangChain Demo",
          "Introduction to LangChain",
          "Setting Up the Environment: LangChain, Python-dotenv",
          "ChatModels: GPT-3.5-Turbo and GPT-4",
          "Caching LLM Responses",
          "LLM Streaming",
          "Prompt Templates",
          "ChatPrompt Templates",
          "Important Update: Handling LLMChain Deprecation Errors",
          "Simple Chains",
          "Sequential Chains",
          "Introduction to LangChain Agents",
          "LangChain Agents in Action: Python REPL",
          "LangChain Tools: DuckDuckGo and Wikipedia",
          "Creating a ReAct Agent",
          "Testing the ReAct Agent"
        ],
        "LangChain and Vector Stores (Pinecone)": [
          "Short Recap of Embeddings",
          "Introduction to Vector Databases",
          "Authenticating to Pinecone",
          "Working with Pinecone Indexes",
          "Working with Vectors",
          "Namespaces",
          "Splitting and Embedding Text Using LangChain",
          "Inserting the Embeddings into a Pinecone Index",
          "Asking Questions (Similarity Search)"
        ],
        "LangChain and Google's Gemini": [
          "Getting a Gemini API Key",
          "Gemini Multimodal Models: Nano, Pro, and Ultra",
          "Installing the Python Libraries for Gemini and Authenticating to Gemini",
          "Integrating Gemini with LangChain",
          "Using a System Prompt and Enabling Streaming",
          "Multimodal AI With Gemini",
          "Gemini Safety Settings"
        ],
        "Jupyter AI": [
          "Jupyter AI",
          "Python Version",
          "Introduction to Jupyter AI and Other Coding Companions",
          "Installing Jupyter AI",
          "Using Jupyter AI in JupyterLab",
          "Setting Up Jupyter AI in Jupyter Notebook",
          "Using Jupyter AI in Jupyter Notebook",
          "Using Interpolation for More Advanced Use Cases",
          "Using Jupyter AI with Other Providers and Models"
        ],
        "Project #1: Building a Custom ChatGPT App with LangChain From Scratch": [
          "Project Introduction",
          "Implementing a ChatGPT App with ChatPromptTemplates and Chains",
          "Adding Chat Memory Using ConversationBufferMemory",
          "Saving Chat Sessions"
        ],
        "Project #2: RAG - Q&A App on Your Private Documents (Pinecone and Chroma)": [
          "Project Introduction",
          "Loading Your Custom (Private) PDF Documents",
          "Loading Different Document Formats",
          "Public and Private Service Loaders",
          "Chunking Strategies and Splitting the Documents",
          "Embedding and Uploading to a Vector Database (Pinecone)",
          "Asking and Getting Answers",
          "Using Chroma as a Vector DB",
          "Adding Memory to the RAG System (Chat History)",
          "Using a Custom Prompt"
        ],
        "Project #3: Building a Front-End for the Question-Answering App Using Streamlit": [
          "LangChain Version",
          "Project Introduction and Library Installation",
          "Defining Functions",
          "Creating the Sidebar",
          "Reading, Chunking, and Embedding Data",
          "Asking Questions and Getting Answers",
          "Saving the Chat History",
          "Clearing Session State History Using Callback Functions"
        ],
        "Project #4: Summarizing With LangChain and OpenAI": [
          "Project Introduction",
          "LangChain Version",
          "Summarizing Using a Basic Prompt",
          "Summarizing using Prompt Templates",
          "Summarizing Using StuffDocumentsChain",
          "Summarizing Large Documents Using map_reduce",
          "map_reduce With Custom Prompts",
          "Summarizing Using the refine CombineDocumentChain",
          "refine With Custom Prompts",
          "Summarizing Using LangChain Agents"
        ],
        "Project #5: Building a Custom ChatGTP App with LangChain and Streamlit": [
          "Project Introduction",
          "Building the App",
          "Displaying the Chat History",
          "Testing the App"
        ]
      },
      "requirements": [
        "Basic Python programming experience is required.",
        "You should be able to sign up to OpenAI API with a valid phone number."
      ],
      "description": "Fully Updated for the latest versions of LangChain, OpenaAI, and Pinecone.\nUnlock the Power of LangChain and Pinecone to Build Advanced LLM Applications with Generative AI and Python!\n\n\nThis LangChain course is the 2nd part of “OpenAI API with Python Bootcamp”. It is not recommended for complete beginners as it requires some essential Python programming experience.\nAre you ready to dive into the world of Large Language Models (LLMs) and Generative AI (GenAI)? This comprehensive course will guide you through building cutting-edge LLM applications using OpenAI or Gemini API, LangChain, and Pinecone.\nBy the end of this course, you'll master LangChain and Pinecone to create powerful, production-ready LLM apps in Python. You'll also develop modern web front-ends with Streamlit, bringing your AI applications to life.\n\n\nIn this course, you will:\nUnderstand the fundamentals of LangChain for simplified LLM app development.\nDive into Generative AI with OpenAI and Google's Gemini.\nBuild real-world LLM applications step-by-step with Python.\nUtilize LangChain Agents and Chains for advanced functionalities.\nExplore Pinecone for efficient vector embeddings and similarity search.\nWork with vector databases like Pinecone and Chroma.\nImplement embeddings and indexing for custom document QA systems.\nCreate RAG (Retrieval-Augemented Generation) Apps with LangChain.\nSummarize large texts using LLMs.\nLearn Prompt Engineering best practices.\nCreate engaging front-ends using Streamlit.\nBecome proficient in using AI Coding Assistants (Jupyter AI)\nCreate LLM-Based Hands-On Projects with LangChain for the Real-Word: RAG, ChatBot, Summarization\n\n\nWho should take this course?\nPython developers interested in AI, LLMs, LangChain and LangGraph.\nData scientists and AI enthusiasts looking to expand their skill set.\nProfessionals aiming to leverage Generative AI (GenAI) and LangChain in real-world applications.\nDon't miss out on the AI revolution! Equip yourself with the skills to build state-of-the-art LLM applications. Enroll now and stay ahead in the rapidly evolving field of AI.\nJoin me on this exciting journey to master LangChain, Pinecone, and Generative AI. Let's build the future together!\nI look forward to seeing you in the course!",
      "target_audience": [
        "Python programmers who want to build LLM-Powered Applications using LangChain, Pinecone and OpenAI.",
        "Any technical person interested in the most disruptive technology of this decade.",
        "Any programmer interested in AI."
      ]
    },
    {
      "title": "Artificial Intelligence & Machine Learning for Business",
      "url": "https://www.udemy.com/course/artificial-intelligence-machine-learning-business/",
      "bio": "The Ultimate Artificial Intelligence & Machine Learning course for CxOs, Managers, Team Leaders and Entrepreneurs",
      "objectives": [
        "Applications of Artificial Intelligence and Machine Learning for Business Leaders (CxOs, Managers, Team Leaders, MBA Students, Entrepreneurs)",
        "How can you apply Artificial Intelligence and Machine Learning in your areas of functions?",
        "Disruption happening in several domains and industries because of Artificial Intelligence and Machine learning"
      ],
      "course_content": {},
      "requirements": [
        "This course does not assume any prior knowledge of Artificial Intelligence or it’s associated terms. Bring your business and managerial experience - the course will help you do the rest !"
      ],
      "description": "★ Note:  Course recently updated to include additional content on Machine Learning — including new sections on Types of ML, Applications, and its Ethics. We've also removed/updated quizzes as requested by learners★\nAre you prepared for the inevitable AI revolution? How can you leverage it in your current role as a business leader (whether that's a manager, team leader or a CxO)? Analytics Vidhya’s ‘Artificial Intelligence (AI) & Machine Learning (ML) for Business’ course, curated and delivered by experienced instructors, will help you understand the answers to these pressing questions.\nArtificial Intelligence has become the centrepiece of strategic decision making for organizations. It is disrupting the way industries function - from sales and marketing to finance and HR, companies are betting on AI to give them a competitive edge.\nAI for Business Leaders is a thoughtfully created course designed specifically for business people and does not require any programming.\nThrough this course you will learn about the current state of AI, how it's disrupting businesses globally and in diverse fields, how it might impact your current role and what you can do about it. This course also dives into the various building blocks of AI and why it's necessary for you to have a high-level overview of these topics in today's data-driven world.\nWe will also provide you with multiple practical case studies towards the end of the course that will test your understanding and add context to all that you've studied.\nBy the time you finish the course, you will be ready to apply your newly-acquired knowledge in your current organization. You will be able to make informed strategic decisions for yourself and your business.",
      "target_audience": [
        "CXOs, Business Managers, MBA students",
        "Entrepreneurs",
        "Any one interested in understanding and applying these technologies for business use"
      ]
    },
    {
      "title": "Scraping and Data Mining for Beginners and Pros",
      "url": "https://www.udemy.com/course/scraping-and-data-mining-for-beginners-and-pros-y/",
      "bio": "Data mining visually or programmatically, learn everything you need in a fast pace 30 minute course",
      "objectives": [
        "Learn about data mining and scraping",
        "Look at alternative approaches",
        "Gain hands-on experience on how to do it"
      ],
      "course_content": {},
      "requirements": [
        "Minimal development experience"
      ],
      "description": "For Busy People Only!\n\n\nWeb scraping is a technique for gathering data or information on web pages. You could revisit your favorite web site every time it updates for new information, or you could write a web scraper to have it do it for you!\nWeb crawling is usually the very first step of data research. Whether you are looking to obtain data from a website, track changes on the internet, or use a website API, web crawlers are a great way to get the data you need.\n\n\nLearn everything you need to know about converting web sit es into data. We'll focus on the 20% that gets the 80% job done.\nWe'll cover data mining approaches for journalists, growth hackers, data scientists and anyone who's fascinated about seeing the big picture.\nPresented are Visual tools and Programmatic tools that you can get started with, making the course accessible for both beginners and more experienced developers.\nWe'll show you how data is represented, navigated and accessed. We'll briefly talk about other mechanisms like API stores, data stores and official APIs and their pros/cons.\nIf you're busy, and want to learn how to unlock the power of data in 30 minutes, check this course out.",
      "target_audience": [
        "People interested in Data"
      ]
    },
    {
      "title": "Natural Language Processing in Python (NEW for 2025!)",
      "url": "https://www.udemy.com/course/nlp-in-python-2025/",
      "bio": "Learn NLP in Python — text preprocessing, machine learning, transformers & LLMs using scikit-learn, spaCy & Hugging Face",
      "objectives": [
        "Review the history and evolution of NLP techniques and applications, from traditional machine learning models to modern LLM approaches",
        "Walk through the NLP text preprocessing pipeline, including cleaning, normalization, linguistic analysis, and vectorization",
        "Use traditional machine learning techniques to perform sentiment analysis, text classification, and topic modeling",
        "Understand the theory behind neural networks and deep learning, the building blocks of modern NLP techniques",
        "Break down the main parts of the Transformers architecture, including embeddings, attention and feedforward neural networks (FFNs)",
        "Use pretrained LLMs with Hugging Face to perform sentiment analysis, NER, zero-shot classification, document similarity, and text summarization & generation"
      ],
      "course_content": {},
      "requirements": [
        "We strongly recommend taking our Data Prep & EDA with Python course first",
        "Jupyter Notebooks (free download, we'll walk through the install)",
        "Familiarity with base Python and Pandas is recommended, but not required"
      ],
      "description": "This is a practical, hands-on course designed to give you a comprehensive overview of all the essential concepts for modern Natural Language Processing (NLP) in Python.\n\n\nWe’ll start by reviewing the history and evolution of NLP over the past 70 years, including the most popular architecture at the moment, Transformers. We'll also walk through the initial text preprocessing steps required for modeling, where you’ll learn how to clean and normalize data with pandas and spaCy, then vectorize that data into a Document-Term Matrix using both word counts and TF-IDF scores.\n\n\nAfter that, the course is split into two parts:\n\n\nThe first half covers traditional machine learning techniques\nThe second half covers modern deep learning and LLM (large language model) approaches\n\n\nFor the traditional NLP applications, we'll begin with Sentiment Analysis to determine the positivity or negativity of text using the VADER library. Then we’ll cover Text Classification on labeled data with Naïve Bayes, as well as Topic Modeling on unlabeled data using Non-Negative Matrix Factorization, all using the scikit-learn library.\n\n\nOnce you have a solid understanding of the foundational NLP concepts, we’ll move on to the second half of the course on modern NLP techniques, which covers the major advancements in NLP and the data science mindset shift over the past decade.\n\n\nWe’ll start with the basic building blocks of modern NLP techniques, which are neural networks. You’ll learn how neural networks are trained, become familiar with key terms like layers, nodes, weights, and activation functions, and then get introduced to popular deep learning architectures and their practical applications.\n\n\nAfter that, we’ll talk about Transformers, the architectures behind popular LLMs like ChatGPT, Gemini, and Claude. We’ll cover how the main layers work and what they do, including embeddings, attention, and feedforward neural networks. We’ll also review the differences between encoder-only, decoder-only, and encoder-decoder models, and the types of LLMs that fall into each category.\n\n\nLast but not least, we’re going to apply what we’ve learned with Python. We’ll be using Hugging Face’s Transformers library and their Model Hub to demo six practical NLP applications, including Sentiment Analysis, Named Entity Recognition, Zero-Shot Classification, Text Summarization, Text Generation, and Document Similarity.\n\n\nCOURSE OUTLINE:\n\n\nInstallation & Setup\nInstall Anaconda, start writing Python code in a Jupyter Notebook, and learn how to create a new conda environment to get set up for this course\n\n\nNatural Language Processing 101\nReview the basics of natural language processing (NLP), including key concepts, the evolution of NLP over the years, and its applications & Python libraries\n\n\nText Preprocessing\nWalk through the text preprocessing steps required before applying machine learning algorithms, including cleaning, normalization, vectorization, and more\n\n\nNLP with Machine Learning\nPerform sentiment analysis, text classification, and topic modeling using traditional NLP methods, including rules-based, supervised, and unsupervised machine learning techniques\n\n\nNeural Networks & Deep Learning\nVisually break down the concepts behind neural networks and deep learning, the building blocks of modern NLP techniques\n\n\nTransformers & LLMs\nDive into the main parts of the transformer architecture, including embeddings, attention, and FFNs, as well as popular LLMs for NLP tasks like BERT, GPT, and more\n\n\nHugging Face Transformers\nIntroduce the Hugging Face Transformers library in Python and walk through examples of how you can use pretrained LLMs to perform NLP tasks, including sentiment analysis, named entity recognition (NER), zero-shot classification, text summarization, text generation, and document similarity\n\n\nNLP Review & Next Steps\nReview the NLP techniques covered in this course, when to use them, and how to dive deeper and stay up-to-date\n\n\n__________\n\n\nReady to dive in? Join today and get immediate, LIFETIME access to the following:\n\n\n12.5 hours of high-quality video\n13 homework assignments\n4 interactive exercises\nNatural Language Processing in Python ebook (200+ pages)\nDownloadable project files & solutions\nExpert support and Q&A forum\n30-day Udemy satisfaction guarantee\n\n\nIf you're an aspiring or seasoned data scientist looking for a practical overview of both traditional and modern NLP techniques in Python, this is the course for you.\n\n\nHappy learning!\n-Alice Zhao (Python Expert & Data Science Instructor, Maven Analytics)\n\n\n__________\nLooking for more data & AI courses? Search for \"Maven Analytics\" to browse our full course library, including Excel, Power BI, MySQL, Tableau, Machine Learning and more!\n\n\nSee why our courses are among the TOP-RATED on Udemy:\n\n\n\"Some of the BEST courses I've ever taken. I've studied several programming languages, Excel, VBA and web dev, and Maven is among the very best I've seen!\" Russ C.\n\n\n\"This is my fourth course from Maven Analytics and my fourth 5-star review, so I'm running out of things to say. I wish Maven was in my life earlier!\" Tatsiana M.\n\n\n\"Maven Analytics should become the new standard for all courses taught on Udemy!\" Jonah M.",
      "target_audience": [
        "Aspiring Data Scientists who want a practical overview of natural language processing techniques in Python",
        "Seasoned Data Scientists looking to learn the latest NLP techniques, such as Transformers, LLMs and Hugging Face"
      ]
    },
    {
      "title": "Math 0-1: Matrix Calculus in Data Science & Machine Learning",
      "url": "https://www.udemy.com/course/matrix-calculus-machine-learning/",
      "bio": "A Casual Guide for Artificial Intelligence, Deep Learning, and Python Programmers",
      "objectives": [
        "Derive matrix and vector derivatives for linear and quadratic forms",
        "Solve common optimization problems (least squares, Gaussian, financial portfolio)",
        "Understand and implement Gradient Descent and Newton's method",
        "Learn to use the Matrix Cookbook"
      ],
      "course_content": {},
      "requirements": [
        "Competence with Calculus and Linear Algebra",
        "Optional: Familiarity with Python, Numpy, and Matplotlib to implement optimization techniques"
      ],
      "description": "Welcome to the exciting world of Matrix Calculus, a fundamental tool for understanding and solving problems in machine learning and data science. In this course, we will dive into the powerful mathematics that underpin many of the algorithms and techniques used in these fields. By the end of this course, you'll have the knowledge and skills to navigate the complex landscape of derivatives, gradients, and optimizations involving matrices.\n\n\nCourse Objectives:\nUnderstand the basics of matrix calculus, linear and quadratic forms, and their derivatives.\nLearn how to utilize the famous Matrix Cookbook for a wide range of matrix calculus operations.\nGain proficiency in optimization techniques like gradient descent and Newton's method in one and multiple dimensions.\nApply the concepts learned to real-world problems in machine learning and data science, with hands-on exercises and Python code examples.\n\n\nWhy Matrix Calculus? Matrix calculus is the language of machine learning and data science. In these fields, we often work with high-dimensional data, making matrices and their derivatives a natural representation for our problems. Understanding matrix calculus is crucial for developing and analyzing algorithms, building predictive models, and making sense of the vast amounts of data at our disposal.\n\n\nSection 1: Linear and Quadratic Forms In the first part of the course, we'll explore the basics of linear and quadratic forms, and their derivatives. The linear form appears in all of the most fundamental and popular machine learning models, including linear regression, logistic regression, support vector machine (SVM), and deep neural networks. We will also dive into quadratic forms, which are fundamental to understanding optimization problems, which appear in regression, portfolio optimization in finance, signal processing, and control theory.\nThe Matrix Cookbook is a valuable resource that compiles a wide range of matrix derivative formulas in one place. You'll learn how to use this reference effectively, saving you time and ensuring the accuracy of your derivations.\n\n\nSection 2: Optimization Techniques Optimization lies at the heart of many machine learning and data science tasks. In this section, we will explore two crucial optimization methods: gradient descent and Newton's method. You'll learn how to optimize not only in one dimension but also in high-dimensional spaces, which is essential for training complex models. We'll provide Python code examples to help you grasp the practical implementation of these techniques.\n\n\nCourse Structure:\nEach lecture will include a theoretical introduction to the topic.\nWe will work through relevant mathematical derivations and provide intuitive explanations.\nHands-on exercises will allow you to apply what you've learned to real-world problems.\nPython code examples will help you implement and experiment with the concepts.\nThere will be opportunities for questions and discussions to deepen your understanding.\n\n\nPrerequisites:\nBasic knowledge of linear algebra, calculus, and Python programming is recommended.\nA strong desire to learn and explore the fascinating world of matrix calculus.\n\n\nConclusion: Matrix calculus is an indispensable tool in the fields of machine learning and data science. It empowers you to understand, create, and optimize algorithms that drive innovation and decision-making in today's data-driven world. This course will equip you with the knowledge and skills to navigate the intricate world of matrix calculus, setting you on a path to become a proficient data scientist or machine learning engineer. So, let's dive in, embrace the world of matrices, and unlock the secrets of data science and machine learning together!",
      "target_audience": [
        "Students and professionals interested in the math behind AI, data science and machine learning"
      ]
    },
    {
      "title": "Machine Learning - Regression and Classification (math Inc.)",
      "url": "https://www.udemy.com/course/machine-learning-regression-and-classification-math-inc/",
      "bio": "A complete Beginner to Advance level guide to Machine Learning. Hands-on Learning approach with in-depth math concepts",
      "objectives": [
        "Understand and implement a Decision Tree in Python",
        "Understand about Gini and Information Gain algorithm",
        "Solve mathematical numerical related decision trees",
        "Learn about regression trees",
        "Learn about simple, multiple, polynomial and multivariate regression",
        "Learn about Ordinary Least Squares Algorithms",
        "Solve numerical related to Ordinary Least Squares algorithm",
        "Learn to create real world predictions and classification projects",
        "Learn about Gradient Descent",
        "Learn about Logistic Regression and hyper parameters"
      ],
      "course_content": {},
      "requirements": [
        "Basic mathematical concepts of addition, multiplication and so on",
        "Knowing python beforehand would be handful"
      ],
      "description": "Machine learning is a branch of artificial intelligence (AI) focused on building applications that learn from data and improve their accuracy over time without being programmed to do so.\nIn data science, an algorithm is a sequence of statistical processing steps. In machine learning, algorithms are 'trained' to find patterns and features in massive amounts of data in order to make decisions and predictions based on new data. The better the algorithm, the more accurate the decisions and predictions will become as it processes more data.\nMachine learning has led to some amazing results, like being able to analyze medical images and predict diseases on-par with human experts.\nGoogle's AlphaGo program was able to beat a world champion in the strategy game go using deep reinforcement learning.\nMachine learning is even being used to program self driving cars, which is going to change the automotive industry forever. Imagine a world with drastically reduced car accidents, simply by removing the element of human error.\nTopics covered in this course:\n1. Lecture on Information Gain and GINI impurity [decision trees]\n2. Numerical problem related to Decision Tree will be solved in tutorial sessions\n3. Implementing Decision Tree Classifier in workshop session [coding]\n4. Regression Trees\n5. Implement Decision Tree Regressor\n6. Simple Linear Regression\n7. Tutorial on cost function and numerical implementing Ordinary Least Squares Algorithm\n8. Multiple Linear Regression\n9. Polynomial Linear Regression\n10. Implement Simple, Multiple, Polynomial Linear Regression [[coding session]]\n11. Write code of Multivariate Linear Regression from Scratch\n12. Learn about gradient Descent algorithm\n13. Lecture on Logistic Regression [[decision boundary, cost function, gradient descent.....]]\n14. Implement Logistic Regression [[coding session]]",
      "target_audience": [
        "Seasonal and Beginners Python developers who want to learn about different AI and ML algorithms",
        "Students who want to learn all the mathematics behind popular regression and classification models",
        "Students who want to learn to implement data science libraries to solve real world Machine Learning problems"
      ]
    },
    {
      "title": "Python Data Science: Regression & Forecasting",
      "url": "https://www.udemy.com/course/data-science-in-python-regression/",
      "bio": "Learn Python for data science and machine learning, and build regression & forecast models w/ a top Python instructor!",
      "objectives": [
        "Master the machine learning foundations for regression analysis in Python",
        "Perform exploratory data analysis on model features, the target, and relationships between them",
        "Build and interpret simple and multiple linear regression models with Statsmodels and Scikit-Learn",
        "Evaluate model performance using tools like hypothesis tests, residual plots, and mean error metrics",
        "Diagnose and fix violations to the assumptions of linear regression models",
        "Tune and test your models with data splitting, validation and cross validation, and model scoring",
        "Leverage regularized regression algorithms to improve test model performance & accuracy",
        "Employ time series analysis techniques to identify trends & seasonality, perform decomposition, and forecast future values"
      ],
      "course_content": {},
      "requirements": [
        "We strongly recommend taking our Data Prep & EDA course first",
        "Jupyter Notebooks (free download, we'll walk through the install)",
        "Familiarity with base Python and Pandas is recommended, but not required"
      ],
      "description": "This is a hands-on, project-based course designed to help you master the foundations for regression analysis and forecasting with Python.\n\n\nWe’ll start by reviewing the Python data science workflow, discussing the primary goals & types of regression analysis, and do a deep dive into the regression modeling steps we’ll be using throughout the course.\n\n\nYou’ll learn to perform exploratory data analysis (EDA), fit simple & multiple linear regression models, and build an intuition for interpreting models and evaluating their performance using tools like hypothesis tests, residual plots, and error metrics. We’ll also review the assumptions of linear regression, and learn how to diagnose and fix each one.\n\n\nFrom there, we’ll cover the model testing & validation steps that help ensure our models perform well on new, unseen data, including the concepts of data splitting, tuning, and model selection. You’ll also learn how to improve model performance by leveraging feature engineering techniques and regularized regression algorithms.\n\n\nThroughout the course, you'll play the role of Associate Data Scientist for Maven Consulting Group on a team that focuses on pricing strategy for their clients. Using the skills you learn throughout the course, you'll use Python to explore their data and build regression models to help firms accurately predict prices and understand the variables that impact them.\n\n\nLast but not least, you'll get an introduction to time series analysis & forecasting techniques. You’ll learn to analyze trends & seasonality, perform decomposition, and forecast future values.\n\n\nCOURSE OUTLINE:\n\n\nIntro to Data Science with Python\nIntroduce the fields of data science and machine learning, review essential skills, and introduce each phase of the data science workflow\n\n\nRegression 101\nReview the basics of regression, including key terms, the types and goals of regression analysis, and the regression modeling workflow\n\n\nPre-Modeling Data Prep & EDA\nRecap the data prep & EDA steps required to perform modeling, including key techniques to explore the target, features, and their relationships\n\n\nSimple Linear Regression\nBuild simple linear regression models in Python and learn about the metrics and statistical tests that help evaluate their quality and output\n\n\nMultiple Linear Regression\nBuild multiple linear regression models in Python and evaluate the model fit, perform variable selection, and compare models using error metrics\n\n\nModel Assumptions\nReview the assumptions of linear regression models that need to be met to ensure that the model’s predictions and interpretation are valid\n\n\nModel Testing & Validation\nTest model performance by splitting data, tuning the model with the train & validation data, selecting the best model, and scoring it on the test data\n\n\nFeature Engineering\nApply feature engineering techniques for regression models, including dummy variables, interaction terms, binning, and more\n\n\nRegularized Regression\nIntroduce regularized regression techniques, which are alternatives to linear regression, including Ridge, Lasso, and Elastic Net regression\n\n\nTime Series Analysis\nLearn methods for exploring time series data and how to perform time series forecasting using linear regression and Facebook Prophet\n\n\n__________\n\n\nReady to dive in? Join today and get immediate, LIFETIME access to the following:\n\n\n8.5 hours of high-quality video\n14 homework assignments\n10 quizzes\n3 projects\nData Science in Python: Regression & forecasting ebook (230+ pages)\nDownloadable project files & solutions\nExpert support and Q&A forum\n30-day Udemy satisfaction guarantee\n\n\nIf you're a business intelligence professional or aspiring data scientist looking for an introduction to the world of regression modeling and forecasting with Python, this is the course for you.\n\n\nHappy learning!\n-Chris Bruehl (Data Science Expert & Lead Python Instructor, Maven Analytics)\n\n\n__________\nLooking for our full business intelligence stack? Search for \"Maven Analytics\" to browse our full course library, including Excel, Power BI, MySQL, Tableau and Machine Learning courses!\n\n\nSee why our courses are among the TOP-RATED on Udemy:\n\n\n\"Some of the BEST courses I've ever taken. I've studied several programming languages, Excel, VBA and web dev, and Maven is among the very best I've seen!\" Russ C.\n\n\n\"This is my fourth course from Maven Analytics and my fourth 5-star review, so I'm running out of things to say. I wish Maven was in my life earlier!\" Tatsiana M.\n\n\n\"Maven Analytics should become the new standard for all courses taught on Udemy!\" Jonah M.",
      "target_audience": [
        "Data analysts or BI experts looking to transition into a data science role",
        "Python users who want to build the core skills for applying regression models in Python",
        "Anyone interested in learning one of the most popular open source programming languages in the world"
      ]
    },
    {
      "title": "Data Science Mega-Course: #Build {120-Projects In 120-Days}",
      "url": "https://www.udemy.com/course/real-world-data-science-projects-using-python/",
      "bio": "Build & Deploy Data Science, Machine Learning, Deep Learning (Python, Flask, Django, AWS, Azure, GCP, Heruko Cloud)",
      "objectives": [
        "Make powerful analysis, Make robust Machine Learning models",
        "Master Machine Learning on Python",
        "Know which Machine Learning model to choose for each type of problem",
        "Implement Machine Learning Algorithms",
        "Explore how to deploy your machine learning models.",
        "Understand the full product workflow for the machine learning lifecycle.",
        "Present Data Science projects to management",
        "Real life case studies and projects to understand how things are done in the real world",
        "Build a portfolio of work to have on your resume"
      ],
      "course_content": {},
      "requirements": [
        "Basic knowledge of machine learning"
      ],
      "description": "In This Course, Solve Business Problems Using Data Science Practically. Learn To Build & Deploy Machine Learning, Data Science, Artificial Intelligence, Auto Ml, Deep Learning, Natural Language Processing (Nlp) Web Applications Projects With Python (Flask, Django, Heroku, AWS, Azure, GCP, IBM Watson, Streamlit Cloud).\n\n\nAccording to Glassdoor, the average salary for a Data Scientist is $117,345/yr. This is above the national average of $44,564. Therefore, a Data Scientist makes 163% more than the national average salary.\nThis makes Data Science a highly lucrative career choice. It is mainly due to the dearth of Data Scientists resulting in a huge income bubble.\nSince Data Science requires a person to be proficient and knowledgeable in several fields like Statistics, Mathematics, and Computer Science, the learning curve is quite steep. Therefore, the value of a Data Scientist is very high in the market.\nA Data Scientist enjoys a position of prestige in the company. The company relies on its expertise to make data-driven decisions and enable them to navigate in the right direction.\nFurthermore, the role of a Data Scientist depends on the specialization of his employer company. For example – A commercial industry will require a data scientist to analyze their sales.\nA healthcare company will require data scientists to help them analyze genomic sequences. The salary of a Data Scientist depends on his role and type of work he has to perform. It also depends on the size of the company which is based on the amount of data they utilize.\nStill, the pay scale of Data scientists is way above other IT and management sectors. However, the salary observed by Data Scientists is proportional to the amount of work that they must put in. Data Science needs hard work and requires a person to be thorough with his/her skills.\nDue to several lucrative perks, Data Science is an attractive field. This, combined with the number of vacancies in Data Science makes it an untouched gold mine. Therefore, you should learn Data Science in order to enjoy a fruitful career.\n\n\nIn This Course, We Are Going To Work On 120 Real World Projects Listed Below:\n\n\nProject-1: Pan Card Tempering Detector App -Deploy On Heroku\nProject-2: Dog breed prediction Flask App\nProject-3: Image Watermarking App -Deploy On Heroku\nProject-4: Traffic sign classification\nProject-5: Text Extraction From Images Application\nProject-6: Plant Disease Prediction Streamlit App\nProject-7: Vehicle Detection And Counting Flask App\nProject-8: Create A Face Swapping Flask App\nProject-9: Bird Species Prediction Flask App\nProject-10: Intel Image Classification Flask App\n\n\nProject-11: Language Translator App Using IBM Cloud Service -Deploy On Heroku\nProject-12: Predict Views On Advertisement Using IBM Watson -Deploy On Heroku\nProject-13: Laptop Price Predictor -Deploy On Heroku\nProject-14: WhatsApp Text Analyzer -Deploy On Heroku\nProject-15: Course Recommendation System -Deploy On Heroku\nProject-16: IPL Match Win Predictor -Deploy On Heroku\nProject-17: Body Fat Estimator App -Deploy On Microsoft Azure\nProject-18: Campus Placement Predictor App -Deploy On Microsoft Azure\nProject-19: Car Acceptability Predictor -Deploy On Google Cloud\nProject-20: Book Genre Classification App -Deploy On Amazon Web Services\n\n\nProject 21 : DNA classification for finding E.Coli - Deploy On AWS\nProject 22 : Predict the next word in a sentence. - AWS - Deploy On AWS\nProject 23 : Predict Next Sequence of numbers using LSTM - Deploy On AWS\nProject 24 : Keyword Extraction from text using NLP - Deploy On Azure\nProject 25 : Correcting wrong spellings - Deploy On Azure\nProject 26 : Music popularity classification - Deploy On Google App Engine\nProject 27 : Advertisement Classification - Deploy On Google App Engine\nProject 28 : Image Digit Classification - Deploy On AWS\nProject 29 : Emotion Recognition using Neural Network - Deploy On AWS\nProject 30 : Breast cancer Classification - Deploy On AWS\n\n\nProject-31: Sentiment Analysis Django App -Deploy On Heroku\nProject-32: Attrition Rate Django Application\nProject-33: Find Legendary Pokemon Django App -Deploy On Heroku\nProject-34: Face Detection Streamlit App\nProject-35: Cats Vs Dogs Classification Flask App\nProject-36: Customer Revenue Prediction App -Deploy On Heroku\nProject-37: Gender From Voice Prediction App -Deploy On Heroku\nProject-38: Restaurant Recommendation System\nProject-39: Happiness Ranking Django App -Deploy On Heroku\nProject-40: Forest Fire Prediction Django App -Deploy On Heroku\n\n\nProject-41: Build Car Prices Prediction App -Deploy On Heroku\nProject-42: Build Affair Count Django App -Deploy On Heroku\nProject-43: Build Shrooming Predictions App -Deploy On Heroku\nProject-44: Google Play App Rating prediction With Deployment On Heroku\nProject-45: Build Bank Customers Predictions Django App -Deploy On Heroku\nProject-46: Build Artist Sculpture Cost Prediction Django App -Deploy On Heroku\nProject-47: Build Medical Cost Predictions Django App -Deploy On Heroku\nProject-48: Phishing Webpages Classification Django App -Deploy On Heroku\nProject-49: Clothing Fit-Size predictions Django App -Deploy On Heroku\nProject-50: Build Similarity In-Text Django App -Deploy On Heroku\n\n\nProject-51: Black Friday Sale Project\nProject-52: Sentiment Analysis Project\nProject-53: Parkinson’s Disease Prediction Project\nProject-54: Fake News Classifier Project\nProject-55: Toxic Comment Classifier Project\nProject-56: IMDB Movie Ratings Prediction\nProject-57: Indian Air Quality Prediction\nProject-58: Covid-19 Case Analysis\nProject-59: Customer Churning Prediction\nProject-60: Create A ChatBot\n\n\nProject-61: Video Game sales Analysis\nProject-62: Zomato Restaurant Analysis\nProject-63: Walmart Sales Forecasting\nProject-64 : Sonic wave velocity prediction using Signal Processing Techniques\nProject-65 : Estimation of Pore Pressure using Machine Learning\nProject-66 : Audio processing using ML\nProject-67 : Text characterisation using Speech recognition\nProject-68 : Audio classification using Neural networks\nProject-69 : Developing a voice assistant\nProject-70 : Customer segmentation\n\n\nProject-71 : FIFA 2019 Analysis\nProject-72 : Sentiment analysis of web scrapped data\nProject-73 : Determining Red Vine Quality\nProject-74 : Customer Personality Analysis\nProject-75 : Literacy Analysis in India\nProject-76: Heart Attack Risk Prediction Using Eval ML (Auto ML)\nProject-77: Credit Card Fraud Detection Using Pycaret (Auto ML)\nProject-78: Flight Fare Prediction Using Auto SK Learn (Auto ML)\nProject-79: Petrol Price Forecasting Using Auto Keras\nProject-80: Bank Customer Churn Prediction Using H2O Auto ML\n\n\nProject-81: Air Quality Index Predictor Using TPOT With End-To-End Deployment (Auto ML)\nProject-82: Rain Prediction Using ML models & PyCaret With Deployment (Auto ML)\nProject-83: Pizza Price Prediction Using ML And EVALML(Auto ML)\nProject-84: IPL Cricket Score Prediction Using TPOT (Auto ML)\nProject-85: Predicting Bike Rentals Count Using ML And H2O Auto ML\nProject-86: Concrete Compressive Strength Prediction Using Auto Keras (Auto ML)\nProject-87: Bangalore House Price Prediction Using Auto SK Learn (Auto ML)\nProject-88: Hospital Mortality Prediction Using PyCaret (Auto ML)\nProject-89: Employee Evaluation For Promotion Using ML And Eval Auto ML\nProject-90: Drinking Water Potability Prediction Using ML And H2O Auto ML\n\n\nProject-91: Image Editor Application With OpenCV And Tkinter\nProject-92: Brand Identification Game With Tkinter And Sqlite3\nProject-93: Transaction Application With Tkinter And Sqlite3\nProject-94: Learning Management System With Django\nProject-95: Create A News Portal With Django\nProject-96: Create A Student Portal With Django\nProject-97: Productivity Tracker With Django And Plotly\nProject-98: Create A Study Group With Django\nProject-99: Building Crop Guide Application with PyQt5, SQLite\nProject-100: Building Password Manager Application With PyQt5, SQLite\n\n\nProject-101: Create A News Application With Python\nProject-102: Create A Guide Application With Python\nProject-103: Building The Chef Web Application with Django, Python\nProject-104: Syllogism-Rules of Inference Solver Web Application\nProject-105: Building Vision Web Application with Django, Python\nProject-106: Building Budget Planner Application With Python\nProject-107: Build Tic Tac Toe Game\nProject-108: Random Password Generator Website using Django\nProject-109: Building Personal Portfolio Website Using Django\nProject-110: Todo List Website For Multiple Users\n\n\nProject-111: Crypto Coin Planner GUI Application\nProject-112: Your Own Twitter Bot -python, request, API, deployment, tweepy\nProject-113: Create A Python Dictionary Using python, Tkinter, JSON\nProject-114: Egg-Catcher Game using python\nProject-115: Personal Routine Tracker Application using python\nProject-116: Building Screen -Pet using Tkinter & Canvas\nProject-117: Building Caterpillar Game Using Turtle and Python\nProject-118: Building Hangman Game Using Python\nProject-119: Developing our own Smart Calculator Using Python and Tkinter\nProject-120: Image-based steganography Using Python and pillows\n\n\nTip: Create A 60 Days Study Plan Or 120 Day Study Plan, Spend 1-3hrs Per Day, Build 120 Projects In 60 Days Or  120 Projects In 120 Days.\n\n\nThe Only Course You Need To Become A Data Scientist, Get Hired And Start A New Career\n\n\nNote (Read This): This Course Is Worth Of Your Time And Money, Enroll Now Before Offer Expires.",
      "target_audience": [
        "Beginners in data science"
      ]
    },
    {
      "title": "Generative Adversarial Networks (GANs): Complete Guide",
      "url": "https://www.udemy.com/course/generative-adversarial-networks-gans-complete-guide/",
      "bio": "Deep Learning and Computer Vision to implement projects using one of the most revolutionary technologies in the world!",
      "objectives": [
        "Understand the basic intuition about GANs",
        "Generate images of digits (0 - 9) using DCGAN and WGAN",
        "Transform satellite images into maps using Pix2Pix architecture",
        "Transform zebras into horses using CycleGAN architecture",
        "Transfer styles between images",
        "Apply super resolution to improve image quality using ESRGAN architecture",
        "Create new faces of people with high quality and definition using StyleGAN",
        "Generate images through textual descriptions",
        "Restore old photos using GFP-GAN",
        "Complete missing parts of images using Boundless architecture",
        "Generate deepfakes to swap faces with SimSwap"
      ],
      "course_content": {
        "Introduction": [
          "Course content",
          "Introduction to GANs",
          "How GANs work",
          "Course materials"
        ],
        "DCGAN and WGAN": [
          "DCGAN - intuition",
          "MNIST dataset",
          "Building the generator",
          "Building the discriminator",
          "Loss (error) calculation",
          "A quick note about the code",
          "Training",
          "Visualizing the results",
          "HOMEWORK and solution",
          "WGAN - intuition 1",
          "WGAN - intuition 2",
          "WGAN-GP - intuition",
          "Preparing the environment",
          "Wassertein loss",
          "Gradient penalty",
          "Training 1",
          "Training 2 and visualization",
          "HOMEWORK and solution"
        ],
        "cGAN - Pix2Pix and CycleGAN": [
          "cGAN - intuition",
          "Pix2Pix - intuition",
          "Map dataset",
          "Preprocessing the images 1",
          "Preprocessing the images 2",
          "Loading the data",
          "Building the generator 1",
          "Building the generator 2",
          "Building the generator 3",
          "Building the discriminator 1",
          "Building the discriminator 2",
          "Generating the images",
          "Training 1",
          "Training 2 and results",
          "Pretrained Pix2Pix with PyTorch",
          "Facades dataset",
          "Visualizing the results",
          "Drawing to photo 1",
          "Drawing to photo 2",
          "Night to day",
          "HOMEWORK and solution",
          "CycleGAN - intuition",
          "Change in the dataset URL",
          "Apples and orange dataset",
          "Preprocessing",
          "Loading the images",
          "Generator and discriminator",
          "Loss function",
          "Optimizers and checkpoint",
          "Training 1",
          "Training 2 and results",
          "Pretrained CycleGAN with PyTorch",
          "Horse to zebra",
          "Style transfer",
          "Van Gogh, Cezanne and Ukiyo-e styles",
          "HOMEWORK and solution"
        ],
        "SRGAN and ESRGAN": [
          "SRGAN - intuition",
          "ESRGAN - intuition",
          "Pretrained model",
          "Testing images",
          "Super resolution",
          "Evaluating the results - PSNR",
          "Improving the results",
          "HOMEWORK and solution"
        ],
        "StyleGAN": [
          "ProGAN - intuition",
          "StyleGAN - intuition",
          "Pretrained model",
          "Generating images 1",
          "Generating images 2",
          "Generating images 3",
          "Interpolation",
          "Other pretrained models",
          "HOMEWORK and solution"
        ],
        "VQGAN + CLIP - text to image": [
          "VQGAN + CLIP - intuition",
          "Warning after lib update",
          "Pretrained model",
          "GAN settings",
          "Visualizing the results",
          "Results in videos",
          "HOMEWORK and solution"
        ],
        "Other types of GANs": [
          "BigGAN - intuition",
          "Pretrained model",
          "GAN settings",
          "Generating new images 1",
          "Generating new images 2",
          "GFP-GAN to restore old photos",
          "Pretrained model",
          "Photo restoration",
          "Boundless for image extension",
          "Processing the image",
          "Visualizing the results",
          "SimSwap for deepfake",
          "Pretrained model",
          "Face swap",
          "Additional: GANs in videos"
        ],
        "Additional content 1: Artificial neural networks": [
          "Biological fundamentals",
          "Single layer perceptron",
          "Multilayer perceptron – sum and activation functions",
          "Multilayer perceptron – error calculation",
          "Gradient descent",
          "Delta parameter",
          "Updating weights with backpropagation",
          "Bias, error, stochastic gradient descent, and more parameters"
        ],
        "Additional content 2: Convolution neural networks": [
          "Introduction to convolutional neural networks",
          "Convolutional operator",
          "Pooling",
          "Flattening",
          "Dense neural network"
        ],
        "Final remarks": [
          "Final remarks",
          "BONUS"
        ]
      },
      "requirements": [
        "Programming logic",
        "Basic Python programming",
        "Knowledge about neural networks is desirable, but not mandatory"
      ],
      "description": "GANs (Generative Adversarial Networks) are considered one of the most modern and fascinating technologies within the field of Deep Learning and Computer Vision. They have gained a lot of attention because they can create fake content. One of the most classic examples is the creation of people who do not exist in the real world to be used to broadcast television programs. This technology is considered a revolution in the field of Artificial Intelligence for producing high quality results, remaining one of the most popular and relevant topics.\nIn this course you will learn the basic intuition and mainly the practical implementation of the most modern architectures of Generative Adversarial Networks! This course is considered a complete guide because it presents everything from the most basic concepts to the most modern and advanced techniques, so that in the end you will have all the necessary tools to build your own projects! See below some of the projects that you are going to implement step by step:\n\nCreating of digits from 0 to 9\nTransforming satellite images into map images, like Google Maps style\nConvert drawings into high-quality photos\nCreate zebras using horse images\nTransfer styles between images using paintings by famous artists such as Van Gogh, Cezanne and Ukiyo-e\nIncrease the resolution of low quality images (super resolution)\nGenerate deepfakes (fake faces) with high quality\nCreate images through textual descriptions\nRestore old photos\nComplete missing parts of images\nSwap the faces of people who are in different environments\nTo implement the projects, you will learn several different architectures of GANs, such as: DCGAN (Deep Convolutional Generative Adversarial Network), WGAN (Wassertein GAN), WGAN-GP (Wassertein GAN-Gradient Penalty), cGAN (conditional GAN), Pix2Pix (Image-to-Image), CycleGAN (Cycle-Consistent Adversarial Network), SRGAN (Super Resolution GAN), ESRGAN (Enhanced Super Resolution GAN), StyleGAN (Style-Based Generator Architecture for GANs), VQ-GAN (Vector Quantized Generative Adversarial Network), CLIP (Contrastive Language–Image Pre-training), BigGAN, GFP-GAN (Generative Facial Prior GAN), Unlimited GAN (Boundless) and SimSwap (Simple Swap).\nDuring the course, we will use the Python programming language and Google Colab online, so you do not have to worry about installing and configuring libraries on your own machine! More than 100 lectures and 16 hours of videos!",
      "target_audience": [
        "People interested in creating complex applications using GANs",
        "Undergraduate and graduate students who are taking courses on Computer Vision, Artificial Intelligence, Digital Image Processing or Computer Vision",
        "People who want to implement their own projects using Computer Vision techniques",
        "Data Scientists who want to increase their project portfolio"
      ]
    },
    {
      "title": "From 0 to 1 : Spark for Data Science with Python",
      "url": "https://www.udemy.com/course/spark-for-data-science-with-python/",
      "bio": "Get your data to fly using Spark for analytics, machine learning and data science",
      "objectives": [
        "Use Spark for a variety of analytics and Machine Learning tasks",
        "Implement complex algorithms like PageRank or Music Recommendations",
        "Work with a variety of datasets from Airline delays to Twitter, Web graphs, Social networks and Product Ratings",
        "Use all the different features and libraries of Spark : RDDs, Dataframes, Spark SQL, MLlib, Spark Streaming and GraphX"
      ],
      "course_content": {
        "You, This Course and Us": [
          "You, This Course and Us",
          "Course Materials"
        ],
        "Introduction to Spark": [
          "What does Donald Rumsfeld have to do with data analysis?",
          "Why is Spark so cool?",
          "An introduction to RDDs - Resilient Distributed Datasets",
          "Built-in libraries for Spark",
          "Installing Spark",
          "The PySpark Shell",
          "Transformations and Actions",
          "See it in Action : Munging Airlines Data with PySpark - I",
          "[For Linux/Mac OS Shell Newbies] Path and other Environment Variables"
        ],
        "Resilient Distributed Datasets": [
          "RDD Characteristics: Partitions and Immutability",
          "RDD Characteristics: Lineage, RDDs know where they came from",
          "What can you do with RDDs?",
          "Create your first RDD from a file",
          "Average distance travelled by a flight using map() and reduce() operations",
          "Get delayed flights using filter(), cache data using persist()",
          "Average flight delay in one-step using aggregate()",
          "Frequency histogram of delays using countByValue()",
          "See it in Action : Analyzing Airlines Data with PySpark - II"
        ],
        "Advanced RDDs: Pair Resilient Distributed Datasets": [
          "Special Transformations and Actions",
          "Average delay per airport, use reduceByKey(), mapValues() and join()",
          "Average delay per airport in one step using combineByKey()",
          "Get the top airports by delay using sortBy()",
          "Lookup airport descriptions using lookup(), collectAsMap(), broadcast()",
          "See it in Action : Analyzing Airlines Data with PySpark - III"
        ],
        "Advanced Spark: Accumulators, Spark Submit, MapReduce , Behind The Scenes": [
          "Get information from individual processing nodes using accumulators",
          "See it in Action : Using an Accumulator variable",
          "Long running programs using spark-submit",
          "See it in Action : Running a Python script with Spark-Submit",
          "Behind the scenes: What happens when a Spark script runs?",
          "Running MapReduce operations",
          "See it in Action : MapReduce with Spark"
        ],
        "Java and Spark": [
          "The Java API and Function objects",
          "Pair RDDs in Java",
          "Running Java code",
          "Installing Maven",
          "See it in Action : Running a Spark Job with Java"
        ],
        "PageRank: Ranking Search Results": [
          "What is PageRank?",
          "The PageRank algorithm",
          "Implement PageRank in Spark",
          "Join optimization in PageRank using Custom Partitioning",
          "See it Action : The PageRank algorithm using Spark"
        ],
        "Spark SQL": [
          "Dataframes: RDDs + Tables",
          "See it in Action : Dataframes and Spark SQL"
        ],
        "MLlib in Spark: Build a recommendations engine": [
          "Collaborative filtering algorithms",
          "Latent Factor Analysis with the Alternating Least Squares method",
          "Music recommendations using the Audioscrobbler dataset",
          "Implement code in Spark using MLlib"
        ],
        "Spark Streaming": [
          "Introduction to streaming",
          "Implement stream processing in Spark using Dstreams",
          "Stateful transformations using sliding windows",
          "See it in Action : Spark Streaming"
        ]
      },
      "requirements": [
        "The course assumes knowledge of Python. You can write Python code directly in the PySpark shell. If you already have IPython Notebook installed, we'll show you how to configure it for Spark",
        "For the Java section, we assume basic knowledge of Java. An IDE which supports Maven, like IntelliJ IDEA/Eclipse would be helpful",
        "All examples work with or without Hadoop. If you would like to use Spark with Hadoop, you'll need to have Hadoop installed (either in pseudo-distributed or cluster mode)."
      ],
      "description": "Taught by a 4 person team including 2 Stanford-educated, ex-Googlers  and 2 ex-Flipkart Lead Analysts. This team has decades of practical experience in working with Java and with billions of rows of data.\nGet your data to fly using Spark for analytics, machine learning and data science\nLet’s parse that.\nWhat's Spark? If you are an analyst or a data scientist, you're used to having multiple systems for working with data. SQL, Python, R, Java, etc. With Spark, you have a single engine where you can explore and play with large amounts of data, run machine learning algorithms and then use the same system to productionize your code.\nAnalytics: Using Spark and Python you can analyze and explore your data in an interactive environment with fast feedback. The course will show how to leverage the power of RDDs and Dataframes to manipulate data with ease.\nMachine Learning and Data Science : Spark's core functionality and built-in libraries make it easy to implement complex algorithms like Recommendations with very few lines of code. We'll cover a variety of datasets and algorithms including PageRank, MapReduce and Graph datasets.\nWhat's Covered:\nLot's of cool stuff ..\nMusic Recommendations using Alternating Least Squares and the Audioscrobbler dataset\nDataframes and Spark SQL to work with Twitter data\nUsing the PageRank algorithm with Google web graph dataset\nUsing Spark Streaming for stream processing\nWorking with graph data using the  Marvel Social network dataset\n\n\n.. and of course all the Spark basic and advanced features:\nResilient Distributed Datasets, Transformations (map, filter, flatMap), Actions (reduce, aggregate)\nPair RDDs , reduceByKey, combineByKey\nBroadcast and Accumulator variables\nSpark for MapReduce\nThe Java API for Spark\nSpark SQL, Spark Streaming, MLlib and GraphFrames (GraphX for Python)",
      "target_audience": [
        "Yep! Analysts who want to leverage Spark for analyzing interesting datasets",
        "Yep! Data Scientists who want a single engine for analyzing and modelling data as well as productionizing it.",
        "Yep! Engineers who want to use a distributed computing engine for batch or stream processing or both"
      ]
    },
    {
      "title": "Text Analysis and Natural Language Processing With Python",
      "url": "https://www.udemy.com/course/text-analysis-and-natural-language-processing-with-python/",
      "bio": "Use Python and Google CoLab For Social Media Mining and Text Analysis and Natural Language Processing (NLP)",
      "objectives": [
        "Students will be able to read in data from different sources- including websites and social media",
        "Social media mining from Twitter",
        "Extract information relating to tweets and posts",
        "Analyze text data for emotions",
        "Carry out Sentiment analysis",
        "Implement natural language processing (NLP) on different types of text data",
        "Introduction to some of the most common Python text analysis packages"
      ],
      "course_content": {
        "Introduction To Social Media Mining With Python": [
          "Welcome to the Course",
          "Data and Code",
          "Python Installation",
          "What Is Google CoLab?",
          "Google Colabs and GPU",
          "Google Colab Packages"
        ],
        "Basic Data Preprocessing": [
          "What Is Pandas?",
          "Basic Data Cleaning With Pandas",
          "Basics of Data Visualization"
        ],
        "Welcome To Social Media": [
          "Can Social Media Be Useful?: The Case of Twitter"
        ],
        "Extracting Tweets (Without An API)": [
          "Obtaining Tweets Without A Twitter Account",
          "Lets Dip Our Toes Into Twitter",
          "Get Elon Musk's Tweet",
          "Obtain The Most Popular Tweets of a User",
          "Obtain Tweets For A User Between A Certain Date",
          "Look With For With a Specific Term",
          "Elon Musk's Bitcoin Tweets",
          "Tweets From a Location",
          "Tweets From Multiple Locations",
          "Tweets From Multiple Locations and Multiple Terms",
          "Another Way of Obtaining Tweets",
          "More Snscrape Tweets"
        ],
        "Other Ways of Obtaining Textual Data": [
          "What is API?",
          "Using APIs: Singapore MRT Stations",
          "Obtain Financial News Headlines",
          "Obtaining Textual Data From Reddit"
        ],
        "Basic Textual Data Preprocessing": [
          "Introduction to Theory",
          "Lets Start Cleaning The Text",
          "Final Cleaned Text",
          "A Function For Text Cleaning",
          "More Text Cleaning",
          "Another NTLK-Based Workflow"
        ],
        "Exploring Text Data": [
          "Tweet Lengths",
          "How People Interact With Tweets",
          "Of Mentions and Hashtags",
          "Identify The Most Popular Hashtags",
          "Identify the Most Common Usernames",
          "What Are Wordclouds?",
          "Basic Wordcloud-Install",
          "A Basic Wordcloud",
          "Word Count of Common Words",
          "N-Grams",
          "Network of Bigrams",
          "Topic Modelling With Gensim"
        ],
        "Exploring Sentiments": [
          "Identify the Polarity of Text",
          "Polarity: Positive or Negative",
          "Dealing With Dates",
          "Introduction to VADER Sentiment Analysis",
          "Dealing With Dates",
          "VADER Sentiment For Financial News",
          "Visualise the Sentiments"
        ],
        "Machine Learning and Deep learning For Text Data": [
          "What Is Machine Learning?",
          "Preprocessing-Toy Example",
          "A Simple Machine Learning Model on Textual Data",
          "Predicting Stock Price Movements Based On Newspaper Headlines",
          "Unsupervised Learning With K-Means Algorithm",
          "Identifying Textual Clusters With K-means",
          "DBSCAN Based Textual Clustering",
          "Classify the Tweet Sentiment-GBM",
          "Keras Installation-Windows",
          "Keras Installation-Mac",
          "Long short-term memory (LSTM): Theory",
          "Brief Lowdown on Word Embeddings",
          "LSTM For Classifying Tweet Sentiment-1"
        ],
        "Miscellaneous Information": [
          "Lets Do Dictionaries",
          "Set up the FourSquare App",
          "NTLK Cleaning",
          "Text Summarisation With AI-Case Study",
          "Of NLP and ChatGPT",
          "Posit On POSIT",
          "Distributed Computing"
        ]
      },
      "requirements": [
        "Should have prior experience of Python data science",
        "Prior experience of statistical and machine learning techniques will be beneficial",
        "Should have an interest in extracting unstructured text data from social media and websites",
        "Should have an interest in extracting insights from text analysis",
        "Should have an interest in applying machine learning models on text data"
      ],
      "description": "ENROLL IN MY LATEST COURSE ON HOW TO LEARN ALL ABOUT PYTHON SOCIAL MEDIA & NATURAL LANGUAGE PROCESSING (NLP)\nDo you want to harness the power of social media to make financial decisions?\nAre you looking to gain an edge in the fields of retail, online selling, real estate and geolocation services?\nDo you want to turn unstructured data from social media and web pages into real insights?\nDo you want to develop cutting edge analytics and visualisations to take advantage of the millions of Twitter posts that appear each day?\nGaining proficiency in social media mining can help you harness the power of the freely available data and information on the world wide web (including popular social media sites such as Twitter) and turn it into actionable insights\nMY COURSE IS A HANDS-ON TRAINING WITH REAL PYTHON SOCIAL MEDIA MINING- You will learn to carry out text analysis and natural language processing (NLP) to gain insights from unstructured text data, including tweets\nMy course provides a foundation to carry out PRACTICAL, real-life social media mining. By taking this course, you are taking an important step forward in your data science journey to become an expert in harnessing the power of social media for deriving insights and identifying trends.\nWhy Should You Take My Course?\nI have an MPhil (Geography and Environment) from the University of Oxford, UK. I also completed a data science intense PhD at Cambridge University (Tropical Ecology and Conservation).\nI have several years of experience in analyzing real-life data from different sources and producing publications for international peer-reviewed journals.\nThis course will help you gain fluency both in the different aspects of text analysis and NLP working through a real-life example of cryptocurrency tweets and financial news using a powerful clouded based python environment called GoogleColab. Specifically, you will\nGain proficiency in setting up and using Google CoLab for Python Data Science tasks\nCarry out common social media mining tasks such as obtaining tweets (e.g. tweets relating to bitcoins)\nWork with complicated web pages and extract information\nProcess the extracted textual information in a usable form via preprocessing techniques implemented via powerful Python packages such as NTLK\nA thorough grounding in text analysis and NLP related Python packages such as NTLK, Snscrape among others\nCarry out common text analytics tasks such as Sentiment Analysis\nImplement machine learning and artificial intelligence techniques on text data\nYou will work on practical mini case studies relating to (a) extracting and  pre-processing tweets from certain users and topics relating to cryptocurrencies (b) identify the sentiments of cryptocurrency tweets(c) classify your tweets using machine learning models\nIn addition to all the above, you’ll have MY CONTINUOUS SUPPORT to make sure you get the most value out of your investment!\nENROLL NOW :)",
      "target_audience": [
        "People who wish to learn practical text mining and natural language processing",
        "People who wish to derive insights from textual and social media data",
        "People wanting to understand the impact of human sentiments on financial markets"
      ]
    },
    {
      "title": "Hands On Natural Language Processing (NLP) using Python",
      "url": "https://www.udemy.com/course/hands-on-natural-language-processing-using-python/",
      "bio": "Learn Natural Language Processing ( NLP ) & Text Mining by creating text classifier, article summarizer, and many more.",
      "objectives": [
        "Understand the various concepts of natural language processing along with their implementation",
        "Build natural language processing based applications",
        "Learn about the different modules available in Python for NLP",
        "Create personal spam filter or sentiment predictor",
        "Create personal text summarizer"
      ],
      "course_content": {
        "Introduction to the Course": [
          "What is NLP?",
          "Getting the Course Resources",
          "Getting the Course Resources - Text"
        ],
        "Getting the required softwares": [
          "Installing Anaconda Python",
          "Installing Anaconda Python - Text",
          "A tour of Spyder IDE",
          "How to take this course?"
        ],
        "Python Crash Course": [
          "Variables and Operations in Python",
          "Conditional Statements",
          "Introduction to Loops",
          "Loop Control Statements",
          "Python Data Structures - Lists",
          "Python Data Structures - Tuples",
          "Python Data Structures - Dictionaries",
          "Console and File I/O in Python",
          "Introduction to Functions",
          "Introduction to Classes and Objects",
          "List Comprehension",
          "Test Your Skills"
        ],
        "Regular Expressions": [
          "Introduction to Regular Expressions",
          "Finding Patterns in Text Part 1",
          "Finding Patterns in Text Part 2",
          "Substituting Patterns in Text",
          "Shorthand Character Classes",
          "Character Ranges - Text",
          "Preprocessing using Regex",
          "Test Your Skills"
        ],
        "Numpy and Pandas": [
          "Introduction to Numpy",
          "Introduction to Pandas"
        ],
        "NLP Core": [
          "Installing NLTK in Python",
          "Tokenizing Words and Sentences",
          "How tokenization works? - Text",
          "Introduction to Stemming and Lemmatization",
          "Stemming using NLTK",
          "Lemmatization using NLTK",
          "Stop word removal using NLTK",
          "Parts Of Speech Tagging",
          "POS Tag Meanings",
          "Named Entity Recognition",
          "Text Modelling using Bag of Words Model",
          "Building the BOW Model Part 1",
          "Building the BOW Model Part 2",
          "Building the BOW Model Part 3",
          "Building the BOW Model Part 4",
          "Text Modelling using TF-IDF Model",
          "Building the TF-IDF Model Part 1",
          "Building the TF-IDF Model Part 2",
          "Building the TF-IDF Model Part 3",
          "Building the TF-IDF Model Part 4",
          "Understanding the N-Gram Model",
          "Building Character N-Gram Model",
          "Building Word N-Gram Model",
          "Understanding Latent Semantic Analysis",
          "LSA in Python Part 1",
          "LSA in Python Part 2",
          "Word Synonyms and Antonyms using NLTK",
          "Word Negation Tracking in Python Part 1",
          "Word Negation Tracking in Python Part 2"
        ],
        "Project 1 - Text Classification": [
          "Getting the data for Text Classification",
          "Getting the data for Text Classification - Text",
          "Importing the dataset",
          "Persisting the dataset",
          "Preprocessing the data",
          "Transforming data into BOW Model",
          "Transform BOW model into TF-IDF Model",
          "Creating training and test set",
          "Understanding Logistic Regression",
          "Training our classifier",
          "Testing Model performance",
          "Saving our Model",
          "Importing and using our Model"
        ],
        "Project 2 - Twitter Sentiment Analysis": [
          "Setting up Twitter Application",
          "Initializing Tokens",
          "Client Authentication",
          "Fetching real time tweets",
          "Loading TF-IDF Model and Classifier",
          "Preprocessing the tweets",
          "Predicting sentiments of tweets",
          "Plotting the results"
        ],
        "Project 3 - Text Summarization": [
          "Understanding Text Summarization",
          "Fetching article data from the web",
          "Parsing the data using Beautiful Soup",
          "Preprocessing the data",
          "Tokenizing Article into sentences",
          "Building the histogram",
          "Calculating the sentence scores",
          "Getting the summary"
        ],
        "Word2Vec Analysis": [
          "Understanding Word Vectors",
          "Importing the data",
          "Preparing the data",
          "Training the Word2Vec Model",
          "Testing Model Performance",
          "Improving the Model",
          "Exploring Pre-trained Models"
        ]
      },
      "requirements": [
        "Basic Programming Experience in any language",
        "Concept of Object Oriented Programming",
        "Knowledge of Basic to Intermediate Mathematics",
        "Knowledge of Matrix operations"
      ],
      "description": "In this course you will learn the various concepts of natural language processing by implementing them hands on in python programming language. This course is completely project based and from the start of the course the main objective would be to learn all the concepts required to finish the different projects. You will be building a text classifier which you will use to predict sentiments of tweets in real time and you will also be building an article summarizer which will fetch articles from websites and find the summary. Apart from these you will also be doing a lot of mini projects through out the course. So, at the end of the course you will have a deep understanding of NLP and how it is applied in real world.",
      "target_audience": [
        "Anyone willing to start a career in data science and natural language processing",
        "Anyone willing to learn the concepts of natural language processing by implementing them",
        "Anyone willing to learn Sentiment Analysis"
      ]
    },
    {
      "title": "Deploying AI & Machine Learning Models for Business | Python",
      "url": "https://www.udemy.com/course/deploy-data-science-nlp-models-with-docker-containers/",
      "bio": "Learn to build Machine Learning, Deep Learning & NLP Models & Deploy them with Docker Containers (DevOps) (in Python)",
      "objectives": [
        "How to synchronize the versatility of DevOps & Machine Learning",
        "Master Docker , Docker Files, Docker Applications & Docker Containers (DevOps)",
        "Flask Basics & Application Program Interface (API)",
        "Build & Deploy a Random Forest Model",
        "Build a Text based (Natural Language Processing : NLP ) CLUSTERING (KMeans) Model and expose it as an API",
        "Build an API which will run a Deep Learning Model (Convolutional Neural Network : CNN) Model for Image Recognition & Classification"
      ],
      "course_content": {
        "Course Overview": [
          "Introduction",
          "I have a model. Now what?",
          "Skills Checklist",
          "Learning Goals"
        ],
        "Docker basics": [
          "Why docker?",
          "What are docker containers?",
          "Importance of docker containers in machine learning",
          "Where devops meets data science",
          "Summary"
        ],
        "Flask basics": [
          "Introduction",
          "Setting up a Flask Project",
          "Simple Flask API to add two numbers",
          "Taking user input with GET requests",
          "POST request with Flask",
          "Using Flask in the context of Machine Learning"
        ],
        "Exposing a Random Forest Machine Learning service as an API": [
          "Introduction",
          "API & Dataset Overview",
          "Training the Random Forest model",
          "Pickling the Random Forest model",
          "Exposing the Random Forest model as a Flask API",
          "Testing the API model",
          "Providing file input to Flask API",
          "Flasgger for autogenerating UI",
          "Summary"
        ],
        "Writing and building the Dockerfile": [
          "Introduction",
          "Base Image & FROM command",
          "COPY and EXPOSE commands",
          "WORKDIR, RUN and CMD commands",
          "Preparing the flask scripts for dockerizing",
          "Writing the Dockerfile",
          "Building the docker image",
          "Running the Random Forest model on Docker"
        ],
        "Building a production grade Docker application": [
          "Introduction",
          "Overall Architecture",
          "Configuring the WSGI file",
          "Writing a production grade Dockerfile",
          "Running and debugging a docker container in production",
          "Docker Quiz 1 – Basic Concepts, Commands"
        ],
        "Building NLP based Text Clustering application": [
          "Introduction",
          "Stemming & Lemmatization for cleaner text",
          "Converting unstructured to structured data",
          "KMeans Clustering",
          "Preparing the excel output",
          "Making the output Downloadable",
          "Finding top keywords for kmeans clusters",
          "Final output with charts",
          "Summary",
          "Dockerizing the text clustering app"
        ],
        "API for image recognition with deep learning": [
          "Introduction",
          "Visualizing the input images",
          "Preparing the input images",
          "Building the deep learning model",
          "Training and saving the trained deep learning model",
          "Generating test images",
          "Flask API wrapper for making predictions",
          "Summary",
          "Dockerizing the deep learning app"
        ]
      },
      "requirements": [
        "Basic programming in any language",
        "Basic Mathematics",
        "Some exposure to Python (but not mandatory)"
      ],
      "description": "Machine Learning, as we know it is the new buzz word in the industry today. This is practiced in every sector of business imaginable to provide data-driven solutions to complex business problems. This poses the challenge of deploying the solution, built by the Machine Learning technique so that it can be used across the intended Business Unit and not operated in silos.\nThis is an extensive and well-thought course created & designed by UNP's elite team of Data Scientists from around the world to focus on the challenges that are being faced by Data Scientists and Computational Solution Architects across the industry  which is summarized the below  sentence :\n\"I HAVE THE MACHINE LEARNING MODEL, IT IS WORKING AS EXPECTED !! NOW, WHAT ?????\"\nThis course will help you create a solid foundation of the essential topics of data science along with a solid foundation of deploying those created solutions through Docker containers which eventually will expose your model as a service (API) which can be used by all who wish for it.\nAt the end of this course, you will be able to:\nLearn about Docker, Docker Files, Docker Containers\nLearn Flask Basics & Application Program Interface (API)\nBuild a Random Forest Model and deploy it.\nBuild a Natural Language Processing based Test Clustering Model (K-Means) and visualize it.\nBuild an API for Image Processing and Recognition with a Deep Learning Model under the hood (Convolutional Neural Network: CNN)\nThis course is a perfect blend of foundations of data science, industry standards, broader understanding of machine learning and practical applications and most importantly deploying them.",
      "target_audience": [
        "Anyone willing to venture into the realm of data science",
        "Anyone who would be interested in deploying a Data Science Solution, can be Regression, NLP or even Deep Learning Models"
      ]
    },
    {
      "title": "Advanced AI: Deep Reinforcement Learning in Python",
      "url": "https://www.udemy.com/course/deep-reinforcement-learning-in-python/",
      "bio": "The Complete Guide to Mastering Artificial Intelligence using Deep Learning and Neural Networks",
      "objectives": [
        "Build various deep learning agents (including DQN and A3C)",
        "Apply a variety of advanced reinforcement learning algorithms to any problem",
        "Q-Learning with Deep Neural Networks",
        "Policy Gradient Methods with Neural Networks",
        "Reinforcement Learning with RBF Networks",
        "Use Convolutional Neural Networks with Deep Q-Learning",
        "Understand important foundations for OpenAI ChatGPT, GPT-4"
      ],
      "course_content": {},
      "requirements": [
        "Know reinforcement learning basics, MDPs, Dynamic Programming, Monte Carlo, TD Learning",
        "College-level math is helpful",
        "Experience building machine learning models in Python and Numpy",
        "Know how to build ANNs and CNNs using Theano or Tensorflow"
      ],
      "description": "Ever wondered how AI technologies like OpenAI ChatGPT and GPT-4 really work? In this course, you will learn the foundations of these groundbreaking applications.\nThis course is all about the application of deep learning and neural networks to reinforcement learning.\nIf you’ve taken my first reinforcement learning class, then you know that reinforcement learning is on the bleeding edge of what we can do with AI.\nSpecifically, the combination of deep learning with reinforcement learning has led to AlphaGo beating a world champion in the strategy game Go, it has led to self-driving cars, and it has led to machines that can play video games at a superhuman level.\nReinforcement learning has been around since the 70s but none of this has been possible until now.\nThe world is changing at a very fast pace. The state of California is changing their regulations so that self-driving car companies can test their cars without a human in the car to supervise.\nWe’ve seen that reinforcement learning is an entirely different kind of machine learning than supervised and unsupervised learning.\nSupervised and unsupervised machine learning algorithms are for analyzing and making predictions about data, whereas reinforcement learning is about training an agent to interact with an environment and maximize its reward.\nUnlike supervised and unsupervised learning algorithms, reinforcement learning agents have an impetus - they want to reach a goal.\nThis is such a fascinating perspective, it can even make supervised / unsupervised machine learning and \"data science\" seem boring in hindsight. Why train a neural network to learn about the data in a database, when you can train a neural network to interact with the real-world?\nWhile deep reinforcement learning and AI has a lot of potential, it also carries with it huge risk.\nBill Gates and Elon Musk have made public statements about some of the risks that AI poses to economic stability and even our existence.\nAs we learned in my first reinforcement learning course, one of the main principles of training reinforcement learning agents is that there are unintended consequences when training an AI.\nAIs don’t think like humans, and so they come up with novel and non-intuitive solutions to reach their goals, often in ways that surprise domain experts - humans who are the best at what they do.\nOpenAI is a non-profit founded by Elon Musk, Sam Altman (Y Combinator), and others, in order to ensure that AI progresses in a way that is beneficial, rather than harmful.\nPart of the motivation behind OpenAI is the existential risk that AI poses to humans. They believe that open collaboration is one of the keys to mitigating that risk.\nOne of the great things about OpenAI is that they have a platform called the OpenAI Gym, which we’ll be making heavy use of in this course.\nIt allows anyone, anywhere in the world, to train their reinforcement learning agents in standard environments.\nIn this course, we’ll build upon what we did in the last course by working with more complex environments, specifically, those provided by the OpenAI Gym:\nCartPole\nMountain Car\nAtari games\nTo train effective learning agents, we’ll need new techniques.\nWe’ll extend our knowledge of temporal difference learning by looking at the TD Lambda algorithm, we’ll look at a special type of neural network called the RBF network, we’ll look at the policy gradient method, and we’ll end the course by looking at Deep Q-Learning (DQN) and A3C (Asynchronous Advantage Actor-Critic).\nThanks for reading, and I’ll see you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nCollege-level math is helpful (calculus, probability)\nObject-oriented programming\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations\nLinear regression\nGradient descent\nKnow how to build ANNs and CNNs in Theano or TensorFlow\nMarkov Decision Proccesses (MDPs)\nKnow how to implement Dynamic Programming, Monte Carlo, and Temporal Difference Learning to solve MDPs\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Professionals and students with strong technical backgrounds who wish to learn state-of-the-art AI techniques"
      ]
    },
    {
      "title": "Complete iOS Machine Learning Masterclass",
      "url": "https://www.udemy.com/course/complete-ios-machine-learning-masterclass/",
      "bio": "The most comprehensive course on Machine Learning for iOS development. Master building smart apps iOS Swift 4",
      "objectives": [
        "Build smart iOS 11 & Swift 4 apps using Machine Learning",
        "Use trained ML models in your apps",
        "Convert ML models to iOS ready models",
        "Create your own ML models",
        "Apply Object Prediction on pictures, videos, speech and text",
        "Discover when and how to apply a smart sense to your apps"
      ],
      "course_content": {
        "Getting started": [
          "About Your Instructor and Course Overview",
          "About Machine Learning",
          "Activity: Playing with Machine Learning Style Transfer"
        ],
        "Optional - iOS Fundamentals": [
          "About this section - start iOS",
          "Download and install xcode for iOS 11",
          "Get the iOS developer license",
          "How to use a MAC on Windows PC or Linux",
          "How to install iOS 11 on your iPhone or iPad",
          "Use the Xcode interface",
          "Xcode configuration files"
        ],
        "Optional - Machine Learning Concepts": [
          "About this section - intro to ML",
          "What is an Artificial Neuron - Neural Network",
          "Parts of an Artificial Neural Network",
          "Explanation - Convolutional Neural Network",
          "Recurrent Neural Networks basics RNNs"
        ],
        "iOS Machine Learning With Photos": [
          "About this section - coreML with Photos",
          "Demo of project using coreML on photos",
          "About ML model and Neural Networks",
          "Project: Create the xcode project",
          "Project: How to add ML models to xcode projects",
          "Project: How to get pre-made ML models for iOS",
          "Project: How to use ML models with images (part 1)",
          "Project: How to use ML models with images (part 2)",
          "Project: Programming the VN request callback method",
          "Testing different ML models",
          "Exercise: Models with Images input",
          "Solution: Models with Images input",
          "Basics of Machine Learning for iOS",
          "Summary: coreML Vision with Photos"
        ],
        "coreML All about custom models": [
          "About this section - model conversion",
          "Project: Finding custom ML models",
          "Project: Converting ML models get Anaconda IDE",
          "Installing Python libraries for core ML",
          "Installing Caffe tools for core ML conversion",
          "Project: Converting scikit model to core ml mlmodel format",
          "Working with Custom Models"
        ],
        "CoreML with Data Set models": [
          "Introduction to Working with Data sets",
          "Project: Create xcode project and add iris model",
          "Project: ML dataset project User Interface",
          "Project: Properties and picker delegate methods",
          "Project: Pickerview data source methods",
          "Project: Coding prediction for data sets",
          "Project: Code improvements",
          "Important data set models information",
          "Working with Data Sets"
        ],
        "Project: coreML with Video Camera": [
          "About CoreML with Video Camera",
          "Project: Create xcode project and add VGG16 model",
          "Project: Building the user interface",
          "Project: Video Stream variables setup",
          "Project: Program camera feed",
          "Project: Capture image from video stream for ML model",
          "Project: Programming the ML prediction launch",
          "Project: Processing the ML model output",
          "Testing the live camera feed with VGG model"
        ],
        "END: iOS coreML fundamentals": [
          "Congratulations"
        ],
        "Optional - Going the extra mile": [
          "Adding converted model metadata",
          "Get a PixelBuffer from a UIImage",
          "UIImage PixelBuffer extension (part 1)",
          "UIImage PixelBuffer extension (part 2)",
          "coreML prediction using UIImage PixelBuffer"
        ],
        "Optional - Numerous Model Conversions": [
          "About model conversion types",
          "Caffe - Get a Caffe ML model with weights and labels",
          "CoreML tools conversion code with Caffe",
          "Exporting Caffe model to mlmodel format",
          "Caffe - Using the Caffe model with iOS",
          "Keras - Load Save Keras models and convert to mlmodel",
          "Vision Image Request parameter options"
        ]
      },
      "requirements": [
        "Basic understanding of programming",
        "Have access to a MAC computer or MACinCloud website"
      ],
      "description": "If you want to learn how to start building professional, career-boosting mobile apps and use Machine Learning to take things to the next level, then this course is for you. The Complete iOS Machine Learning Masterclass™ is the only course that you need for machine learning on iOS. Machine Learning is a fast-growing field that is revolutionizing many industries with tech giants like Google and IBM taking the lead. In this course, you’ll use the most cutting-edge iOS Machine Learning technology stacks to add a layer of intelligence and polish to your mobile apps. We’re approaching a new era where only apps and games that are considered “smart” will survive. (Remember how Blockbuster went bankrupt when Netflix became a giant?) Jump the curve and adopt this innovative approach; the Complete iOS Machine Learning Masterclass™ will introduce Machine Learning in a way that’s both fun and engaging.\nIn this course, you will:\nMaster the 3 fundamental branches of applied Machine Learning: Image & Video Processing, Text Analysis, and Speech & Language Recognition\nDevelop an intuitive sense for using Machine Learning in your iOS apps\nCreate 7 projects from scratch in practical code-along tutorials\nFind pre-trained ML models and make them ready to use in your iOS apps\nCreate your own custom models\nAdd Image Recognition capability to your apps\nIntegrate Live Video Camera Stream Object Recognition to your apps\nAdd Siri Voice speaking feature to your apps\nDive deep into key frameworks such as coreML, Vision, CoreGraphics, and GamePlayKit.\nUse Python, Keras, Caffee, Tensorflow, sci-kit learn, libsvm, Anaconda, and Spyder–even if you have zero experience\nGet FREE unlimited hosting for one year\nAnd more!\nThis course is also full of practical use cases and real-world challenges that allow you to practice what you’re learning. Are you tired of courses based on boring, over-used examples?  Yes? Well then, you’re in a treat. We’ll tackle 5 real-world projects in this course so you can master topics such as image recognition, object recognition, and modifying existing trained ML models. You’ll also create an app that classifies flowers and another fun project inspired by Silicon Valley™ Jian Yang’s masterpiece: a Not-Hot Dog classifier app!\nWhy Machine Learning on iOS\nOne of the hottest growing fields in technology today, Machine Learning is an excellent skill to boost your your career prospects and expand your professional tool kit. Many of Silicon Valley’s hottest companies are working to make Machine Learning an essential part of our daily lives. Self-driving cars are just around the corner with millions of miles of successful training. IBM’s Watson can diagnose patients more effectively than highly-trained physicians. AlphaGo, Google DeepMind’s computer, can beat the world master of the game Go, a game where it was thought only human intuition could excel.\nIn 2017, Apple has made Machine Learning available in iOS so that anyone can build smart apps and games for iPhones, iPads, Apple Watches and Apple TVs. Nowadays, apps and games that do not have an ML layer will not be appealing to users. Whether you wish to change careers or create a second stream of income, Machine Learning is a highly lucrative skill that can give you an amazing sense of gratification when you can apply it to your mobile apps and games.\nWhy This Course Is Different\nMachine Learning is very broad and complex; to navigate this maze, you need a clear and global vision of the field. Too many tutorials just bombard you with the theory, math, and coding. In this course, each section focuses on distinct use cases and real projects so that your learning experience is best structured for mastery.\nThis course brings my teaching experience and technical know-how to you. I’ve taught programming for over 10 years, and I’m also a veteran iOS developer with hands-on experience making top-ranked apps. For each project, we will write up the code line by line to create it from scratch. This way you can follow along and understand exactly what each line means and how to code comes together. Once you go through the hands-on coding exercises, you will see for yourself how much of a game-changing experience this course is.\nAs an educator, I also want you to succeed. I’ve put together a team of professionals to help you master the material. Whenever you ask a question, you will get a response from my team within 48 hours. No matter how complex your question, we will be there–because we feel a personal responsibility in being fully committed to our students.\nBy the end of the course, you will confidently understand the tools and techniques of Machine Learning for iOS on an instinctive level.\nDon’t be the one to get left behind.  Get started today and join millions of people taking part in the Machine Learning revolution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntopics: ios swift 4 coreml vision deep learning machine learning neural networks python anaconda trained models keras tensorflow scikit learn core ml ios12 Swift4 scikitlearn artificial neural network ANN recurrent neural network RNN  convolutional neural network CNN ocr character recognition face detection  ios swift 4 coreml vision deep learning machine learning neural networks python anaconda trained models keras tensorflow scikit learn core ml ios12 Swift4 scikitlearn artificial neural network ANN recurrent neural network RNN  convolutional neural network CNN ocr character recognition face detection  ios swift 4 coreml vision deep learning machine learning neural networks python anaconda trained models keras tensorflow scikit learn core ml ios12 Swift4 scikitlearn artificial neural network ANN recurrent neural network RNN  convolutional neural network CNN ocr character recognition face detection  ios swift 4 coreml vision deep learning machine learning neural networks python anaconda trained models keras tensorflow scikit learn core ml ios12 Swift4 scikitlearn artificial neural network ANN recurrent neural network RNN  convolutional neural network CNN ocr character recognition face detection  ios  swift 4 coreml vision deep learning machine learning neural networks python anaconda trained models keras tensorflow scikit learn core ml ios12 Swift4 scikitlearn artificial neural network ANN recurrent neural network RNN  convolutional neural network CNN ocr character recognition face detection",
      "target_audience": [
        "People with a basic foundation in iOS programming who would like to discover Machine Learning, a branch of Artificial Intelligence",
        "People who want to pursue a career combining app development and Machine Learning to become a hybrid iOS developer and ML expert",
        "Developers who would like to apply their Machine Learning skills by creating practical mobile apps",
        "Entrepreneurs who want to leverage the exponential technology of Machine Learning to create added value to their business could also take this course. However, this course does assume that you are familiar with basic programming concepts such as object oriented programming, variables, methods, classes, and conditional statements"
      ]
    },
    {
      "title": "Mastering Data Science and Machine Learning Fundamentals",
      "url": "https://www.udemy.com/course/mastering-data-science-and-machine-learning-fundamentals/",
      "bio": "Data Science & Machine Learning- Data Science, Machine Learning, Regression, Classification and Clustering [THEORY ONLY]",
      "objectives": [
        "Mastering Data Science fundamentals",
        "Mastering Machine Learning Fundamentals",
        "How and when to use each Machine Learning model",
        "Make regression using Linear Regression, SVM, Decsision Trees and Ensemble Modeling",
        "Classify data using K-Means clustering, Support Vector Machines (SVM), KNN, Decision Trees, Naive Bayes, and PCA"
      ],
      "course_content": {
        "Welcome and Introduction": [
          "Welcome",
          "Introduction to Data Science",
          "Request for Your Honest Review"
        ],
        "Preliminary to understand Data Science and Machine Learning": [
          "Understand Data Science Terms",
          "Understand Machine Learning",
          "Type of Learning",
          "Understand Data Science Modelling",
          "what is the difference between model Parameters and Hyperparameters"
        ],
        "Machine learning Models": [
          "How Linear Regression works?",
          "How Decision Trees works?",
          "How Bagging and Random Forest work?",
          "How Support Vector Machine Works?",
          "Neural Networks Part 1",
          "Neural Networks Part 2",
          "How Logistic Regression works?",
          "How KNN works?",
          "Clustering techniques"
        ],
        "Models Performances": [
          "Models performance"
        ],
        "Best Practices for Data Scientist": [
          "Best Practices for Data Scientist",
          "THANK YOU"
        ]
      },
      "requirements": [
        "Absolutely no prior knowledge or experience needed. Only a passion to be successful!",
        "Admin permissions to download files"
      ],
      "description": "Embark on a Journey into the World of Data Science and Machine Learning!\nWelcome to the Mastering Data Science & Machine Learning Fundamentals for Beginners course, a comprehensive and illuminating exploration of the captivating realms of Data Science and Machine Learning!\nIn today's rapidly evolving landscape, Data Science and Machine Learning are not mere buzzwords; they are the driving forces behind innovation in diverse domains, including IT, security, marketing, automation, and healthcare. These technologies underpin the very foundations of modern conveniences, from email spam filters and efficient Google searches to personalized advertisements, precise weather forecasts, and uncanny sports predictions. This course is your gateway to understanding the magic behind these advancements.\n\n\nDesigned with students and learners in mind, this course aims to demystify complex machine learning algorithms, statistics, and mathematics. It caters to those curious minds eager to solve real-world problems using the power of machine learning. Starting with the fundamentals, the course progressively deepens your understanding of a vast array of machine learning and data science concepts.\n\n\nNo prior knowledge or experience is required to embark on this enriching learning journey. This course not only simplifies intricate machine learning concepts but also provides hands-on guidance on implementing them successfully.\n\n\nOur esteemed instructors, experts in data science and AI, are your trusted guides throughout this course. They are committed to making each concept crystal clear, steering away from confusing mathematical notations and jargon, and ensuring that everything is explained in plain English.\n\n\nHere's a glimpse of what you'll delve into:\nMastering Machine Learning Fundamentals\nDistinguishing between Supervised and Unsupervised Learning\nUnveiling the Power of Linear Regression\nHarnessing the Potential of Support Vector Machines (SVM)\nNavigating Decision Trees and the Enchanting Realm of Random Forests\nDemystifying Logistic Regression\nGetting Acquainted with K-Nearest Neighbors (K-NN)\nEmbracing Naive Bayes\nDelving into K-Means Clustering\nExploring the World of Hierarchical Clustering\nAssessing Machine Learning Model Performance with Confidence\nVenturing into the Realm of Neural Networks\nUncovering Best Practices for Data Scientists\nAnd so much more!\n\n\nWhether you're a programmer seeking to pivot into an exciting new career or a data analyst with aspirations in the AI industry, this course equips you with essential techniques used by real-world data scientists. These are the skills every aspiring technologist should possess, making your learning journey a vital investment in your future.\n\n\nSo, don't hesitate! Enroll in this course today to begin your transformation into a Data Scientist. Whether you're taking your first steps into this exciting field or you're an experienced data scientist looking to refine your skills, this course is your ticket to mastering Data Science and Machine Learning.\n\n\nSeize this opportunity to unlock the fascinating world of Data Science and Machine Learning. Enroll now!\n\n\n\n\nList of Keywords:\nData Science\nMachine Learning\nBeginner's Guide\nFundamentals\nData Analysis\nStatistics\nLinear Regression\nSupervised Learning\nUnsupervised Learning\nSupport Vector Machine\nDecision Trees\nRandom Forest\nLogistic Regression\nK-Nearest Neighbors\nNaive Bayes\nClustering\nPerformance Evaluation\nNeural Networks\nBest Practices\nHands-on\nPractical Implementation\nData Scientist\nAI Industry\nCareer Transition\nReal-world Problems\nPlain English Explanation\nExpert Instructors\nOnline Learning\nEnroll Now\nComprehensive Course\nBeginner-Friendly\nData Analysis Techniques\nPython Programming\nMachine Learning Models\nLearning Path\nAlgorithmic Concepts\nHands-on Exercises\nInteractive Learning\nMaster Data Science\nBuild Machine Learning Models",
      "target_audience": [
        "Beginners who want to approach Machine Learning, but are too afraid of complex math to start",
        "Students and academicians, especially those focusing on Machine Learning",
        "Professors, lecturers or tutors who are looking to find better ways to explain the content to their students in the simplest and easiest way"
      ]
    },
    {
      "title": "The 10-Day Python Bootcamp for Engineers and Scientists 2025",
      "url": "https://www.udemy.com/course/python-for-engineers-scientists-and-analysts/",
      "bio": "Bite-sized chunks of learning: practical coding skills for working with data, visualisation, modelling and simulation.",
      "objectives": [
        "Learn Practical Python Programming for Data Science, Engineering, Modelling and Simulation tasks",
        "Analyze and Manipulate Data Using Pandas and NumPy",
        "Visualise Data with Matplotlib and Seaborn",
        "Develop Predictive Models and Simulate Real-World Scenarios",
        "Automate Data Processes to Produce Immediate, Actionable Insights",
        "Understand and Apply Statistical Methods to Analyse and Interpret Data",
        "Create Professional Visualisations to Present Your Findings",
        "Write Efficient Python Scripts and Functions for Data Analysis",
        "Solve Engineering, Scientific, and Analytical Problems Using Python",
        "Build Practical Projects to Showcase Your Skills",
        "Prepare for a Career in Data Science, Analytics or Engineering",
        "Apply Your Skills to Real-Life Business Cases and Projects",
        "Understand the Concept of Modular Programming and Apply It to Real-World Problems",
        "Work with Real Datasets to Solve Complex Analytical Challenges"
      ],
      "course_content": {
        "Day 1 - Introduction": [
          "Introduction to the Section",
          "Course Outcomes",
          "Real World Applications",
          "Course Structure",
          "Wrap Up"
        ],
        "Day 1 - continued: Getting Python Up and Running": [
          "Introduction",
          "Why Thonny",
          "Comparing Different Interactive Development Environments (IDEs)",
          "Installing Thonny",
          "A Brief Tour of Thonny",
          "Example Code",
          "Wrap Up",
          "Try running your first few lines of code",
          "Getting Python Up and Running Quiz"
        ],
        "Day 2: Print Statements, Variables and the Order of Operations": [
          "Introduction",
          "\"Hello World\"",
          "Variables and Basic Mathematics",
          "Order of Operations with BIDMAS",
          "Example with Geometry",
          "Wrap Up",
          "Understanding BIDMAS in Python",
          "Calculating Kinetic Energy"
        ],
        "Day 3: Understanding Python's Data Types": [
          "Introduction",
          "Integers and Floats",
          "Strings",
          "Lists",
          "Dictionaries",
          "Recap Summary",
          "Wrap Up",
          "Understanding Data Types in Python",
          "Analysing Sensor Data"
        ],
        "Day 4: NumPy: Arrays, Vectorisation and Statistics": [
          "Introduction",
          "Introducing NumPy",
          "NumPy Arrays",
          "Practical Examples",
          "Statistics with NumPy",
          "Simulating Vibration Data by Sampling from a Distribution",
          "Wrap Up",
          "Introduction to NumPy – Numerical Computations in Python",
          "Analyzing Vibration Data Using NumPy"
        ],
        "Day 5: Importing and Inspecting Data with Pandas DataFrames": [
          "Decoding Data: Working with Pandas and CSV Files",
          "Installing and Importing Pandas",
          "Reading in Data from a .csv File",
          "Inspecting Data with Pandas",
          "Wrap Up",
          "Introduction to Pandas – Exploring DataFrames in Python",
          "Exploring Manufacturing Production Data with Pandas"
        ],
        "Day 6: Statistics with Pandas": [
          "Introduction",
          "Statistical Analysis with Pandas",
          "Central Tendency Measures",
          "Outliers and Variability",
          "Manufacturing Quality Control Example",
          "Wrap Up",
          "Statistics in Python",
          "Analysing Concrete Strength Data Using Statistics"
        ],
        "Day 7: Visualisation with Matplotlib and Seaborn Part 1": [
          "Introduction",
          "Installing and Importing Seaborn",
          "Working with Time Based Data",
          "Your First Plot",
          "Grouping and Visualising Categorical Data",
          "Working with Time Data in Pandas – Extracting and Aggregating Insights",
          "Visualising Hourly Traffic Flow with Seaborn"
        ],
        "Day 8: Visualisation with Matplotlib and Seaborn Part 2": [
          "Introduction to Part 2 - Recap and Loading in Our Data",
          "Visualising the Relationship Between Two Variables",
          "Visualising a Distribution of Data with a Histogram",
          "Visualising the Distributions of Categorical Data with Box Plots",
          "Visualising the Distributions of Categorical Data with Violin Plots",
          "Wrap Up",
          "Visualising Data with Seaborn – Categorical and Distribution Plots",
          "Visualising Temperature and Humidity Data with Seaborn"
        ],
        "Day 9: Introduction to Functions for Modularity, Modelling and Simulation": [
          "Introduction",
          "Your First Function",
          "Specifying Parameters with Keyword Arguments",
          "The Importance of Documenting Functions",
          "Using Functions to Transform Data",
          "Using Functions for Simulation",
          "Wrap Up",
          "Understanding Python Functions and Data Transformation",
          "Calculating Pressure Drop in a Pipe Using Functions",
          "Monitoring Power Plant Emissions"
        ]
      },
      "requirements": [
        "Basic Computer Literacy: Learners should be comfortable using a computer, navigating files, and installing software.",
        "Curiosity and a Willingness to Learn: No prior programming experience is required! This course is designed to take beginners through the foundations of Python and data analysis step by step.",
        "(Optional) Basic Understanding of Mathematics and Statistics: While not required, familiarity with basic statistical concepts (e.g., mean, median, and standard deviation) will help learners get the most out of the course, particularly when we explore data analysis and modelling.",
        "No Prior Python Experience Required! This course will cover everything you need to know from scratch. If you’re new to programming please note that the pace of the course is quite aggressive, so please repeat sections and take your time with it if you need additional practice."
      ],
      "description": "Python Skills for Professionals – Guaranteed in Just 10 Days or Your Money Back\n\n\nif engineer_or_scientist and wants_to_learn_python:\nprint(\"Welcome! You are in the right place, please read on!\")\nelse:\nprint(\"This probably isn't for you but please feel free to check it out anyway.\")\n\n\nWhat’s Included:\nBite-Sized, Actionable Lessons: No fluff, no wasted time - just concise, practical Python training tailored for busy professionals.\nReal-World Application: From visualising data to simulating an investment portfolio, every exercise is designed to mimic real-world challenges in engineering, science, and finance.\nIndustry-Tested Techniques: Learn from proven tools and workflows with Pandas, NumPy, and Seaborn – essentials for modern data analysis and modelling.\nComprehensive Resources: Downloadable code snippets, quizzes, and a final project to simulate and analyse an investment portfolio’s performance.\nLifetime Access and Updates: Enjoy free lifetime access to all lessons and updates, including restructured modules and harder exercises based on feedback.\n\n\n3 Reasons Why this Course is a No-Brainer:\nMade by an engineer, for engineers: I'm a chartered mechanical engineer and I've coded in Python for over a decade. Skip endless YouTube tutorials and blog posts. Get straight to practical skills that are actually relevant for industry application.\nGain Immediate Results: Test your knowledge with coding exercises, assignments and quizzes throughout the course.\nBe Ready for the Future: Python is essential for modern fields like AI and data science. You can magnify your existing skillset by leveraging the power of Python.\n\n\nCourse updates:\n4th June 2025: Extensive improvements to capstone exercise on simulating portfolio returns\n2nd April 2025: Landing page updates and bug fixes.\n25th February 2025: Changed the structure to more evenly spread out the course material over the 10 days.\n5th December 2024: Some general bug fixes.\n24th November 2024: Added a big final coding exercise where you will simulate the returns of an investment portfolio and analyse the performance. Also made some of the coding exercises a little harder in response to feedback.\n28th October 2024: Big update - complete restructure to the course, breaking down lessons into much smaller bite-sized chunks to make it far easier to reference different sections.\n\n\n\"This is exactly what I was looking for to help jumpstart my Python skills.\" - Rhys Feeney, Product Manager at Ocula Technologies\n\n\n“Perfect Course for Busy Professionals Who Want Real-World Python Skills! The bite-sized lessons are perfect for someone like me who’s juggling work and learning. The course goes straight to the point, cutting out unnecessary fluff, and dives right into practical, industry-relevant examples.” - Grace, Actuary\n\n\n“This course is fantastic! Just the right level of detail and pacing to help get up to speed with Python and then apply the knowledge to real-world scenarios. I’d especially recommend this course for mechanical, chemical, or civil engineers and scientists who have resisted trying Python but are now ready to dive in.” - Chris, Engineer\n\n\nEnrol today, enjoy lifetime access (including access to all future course updates) and level up your Python skills in 10 days or your money back!",
      "target_audience": [
        "Engineers, Scientists, and Analysts",
        "Students and Academics",
        "Industry Professionals Looking to Upskill",
        "Beginners Interested in Python for Data Science"
      ]
    },
    {
      "title": "Machine Learning - Fundamental of Python Machine Learning",
      "url": "https://www.udemy.com/course/machine-learning-fundamental-of-python-machine-learning/",
      "bio": "Learn The Most Effective Machine Learning Techniques in Python",
      "objectives": [
        "The Machine Learning Process",
        "Standard Deviation",
        "Linear Regression",
        "Polynomial Regression",
        "Multiple Regression",
        "Hierarchical Clustering",
        "Logistic Regression",
        "Bootstrap Aggregation",
        "Cross Validation"
      ],
      "course_content": {
        "Introduction": [
          "Mean, Median and Mode",
          "Standard Deviation",
          "Percentiles",
          "Data Distribution",
          "Normal Data Distribution",
          "Scatter Plot",
          "Linear Regression",
          "Polynomial Regression",
          "Multiple Regression",
          "Scale",
          "Train or Test data",
          "Decision Tree",
          "Confusion Matrix",
          "Hierarchical Clustering",
          "Logistic Regression",
          "Grid Search",
          "Categorical Data",
          "K-Means",
          "Bootstrap Aggregation",
          "Cross Validation",
          "AUC-ROC Curve",
          "K-nearest neighbors"
        ]
      },
      "requirements": [
        "No prior knowledge of Python Machine Learning is required"
      ],
      "description": "Are you ready to learn on a journey into the captivating world of machine learning using Python? Welcome to \"Machine Learning - Fundamentals of Python Machine Learning,\" your gateway to understanding and applying the core principles of machine learning.\n\n\nMachine learning is transforming industries, from healthcare to finance, and Python is at the forefront of this revolution. Whether you're a budding data scientist, aspiring machine learning engineer, or simply curious about the potential of AI, this course will equip you with the foundational knowledge and practical skills to harness the power of Python for machine learning.\n\n\nKey Learning Objectives:\n\n\nIntroduction to Machine Learning: Get a comprehensive overview of machine learning, its significance, and the Python ecosystem's role in the field.\n\n\nPython for Machine Learning: Learn the basics of Python programming, data structures, and libraries essential for machine learning.\n\n\nModel Evaluation and Selection: Discover techniques for evaluating machine learning models and selecting the best model for your tasks.\n\n\nFeature Engineering: Master the art of feature selection and engineering to enhance the performance of your machine learning models.\n\n\nWhy Choose This Course?\n\n\nComprehensive Curriculum: This course is designed to take you from a machine learning novice to a proficient practitioner, ensuring you have a deep understanding of the fundamentals.\n\n\nHands On Learning: Practice your skills with coding exercises, hands-on projects, and machine learning challenges that replicate real-world scenarios.\n\n\nExpert Instruction: Benefit from the guidance of experienced instructors who have worked on machine learning projects and are passionate about sharing their knowledge.\n\n\nLifetime Access: Enroll once and have lifetime access to the course materials, ensuring your skills stay up to date with the latest developments in machine learning.\n\n\nUnlock the potential of Python in the world of machine learning. Enroll today in \"Machine Learning - Fundamentals of Python Machine Learning\" and acquire the knowledge and skills you need to excel in the exciting field of machine learning. Don't miss this opportunity to become a proficient machine learning practitioner.\n\n\nYour journey to mastering machine learning with Python starts now!",
      "target_audience": [
        "Anyone interested in Machine Learning",
        "Beginners Who want to Learn Machine Learning"
      ]
    },
    {
      "title": "Data Science Career Guide - Interview Preparation",
      "url": "https://www.udemy.com/course/data-science-career-guide-interview-preparation/",
      "bio": "Prepare for your Data Science Interview with this full guide on a career in Data Science including practice questions!",
      "objectives": [
        "Create a great data science resume!",
        "Understand various positions and titles available in the data science ecosystem.",
        "Get practice with probability and statistics interview questions.",
        "Build an understanding of good experiment design.",
        "Get practice with SQL interview questions."
      ],
      "course_content": {
        "Course Overview": [
          "Course Overview Lecture",
          "Curriculum Overview",
          "Frequently Asked Questions"
        ],
        "Data Science Career Overview": [
          "Why a choose a career in Data Science?",
          "Data Science is Interdisciplinary",
          "Data Science Position and Titles",
          "Thoughts on Higher Education"
        ],
        "Data Science Interview Preparation": [
          "Introduction to Interview Preparation",
          "Technical Tools of the Trade",
          "Theory Knowledge",
          "Machine Learning Knowledge",
          "Software Knowledge",
          "How do I know when I'm ready?"
        ],
        "The Data Science Interview Process": [
          "Resumes",
          "Interview Process",
          "Landing Interviews",
          "Negotiating Offers"
        ],
        "Probability Theory Interview Questions": [
          "Introduction to Probability Interview Questions",
          "Probability Question 1",
          "Solution for Probability Question 1",
          "Probability Question 2",
          "Solution for Probability Question 2",
          "Probability Question 3",
          "Solution for Probability Question 3",
          "Probability Question 4",
          "Solution for Probability Question 4",
          "Probability Question 5",
          "Solution for Probability Question 5",
          "Probability Question 6",
          "Solution for Probability Question 6",
          "Note about Probability Interview Question 7",
          "Probability Question 7",
          "Solution for Probability Question 7",
          "Probability Question 8",
          "Solution for Probability Question 8",
          "Probability Question 9",
          "Solution for Probability Question 9"
        ],
        "Statistics Interview Questions": [
          "Introduction to Statistics Interview Questions",
          "Statistics Interview Question 1",
          "Solution for Statistics Interview Question 1",
          "Statistics Interview Question 2",
          "Solution for Statistics Interview Question 2",
          "Statistics Interview Question 3",
          "Solution for Statistics Interview Question 3",
          "Statistics Interview Question 4",
          "Solution for Statistics Interview Question 4",
          "Statistics Interview Question 5",
          "Solution for Statistics Interview Question 5"
        ],
        "Product Analysis and Business Metrics Interview Questions": [
          "Introduction to Product Design and Metrics",
          "Product Design and Metrics - Interview Question 1",
          "Product Design and Metrics - Interview Question 1 - Solution",
          "Product Design and Metrics - Interview Question 2",
          "Product Design and Metrics - Interview Question 2 - Solution",
          "Product Design and Metrics - Interview Question 3",
          "Product Design and Metrics - Interview Question 3 - Solution",
          "Product Design and Metrics - Interview Question 4",
          "Product Design and Metrics - Interview Question 4 - Solution",
          "Product Design and Metrics - Interview Question 5",
          "Product Design and Metrics - Interview Question 5 - Solution"
        ],
        "Working with Data Interview Questions": [
          "Introduction to SQL Questions",
          "Data with SQL - Interview Question 1",
          "Data with SQL - Interview Question 1 -Solution",
          "Data with SQL - Interview Question 2",
          "Data with SQL - Interview Question 2 - Solution",
          "Data with SQL - Interview Question 3",
          "Data with SQL - Interview Question 3 - Solution",
          "Data with SQL - Interview Question 4",
          "Data with SQL - Interview Question 4 - Solution",
          "Data with SQL - Interview Question 5",
          "Data with SQL - Interview Question 5 - Solution"
        ],
        "Machine Learning Interview Questions": [
          "Introduction to Machine Learning Interview Questions",
          "Machine Learning Interview Question 1",
          "Machine Learning Interview Question 1 - Solution",
          "Machine Learning Interview Question 2",
          "Machine Learning Interview Question 2 - Solution",
          "Machine Learning Interview Question 3",
          "Machine Learning Interview Question 3 - Solution",
          "Machine Learning Interview Question 4",
          "Machine Learning Interview Question 4 - Solution",
          "Machine Learning Interview Question 5",
          "Machine Learning Interview Question 5 - Solution",
          "Machine Learning Interview Question 6",
          "Machine Learning Interview Question 6 - Solution",
          "Machine Learning Interview Question 7",
          "Machine Learning Interview Question 7 - Solution",
          "Machine Learning Interview Question 8",
          "Machine Learning Interview Question 8 - Solution",
          "Machine Learning Interview Question 9",
          "Machine Learning Interview Question 9 - Solution",
          "Machine Learning Interview Question 10",
          "Machine Learning Interview Question 10 - Solution"
        ],
        "Design of Experiments Interview Questions": [
          "Introduction to Design of Experiments",
          "Design of Experiments Interview Question 1",
          "Design of Experiments Interview Question 1 - Solution",
          "Design of Experiments Interview Question 2",
          "Design of Experiments Interview Question 2 - Solution",
          "Design of Experiments Interview Question 3",
          "Design of Experiments Interview Question 3 - Solution",
          "Design of Experiments Interview Question 4",
          "Design of Experiments Interview Question 4 - Solution"
        ]
      },
      "requirements": [
        "An understanding of Probability and Statistics",
        "Programming Experience in either Python or R",
        "Experience in SQL",
        "An understanding of Machine Learning Algorithms"
      ],
      "description": "According to Glassdoor, a career as a Data Scientist is the best job in America! With an average base salary of over $120,000, not only do Data Scientists earn fantastic compensation, but they also get to work on some of the world's most interesting problems! Data Scientist positions are also rated as having some of the best work-life balances by Glassdoor. Companies are in dire need of filling out this unique role, and you can use this course to help you rock your Data Scientist Interview!\nThis course is designed to be the ultimate resource for getting a career as a Data Scientist. We'll start off with an general overview of the field and discuss multiple career paths, including Product Analyst, Data Engineering, Data Scientist, and many more. You'll understand the various opportunities available and the best way to pursue each of them. The course touches upon a wide variety of topics, including questions on probability, statistics, machine learning, product metrics, example data sets, A/B testing, market analysis, and much more!\nThe course will be full of real questions sourced from employees working at some of the world's top technology companies, including Amazon, Square, Facebook, Google, Microsoft, AirBnb and more!\n\nThe course contains real questions with fully detailed explanations and solutions. Not only is the course designed for candidates to achieve a full understanding of possible interview questions, but also for recruiters to learn about what to look for in each question response. For questions requiring coded solutions, fully commented code examples will be shown for both Python and R. This way you can focus on understanding the code in a programming language you're already familiar with, instead of worrying about syntax!",
      "target_audience": [
        "Anyone who wants to prepare for a Data Science Interview",
        "Anyone interested in a career in Data Science"
      ]
    },
    {
      "title": "Feature Selection for Machine Learning",
      "url": "https://www.udemy.com/course/feature-selection-for-machine-learning/",
      "bio": "Learn filter, wrapper, and embedded methods, recursive feature elimination, exhaustive search, feature shuffling & more.",
      "objectives": [
        "Learn about filter, embedded and wrapper methods for feature selection",
        "Find out about hybdrid methods for feature selection",
        "Select features with Lasso and decision trees",
        "Implement different methods of feature selection with Python",
        "Learn why less (features) is more",
        "Reduce the feature space in a dataset",
        "Build simpler, faster and more reliable machine learning models",
        "Analyse and understand the selected features",
        "Discover feature selection techniques used in data science competitions"
      ],
      "course_content": {
        "Introduction": [
          "Course Curriculum Overview",
          "Course requirements",
          "Course Aim",
          "Optional: How to approach this course",
          "Course Material",
          "The code | Jupyter notebooks",
          "Presentations covered in this course",
          "Download the data sets",
          "Resources to learn machine learning skills"
        ],
        "Feature Selection": [
          "What is feature selection?",
          "Feature selection methods | Overview",
          "Filter Methods",
          "Wrapper methods",
          "Embedded Methods",
          "Moving Forward",
          "Open-source packages for feature selection"
        ],
        "Filter Methods | Basics": [
          "Constant, quasi constant, and duplicated features – Intro",
          "Constant features",
          "Quasi-constant features",
          "Duplicated features",
          "Install Feature-engine",
          "Drop constant and quasi-constant with Feature-engine",
          "Drop duplicates with Feature-engine"
        ],
        "Filter methods | Correlation": [
          "Correlation - Intro",
          "Correlation Feature Selection",
          "Correlation procedures to select features",
          "Correlation | Notebook demo",
          "Basic methods plus Correlation pipeline",
          "Correlation with Feature-engine",
          "Feature Selection Pipeline with Feature-engine",
          "Additional reading resources",
          "? Bonus! An eye-opening movie experience! ?"
        ],
        "Filter methods | Statistical measures": [
          "Statistical methods – Intro",
          "Mutual information",
          "Mutual information demo",
          "Chi-square test",
          "Chi-square | Demo",
          "Chi-square considerations",
          "Chi2 - calculating the expected frequencies (Optional)",
          "Chi-square quiz",
          "Anova",
          "Anova | Demo",
          "Select features based of p-values",
          "Do you want to learn more about stats?",
          "Basic methods + Correlation + Filter with stats pipeline"
        ],
        "Filter Methods | Other methods and metrics": [
          "Filter Methods with other metrics",
          "Univariate model performance metrics",
          "Univariate model performance metrics | Demo",
          "KDD 2009: Select features by target mean encoding",
          "KDD 2009: Select features by mean encoding | Demo",
          "Univariate model performance with Feature-engine",
          "Target Mean Encoding Selection with Feature-engine",
          "? Unveiling the Dark Side of Algorithms: A Captivating Book Recommendation!"
        ],
        "Wrapper methods": [
          "Wrapper methods – Intro",
          "MLXtend",
          "Step forward feature selection",
          "SFS - MLXtend vs Sklearn",
          "Step forward feature selection | MLXtend",
          "Step forward feature selection | sklearn",
          "Step backward feature selection",
          "Step backward feature selection | MLXtend",
          "Step backward feature selection | Sklearn",
          "Exhaustive search",
          "Exhaustive search | Demo"
        ],
        "Embedded methods | Linear models": [
          "Regression Coefficients – Intro",
          "Selection by Logistic Regression Coefficients",
          "Selection by Linear Regression Coefficients",
          "Coefficients change with penalty",
          "Basic methods + Correlation + Embedded method using coefficients"
        ],
        "Embedded methods – Lasso regularisation": [
          "Regularisation – Intro",
          "Lasso",
          "A note on SelectFromModel",
          "Basic filter methods + LASSO pipeline"
        ],
        "Embedded methods | Trees": [
          "Feature Selection by Tree importance | Intro",
          "Feature Selection by Tree importance | Demo",
          "Feature Selection by Tree importance | Recursively",
          "Feature selection with decision trees | review"
        ]
      },
      "requirements": [
        "A Python installation",
        "Jupyter notebook installation",
        "Python coding skills",
        "Some experience with Numpy and Pandas",
        "Familiarity with Machine Learning algorithms",
        "Familiarity with scikit-learn"
      ],
      "description": "Welcome to Feature Selection for Machine Learning, the most comprehensive course on feature selection available online.\nIn this course, you will learn how to select the variables in your data set and build simpler, faster, more reliable and more interpretable machine learning models.\n\n\nWho is this course for?\nYou’ve given your first steps into data science, you know the most commonly used machine learning models, you probably built a few linear regression or decision tree based models. You are familiar with data pre-processing techniques like removing missing data, transforming variables, encoding categorical variables. At this stage you’ve probably realized that many data sets contain an enormous amount of features, and some of them are identical or very similar, some of them are not predictive at all, and for some others it is harder to say.\nYou wonder how you can go about to find the most predictive features. Which ones are OK to keep and which ones could you do without? You also wonder how to code the methods in a professional manner. Probably you did your online search and found out that there is not much around there about feature selection. So you start to wonder: how are things really done in tech companies?\nThis course will help you! This is the most comprehensive online course in variable selection. You will learn a huge variety of feature selection procedures used worldwide in different organizations and in data science competitions, to select the most predictive features.\n\n\nWhat will you learn?\nI have put together a fantastic collection of feature selection techniques, based on scientific articles, data science competitions and of course my own experience as a data scientist.\nSpecifically, you will learn:\nHow to remove features with low variance\nHow to identify redundant features\nHow to select features based on statistical tests\nHow to select features based on changes in model performance\nHow to find predictive features based on importance attributed by models\nHow to code procedures elegantly and in a professional manner\nHow to leverage the power of existing Python libraries for feature selection\n\n\nThroughout the course, you are going to learn multiple techniques for each of the mentioned tasks, and you will learn to implement these techniques in an elegant, efficient, and professional manner, using Python, Scikit-learn, pandas and mlxtend.\n\n\nAt the end of the course, you will have a variety of tools to select and compare different feature subsets and identify the ones that returns the simplest, yet most predictive machine learning model. This will allow you to minimize the time to put your predictive models into production.\n\n\nThis comprehensive feature selection course includes about 70 lectures spanning ~8 hours of video, and ALL topics include hands-on Python code examples which you can use for reference and for practice, and re-use in your own projects.\n\n\nIn addition, I update the course regularly, to keep up with the Python libraries new releases and include new techniques when they appear.\nSo what are you waiting for? Enroll today, embrace the power of feature selection and build simpler, faster and more reliable machine learning models.",
      "target_audience": [
        "Beginner Data Scientists who want to understand how to select variables for machine learning",
        "Intermediate Data Scientists who want to level up their experience in feature selection for machine learning",
        "Advanced Data Scientists who want to discover alternative methods for feature selection",
        "Software engineers and academics switching careers into data science",
        "Software engineers and academics stepping into data science",
        "Data analysts who want to level up their skills in data science"
      ]
    },
    {
      "title": "Complete Linear Regression Analysis in Python",
      "url": "https://www.udemy.com/course/machine-learning-basics-building-regression-model-in-python/",
      "bio": "Linear Regression in Python| Simple Regression, Multiple Regression, Ridge Regression, Lasso and subset selection also",
      "objectives": [
        "Learn how to solve real life problem using the Linear Regression technique",
        "Preliminary analysis of data using Univariate and Bivariate analysis before running Linear regression",
        "Predict future outcomes basis past data by implementing Simplest Machine Learning algorithm",
        "Understand how to interpret the result of Linear Regression model and translate them into actionable insight",
        "Understanding of basics of statistics and concepts of Machine Learning",
        "Indepth knowledge of data collection and data preprocessing for Machine Learning Linear Regression problem",
        "Learn advanced variations of OLS method of Linear Regression",
        "Course contains a end-to-end DIY project to implement your learnings from the lectures",
        "How to convert business problem into a Machine learning Linear Regression problem",
        "Basic statistics using Numpy library in Python",
        "Data representation using Seaborn library in Python",
        "Linear Regression technique of Machine Learning using Scikit Learn and Statsmodel libraries of Python"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course!",
          "Course contents",
          "Course Resources"
        ],
        "Setting up Python": [
          "Installing Python and Anaconda",
          "This is a Milestone!",
          "Opening Jupyter Notebook",
          "Introduction to Jupyter Notebook - Part 1",
          "Introduction to Jupyter Notebook - Part 2"
        ],
        "Python crash course - Working with different data types": [
          "Arithmetic operators in Python",
          "Quick coding exercise on arithmetic operators",
          "Strings in Python - Part 1",
          "Strings in Python - Part 2",
          "Quick coding exercise on String operations",
          "Lists - Part 1",
          "Lists - Part 2",
          "Tuples and Directories",
          "Quick coding exercise on Tuples",
          "Quiz"
        ],
        "Important Python Libraries": [
          "Working with Numpy Library of Python",
          "Quick coding exercise on NumPy Library",
          "Working with Pandas Library of Python",
          "Quick coding exercise on Pandas Library",
          "Working with Seaborn Library of Python",
          "Quiz",
          "Python file for additional practice"
        ],
        "Integrating ChatGPT with Python": [
          "Integrating ChatGPT with Jupyter notebook"
        ],
        "Basics of Statistics": [
          "Types of Data",
          "Types of Statistics",
          "Describing data Graphically",
          "Measures of Centers",
          "Practice Exercise 1",
          "Measures of Dispersion",
          "Practice Exercise 2",
          "Quiz"
        ],
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning",
          "Building a Machine Learning Model",
          "Introduction to Machine learning quiz"
        ],
        "Data Preprocessing": [
          "Gathering Business Knowledge",
          "Data Exploration",
          "The Dataset and the Data Dictionary",
          "Importing Data in Python",
          "Project exercise 1",
          "Univariate analysis and EDD",
          "EDD in Python",
          "Project Exercise 2",
          "What is outlier treatment?",
          "Outlier Treatment in Python",
          "Project Exercise 3",
          "Missing Value Imputation",
          "Missing Value Imputation in Python",
          "Project Exercise 4",
          "Seasonality in Data",
          "Bi-variate analysis and Variable transformation",
          "Variable transformation and deletion in Python",
          "Project Exercise 5",
          "Non-usable variables",
          "Handling qualitative data by using dummy variables",
          "Dummy variable creation in Python",
          "Project Exercise 6",
          "Correlation Analysis",
          "Correlation Analysis in Python",
          "Project Exercise 7",
          "Quiz"
        ],
        "Linear Regression": [
          "The Problem Statement",
          "Basic Equations and Ordinary Least Squares (OLS) method",
          "Assessing accuracy of predicted coefficients",
          "Assessing Model Accuracy: RSE and R squared",
          "Simple Linear Regression in Python",
          "Project Exercise 8",
          "Multiple Linear Regression",
          "The F - statistic",
          "Quiz",
          "Interpreting results of Categorical variables",
          "Multiple Linear Regression in Python",
          "Quiz",
          "Project Exercise 9",
          "Test-train split",
          "Bias Variance trade-off",
          "More about test-train split",
          "Test train split in Python",
          "Quiz",
          "Linear models other than OLS",
          "Subset selection techniques",
          "Shrinkage methods: Ridge and Lasso",
          "Ridge regression and Lasso in Python",
          "Heteroscedasticity",
          "Project Exercise 10",
          "Final Project Exercise",
          "Quiz",
          "Practical Task",
          "Quiz",
          "Comprehensive Interview Preparation Questions"
        ],
        "Congratulations & About your certificate": [
          "The final milestone!",
          "About your certificate",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Linear Regression course that teaches you everything you need to create a Linear Regression model in Python, right?\nYou've found the right Linear Regression course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using linear regression technique of Machine Learning.\nCreate a linear regression model in Python and analyze its result.\nConfidently practice, discuss and understand Machine Learning concepts\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nHow this course will help you?\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you the most popular technique of machine learning, which is Linear Regression\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear regression.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Linear Regression model, which is the most popular Machine Learning model, to solve business problems.\nBelow are the course contents of this course on Linear Regression:\nSection 1 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics\nthen graphical representations to describe the data and then a lecture on measures of center like mean\nmedian and mode and lastly measures of dispersion like range and standard deviation\nSection 2 - Python basic\nThis section gets you started with Python.\nThis section will help you set up the python and Jupyter environment on your system and it'll teach\nyou how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nSection 3 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\nSection 4 - Data Preprocessing\nIn this section you will learn what actions you need to take a step by step to get the data and then prepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment, missing value imputation, variable transformation and correlation.\nSection 5 - Regression Model\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you understand where the concept is coming from and how it is important. But even if you don't understand it,  it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results, what are other variations to the ordinary least squared method and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a regression model in Python will soar. You'll have a thorough understanding of how to use regression modelling to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhat is the Linear regression technique of Machine learning?\nLinear Regression is a simple machine learning model for regression problems, i.e., when the target variable is a real value.\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\nWhen there is a single input variable (x), the method is referred to as simple linear regression.\nWhen there are multiple input variables, the method is known as multiple linear regression.\nWhy learn Linear regression technique of Machine learning?\nThere are four reasons to learn Linear regression technique of Machine learning:\n1. Linear Regression is the most popular machine learning technique\n2. Linear Regression has fairly good prediction accuracy\n3. Linear Regression is simple to implement and easy to interpret\n4. It gives you a firm base to start learning other advanced techniques of Machine Learning\nHow much time does it take to learn Linear regression technique of machine learning?\nLinear Regression is easy but no one can determine the learning time it takes. It totally depends on you. The method we adopted to help you learn Linear regression starts from the basics and takes you to advanced level within hours. You can follow the same, but remember you can learn nothing without practicing it. Practice is the only way to remember whatever you have learnt. Therefore, we have also provided you with another data set to work on as a separate project of Linear regression.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 4 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of Linear Regression modelling - Having a good knowledge of Linear Regression gives you a solid understanding of how machine learning works. Even though Linear regression is the simplest technique of Machine learning, it is still the most popular one with fairly good prediction ability. Fifth and sixth section cover Linear regression topic end-to-end and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.\nWhy use Python for data Machine Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Machine Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nMachine Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master Linear Regression from beginner to Advanced in short span of time"
      ]
    },
    {
      "title": "Deep learning for object detection using Tensorflow 2",
      "url": "https://www.udemy.com/course/deep-learning-for-object-detection-using-tensorflow-2/",
      "bio": "Understand, train and evaluate Faster RCNN, SSD and YOLO v3 models using Tensorflow 2 and Google AI Platform",
      "objectives": [
        "You will learn how Faster RCNN deep neural network works",
        "You will learn how SSD deep neural network works",
        "You will learn how YOLO deep neural network works",
        "You will learn how to use Tensorflow 2 object detection API",
        "You will learn how to train and evaluate deep neural networks for object detection such as Faster RCNN, SSD and YOLOv3 using your own custom data",
        "You will learn how to \"freeze\" your model to get a final model that is ready for production",
        "You will learn how to use your \"frozen\" model to make predictions on a set of new images using openCV and Tensorflow 2",
        "You will learn how to use Google Cloud AI platform in order to train your object detection models on powerful cloud GPUs",
        "You will learn how to use Tensorboard to visualize the development of the loss function and the mean average precision of your model",
        "You will learn how to change different parameters in order to improve your model's performance"
      ],
      "course_content": {
        "Object detection as a concept in computer vision": [
          "Introduction and course content",
          "What is object detection for computer vision?",
          "Object detection can be for multiple objects in the image",
          "Why deep learning for object detection?",
          "The 2 categories of neural networks used for object detection",
          "High level overview of Faster R CNN",
          "High level overview of SSD",
          "High level overview of YOLO v3"
        ],
        "How to choose the right neural network for your object detection task": [
          "Introduction",
          "Speed and accuracy tradeoffs in object detection (reference article)",
          "Accuracy VS inference time",
          "Object detector accuracy with respect to feature extractor accuracy",
          "Accuracy stratified by object size",
          "How the image resolution affects your object detection model",
          "How YOLO compares to SSD and Faster R-CNN",
          "Summary on how to choose the right model for your object detection task"
        ],
        "Software setup": [
          "Linux installation : How to install tensorflow with GPU support (part 1)",
          "Linux installation : How to install tensorflow with GPU support (part 2)",
          "Linux installation : How to install Tensorflow 2 object detection API",
          "Windows installation : Installing miniconda",
          "Windows installation : Create virtual environment",
          "Windows installation : Installing tensorflow 2 object detection API",
          "Windows installation : Installing tensorflow with GPU support"
        ],
        "Data for object detection": [
          "Data preparation for object detection",
          "The dataset that you will use to build your object detection model",
          "Downloading and setting up our annotation tool : labelImg",
          "Annotating your dataset",
          "Transforming our xml files into one csv file",
          "Creating a labelmap file for our dataset",
          "The tool that we will use to generate tfrecords",
          "Generating tfrecords"
        ],
        "Training an object detection model on your local machine": [
          "Overview of the steps needed to build an object detector",
          "Transfer learning",
          "Downloading the pretrained model and getting its corresponding config file",
          "Preparing your config file",
          "Running the training and testing for experimentation",
          "Running Tensorboard to analyse the development of the loss and average precision",
          "Settings for training and evaluating a Faster RCNN model on your local machine",
          "Training and evaluating an SSD based object detection model",
          "Solution : Training and evaluating an SSD based object detection model",
          "Running the training for SSD object detector",
          "Other important settings in your config file : anchor boxes",
          "Other important settings in your config file : data augmentation"
        ],
        "Training object detection API models using Google Cloud AI Platform": [
          "What is cloud computing and what is AI-Platform? (optional)",
          "Creating a Google Cloud account and creating your first project",
          "Downloading Google Cloud SDK",
          "Creating a google bucket and uploading data to it",
          "Preparing our config file for training on google cloud",
          "Running the training using Faster RCNN model",
          "Running the evaluation during the training",
          "Analyzing the results after the training of Faster RCNN model is finished",
          "Possible things to do to improve our model performance",
          "Downloading the trained model and exporting the frozen model from checkpoints",
          "Running the frozen model on new examples locally",
          "Run a training and evaluation using SSD model",
          "Solution : Run a training and evaluation using SSD model",
          "Analyzing the results after the training of SSD model is finished"
        ],
        "YOLO v3 for object detection": [
          "The new setup for training YOLOv3 model",
          "Installing the necessary requirements to run the code",
          "How will the dataset change to account for YOLO neural network",
          "Preparing the dataset : problem with our current dataset",
          "Preparing the dataset : getting the data from the original source",
          "Preparing the dataset : transforming our previous dataset to the right format",
          "Preparing the dataset : adding classes names",
          "Preparing the dataset : exploring and changing the config file",
          "Training YOLO v3 based object detection model",
          "Analyzing the results of the training",
          "Evaluating our YOLOv3 trained model using new images",
          "Computing the mean average precision of our trained model",
          "Other quantitative results",
          "Saving our model as a SavedModel format for production deployment",
          "Using our trained model to make predictions on new images",
          "Steps needed to run the training on google AI platform",
          "Bonus materials"
        ]
      },
      "requirements": [
        "You need to have a basic level of Python (if you know what classes and functions are then you are good to go!)",
        "You need to have a basic understanding of what Tensorflow is.",
        "You don't need any prior understanding of what object detection is, this is the mission of the course!"
      ],
      "description": "This course is designed to make you proficient in training and evaluating deep learning based object detection models. Specifically, you will learn about Faster R-CNN, SSD and YOLO models.\nFor each of these models, you will first learn about how they function from a high level perspective. This will help you build the intuition about how they work.\nAfter this, you will learn how to leverage the power of Tensorflow 2 to train and evaluate these models on your local machine.\nFinally, you will learn how to leverage the power of cloud computing to improve your training process. For this last part, you will learn how to use Google Cloud AI Platform in order to train and evaluate your models on powerful GPUs offered by google.\nI designed this course to help you become proficient in training and evaluating object detection models. This is done by helping you in several different ways, including :\nBuilding the necessary intuition that will help you answer most questions about object detection using deep learning, which is a very common topic in interviews for positions in the fields of computer vision and deep learning.\nBy teaching you how to create your own models using your own custom dataset. This will enable you to create some powerful AI solutions.\nBy teaching you how to leverage the power of Google Cloud AI Platform in order to push your model's performance by having access to powerful GPUs.",
      "target_audience": [
        "AI enthusiasts",
        "Data scientists",
        "Computer vision and machine learning students",
        "software developers",
        "Entrepreneurs"
      ]
    },
    {
      "title": "Data Engineering using AWS Data Analytics",
      "url": "https://www.udemy.com/course/data-engineering-using-aws-analytics-services/",
      "bio": "Build Data Engineering Pipelines on AWS using Data Analytics Services - Glue, EMR, Athena, Kinesis, Lambda, Redshift",
      "objectives": [
        "Data Engineering leveraging Services under AWS Data Analytics",
        "AWS Essentials such as s3, IAM, EC2, etc",
        "Understanding AWS s3 for cloud based storage",
        "Understanding details related to virtual machines on AWS known as EC2",
        "Managing AWS IAM users, groups, roles and policies for RBAC (Role Based Access Control)",
        "Managing Tables using AWS Glue Catalog",
        "Engineering Batch Data Pipelines using AWS Glue Jobs",
        "Orchestrating Batch Data Pipelines using AWS Glue Workflows",
        "Running Queries using AWS Athena - Server less query engine service",
        "Using AWS Elastic Map Reduce (EMR) Clusters for building Data Pipelines",
        "Using AWS Elastic Map Reduce (EMR) Clusters for reports and dashboards",
        "Data Ingestion using AWS Lambda Functions",
        "Scheduling using AWS Events Bridge",
        "Engineering Streaming Pipelines using AWS Kinesis",
        "Streaming Web Server logs using AWS Kinesis Firehose",
        "Overview of data processing using AWS Athena",
        "Running AWS Athena queries or commands using CLI",
        "Running AWS Athena queries using Python boto3",
        "Creating AWS Redshift Cluster, Create tables and perform CRUD Operations",
        "Copy data from s3 to AWS Redshift Tables",
        "Understanding Distribution Styles and creating tables using Distkeys",
        "Running queries on external RDBMS Tables using AWS Redshift Federated Queries",
        "Running queries on Glue or Athena Catalog tables using AWS Redshift Spectrum"
      ],
      "course_content": {
        "Introduction to the course": [
          "Introduction to Data Engineering using AWS Analytics Services",
          "Video Lectures and Reference Material",
          "Taking the Udemy Course for new Udemy Users",
          "Additional Costs for AWS Infrastructure for Hands-on Practice",
          "Signup for AWS Account",
          "Logging in into AWS Account",
          "Overview of AWS Billing Dashboard - Cost Explorer and Budgets"
        ],
        "Setup Local Development Environment for AWS on Windows 10 or Windows 11": [
          "Setup Local Environment on Windows for AWS",
          "Overview of Powershell on Windows 10 or Windows 11",
          "Setup Ubuntu VM on Windows 10 or 11 using wsl",
          "Setup Ubuntu VM on Windows 10 or 11 using wsl - Contd...",
          "Setup Python venv and pip on Ubuntu",
          "Setup AWS CLI on Windows and Ubuntu using Pip",
          "Create AWS IAM User and Download Credentials",
          "Configure AWS CLI on Windows",
          "Create Python Virtual Environment for AWS Projects",
          "Setup Boto3 as part of Python Virtual Environment",
          "Setup Jupyter Lab and Validate boto3"
        ],
        "Setup Local Development Environment for AWS on Mac": [
          "Setup Local Environment for AWS on Mac",
          "Setup AWS CLI on Mac",
          "Setup AWS IAM User to configure AWS CLI",
          "Configure AWS CLI using IAM User Credentials",
          "Setup Python Virtual Environment on Mac using Python 3",
          "Setup Boto3 as part of Python Virtual Environment",
          "Setup Jupyter Lab and Validate boto3"
        ],
        "Setup Environment for Practice using Cloud9": [
          "Introduction to Cloud9",
          "Setup Cloud9",
          "Overview of Cloud9 IDE",
          "Docker and AWS CLI on Cloud9",
          "Cloud9 and EC2",
          "Accessing Web Applications",
          "Allocate and Assign Static IP",
          "Changing Permissions using IAM Policies",
          "Increasing Size of EBS Volume",
          "Opening ports for Cloud9 Instance",
          "Setup Jupyter lab on Cloud9 Instance",
          "Open SSH Port for Cloud9 EC2 Instance",
          "Connect to Cloud9 EC2 Instance using SSH"
        ],
        "AWS Getting Started with s3, IAM and CLI": [
          "Introduction - AWS Getting Started",
          "[Instructions] Introduction - AWS Getting Started",
          "Create AWS s3 Bucket using AWS Web Console",
          "[Instructions] Create s3 Bucket",
          "Create AWS IAM Group and User using AWS Web Console",
          "[Instructions] Create IAM Group and User",
          "Overview of AWS IAM Roles to grant permissions between AWS Services",
          "[Instructions] Overview of Roles",
          "Create and Attach AWS IAM Custom Policy using AWS Web Console",
          "[Instructions and Code] Create and Attach Custom Policy",
          "Configure and Validate AWS Command Line Interface to run AWS Commands",
          "[Instructions and Code] Configure and Validate AWS CLI"
        ],
        "Storage -Deep Dive into AWS Simple Storage Service aka s3": [
          "Getting Started with AWS Simple Storage aka S3",
          "[Instructions] Getting Started with AWS S3",
          "Setup Data Set locally to upload into AWS s3",
          "[Instructions] Setup Data Set locally to upload into AWS s3",
          "Adding AWS S3 Buckets and Objects using AWS Web Console",
          "[Instruction] Adding AWS s3 Buckets and Objects",
          "Version Control of AWS S3 Objects or Files",
          "[Instructions] Version Control in AWS S3",
          "AWS S3 Cross-Region Replication for fault tolerance",
          "[Instructions] AWS S3 Cross-Region Replication for fault tolerance",
          "Overview of AWS S3 Storage Classes or Storage Tiers",
          "[Instructions] Overview of AWS S3 Storage Classes or Storage Tiers",
          "Overview of Glacier in AWS s3",
          "[Instructions] Overview of Glacier in AWS s3",
          "Managing AWS S3 buckets and objects using AWS CLI",
          "[Instructions and Commands] Managing AWS S3 buckets and objects using AWS CLI",
          "Managing Objects in AWS S3 using AWS CLI - Lab",
          "[Instructions] Managing Objects in AWS S3 using AWS CLI - Lab"
        ],
        "AWS Security using IAM - Managing AWS Users, Roles and Policies using AWS IAM": [
          "Creating AWS IAM Users with Programmatic and Web Console Access",
          "[Instructions] Creating IAM Users",
          "Logging into AWS Management Console using AWS IAM User",
          "[Instructions] Logging into AWS Management Console using IAM User",
          "Validate Programmatic Access to AWS IAM User via AWS CLI",
          "[Instructions and Commands] Validate Programmatic Access to IAM User",
          "Getting Started with AWS IAM Identity-based Policies",
          "[Instructions and Commands] IAM Identity-based Policies",
          "Managing AWS IAM User Groups",
          "[Instructions and Commands] Managing IAM Groups",
          "Managing AWS IAM Roles for Service Level Access",
          "[Instructions and Commands] Managing IAM Roles",
          "Overview of AWS Custom Policies to grant permissions to Users, Groups, and Roles",
          "[Instructions and Commands] Overview of Custom Policies",
          "Managing AWS IAM Groups, Users, and Roles using AWS CLI",
          "[Instructions and Commands] Managing IAM using AWS CLI"
        ],
        "Infrastructure - Getting Started with AWS Elastic Cloud Compute aka EC2": [
          "Getting Started with AWS Elastic Cloud Compute aka EC2",
          "[Instructions] Getting Started with EC2",
          "Create AWS EC2 Key Pair for SSH Access",
          "[Instructions] Create EC2 Key Pair",
          "Launch AWS EC2 Instance or Virtual Machine",
          "[Instructions] Launch EC2 Instance",
          "Connecting to AWS EC2 Instance or Virtual Machine using SSH",
          "[Instructions and Commands] Connecting to EC2 Instance",
          "Overview of AWS Security Groups for firewall security of AWS EC2 Instance",
          "[Instructions and Commands] Security Groups Basics",
          "Overview of Public and Private IP Addresses of AWS EC2 Instance",
          "[Instructions] Public and Private IP Addresses",
          "Understanding AWS EC2 Instance or Virtual Machine Life Cycle",
          "[Instructions] EC2 Life Cycle",
          "Allocating and Assigning AWS Elastic IP or Static IP address to AWS EC2 Instance",
          "[Instructions] Allocating and Assigning Elastic IP Addresses",
          "Managing AWS EC2 Instances or Virtual Machines Using AWS CLI",
          "[Instructions and Commands] Managing EC2 Using AWS CLI",
          "Upgrade or Downgrade of AWS EC2 Instances or Virtual Machines",
          "[Instructions and Commands] Upgrade or Downgrade EC2 Instances"
        ],
        "Infrastructure - AWS EC2 Advanced": [
          "Understanding AWS EC2 Instance or Virtual Machine Metadata",
          "[Instructions and Commands] Understanding EC2 Metadata",
          "Querying on AWS EC2 Instance or Virtual Machine Metadata",
          "[Instructions and Commands] Querying on EC2 Metadata",
          "Fitering on AWS EC2 Instance or Virtual Machine Metadata",
          "[Instructions and Commands] Filtering on EC2 Metadata",
          "Using Bootstrapping Scripts on AWS EC2 Instance or Virtual Machine",
          "[Instructions and Commands] Using Bootstrapping Scripts",
          "Create an Amazon Machine Image aka AMI using AWS EC2 Instance",
          "[Instructions and Commands] Create an AMI",
          "Validate Amazon Machine Image aka AMI - Lab",
          "[Instructions and Commands] Validate AMI - Lab"
        ],
        "Data Ingestion using Lambda Functions": [
          "Hello World using AWS Lambda",
          "[Instructions] Hello World using AWS Lambda",
          "Setup Project for local development",
          "[Instructions and Code] Setup Project for local development",
          "Deploy Project to AWS Lambda console",
          "[Instructions and Code] Deploy Project to AWS Lambda console",
          "Develop download functionality using requests",
          "[Instructions and Code] Develop download functionality using requests",
          "Using 3rd party libraries in AWS Lambda",
          "[Instructions and Code] Using 3rd party libraries in AWS Lambda",
          "Validating s3 access for local development",
          "[Instructions and Code] Validating s3 access for local development",
          "Develop upload functionality to s3",
          "[Instructions and Code] Develop upload functionality to s3",
          "Validating using AWS Lambda Console",
          "[Instructions and Code] Validating using AWS Lambda Console",
          "Run using AWS Lambda Console",
          "[Instructions] Run using AWS Lambda Console",
          "Validating files incrementally",
          "[Instructions and Code] Validating files incrementally",
          "Reading and Writing Bookmark using s3",
          "[Instructions and Code] Reading and Writing Bookmark using s3",
          "Maintaining Bookmark using s3",
          "[Instructions and Code] Maintaining Bookmark using s3",
          "Review the incremental upload logic",
          "Deploying lambda function",
          "[Instructions and Source Code] - ghactivity-downloader Lambda Function",
          "Schedule Lambda Function using AWS Event Bridge",
          "[Instructions] Schedule Lambda Function using AWS Event Bridge"
        ]
      },
      "requirements": [
        "A Computer with at least 8 GB RAM",
        "Programming Experience using Python is highly desired as some of the topics are demonstrated using Python",
        "SQL Experience is highly desired as some of the topics are demonstrated using SQL",
        "Nice to have Data Engineering Experience using Pandas or Pyspark",
        "This course is ideal for experienced data engineers to add AWS Analytics Services as key skills to their profile"
      ],
      "description": "Data Engineering is all about building Data Pipelines to get data from multiple sources into Data Lakes or Data Warehouses and then from Data Lakes or Data Warehouses to downstream systems. As part of this course, I will walk you through how to build Data Engineering Pipelines using AWS Data Analytics Stack. It includes services such as Glue, Elastic Map Reduce (EMR), Lambda Functions, Athena, EMR, Kinesis, and many more.\nHere are the high-level steps which you will follow as part of the course.\nSetup Development Environment\nGetting Started with AWS\nStorage - All about AWS s3 (Simple Storage Service)\nUser Level Security - Managing Users, Roles, and Policies using IAM\nInfrastructure - AWS EC2 (Elastic Cloud Compute)\nData Ingestion using AWS Lambda Functions\nOverview of AWS Glue Components\nSetup Spark History Server for AWS Glue Jobs\nDeep Dive into AWS Glue Catalog\nExploring AWS Glue Job APIs\nAWS Glue Job Bookmarks\nDevelopment Life Cycle of Pyspark\nGetting Started with AWS EMR\nDeploying Spark Applications using AWS EMR\nStreaming Pipeline using AWS Kinesis\nConsuming Data from AWS s3 using boto3 ingested using AWS Kinesis\nPopulating GitHub Data to AWS Dynamodb\nOverview of Amazon AWS Athena\nAmazon AWS Athena using AWS CLI\nAmazon AWS Athena using Python boto3\nGetting Started with Amazon AWS Redshift\nCopy Data from AWS s3 into AWS Redshift Tables\nDevelop Applications using AWS Redshift Cluster\nAWS Redshift Tables with Distkeys and Sortkeys\nAWS Redshift Federated Queries and Spectrum\nHere are the details about what you will be learning as part of this course. We will cover most of the commonly used services with hands-on practice which are available under AWS Data Analytics.\nGetting Started with AWS\nAs part of this section, you will be going through the details related to getting started with AWS.\nIntroduction - AWS Getting Started\nCreate s3 Bucket\nCreate AWS IAM Group and AWS IAM User to have required access on s3 Bucket and other services\nOverview of AWS IAM Roles\nCreate and Attach Custom AWS IAM Policy to both AWS IAM Groups as well as Users\nConfigure and Validate AWS CLI to access AWS Services using AWS CLI Commands\nStorage - All about AWS s3 (Simple Storage Service)\nAWS s3 is one of the most prominent fully managed AWS services. All IT professionals who would like to work on AWS should be familiar with it. We will get into quite a few common features related to AWS s3 in this section.\nGetting Started with AWS S3\nSetup Data Set locally to upload to AWS s3\nAdding AWS S3 Buckets and Managing Objects (files and folders) in AWS s3 buckets\nVersion Control for AWS S3 Buckets\nCross-Region Replication for AWS S3 Buckets\nOverview of AWS S3 Storage Classes\nOverview of AWS S3 Glacier\nManaging AWS S3 using AWS CLI Commands\nManaging Objects in AWS S3 using CLI - Lab\nUser Level Security - Managing Users, Roles, and Policies using IAM\nOnce you start working on AWS, you need to understand the permissions you have as a non-admin user. As part of this section, you will understand the details related to AWS IAM users, groups, roles as well as policies.\nCreating AWS IAM Users\nLogging into AWS Management Console using AWS IAM User\nValidate Programmatic Access to AWS IAM User\nAWS IAM Identity-based Policies\nManaging AWS IAM Groups\nManaging AWS IAM Roles\nOverview of Custom AWS IAM Policies\nManaging AWS IAM users, groups, roles as well as policies using AWS CLI Commands\nInfrastructure - AWS EC2 (Elastic Cloud Compute) Basics\nAWS EC2 Instances are nothing but virtual machines on AWS. As part of this section, we will go through some of the basics related to AWS EC2 Basics.\nGetting Started with AWS EC2\nCreate AWS EC2 Key Pair\nLaunch AWS EC2 Instance\nConnecting to AWS EC2 Instance\nAWS EC2 Security Groups Basics\nAWS EC2 Public and Private IP Addresses\nAWS EC2 Life Cycle\nAllocating and Assigning AWS Elastic IP Address\nManaging AWS EC2 Using AWS CLI\nUpgrade or Downgrade AWS EC2 Instances\nInfrastructure - AWS EC2 Advanced\nIn this section, we will continue with AWS EC2 to understand how we can manage EC2 instances using AWS Commands and also how to install additional OS modules leveraging bootstrap scripts.\nGetting Started with AWS EC2\nUnderstanding AWS EC2 Metadata\nQuerying on AWS EC2 Metadata\nFitering on AWS EC2 Metadata\nUsing Bootstrapping Scripts with AWS EC2 Instances to install additional softwares on AWS EC2 instances\nCreate an AWS AMI using AWS EC2 Instances\nValidate AWS AMI - Lab\nData Ingestion using Lambda Functions\nAWS Lambda functions are nothing but serverless functions. In this section, we will understand how we can develop and deploy Lambda functions using Python as a programming language. We will also see how to maintain a bookmark or checkpoint using s3.\nHello World using AWS Lambda\nSetup Project for local development of AWS Lambda Functions\nDeploy Project to AWS Lambda console\nDevelop download functionality using requests for AWS Lambda Functions\nUsing 3rd party libraries in AWS Lambda Functions\nValidating AWS s3 access for local development of AWS Lambda Functions\nDevelop upload functionality to s3 using AWS Lambda Functions\nValidating AWS Lambda Functions using AWS Lambda Console\nRun AWS Lambda Functions using AWS Lambda Console\nValidating files incrementally downloaded using AWS Lambda Functions\nReading and Writing Bookmark to s3 using AWS Lambda Functions\nMaintaining Bookmark on s3 using AWS Lambda Functions\nReview the incremental upload logic developed using AWS Lambda Functions\nDeploying AWS Lambda Functions\nSchedule AWS Lambda Functions using AWS Event Bridge\nOverview of AWS Glue Components\nIn this section, we will get a broad overview of all important Glue Components such as Glue Crawler, Glue Databases, Glue Tables, etc. We will also understand how to validate Glue tables using AWS Athena. AWS Glue (especially Glue Catalog) is one of the key components in the realm of AWS Data Analytics Services.\nIntroduction - Overview of AWS Glue Components\nCreate AWS Glue Crawler and AWS Glue Catalog Database as well as Table\nAnalyze Data using AWS Athena\nCreating AWS S3 Bucket and Role to create AWS Glue Catalog Tables using Crawler on the s3 location\nCreate and Run the AWS Glue Job to process data in AWS Glue Catalog Tables\nValidate using AWS Glue Catalog Table and by running queries using AWS Athena\nCreate and Run AWS Glue Trigger\nCreate AWS Glue Workflow\nRun AWS Glue Workflow and Validate\nSetup Spark History Server for AWS Glue Jobs\nAWS Glue uses Apache Spark under the hood to process the data. It is important we setup Spark History Server for AWS Glue Jobs to troubleshoot any issues.\nIntroduction - Spark History Server for AWS Glue\nSetup Spark History Server on AWS\nClone AWS Glue Samples repository\nBuild AWS Glue Spark UI Container\nUpdate AWS IAM Policy Permissions\nStart AWS Glue Spark UI Container\nDeep Dive into AWS Glue Catalog\nAWS Glue has several components, but the most important ones are nothing but AWS Glue Crawlers, Databases as well as Catalog Tables. In this section, we will go through some of the most important and commonly used features of the AWS Glue Catalog.\nPrerequisites for AWS Glue Catalog Tables\nSteps for Creating AWS Glue Catalog Tables\nDownload Data Set to use to create AWS Glue Catalog Tables\nUpload data to s3 to crawl using AWS Glue Crawler to create required AWS Glue Catalog Tables\nCreate AWS Glue Catalog Database - itvghlandingdb\nCreate AWS Glue Catalog Table - ghactivity\nRunning Queries using AWS Athena - ghactivity\nCrawling Multiple Folders using AWS Glue Crawlers\nManaging AWS Glue Catalog using AWS CLI\nManaging AWS Glue Catalog using Python Boto3\nExploring AWS Glue Job APIs\nOnce we deploy AWS Glue jobs, we can manage them using AWS Glue Job APIs. In this section we will get overview of AWS Glue Job APIs to run and manage the jobs.\nUpdate AWS IAM Role for AWS Glue Job\nGenerate baseline AWS Glue Job\nRunning baseline AWS Glue Job\nAWS Glue Script for Partitioning Data\nValidating using AWS Athena\nUnderstanding AWS Glue Job Bookmarks\nAWS Glue Job Bookmarks can be leveraged to maintain the bookmarks or checkpoints for incremental loads. In this section, we will go through the details related to AWS Glue Job Bookmarks.\nIntroduction to AWS Glue Job Bookmarks\nCleaning up the data to run AWS Glue Jobs\nOverview of AWS Glue CLI and Commands\nRun AWS Glue Job using AWS Glue Bookmark\nValidate AWS Glue Bookmark using AWS CLI\nAdd new data to the landing zone to run AWS Glue Jobs using Bookmarks\nRerun AWS Glue Job using Bookmark\nValidate AWS Glue Job Bookmark and Files for Incremental run\nRecrawl the AWS Glue Catalog Table using AWS CLI Commands\nRun AWS Athena Queries for Data Validation\nDevelopment Lifecycle for Pyspark\nIn this section, we will focus on the development of Spark applications using Pyspark. We will use this application later while exploring EMR in detail.\nSetup Virtual Environment and Install Pyspark\nGetting Started with Pycharm\nPassing Run Time Arguments\nAccessing OS Environment Variables\nGetting Started with Spark\nCreate Function for Spark Session\nSetup Sample Data\nRead data from files\nProcess data using Spark APIs\nWrite data to files\nValidating Writing Data to Files\nProductionizing the Code\nGetting Started with AWS EMR (Elastic Map Reduce)\nAs part of this section, we will understand how to get started with AWS EMR Cluster. We will primarily focus on AWS EMR Web Console. Elastic Map Reduce is one of the key service in AWS Data Analytics Services which provide capability to run applications which process large scale data leveraging distributed computing frameworks such as Spark.\nPlanning for AWS EMR Cluster\nCreate AWS EC2 Key Pair for AWS EMR Cluster\nSetup AWS EMR Cluster with Apache Spark\nUnderstanding Summary of AWS EMR Cluster\nReview AWS EMR Cluster Application User Interfaces\nReview AWS EMR Cluster Monitoring\nReview AWS EMR Cluster Hardware and Cluster Scaling Policy\nReview AWS EMR Cluster Configurations\nReview AWS EMR Cluster Events\nReview AWS EMR Cluster Steps\nReview AWS EMR Cluster Bootstrap Actions\nConnecting to AWS EMR Master Node using SSH\nDisabling Termination Protection for AWS EMR Cluster and Terminating the AWS EMR Cluster\nClone and Create a New AWS EMR Cluster\nListing AWS S3 Buckets and Objects using AWS CLI on AWS EMR Cluster\nListing AWS S3 Buckets and Objects using HDFS CLI on AWS EMR Cluster\nManaging Files in AWS S3 using HDFS CLI on AWS EMR Cluster\nReview AWS Glue Catalog Databases and Tables\nAccessing AWS Glue Catalog Databases and Tables using AWS EMR Cluster\nAccessing spark-sql CLI of AWS EMR Cluster\nAccessing pyspark CLI of AWS EMR Cluster\nAccessing spark-shell CLI of AWS EMR Cluster\nCreate AWS EMR Cluster for Notebooks\nDeploying Spark Applications using AWS EMR\nAs part of this section, we will understand how we typically deploy Spark Applications using AWS EMR. We will be using the Spark Application we deployed earlier.\nDeploying Applications using AWS EMR - Introduction\nSetup AWS EMR Cluster to deploy applications\nValidate SSH Connectivity to Master node of AWS EMR Cluster\nSetup Jupyter Notebook Environment on AWS EMR Cluster\nCreate required AWS s3 Bucket for AWS EMR Cluster\nUpload GHActivity Data to s3 so that we can process using Spark Application deployed on AWS EMR Cluster\nValidate Application using AWS EMR Compatible Versions of Python and Spark\nDeploy Spark Application to AWS EMR Master Node\nCreate user space for ec2-user on AWS EMR Cluster\nRun Spark Application using spark-submit on AWS EMR Master Node\nValidate Data using Jupyter Notebooks on AWS EMR Cluster\nClone and Start Auto Terminated AWS EMR Cluster\nDelete Data Populated by GHAcitivity Application using AWS EMR Cluster\nDifferences between Spark Client and Cluster Deployment Modes on AWS EMR Cluster\nRunning Spark Application using Cluster Mode on AWS EMR Cluster\nOverview of Adding Pyspark Application as Step to AWS EMR Cluster\nDeploy Spark Application to AWS S3 to run using AWS EMR Steps\nRunning Spark Applications as AWS EMR Steps in client mode\nRunning Spark Applications as AWS EMR Steps in cluster mode\nValidate AWS EMR Step Execution of Spark Application\nStreaming Data Ingestion Pipeline using AWS Kinesis\nAs part of this section, we will go through details related to the streaming data ingestion pipeline using AWS Kinesis which is a streaming service of AWS Data Analytics Services. We will use AWS Kinesis Firehose Agent and AWS Kinesis Delivery Stream to read the data from log files and ingest it into AWS s3.\nBuilding Streaming Pipeline using AWS Kinesis Firehose Agent and Delivery Stream\nRotating Logs so that the files are created frequently which will be eventually ingested using AWS Kinesis Firehose Agent and AWS Kinesis Firehose Delivery Stream\nSet up AWS Kinesis Firehose Agent to get data from logs into AWS Kinesis Delivery Stream.\nCreate AWS Kinesis Firehose Delivery Stream\nPlanning the Pipeline to ingest data into s3 using AWS Kinesis Delivery Stream\nCreate AWS IAM Group and User for Streaming Pipelines using AWS Kinesis Components\nGranting Permissions to AWS IAM User using Policy for Streaming Pipelines using AWS Kinesis Components\nConfigure AWS Kinesis Firehose Agent to read the data from log files and ingest it into AWS Kinesis Firehose Delivery Stream.\nStart and Validate AWS Kinesis Firehose Agent\nConclusion - Building Simple Steaming Pipeline using AWS Kinesis Firehose\nConsuming Data from AWS s3 using Python boto3 ingested using AWS Kinesis\nAs data is ingested into AWS S3, we will understand how data can ingested in AWS s3 can be processed using boto3.\nCustomizing AWS s3 folder using AWS Kinesis Delivery Stream\nCreate AWS IAM Policy to read from AWS s3 Bucket\nValidate AWS s3 access using AWS CLI\nSetup Python Virtual Environment to explore boto3\nValidating access to AWS s3 using Python boto3\nRead Content from AWS s3 object\nRead multiple AWS s3 Objects\nGet the number of AWS s3 Objects using Marker\nGet the size of AWS s3 Objects using Marker\nPopulating GitHub Data to AWS Dynamodb\nAs part of this section, we will understand how we can populate data to AWS Dynamodb tables using Python as a programming language.\nInstall required libraries to get GitHub Data to AWS Dynamodb tables.\nUnderstanding GitHub APIs\nSetting up GitHub API Token\nUnderstanding GitHub Rate Limit\nCreate New Repository for since\nExtracting Required Information using Python\nProcessing Data using Python\nGrant Permissions to create AWS dynamodb tables using boto3\nCreate AWS Dynamodb Tables\nAWS Dynamodb CRUD Operations\nPopulate AWS Dynamodb Table\nAWS Dynamodb Batch Operations\nOverview of Amazon AWS Athena\nAs part of this section, we will understand how to get started with AWS Athena using AWS Web console. We will also focus on basic DDL and DML or CRUD Operations using AWS Athena Query Editor.\nGetting Started with Amazon AWS Athena\nQuick Recap of AWS Glue Catalog Databases and Tables\nAccess AWS Glue Catalog Databases and Tables using AWS Athena Query Editor\nCreate a Database and Table using AWS Athena\nPopulate Data into Table using AWS Athena\nUsing CTAS to create tables using AWS Athena\nOverview of Amazon AWS Athena Architecture\nAmazon AWS Athena Resources and relationship with Hive\nCreate a Partitioned Table using AWS Athena\nDevelop Query for Partitioned Column\nInsert into Partitioned Tables using AWS Athena\nValidate Data Partitioning using AWS Athena\nDrop AWS Athena Tables and Delete Data Files\nDrop Partitioned Table using AWS Athena\nData Partitioning in AWS Athena using CTAS\nAmazon AWS Athena using AWS CLI\nAs part of this section, we will understand how to interact with AWS Athena using AWS CLI Commands.\nAmazon AWS Athena using AWS CLI - Introduction\nGet help and list AWS Athena databases using AWS CLI\nManaging AWS Athena Workgroups using AWS CLI\nRun AWS Athena Queries using AWS CLI\nGet AWS Athena Table Metadata using AWS CLI\nRun AWS Athena Queries with a custom location using AWS CLI\nDrop AWS Athena table using AWS CLI\nRun CTAS under AWS Athena using AWS CLI\nAmazon AWS Athena using Python boto3\nAs part of this section, we will understand how to interact with AWS Athena using Python boto3.\nAmazon AWS Athena using Python boto3 - Introduction\nGetting Started with Managing AWS Athena using Python boto3\nList Amazon AWS Athena Databases using Python boto3\nList Amazon AWS Athena Tables using Python boto3\nRun Amazon AWS Athena Queries with boto3\nReview AWS Athena Query Results using boto3\nPersist Amazon AWS Athena Query Results in Custom Location using boto3\nProcessing AWS Athena Query Results using Pandas\nRun CTAS against Amazon AWS Athena using Python boto3\nGetting Started with Amazon AWS Redshift\nAs part of this section, we will understand how to get started with AWS Redshift using AWS Web console. We will also focus on basic DDL and DML or CRUD Operations using AWS Redshift Query Editor.\nGetting Started with Amazon AWS Redshift - Introduction\nCreate AWS Redshift Cluster using Free Trial\nConnecting to Database using AWS Redshift Query Editor\nGet a list of tables querying information schema\nRun Queries against AWS Redshift Tables using Query Editor\nCreate AWS Redshift Table using Primary Key\nInsert Data into AWS Redshift Tables\nUpdate Data in AWS Redshift Tables\nDelete data from AWS Redshift tables\nRedshift Saved Queries using Query Editor\nDeleting AWS Redshift Cluster\nRestore AWS Redshift Cluster from Snapshot\nCopy Data from s3 into AWS Redshift Tables\nAs part of this section, we will go through the details about copying data from s3 into AWS Redshift tables using the AWS Redshift Copy command.\nCopy Data from s3 to AWS Redshift - Introduction\nSetup Data in s3 for AWS Redshift Copy\nCopy Database and Table for AWS Redshift Copy Command\nCreate IAM User with full access on s3 for AWS Redshift Copy\nRun Copy Command to copy data from s3 to AWS Redshift Table\nTroubleshoot Errors related to AWS Redshift Copy Command\nRun Copy Command to copy from s3 to AWS Redshift table\nValidate using queries against AWS Redshift Table\nOverview of AWS Redshift Copy Command\nCreate IAM Role for AWS Redshift to access s3\nCopy Data from s3 to AWS Redshift table using IAM Role\nSetup JSON Dataset in s3 for AWS Redshift Copy Command\nCopy JSON Data from s3 to AWS Redshift table using IAM Role\nDevelop Applications using AWS Redshift Cluster\nAs part of this section, we will understand how to develop applications against databases and tables created as part of AWS Redshift Cluster.\nDevelop application using AWS Redshift Cluster - Introduction\nAllocate Elastic Ip for AWS Redshift Cluster\nEnable Public Accessibility for AWS Redshift Cluster\nUpdate Inbound Rules in Security Group to access AWS Redshift Cluster\nCreate Database and User in AWS Redshift Cluster\nConnect to the database in AWS Redshift using psql\nChange Owner on AWS Redshift Tables\nDownload AWS Redshift JDBC Jar file\nConnect to AWS Redshift Databases using IDEs such as SQL Workbench\nSetup Python Virtual Environment for AWS Redshift\nRun Simple Query against AWS Redshift Database Table using Python\nTruncate AWS Redshift Table using Python\nCreate IAM User to copy from s3 to AWS Redshift Tables\nValidate Access of IAM User using Boto3\nRun AWS Redshift Copy Command using Python\nAWS Redshift Tables with Distkeys and Sortkeys\nAs part of this section, we will go through AWS Redshift-specific features such as distribution keys and sort keys to create AWS Redshift tables.\nAWS Redshift Tables with Distkeys and Sortkeys - Introduction\nQuick Review of AWS Redshift Architecture\nCreate multi-node AWS Redshift Cluster\nConnect to AWS Redshift Cluster using Query Editor\nCreate AWS Redshift Database\nCreate AWS Redshift Database User\nCreate AWS Redshift Database Schema\nDefault Distribution Style of AWS Redshift Table\nGrant Select Permissions on Catalog to AWS Redshift Database User\nUpdate Search Path to query AWS Redshift system tables\nValidate AWS Redshift table with DISTSTYLE AUTO\nCreate AWS Redshift Cluster from Snapshot to the original state\nOverview of Node Slices in AWS Redshift Cluster\nOverview of Distribution Styles related to AWS Redshift tables\nDistribution Strategies for retail tables in AWS Redshift Databases\nCreate AWS Redshift tables with distribution style all\nTroubleshoot and Fix Load or Copy Errors\nCreate AWS Redshift Table with Distribution Style Auto\nCreate AWS Redshift Tables using Distribution Style Key\nDelete AWS Redshift Cluster with a manual snapshot\nAWS Redshift Federated Queries and Spectrum\nAs part of this section, we will go through some of the advanced features of Redshift such as AWS Redshift Federated Queries and AWS Redshift Spectrum.\nAWS Redshift Federated Queries and Spectrum - Introduction\nOverview of integrating AWS RDS and AWS Redshift for Federated Queries\nCreate IAM Role for AWS Redshift Cluster\nSetup Postgres Database Server for AWS Redshift Federated Queries\nCreate tables in Postgres Database for AWS Redshift Federated Queries\nCreating Secret using Secrets Manager for Postgres Database\nAccessing Secret Details using Python Boto3\nReading Json Data to Dataframe using Pandas\nWrite JSON Data to AWS Redshift Database Tables using Pandas\nCreate AWS IAM Policy for Secret and associate with Redshift Role\nCreate AWS Redshift Cluster using AWS IAM Role with permissions on secret\nCreate AWS Redshift External Schema to Postgres Database\nUpdate AWS Redshift Cluster Network Settings for Federated Queries\nPerforming ETL using AWS Redshift Federated Queries\nClean up resources added for AWS Redshift Federated Queries\nGrant Access on AWS Glue Data Catalog to AWS Redshift Cluster for Spectrum\nSetup AWS Redshift Clusters to run queries using Spectrum\nQuick Recap of AWS Glue Catalog Database and Tables for AWS Redshift Spectrum\nCreate External Schema using AWS Redshift Spectrum\nRun Queries using AWS Redshift Spectrum\nCleanup the AWS Redshift Cluster",
      "target_audience": [
        "Beginner or Intermediate Data Engineers who want to learn AWS Analytics Services for Data Engineering",
        "Intermediate Application Engineers who want to explore Data Engineering using AWS Analytics Services",
        "Data and Analytics Engineers who want to learn Data Engineering using AWS Analytics Services",
        "Testers who want to learn key skills to test Data Engineering applications built using AWS Analytics Services"
      ]
    },
    {
      "title": "Tensorflow and Keras For Neural Networks and Deep Learning",
      "url": "https://www.udemy.com/course/tensorflow-and-keras-for-neural-networks-and-deep-learning/",
      "bio": "Master the Most Important Deep Learning Frameworks (Tensorflow & Keras) for Python Data Science",
      "objectives": [
        "Harness The Power Of Anaconda/iPython For Practical Data Science",
        "Learn How To Install & Use Tensorflow Within Anaconda",
        "Implement Statistical & Machine Learning With Tensorflow",
        "Implement Neural Network Modelling With Tensorflow & Keras",
        "Implement Deep Learning Based Unsupervised Learning With Tensorflow and Keras",
        "Implement Deep Learning Based Supervised Learning With Tensorflow & Keras",
        "Implement Convolution Neural Networks With Tensorflow & Keras"
      ],
      "course_content": {
        "INTRODUCTION TO THE COURSE: The Key Concepts and Software Tools": [
          "Introduction to the Course",
          "Data and Scripts For the Course",
          "Python Data Science Environment",
          "For Mac Users",
          "Introduction to IPython",
          "Install Tensorflow",
          "Written Tensorflow Installation Instructions",
          "Install Keras on Windows 10",
          "Install Keras on Mac",
          "Written Keras Installation Instructions"
        ],
        "Introduction to Python Data Science Packages": [
          "Python Packages for Data Science",
          "Introduction to Numpy",
          "Create Numpy Arrays",
          "Numpy Operations",
          "Numpy for Statistical Operation",
          "Introduction to Pandas",
          "Read in Data from CSV",
          "Read in Data from Excel",
          "Basic Data Cleaning"
        ],
        "Introduction to TensorFlow": [
          "A Brief Touchdown",
          "A Brief Touchdown: Computational Graphs",
          "Common Mathematical Operators in Tensorflow",
          "A Tensorflow Session",
          "Interactive Tensorflow Session",
          "Constants and Variables in Tensorflow",
          "Placeholders in Tensorflow"
        ],
        "Introduction to Keras": [
          "What is Keras"
        ],
        "Some Preliminary Tensorflow and Keras Applications": [
          "Theory of Linear Regression (OLS)",
          "OLS From First Principles",
          "Visualize the Results of OLS",
          "Multiple Regression With Tensorflow-Part 1",
          "Estimate With Tensorflow Estimators",
          "Multiple Regression With Tensorflow Estimators",
          "More on Linear Regressor Estimator",
          "GLM: Generalized Linear Model",
          "Linear Classifier For Binary Classification",
          "Accuracy Assessment For Binary Classification",
          "Linear Classification with Binary Classification With Mixed Predictors",
          "Softmax Classification With Tensorflow"
        ],
        "Some Basic Concepts": [
          "What is Machine Learning?",
          "Theory Behind ANN (Artificial Neural Network) and DNN (Deep Neural Networks)"
        ],
        "Unsupervised Learning With Tensorflow and Keras": [
          "What is Unsupervised Learning?",
          "Autoencoders for Unsupervised Classification",
          "Autoencoders in Tensorflow (Binary Class Problem)",
          "Autoencoders in Tensorflow (Multiple Classes)",
          "Autoencoders in Keras (Sparsity Constraints)",
          "Autoencoders in Keras (Simple)",
          "Deep Autoencoder With Keras"
        ],
        "Neural Network for Tensorflow & Keras": [
          "Multi Layer Perceptron (MLP) with Tensorflow",
          "Multi Layer Perceptron (MLP) With Keras",
          "Keras MLP For Binary Classification",
          "Keras MLP for Multiclass Classification",
          "Keras MLP for Regression"
        ],
        "Deep Learning For Tensorflow & Keras": [
          "What is Artificial Intelligence?",
          "Deep Neural Network (DNN) Classifier With Tensorflow",
          "Deep Neural Network (DNN) Classifier With Mixed Predictors",
          "Deep Neural Network (DNN) Regression With Tensorflow",
          "Wide & Deep Learning (Tensorflow)",
          "DNN Classifier With Keras",
          "DNN Classifier With Keras-Example 2"
        ],
        "Convolution Neural Network (CNN) For Image Analysis": [
          "Introduction to CNN",
          "Implement a CNN for Multi-Class Supervised Classification",
          "Activation Functions",
          "More on CNN",
          "Pre-Requisite For Working With Imagery Data",
          "CNN on Image Data-Part 1",
          "CNN on Image Data-Part 2",
          "More on TFLearn",
          "CNN Workflow for Keras",
          "CNN With Keras",
          "CNN on Image Data with Keras-Part 1",
          "CNN on Image Data with Keras-Part 2"
        ]
      },
      "requirements": [
        "Be Able To Operate & Install Software On A Computer",
        "Prior Exposure To Python based Data Science Will Be Beneficial",
        "Prior Exposure To Basic Statistical Concepts & Implementation Will be Useful",
        "Have Prior Exposure To Common Machine Learning Terms such as cross-validation"
      ],
      "description": "THIS IS A COMPLETE NEURAL NETWORKS & DEEP LEARNING TRAINING WITH TENSORFLOW & KERAS IN PYTHON!\nIt is a full 7-Hour Python Tensorflow & Keras Neural Network & Deep Learning Boot Camp that will help you learn basic machine learning, neural networks and deep learning  using two of the most important Deep Learning frameworks- Tensorflow and Keras.\nHERE IS WHY YOU SHOULD ENROLL IN THIS COURSE:\nThis course is your complete guide to practical machine & deep learning using the Tensorflow & Keras framework in Python..\nThis means, this course covers the important aspects of Keras and Tensorflow (Google's powerful Deep Learning framework) and if you take this course, you can do away with taking other courses or buying books on Python Tensorflow and Keras based data science.\nIn this age of big data, companies across the globe use Python to sift through the avalanche of information at their disposal and advent of Tensorflow and Keras is revolutionizing Deep Learning...\nBy gaining proficiency in Keras and and Tensorflow, you can give your company a competitive edge and boost your career to the next level.\nTHIS IS MY PROMISE TO YOU: COMPLETE THIS ONE COURSE & BECOME A PRO IN PRACTICAL KERAS & TENSORFLOW BASED DATA SCIENCE!\nBut first things first. My name is Minerva Singh and I am an Oxford University MPhil (Geography and Environment) graduate. I recently finished a PhD at Cambridge University (Tropical Ecology and Conservation).\nI have several years of experience in analyzing real life data from different sources  using data science related techniques and producing publications for international peer reviewed journals.\nOver the course of my research I realized almost all the Python data science courses and books out there do not account for the multidimensional nature of the topic and use data science interchangeably with machine learning..\nThis gives students an incomplete knowledge of the subject. My course, on the other hand, will give you a robust grounding in all aspects of data science within the Tensorflow framework.\nUnlike other Python courses, we dig deep into the statistical modeling features of Tensorflow & Keras and give you a one-of-a-kind grounding in these frameworks!\nDISCOVER 8 COMPLETE SECTIONS ADDRESSING EVERY ASPECT OF PYTHON BASED TENSORFLOW DATA SCIENCE:\n• A full introduction to Python Data Science and powerful Python driven framework for data science, Anaconda\n• Getting started with Jupyter notebooks for implementing data science techniques in Python\n• A comprehensive presentation about Tensorflow & Keras installation and a brief introduction to the other Python data science packages\n• Brief introduction to the working of Pandas and Numpy\n• The basics of the Tensorflow syntax and graphing environment\n• The basics of the Keras syntax\n• Machine Learning, Supervised Learning, Unsupervised Learning in the Tensorflow & Keras frameworks\n• You’ll even discover how to create artificial neural networks and deep learning structures with Tensorflow & Keras\nBUT,  WAIT! THIS ISN'T JUST ANY OTHER DATA SCIENCE COURSE:\nYou’ll start by absorbing the most valuable Python Tensorflow and Keras basics and techniques.\nI use easy-to-understand, hands-on methods to simplify and address even the most difficult concepts.\nMy course will help you implement the methods using real data obtained from different sources. Many courses use made-up data that does not empower students to implement Python based data science in real -life.\nAfter taking this course, you’ll easily use packages like Numpy, Pandas, and Matplotlib to work with real data in Python along with gaining fluency in Tensorflow and Keras. I will even introduce you to deep learning models such as Convolution Neural network (CNN) !!\nThe underlying motivation for the course is to ensure you can apply Python based data science on real data into practice today, start analyzing  data for your own projects whatever your skill level, and impress your potential employers with actual examples of your data science abilities.\nThis course will take students without a prior Python and/or statistics background background from a basic level to performing some of the most common advanced data science techniques using the powerful Python based Jupyter notebooks\nIt is a practical, hands-on course, i.e. we will spend some time dealing with some of the theoretical concepts related to data science. However, majority of the course will focus on implementing different  techniques on real data and interpret the results..\nAfter each video you will learn a new concept or technique which you may apply to your own projects!\nJOIN THE COURSE NOW!",
      "target_audience": [
        "People Interested In Learning Python Based Tensorflow and Keras For Data Science Applications",
        "People With Prior Exposure To Python Programming &/Or Data Science Concepts",
        "People Interested In Implementing Neural Networks & Deep Learning Models With Tensorflow",
        "People Interested In Implementing Neural Networks & Deep Learning Models With Keras"
      ]
    },
    {
      "title": "Deep Learning A-Z 2025: Neural Networks, AI & ChatGPT Prize",
      "url": "https://www.udemy.com/course/deeplearning/",
      "bio": "Learn to create Deep Learning models in Python from two Machine Learning, Data Science experts. Code templates included.",
      "objectives": [
        "Understand the intuition behind Artificial Neural Networks",
        "Apply Artificial Neural Networks in practice",
        "Understand the intuition behind Convolutional Neural Networks",
        "Apply Convolutional Neural Networks in practice",
        "Understand the intuition behind Recurrent Neural Networks",
        "Apply Recurrent Neural Networks in practice",
        "Understand the intuition behind Self-Organizing Maps",
        "Apply Self-Organizing Maps in practice",
        "Understand the intuition behind Boltzmann Machines",
        "Apply Boltzmann Machines in practice",
        "Understand the intuition behind AutoEncoders",
        "Apply AutoEncoders in practice"
      ],
      "course_content": {
        "Welcome to the course!": [
          "Introduction to Deep Learning: From Historical Context to Modern Applications",
          "Get the Codes, Datasets and Slides Here",
          "Recommended Workshops before we dive in!",
          "Prizes $$ for Learning"
        ],
        "--------------------- Part 1 - Artificial Neural Networks ---------------------": [
          "Welcome to Part 1 - Artificial Neural Networks"
        ],
        "ANN Intuition": [
          "What You'll Need for ANN",
          "How Neural Networks Learn: Gradient Descent and Backpropagation Explained",
          "Understanding Neurons: The Building Blocks of Artificial Neural Networks",
          "Understanding Activation Functions in Neural Networks: Sigmoid, ReLU, and More",
          "How Do Neural Networks Work? Step-by-Step Guide to Property Valuation Example",
          "How Do Neural Networks Learn? Understanding Backpropagation and Cost Functions",
          "Mastering Gradient Descent: Key to Efficient Neural Network Training",
          "How to Use Stochastic Gradient Descent for Deep Learning Optimization",
          "Understanding Backpropagation Algorithm: Key to Optimizing Deep Learning Models"
        ],
        "Building an ANN": [
          "Get the code and dataset ready",
          "Step 1 - Data Preprocessing for Deep Learning: Preparing Neural Network Dataset",
          "Check out our free course on ANN for Regression",
          "Step 2 - Data Preprocessing for Neural Networks: Essential Steps and Techniques",
          "Step 3 - Constructing an Artificial Neural Network: Adding Input & Hidden Layers",
          "Step 4 - Compile and Train Neural Network: Optimizers, Loss Functions & Metrics",
          "Step 5 - How to Make Predictions and Evaluate Neural Network Model in Python"
        ],
        "-------------------- Part 2 - Convolutional Neural Networks --------------------": [
          "Welcome to Part 2 - Convolutional Neural Networks"
        ],
        "CNN Intuition": [
          "What You'll Need for CNN",
          "Understanding CNN Architecture: From Convolution to Fully Connected Layers",
          "How Do Convolutional Neural Networks Work? Understanding CNN Architecture",
          "How to Apply Convolution Filters in Neural Networks: Feature Detection Explained",
          "Rectified Linear Units (ReLU) in Deep Learning: Optimizing CNN Performance",
          "Understanding Spatial Invariance in CNNs: Max Pooling Explained for Beginners",
          "How to Flatten Pooled Feature Maps in Convolutional Neural Networks (CNNs)",
          "How Do Fully Connected Layers Work in Convolutional Neural Networks (CNNs)?",
          "CNN Building Blocks: Feature Maps, ReLU, Pooling, and Fully Connected Layers",
          "Understanding Softmax Activation and Cross-Entropy Loss in Deep Learning"
        ],
        "Building a CNN": [
          "Get the code and dataset ready",
          "Step 1 - Convolutional Neural Networks Explained: Image Classification Tutorial",
          "Step 2 - Deep Learning Preprocessing: Scaling & Transforming Images for CNNs",
          "Step 3 - Building CNN Architecture: Convolutional Layers & Max Pooling Explained",
          "Step 4 - Train CNN for Image Classification: Optimize with Keras & TensorFlow",
          "Step 5 - Deploying a CNN for Real-World Image Recognition",
          "Develop an Image Recognition System Using Convolutional Neural Networks"
        ],
        "---------------------- Part 3 - Recurrent Neural Networks ----------------------": [
          "Welcome to Part 3 - Recurrent Neural Networks"
        ],
        "RNN Intuition": [
          "What You'll Need for RNN",
          "How Do Recurrent Neural Networks (RNNs) Work? Deep Learning Explained",
          "What is a Recurrent Neural Network (RNN)? Deep Learning for Sequential Data",
          "Understanding the Vanishing Gradient Problem in Recurrent Neural Networks (RNNs)",
          "Understanding Long Short-Term Memory (LSTM) Architecture for Deep Learning",
          "How LSTMs Work in Practice: Visualizing Neural Network Predictions",
          "LSTM Variations: Peepholes, Combined Gates, and GRUs in Deep Learning"
        ],
        "Building a RNN": [
          "Get the code and dataset ready",
          "Step 1 - Building a Robust LSTM Neural Network for Stock Price Trend Prediction",
          "Step 2 - Importing Training Data for LSTM Stock Price Prediction Model",
          "Step 3 - Applying Min-Max Normalization for Time Series Data in Neural Networks",
          "Step 4 - Building X_train and y_train Arrays for LSTM Time Series Forecasting",
          "Step 5 - Preparing Time Series Data for LSTM Neural Network in Stock Forecasting",
          "Step 6 - Create RNN Architecture: Sequential Layers vs Computational Graphs",
          "Step 7 - Adding First LSTM Layer: Key Components for Stock Market Prediction",
          "Step 8 - Implementing Dropout Regularization in LSTM Networks for Forecasting",
          "Step 9 - Finalizing RNN Architecture: Dense Layer for Stock Price Forecasting",
          "Step 10 - Compile RNN with Adam Optimizer for Stock Price Prediction in Python",
          "Step 11 - Optimizing Epochs and Batch Size for LSTM Stock Price Forecasting",
          "Step 12 - Visualizing LSTM Predictions: Real vs Forecasted Google Stock Prices",
          "Step 13 - Preparing Historical Stock Data for LSTM Model: Scaling and Reshaping",
          "Step 14 - Creating 3D Input Structure for LSTM Stock Price Prediction in Python",
          "Step 15 - Visualizing LSTM Predictions: Plotting Real vs Predicted Stock Prices"
        ]
      },
      "requirements": [
        "High school mathematics level",
        "Basic Python programming knowledge"
      ],
      "description": "*** As seen on Kickstarter ***\nArtificial intelligence is growing exponentially. There is no doubt about that. Self-driving cars are clocking up millions of miles, IBM Watson is diagnosing patients better than armies of doctors and Google Deepmind's AlphaGo beat the World champion at Go - a game where intuition plays a key role.\nBut the further AI advances, the more complex become the problems it needs to solve. And only Deep Learning can solve such complex problems and that's why it's at the heart of Artificial intelligence.\n--- Why Deep Learning A-Z? ---\nHere are five reasons we think Deep Learning A-Z really is different, and stands out from the crowd of other training programs out there:\n1. ROBUST STRUCTURE\nThe first and most important thing we focused on is giving the course a robust structure. Deep Learning is very broad and complex and to navigate this maze you need a clear and global vision of it.\nThat's why we grouped the tutorials into two volumes, representing the two fundamental branches of Deep Learning: Supervised Deep Learning and Unsupervised Deep Learning. With each volume focusing on three distinct algorithms, we found that this is the best structure for mastering Deep Learning.\n2. INTUITION TUTORIALS\nSo many courses and books just bombard you with the theory, and math, and coding... But they forget to explain, perhaps, the most important part: why you are doing what you are doing. And that's how this course is so different. We focus on developing an intuitive *feel* for the concepts behind Deep Learning algorithms.\nWith our intuition tutorials you will be confident that you understand all the techniques on an instinctive level. And once you proceed to the hands-on coding exercises you will see for yourself how much more meaningful your experience will be. This is a game-changer.\n3. EXCITING PROJECTS\nAre you tired of courses based on over-used, outdated data sets?\nYes? Well then you're in for a treat.\nInside this class we will work on Real-World datasets, to solve Real-World business problems. (Definitely not the boring iris or digit classification datasets that we see in every course). In this course we will solve six real-world challenges:\nArtificial Neural Networks to solve a Customer Churn problem\nConvolutional Neural Networks for Image Recognition\nRecurrent Neural Networks to predict Stock Prices\nSelf-Organizing Maps to investigate Fraud\nBoltzmann Machines to create a Recomender System\nStacked Autoencoders* to take on the challenge for the Netflix $1 Million prize\n*Stacked Autoencoders is a brand new technique in Deep Learning which didn't even exist a couple of years ago. We haven't seen this method explained anywhere else in sufficient depth.\n4. HANDS-ON CODING\nIn Deep Learning A-Z we code together with you. Every practical tutorial starts with a blank page and we write up the code from scratch. This way you can follow along and understand exactly how the code comes together and what each line means.\nIn addition, we will purposefully structure the code in such a way so that you can download it and apply it in your own projects. Moreover, we explain step-by-step where and how to modify the code to insert YOUR dataset, to tailor the algorithm to your needs, to get the output that you are after.\nThis is a course which naturally extends into your career.\n5. IN-COURSE SUPPORT\nHave you ever taken a course or read a book where you have questions but cannot reach the author?\nWell, this course is different. We are fully committed to making this the most disruptive and powerful Deep Learning course on the planet. With that comes a responsibility to constantly be there when you need our help.\nIn fact, since we physically also need to eat and sleep we have put together a team of professional Data Scientists to help us out. Whenever you ask a question you will get a response from us within 48 hours maximum.\nNo matter how complex your query, we will be there. The bottom line is we want you to succeed.\n--- The Tools ---\nTensorflow and Pytorch are the two most popular open-source libraries for Deep Learning. In this course you will learn both!\nTensorFlow was developed by Google and is used in their speech recognition system, in the new google photos product, gmail, google search and much more. Companies using Tensorflow include AirBnb, Airbus, Ebay, Intel, Uber and dozens more.\nPyTorch is as just as powerful and is being developed by researchers at Nvidia and leading universities: Stanford, Oxford, ParisTech. Companies using PyTorch include Twitter, Saleforce and Facebook.\nSo which is better and for what?\nWell, in this course you will have an opportunity to work with both and understand when Tensorflow is better and when PyTorch is the way to go. Throughout the tutorials we compare the two and give you tips and ideas on which could work best in certain circumstances.\nThe interesting thing is that both these libraries are barely over 1 year old. That's what we mean when we say that in this course we teach you the most cutting edge Deep Learning models and techniques.\n--- More Tools ---\nTheano is another open source deep learning library. It's very similar to Tensorflow in its functionality, but nevertheless we will still cover it.\nKeras is an incredible library to implement Deep Learning models. It acts as a wrapper for Theano and Tensorflow. Thanks to Keras we can create powerful and complex Deep Learning models with only a few lines of code. This is what will allow you to have a global vision of what you are creating. Everything you make will look so clear and structured thanks to this library, that you will really get the intuition and understanding of what you are doing.\n--- Even More Tools ---\nScikit-learn the most practical Machine Learning library. We will mainly use it:\nto evaluate the performance of our models with the most relevant technique, k-Fold Cross Validation\nto improve our models with effective Parameter Tuning\nto preprocess our data, so that our models can learn in the best conditions\nAnd of course, we have to mention the usual suspects. This whole course is based on Python and in every single section you will be getting hours and hours of invaluable hands-on practical coding experience.\nPlus, throughout the course we will be using Numpy to do high computations and manipulate high dimensional arrays, Matplotlib to plot insightful charts and Pandas to import and manipulate datasets the most efficiently.\n--- Who Is This Course For? ---\nAs you can see, there are lots of different tools in the space of Deep Learning and in this course we make sure to show you the most important and most progressive ones so that when you're done with Deep Learning A-Z your skills are on the cutting edge of today's technology.\nIf you are just starting out into Deep Learning, then you will find this course extremely useful. Deep Learning A-Z is structured around special coding blueprint approaches meaning that you won't get bogged down in unnecessary programming or mathematical complexities and instead you will be applying Deep Learning techniques from very early on in the course. You will build your knowledge from the ground up and you will see how with every tutorial you are getting more and more confident.\nIf you already have experience with Deep Learning, you will find this course refreshing, inspiring and very practical. Inside Deep Learning A-Z you will master some of the most cutting-edge Deep Learning algorithms and techniques (some of which didn't even exist a year ago) and through this course you will gain an immense amount of valuable hands-on experience with real-world business challenges. Plus, inside you will find inspiration to explore new Deep Learning skills and applications.\n--- Real-World Case Studies ---\nMastering Deep Learning is not just about knowing the intuition and tools, it's also about being able to apply these models to real-world scenarios and derive actual measurable results for the business or project. That's why in this course we are introducing six exciting challenges:\n#1 Churn Modelling Problem\nIn this part you will be solving a data analytics challenge for a bank. You will be given a dataset with a large sample of the bank's customers. To make this dataset, the bank gathered information such as customer id, credit score, gender, age, tenure, balance, if the customer is active, has a credit card, etc. During a period of 6 months, the bank observed if these customers left or stayed in the bank.\nYour goal is to make an Artificial Neural Network that can predict, based on geo-demographical and transactional information given above, if any individual customer will leave the bank or stay (customer churn). Besides, you are asked to rank all the customers of the bank, based on their probability of leaving. To do that, you will need to use the right Deep Learning model, one that is based on a probabilistic approach.\nIf you succeed in this project, you will create significant added value to the bank. By applying your Deep Learning model the bank may significantly reduce customer churn.\n#2 Image Recognition\nIn this part, you will create a Convolutional Neural Network that is able to detect various objects in images. We will implement this Deep Learning model to recognize a cat or a dog in a set of pictures. However, this model can be reused to detect anything else and we will show you how to do it - by simply changing the pictures in the input folder.\nFor example, you will be able to train the same model on a set of brain images, to detect if they contain a tumor or not. But if you want to keep it fitted to cats and dogs, then you will literally be able to a take a picture of your cat or your dog, and your model will predict which pet you have. We even tested it out on Hadelin’s dog!\n#3 Stock Price Prediction\nIn this part, you will create one of the most powerful Deep Learning models. We will even go as far as saying that you will create the Deep Learning model closest to “Artificial Intelligence”. Why is that? Because this model will have long-term memory, just like us, humans.\nThe branch of Deep Learning which facilitates this is Recurrent Neural Networks. Classic RNNs have short memory, and were neither popular nor powerful for this exact reason. But a recent major improvement in Recurrent Neural Networks gave rise to the popularity of LSTMs (Long Short Term Memory RNNs) which has completely changed the playing field. We are extremely excited to include these cutting-edge deep learning methods in our course!\nIn this part you will learn how to implement this ultra-powerful model, and we will take the challenge to use it to predict the real Google stock price. A similar challenge has already been faced by researchers at Stanford University and we will aim to do at least as good as them.\n#4 Fraud Detection\nAccording to a recent report published by Markets & Markets the Fraud Detection and Prevention Market is going to be worth $33.19 Billion USD by 2021. This is a huge industry and the demand for advanced Deep Learning skills is only going to grow. That’s why we have included this case study in the course.\nThis is the first part of Volume 2 - Unsupervised Deep Learning Models. The business challenge here is about detecting fraud in credit card applications. You will be creating a Deep Learning model for a bank and you are given a dataset that contains information on customers applying for an advanced credit card.\nThis is the data that customers provided when filling the application form. Your task is to detect potential fraud within these applications. That means that by the end of the challenge, you will literally come up with an explicit list of customers who potentially cheated on their applications.\n#5 & 6 Recommender Systems\nFrom Amazon product suggestions to Netflix movie recommendations - good recommender systems are very valuable in today's World. And specialists who can create them are some of the top-paid Data Scientists on the planet.\nWe will work on a dataset that has exactly the same features as the Netflix dataset: plenty of movies, thousands of users, who have rated the movies they watched. The ratings go from 1 to 5, exactly like in the Netflix dataset, which makes the Recommender System more complex to build than if the ratings were simply “Liked” or “Not Liked”.\nYour final Recommender System will be able to predict the ratings of the movies the customers didn’t watch. Accordingly, by ranking the predictions from 5 down to 1, your Deep Learning model will be able to recommend which movies each user should watch. Creating such a powerful Recommender System is quite a challenge so we will give ourselves two shots. Meaning we will build it with two different Deep Learning models.\nOur first model will be Deep Belief Networks, complex Boltzmann Machines that will be covered in Part 5. Then our second model will be with the powerful AutoEncoders, my personal favorites. You will appreciate the contrast between their simplicity, and what they are capable of.\nAnd you will even be able to apply it to yourself or your friends. The list of movies will be explicit so you will simply need to rate the movies you already watched, input your ratings in the dataset, execute your model and voila! The Recommender System will tell you exactly which movies you would love one night you if are out of ideas of what to watch on Netflix!\n--- Summary ---\nIn conclusion, this is an exciting training program filled with intuition tutorials, practical exercises and real-World case studies.\nWe are super enthusiastic about Deep Learning and hope to see you inside the class!\nKirill & Hadelin",
      "target_audience": [
        "Anyone interested in Deep Learning",
        "Students who have at least high school knowledge in math and who want to start learning Deep Learning",
        "Any intermediate level people who know the basics of Machine Learning or Deep Learning, including the classical algorithms like linear regression or logistic regression and more advanced topics like Artificial Neural Networks, but who want to learn more about it and explore all the different fields of Deep Learning",
        "Anyone who is not that comfortable with coding but who is interested in Deep Learning and wants to apply it easily on datasets",
        "Any students in college who want to start a career in Data Science",
        "Any data analysts who want to level up in Deep Learning",
        "Any people who are not satisfied with their job and who want to become a Data Scientist",
        "Any people who want to create added value to their business by using powerful Deep Learning tools",
        "Any business owners who want to understand how to leverage the Exponential technology of Deep Learning in their business",
        "Any Entrepreneur who wants to create disruption in an industry using the most cutting edge Deep Learning algorithms"
      ]
    },
    {
      "title": "Data science with R: tidyverse",
      "url": "https://www.udemy.com/course/data-science-with-r-tidyverse/",
      "bio": "R Programming Language, Data Analysis, Data Cleaning, Data Science, Data Wrangling, tidyverse, dplyr, ggplot2, RStudio",
      "objectives": [
        "How to use R's tidyverse libraries in your data science projects",
        "How to write efficient R code for data science related tasks",
        "What is clean data",
        "How to clean your data with R",
        "What is grammar of data wrangling",
        "How to wrangle data with dplyr and tidyr",
        "How to import data into R",
        "How to properly parse imported data",
        "How to chain R's functions into a pipeline",
        "How to manipulate strings",
        "What are Regular Expressions",
        "How to use stringr library with Regular Expressions",
        "How to use forcats library to manipulate categorical variables",
        "What is Grammar of Graphics",
        "How to visualize data with ggplot2 library",
        "What is functional programing",
        "How to use purrr library for mapping functions, nesting data, manipulating lists, etc.",
        "What is relational data",
        "How to use dplyr library for relational data",
        "What is tidy evaluation",
        "How to use tidyverse tools to finish a practical project"
      ],
      "course_content": {},
      "requirements": [
        "R and RStudio already installed on your computer.",
        "Basic knowledge of statistics is a plus.",
        "Basic to intermediate R knowledge is a plus.",
        "Complete R beginners will find course more challenging.",
        "For complete R beginners I recommend first taking one of the R beginners courses.",
        "Interest in data science and data science related tasks.",
        "Interest in how to write efficient R code.",
        "Please update R or R's libraries if necessary. List of versions ( R and all R's libraries used in the exercises) provided at the beginning and at the end of course material."
      ],
      "description": "Data Science skills are still one of the most in-demand skills on the job market today. Many people see only the fun part of data science, tasks like:  \"search for data insight\", \"reveal the hidden truth behind the data\", \"build predictive models\", \"apply machine learning algorithms\", and so on. The reality, which is known to most data scientists, is, that when you deal with real data, the most time-consuming operations of any data science project are: \"data importing\", \"data cleaning\", \"data wrangling\", \"data exploring\" and so on. So it is necessary to have an adequate tool for addressing given data-related tasks. What if I say, there is a freely accessible tool, that falls into the provided description above!\n\n\nR is one of the most in-demand programming languages when it comes to applied statistics, data science, data exploration, etc. If you combine R with R's collection of libraries called tidyverse, you get one of the deadliest tools, which was designed for data science-related tasks. All tidyverse libraries share a unique philosophy, grammar, and data types. Therefore libraries can be used side by side, and enable you to write efficient and more optimized R code, which will help you finish projects faster.\n\n\nThis course includes several chapters, each chapter introduces different aspects of data-related tasks, with the proper tidyverse tool to help you deal with a given task. Also, the course brings to the table theory related to the topic, and practical examples, which are covered in R. If you dive into the course, you will be engaged with many different data science challenges, here are just a few of them from the course:\nTidy data, how to clean your data with tidyverse?\nGrammar of data wrangling.\nHow to wrangle data with dplyr and tidyr.\nCreate table-like objects called tibble.\nImport and parse data with readr and other libraries.\nDeal with strings in R using stringr.\nApply Regular Expressions concepts when dealing with strings.\nDeal with categorical variables using forcats.\nGrammar of Data Visualization.\nExplore data and draw statistical plots using ggplot2.\nUse concepts of functional programming, and map functions using purrr.\nEfficiently deal with lists with the help of purrr.\nPractical applications of relational data.\nUse dplyr for relational data.\nTidy evaluation inside tidyverse.\nApply tidyverse tools for the final practical data science project.\n\n\nCourse includes:\nover 25 hours of lecture videos,\nR scripts and additional data (provided in the course material),\nengagement with assignments at the end of each chapter,\nassignments walkthrough videos (where you can check your results).\nAll being said this makes one of Udemy's most comprehensive courses for data science-related tasks using R and tidyverse.\n\n\nEnroll today and become the master of R's tidyverse!!!",
      "target_audience": [
        "Anyone who is interested in data science",
        "Anyone who is interested in data analysis",
        "Anyone who is interested in writing efficient R code",
        "Anyone whose job, research or hobby is related to data cleaning or data visualizing",
        "Aspiring data scientists, statisticians or data (business) analysts",
        "Anyone who deals with data modeling and is usually struggling with data preparation / cleaning step",
        "Students working with data"
      ]
    },
    {
      "title": "Deploy ML Model in Production with FastAPI and Docker",
      "url": "https://www.udemy.com/course/nlp-with-bert-in-python/",
      "bio": "Deploy ML Model with ViT, BERT and TinyBERT HuggingFace Transformers with Streamlit, FastAPI and Docker at AWS",
      "objectives": [
        "Deploy Machine Learning Models with FastAPI: Learn to build and deploy RESTful APIs for serving ML models efficiently.",
        "Master Cloud-Based ML Deployments with AWS: Gain hands-on experience deploying, managing, and scaling ML models on AWS EC2 and S3.",
        "Automate ML Operations with Boto3 and Python: Automate cloud tasks like instance creation, data storage, and security configuration using Boto3.",
        "Containerize ML Applications Using Docker: Build and manage Docker containers to ensure consistent and scalable ML deployments across environments.",
        "Streamline Model Inference with Real-Time APIs: Develop high-performance APIs that deliver fast and accurate predictions for production-grade applications.",
        "Optimize Machine Learning Pipelines for Production: Design and implement end-to-end ML pipelines, from data ingestion to model deployment, using best practices.",
        "Implement Secure and Scalable ML Infrastructure: Learn to integrate security protocols and scalability features into your cloud-based ML deployments.",
        "Create Interactive Web Apps with Streamlit: Build and deploy interactive ML-powered web applications that are accessible and user-friendly.",
        "Deploy Transformers for NLP and Computer Vision: Fine-tune and deploy TinyBERT and Vision Transformers for sentiment analysis, disaster tweets, and images.",
        "Monitor and Maintain ML Models in Production: Implement monitoring, A/B testing, and bias detection to ensure your models remain reliable and effective in prod."
      ],
      "course_content": {
        "Introduction": [
          "Course Introduction",
          "Install Requirements.txt",
          "Resources [Code Files]"
        ],
        "Introduction to ML Ops and Pipeline": [
          "What is Machine Learning Pipeline",
          "Type of ML Model Deployment",
          "ML Infrastructure and Integration Tools",
          "Benefits of ML Pipeline",
          "Challenges in ML Model Deployment",
          "Data and Model Management",
          "A/B Testing for ML Model",
          "ML Model Bias and Security"
        ],
        "Introduction to AWS Services": [
          "Getting Started with AWS",
          "Exploring EC2 Dashboard",
          "AWS Cost Calculator",
          "Creating EC2 Windows Server Part 1",
          "Creating EC2 Windows Server Part 2",
          "Create EC2 Ubuntu Machine",
          "Create S3 Bucket"
        ],
        "Managing AWS EC2 Servers with Boto3": [
          "Introduction",
          "AWS Credentials Setup with AWS CLI for Boto3",
          "AWS Boto3 - SSH Key Pair Setup",
          "AWS Boto3 - Launch EC2 with Python Part 1",
          "AWS Boto3 - Launch EC2 with Python Part 2",
          "AWS Boto3 - Exploring Security Groups using AWS Console",
          "AWS Boto3 - Create Custom Security Group with Python",
          "AWS Boto3 - Configure Custom Security Group Using Python",
          "AWS Boto 3 - Attach and Detach Security Groups to EC2 with Python",
          "AWS Boto3 - Check EC2 Instance Status",
          "AWS Boto3 - Start, Stop and Terminate EC2 Instance with Python"
        ],
        "Managing AWS S3 with Boto3": [
          "Introduction",
          "AWS Boto3 - Create S3 Bucket with Python",
          "AWS Boto3 - Upload File to S3 with Python",
          "AWS Boto3 - List S3 Objects or Files in the Bucket",
          "AWS Boto3 - Download S3 File to Local System",
          "AWS Boto3 - Upload All Files in Dir to S3 using Python",
          "AWS Boto3 - Download Full S3 Dir to Local",
          "AWS Boto3 - Delete All Files in an S3 Bucket"
        ],
        "Sentiment Classification using TinyBERT Transformer": [
          "Introduction",
          "Getting Started with Sentiment Classification",
          "Load Data with Hugging Face Datasets Library",
          "Data Tokenization",
          "Build Model Evaluation Function",
          "Model Building and Training",
          "Model Save and Load for Inference",
          "Push Model to AWS S3 Part 1",
          "Push Model to AWS S3 Part 2"
        ],
        "Disaster Tweets Classification using TinyBERT Transformer": [
          "Introduction",
          "Getting Started with Disaster Tweets Classification",
          "Loading Disaster Tweets",
          "Load Data with Hugging Face Datasets Library",
          "Tweets Tokenization",
          "Model Building and Training",
          "Model Save and Load for Inference",
          "Push Model to AWS S3"
        ],
        "Human Pose Image Classification using Vision Transformers (ViT)": [
          "Introduction",
          "Getting Started with Image Classification",
          "Human Pose Dataset Preparation",
          "Image Preprocessing Part 1",
          "Image Preprocessing Part 2",
          "ViT Model Building",
          "Vision Transformer (ViT) Fine Tuning",
          "Model Prediction on Test Dataset",
          "Classification Report and Confusion Matrix",
          "Human Pose Prediction on Real Images",
          "Push Model to AWS S3"
        ],
        "Deploy ML Model at Streamlit Server": [
          "Introduction",
          "Streamlit Application Walkthrough",
          "Streamlit Basics Part 1",
          "Streamlit Basics Part 2",
          "Streamlit Basics Part 3",
          "Streamlit Basics Part 4",
          "Streamlit Basics Part 5",
          "Streamlit Basics Part 6",
          "Streamlit App with ML Model Part 1",
          "Streamlit App with ML Model Part 2",
          "Upload Streamlit App on GitHub for Pipeline",
          "Deploying ML App at Streamlit Server"
        ],
        "ML Model Deployment with Streamlit at AWS EC2 Server": [
          "Introduction",
          "Automating AWS EC2 Instance Creation with Boto3",
          "Automating Security Group Configuration",
          "Automating Server Start and Stop",
          "Create Instance with Automated Script",
          "Terminate Instance",
          "Fasted and Shortest Way to Create Server During POC",
          "Connect with SSH to Your Server",
          "Connect VS Code with the Server",
          "Connect GitHub with the Server",
          "Run Streamlit on AWS Server",
          "Create S3 Full Access and Attach to EC2 Server",
          "Run Streamlit Application Automatically and Server Start Up",
          "Attach Permanent Fixed IP with EC2 Server"
        ]
      },
      "requirements": [
        "Introductory knowledge of NLP",
        "Comfortable in Python, Keras, and TensorFlow 2",
        "Basic Elementary Mathematics"
      ],
      "description": "Welcome to Production-Grade ML Model Deployment with FastAPI, AWS, Docker, and NGINX!\nUnlock the power of seamless ML model deployment with our comprehensive course, Production-Grade ML Model Deployment with FastAPI, AWS, Docker, and NGINX. This course is designed for data scientists, machine learning engineers, and cloud practitioners who are ready to take their models from development to production. You'll gain the skills needed to deploy, scale, and manage your machine learning models in real-world environments, ensuring they are robust, scalable, and secure.\nWhat You Will Learn:\nStreamline ML Operations with FastAPI: Master the art of serving machine learning models using FastAPI, one of the fastest-growing web frameworks. Learn to build robust RESTful APIs that facilitate quick and efficient model inference, ensuring your ML solutions are both accessible and scalable.\nHarness the Power of AWS for Scalable Deployments: Leverage AWS services like EC2, S3, ECR, and Fargate to deploy and manage your ML models in the cloud. Gain hands-on experience automating deployments with Boto3, integrating models with AWS infrastructure, and ensuring they are secure, reliable, and cost-efficient.\nContainerize Your Applications with Docker: Discover the flexibility of Docker to containerize your ML applications. Learn how to build, deploy, and manage Docker containers, ensuring your models run consistently across different environments, from development to production.\nBuild and Deploy End-to-End ML Pipelines: Understand the intricacies of ML Ops by constructing end-to-end machine learning pipelines. Explore data management, model monitoring, A/B testing, and more, ensuring your models perform optimally at every stage of the lifecycle.\nAutomate Deployments with Boto3: Automate the deployment of your ML models using Python and Boto3. From launching EC2 instances to managing S3 buckets, streamline cloud operations, making your deployments faster and more efficient.\nScale ML Models with NGINX: Learn to use NGINX with Docker-Compose to scale your ML applications across multiple instances, ensuring high availability and performance in production.\nDeploy Serverless ML Models with AWS Fargate: Dive into serverless deployment using AWS Fargate, and learn how to package, deploy, and manage ML models with AWS ECR and ECS for scalable, serverless applications.\nReal-World ML Use Cases: Apply your knowledge to real-world scenarios by deploying models for sentiment analysis, disaster tweet classification, and human pose estimation. Using cutting-edge transformers and computer vision techniques, you’ll gain practical experience in bringing AI to life.\nDeploy Interactive ML Applications with Streamlit: Create and deploy interactive web applications using Streamlit. Integrate your FastAPI-powered models into user-friendly interfaces, making your ML solutions accessible to non-technical users.\nMonitor and Optimize Production ML Models: Implement load testing, monitoring, and performance optimization techniques to ensure your models remain reliable and efficient in production environments.\n\n\nWhy This Course?\nIn today’s fast-paced tech landscape, the ability to deploy machine learning models into production is a highly sought-after skill. This course combines the latest technologies—FastAPI, AWS, Docker, NGINX, and Streamlit—into one powerful learning journey. Whether you're looking to advance your career or enhance your skill set, this course provides everything you need to deploy, scale, and manage production-grade ML models with confidence.\nBy the end of this course, you’ll have the expertise to deploy machine learning models that are not only effective but also scalable, secure, and ready for production in real-world environments. Join us and take the next step in your machine-learning journey!",
      "target_audience": [
        "Machine learning engineers who want to gain hands-on experience in setting up and configuring an end-to-end machine learning production pipeline.",
        "Data Scientists and Machine Learning Engineers: Professionals looking to advance their skills in deploying machine learning models in production environments using FastAPI, AWS, and Docker.",
        "Cloud Engineers and DevOps Professionals: Individuals who want to master cloud-based deployments, automate ML pipelines, and manage scalable infrastructure on AWS.",
        "Software Developers and Engineers: Developers interested in integrating machine learning models into applications and services, with a focus on API development and containerization.",
        "AI Enthusiasts and Practitioners: Anyone passionate about AI and machine learning who wants to gain hands-on experience in taking models from development to deployment.",
        "Tech Professionals Transitioning into ML Ops: IT professionals or developers transitioning into machine learning operations (ML Ops) who need practical knowledge of production-grade deployment and automation tools."
      ]
    },
    {
      "title": "Probability and Statistics for Business and Data Science",
      "url": "https://www.udemy.com/course/probability-and-statistics-for-business-and-data-science/",
      "bio": "Learn how to apply probability and statistics to real data science and business applications!",
      "objectives": [
        "Understand the basics of probability",
        "Be able to implement basic statistics",
        "Understand how to use various statistical distributions",
        "Apply statistical methods and hypothesis testing to business problems",
        "Understand how regression models work",
        "Implement one way and two way ANOVA",
        "Understand Chi Squared Tests",
        "Be able to understand different types of data"
      ],
      "course_content": {
        "Introduction": [
          "Course Overview Lecture - PLEASE DO NOT SKIP THIS!",
          "FAQ - Frequently Asked Questions"
        ],
        "Data": [
          "What is Data?",
          "Measuring Data",
          "Quiz #1 - Measuring Data",
          "Measurements of Central Tendency",
          "Quiz #2 - Measures of Central Tendency",
          "Measurements of Dispersion",
          "Quiz #3 - Measures of Dispersion",
          "Measurements - Quartiles",
          "Quiz #4 - Quartiles and IQR",
          "Bi-variate Data and Covariance",
          "Pearson Correlation Coefficient",
          "Section Assessment - Data"
        ],
        "Probability": [
          "What is Probability?",
          "Permutations",
          "Quiz #5 - Permutations",
          "Combinations",
          "Quiz #6 - Combinations",
          "Intersections, Unions, and Complements",
          "Independent and Dependent Events",
          "Quiz #7 - Independent & Dependent Events",
          "Conditional Probability",
          "Quiz #8 - Conditional Probability",
          "Addition and Multiplication Rules",
          "Bayes Theorem",
          "Quiz #9 - Bayes Theorem",
          "Section Assessment - Probability"
        ],
        "Distributions": [
          "Introduction to Distributions",
          "Uniform Distribution",
          "Quiz #10 - Uniform Distribution",
          "Binomial Distribution",
          "Quiz #11 - Binomial Distribution",
          "Poisson Distribution",
          "Quiz #12 - Poisson Distribution",
          "Normal Distribution",
          "Quiz #13 - Normal Distribution",
          "Normal Distribution - Formulas and Z Scores",
          "Quiz #14 - Z Score",
          "Section Assessment - Distributions",
          "Optional Resource - Dash Scripts"
        ],
        "Statistics": [
          "What is Statistics?",
          "Sampling",
          "Central Limit Theorem",
          "Quiz #15 - Sampling and CLT",
          "Standard Error",
          "Hypothesis Testing",
          "Hypothesis Testing Example Exercise #1",
          "Hypothesis Testing Example Exercise #2",
          "Quiz #16 - Hypothesis Testing #1",
          "Type 1 and Type 2 Errors",
          "Quiz #17 - Hypothesis Testing #2",
          "Student's T Distribution",
          "Student's T Distribution Example Exercise",
          "Section Assessment - Statistics"
        ],
        "Analysis of Variance (ANOVA)": [
          "Introduction to ANOVA",
          "ANOVA - Analysis of Variance",
          "F Distribution",
          "Two Way ANOVA Overview",
          "Two Way ANOVA Example Exercise",
          "Two Way ANOVA with Replication",
          "Section Assessment - ANOVA"
        ],
        "Regression": [
          "Linear Regression",
          "Regression Example",
          "Multiple Regression",
          "Section Assessment - Regression"
        ],
        "Chi-Square Analysis": [
          "Chi-Square Analysis",
          "Chi Squared Analysis - Exercise Example",
          "Section Assessment - Chi Square Analysis"
        ],
        "BONUS SECTION: THANK YOU!": [
          "BONUS LECTURE"
        ]
      },
      "requirements": [
        "Paper and Pencil to take notes",
        "Optional: Excel or Python to run simulations"
      ],
      "description": "Welcome to Probability and Statistics for Business and Data Science!\nIn this course we cover what you need to know about probability and statistics to succeed in business and the data science field!\nThis practical course will go over theory and implementation of statistics to real world problems. Each section has example problems, in course quizzes, and assessment tests.\nWe’ll start by talking about the basics of data, understanding how to examine it with measurements of central tendency, dispersion, and also building an understanding of how bivariate data sources can relate to each other.\nAfterwards we’ll dive into probability , learning about combinations and permutations, as well as conditional probability and how to apply bayes theorem.\nThen we’ll move on to discussing the most common distributions found in statistics, creating a solid foundation of understanding how to work with uniform, binomial, poisson, and normal distributions.\nUp next we’ll talk about statistics, applying what we’ve learned so far to real world business cases, including hypothesis testing and the student's T distribution.\nWe’ll end the course with 3 sections on advanced topics, such as ANOVA (analysis of variance), understanding regression analysis, and finally performing chi squared analysis.\nThe sections are modular and organized by topic, so you can reference what you need and jump right in!\nOur course includes HD Video with clear explanations and high quality animations, we also include extensive case studies to show you how to apply this knowledge to the real world.\nWe'll cover everything you need to know about statistics and probability to clearly tackle real world business and data science problems!\nIncluding:\nMeasurements of Data\nMean, Median, and Mode\nVariance and Standard Deviation\nCo-variance and Correlation\nPermutations and Combinations\nUnions and Intersections\nConditional Probability\nBayes Theorem\nBinomial Distribution\nPoisson Distribution\nNormal Distribution\nSampling\nCentral Limit Theorem\nHypothesis Testing\nT-Distribution Testing\nRegression Analysis\nANOVA\nChi Squared\nand much more!\nNot only do you get great technical content, but you’ll also have access to our online QA forums as well as our student chat channel. Where the TAs and myself are happy to help out with any questions you encounter! Upon finishing this course you’ll receive a certificate of completion you can post on your linkedin profile to show off to your colleagues, or even potential employers!\nAll of this content comes with a 30 day money back guarantee, so you can try out the course risk free!\n\n\nSo what are you waiting for? Enroll today and we'll see you inside the course!",
      "target_audience": [
        "Someone interested in learning how to apply probability and statistics to business or data science"
      ]
    },
    {
      "title": "Complete Pandas for Absolute Beginners",
      "url": "https://www.udemy.com/course/complete-pandas-for-absolute-beginners/",
      "bio": "Learn how to use the powerful Python pandas library to analyze and manipulate data.",
      "objectives": [
        "Perform a multitude of data operations in Python's popular pandas library including grouping, pivoting, joining and more!",
        "Group, aggregate and summarise your data.",
        "Learn everything there is to know about pandas - from absolute scratch!",
        "Gain a deep and hands-on understanding of pandas data structures.",
        "Practice reading data from the web, pickles, Excel files right within pandas."
      ],
      "course_content": {
        "Introduction": [
          "Introduction to Pandas"
        ],
        "Dataframes and Series": [
          "Pandas Dataframes and Series"
        ],
        "Indexes in Pandas": [
          "Indexes in Pandas"
        ],
        "Conditional Filtering in Pandas": [
          "Conditional Filtering in Pandas"
        ],
        "Update Rows and Columns in Pandas": [
          "Update Rows and Columns in Pandas"
        ],
        "Add/Remove Rows and Columns From Pandas DataFrames": [
          "Add/Remove Rows and Columns From Pandas DataFrames"
        ],
        "Data Sorting in Pandas": [
          "Data Sorting in Pandas"
        ],
        "Cleaning & Saving Pandas DataFrames": [
          "Cleaning & Saving Pandas DataFrames"
        ]
      },
      "requirements": [
        "No prior knowledge of pandas is required.",
        "Basic knowledge of Python",
        "Access to a computer with an internet connection."
      ],
      "description": "This course is designed for individuals with little or no experience with the Pandas library for Python. Pandas is a powerful and flexible open-source data analysis and manipulation tool that is widely used in data science and data analysis. This course will provide a comprehensive introduction to the library, starting with basic concepts and gradually building up to more advanced topics.\nThe course will begin by introducing the basics of Pandas, including its data structures (Series and DataFrames) and the various ways to import and export data. You will learn how to perform basic data cleaning and preprocessing tasks, including handling missing values, renaming columns, and filtering and sorting data. You will also learn how to use Pandas to perform basic statistical operations and data visualization.\nAs the course progresses, you will dive deeper into more advanced topics, such as merging and joining data, groupby operations, and advanced indexing techniques. You will also learn how to use Pandas to work with time series data, including how to handle and manipulate date and time data.\nThroughout the course, you will work with real-world data sets, giving you hands-on experience with the tools and techniques covered. You will also complete a number of practical exercises and projects, allowing you to apply what you've learned to real-world problems.\nBy the end of this course, you will have a solid understanding of the Pandas library and be able to use it confidently to perform data analysis and manipulation tasks. Whether you're a beginner looking to start a career in data science or an experienced data analyst looking to improve your skills, this course is the perfect starting point.\nPrerequisites: This course is designed for absolute beginners, and it will be helpful if you have basic knowledge of Python programming.\nCourse Outline:\nIntroduction to Pandas\nPandas Dataframes and Series\nIndexes in Pandas\nConditional Filtering in Pandas\nUpdate Rows and Columns in Pandas\nAdd/Remove Columns of Data\nMaster Data Sorting in Pandas\nClean & Save DataFrames\n\n\nBy the end of this course, you will be able to:\nUnderstand the basics of the Pandas library and its data structures\nImport and export data using Pandas\nPerform basic data cleaning and preprocessing tasks\nUse Pandas to perform basic statistical operations and data visualization\nMerge and join data using Pandas\nUse the groupby function in Pandas\nApply advanced indexing techniques in Pandas\nWork with time series data using Pandas\nApply your knowledge to real-world projects",
      "target_audience": [
        "If you are a complete beginner then this course will be everything you need to learn Pandas",
        "Python students that want to learn how to manipulate data professionally.",
        "Anyone looking to deeply understand and master pandas",
        "Aspiring data analysts and scientists looking to upgrade their skillset.",
        "Anyone interested in mastering data analysis with python"
      ]
    },
    {
      "title": "Machine Learning Classification Bootcamp in Python",
      "url": "https://www.udemy.com/course/machine-learning-classification/",
      "bio": "Build 10 Practical Projects and Advance Your Skills in Machine Learning Using Python and Scikit Learn",
      "objectives": [
        "Apply advanced machine learning models to perform sentiment analysis and classify customer reviews such as Amazon Alexa products reviews",
        "Understand the theory and intuition behind several machine learning algorithms",
        "Implement classification algorithms in Scikit-Learn for K-Nearest Neighbors, (SVM), Decision Trees, Random Forest, Naive Bayes, and Logistic Regression",
        "Build an e-mail spam classifier using Naive Bayes classification Technique",
        "Apply machine learning models to Healthcare applications such as Cancer and Kyphosis diseases classification",
        "Develop Models to predict customer behavior towards targeted Facebook Ads",
        "Classify data using K-Nearest Neighbors, Support Vector Machines (SVM), Decision Trees, Random Forest, Naive Bayes, and Logistic Regression",
        "Build an in-store feature to predict customer's size using their features",
        "Develop a fraud detection classifier using Machine Learning Techniques",
        "Master Python Seaborn library for statistical plots",
        "Understand the difference between Machine Learning, Deep Learning and Artificial Intelligence",
        "Perform feature engineering and clean your training and testing data to remove outliers",
        "Master Python and Scikit-Learn for Data Science and Machine Learning",
        "Learn to use Python Matplotlib library for data Plotting"
      ],
      "course_content": {
        "Introduction": [
          "Introduction and Welcome Message",
          "Introduction and Welcome Message [Course Material Download]",
          "EXTRA: Learning Paths",
          "Updates on Udemy Reviews",
          "Course Overview",
          "Get the Materials"
        ],
        "What is Machine Learning? The Big Picture": [
          "What is Machine Learning? The Big Picture Part #1",
          "What is Machine Learning? The Big Picture Part #2"
        ],
        "Installation & Setup [Optional][Skip if you are familiar with Jupyter Notebooks]": [
          "What is Anaconda and How to download it?",
          "What are Jupyter Notebooks?",
          "How to run a Jupyter Notebook?"
        ],
        "Logistic Regression": [
          "Logistic Regression Introduction and Learning Outcomes",
          "Logistic Regression Intuition",
          "Confusion Matrix Overview",
          "Logistic Regression - Project #1 - Project Overview",
          "Logistic Regression - Project #1 - Loading Data",
          "Logistic Regression - Project #1 - Visualization",
          "Logistic Regression - Project #1 - Data Cleaning",
          "Logistic Regression - Project #1 - Data Cleaning part 2",
          "Logistic Regression - Project #1 - Training",
          "Logistic Regression - Project #1 - Testing",
          "Logistic Regression - Project #2 Overview",
          "Logistic Regression - Project #2 - Importing data",
          "Logistic Regression - Project #2 - Data visualization",
          "Logistic Regression - Project #2 - Cleaning data",
          "Logistic Regression - Project #2 - Training/Testing",
          "Logistic Regression - Project #2 - Testing/Visualization"
        ],
        "Support Vector Machines": [
          "Support Vector Machines Intro and Learning Outcomes",
          "Support Vector Machines - Intuition",
          "Support Vector Machines - Project #1 - Project Overview",
          "Support Vector Machines - Project #1 - Importing data",
          "Support Vector Machines - Project #1 - Data Visualization",
          "Support Vector Machines - Project #1 - Training",
          "Support Vector Machines - Project #1 - Testing",
          "Support Vector Machines - Project #1 - Improvements 1",
          "Support Vector Machines - Project #1 - Improvements 2",
          "Project #2 Overview",
          "Support Vector Machines - Project #2 - Data import and visualization",
          "Support Vector Machines - Project #2 - Training and evaluating the model",
          "Support Vector Machines - Project #2 - Improvements 1",
          "Support Vector Machines - Project #2 - Improvements 2"
        ],
        "K-Nearest Neighbors": [
          "K-Nearest Neighbors Intro and Learning Outcomes",
          "K-Nearest Neighbors - Intuition",
          "KNN - Project #1 - Project overview",
          "KNN - Project #1 - Data import and cleaning",
          "KNN - Project #1 - Training/Testing",
          "KNN - Project #1 - Model visualization",
          "KNN - Project #2 Overview",
          "KNN - Project #2 - Data Visualization",
          "KNN - Project #2 - Training",
          "KNN - Project #2 - Evaluation"
        ],
        "Decision Trees and Random Forest": [
          "Decision Trees and Random Forest Intro and Learning Outcomes",
          "Decision Trees - Intuition",
          "Random Forest - Intuition",
          "Decision Trees & Random Forest - Project #1 - Project Overview",
          "Decision Trees & Random Forest - Project #1 - Importing data",
          "Decision Trees & Random Forest - Project #1 - Visualization",
          "Decision Trees & Random Forest - Project #1 - Feature Engineering 1",
          "Decision Trees & Random Forest - Project #1 - Feature Engineering 2",
          "Decision Trees & Random Forest - Project #1 - Training",
          "Decision Trees & Random Forest - Project #1 - Evaluation",
          "Decision Trees & Random Forest - Project #1 - Improvements",
          "Decision Trees & Random Forest - Project #2 Overview",
          "Decision Trees & Random Forest - Project #2 - Overview and set up",
          "Decision Trees & Random Forest - Project #2 - Data cleaning/Model training",
          "Decision Trees & Random Forest - Project #3 - Training/Testing",
          "Decision Trees & Random Forest - Project #2 - Evaluation"
        ],
        "Naive Bayes Classifiers": [
          "Naive Bayes Intro and Learning Outcomes",
          "Naive Bayes Intuition",
          "Naive Bayes - Mathematics",
          "Project #1 - Project overview",
          "Project #1 - Data Visualization",
          "Project #1 - Count vectorizer",
          "Project #1 - Training part 1",
          "Project #1 - Training part 2",
          "Project #1 - Testing",
          "Project #2 - Overview",
          "Project #2 - Importing data",
          "Project #2 - Training/Testing",
          "Project #2 - Testing",
          "Project #2 - Improvements"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic knowledge of Python Programming",
        "Experienced computer user"
      ],
      "description": "Are you ready to master Machine Learning techniques and Kick-off your career as a Data Scientist?!\nYou came to the right place!\nMachine Learning is one of the top skills to acquire in 2022, with an average salary of over $114,000 in the United States, according to PayScale! Over the past two years, the total number of ML jobs has grown around 600 percent and is expected to grow even more by 2025.\nThis course provides students with the knowledge and hands-on experience of state-of-the-art machine learning classification techniques such as\nLogistic Regression\nDecision Trees\nRandom Forest\nNaïve Bayes\nSupport Vector Machines (SVM)\nThis course will provide students with knowledge of key aspects of state-of-the-art classification techniques. We are going to build 10 projects from scratch using a real-world dataset. Here’s a sample of the projects we will be working on:\nBuild an e-mail spam classifier.\nPerform sentiment analysis and analyze customer reviews for Amazon Alexa products.\nPredict the survival rates of the titanic based on the passenger features.\nPredict customer behavior towards targeted marketing ads on Facebook.\nPredicting bank clients’ eligibility to retire given their features such as age and 401K savings.\nPredict cancer and Kyphosis diseases.\nDetect fraud in credit card transactions.\nKey Course Highlights:\nThis comprehensive machine learning course includes over 75 HD video lectures with over 11 hours of video content.\nThe course contains 10 practical hands-on python coding projects that students can add to their portfolio of projects.\nNo intimidating mathematics, we will cover the theory and intuition in a clear, simple, and easy way.\nAll Jupyter notebooks (codes) and slides are provided.\n10+ years of experience in machine learning and deep learning in both academic and industrial settings have been compiled in this course.\nStudents who enroll in this course will master machine learning classification models and can directly apply these skills to solve challenging real-world problems.",
      "target_audience": [
        "Data Science Enthusiasts wanting to enhance their machine learning skills",
        "Python programmers curious about Machine Learning and Data Science",
        "Programmers or developers who want to make a shift into the lucrative data science and machine learning career path",
        "Technologists wanting to gain an understanding of how machine learning models work",
        "Data analysts who want to transition into the Tech industry"
      ]
    },
    {
      "title": "Dell Boomi AtomSphere - IPaaS Beginner Training",
      "url": "https://www.udemy.com/course/dellboomi/",
      "bio": "Master the fundamentals of Dell Boomi AtomSphere",
      "objectives": [
        "Atom Configuration",
        "Profiles and Processes",
        "Different Connectors in Dell Boomi",
        "Process design Shapes",
        "Exception Handling",
        "Consuming SOAP and REST Services",
        "Building SOAP and REST in Boomi",
        "Deploy and Un-deploy process",
        "Scheduling the process",
        "Processing of Documents"
      ],
      "course_content": {
        "Dell Boomi Overview": [
          "Dell Boomi Introduction",
          "Boomi Trial Account",
          "MYSQL Installation",
          "Runtime Agent in Dell Boomi"
        ],
        "Dell Boomi Integration Process": [
          "Analyse your Data Set First",
          "First Integration Process",
          "Data Transformation and Mapping to Database",
          "Data Loading and Process Deployment"
        ],
        "Dell Boomi: Second Integration Process": [
          "Amazon S3 Integration With FTP Server Part-01",
          "Amazon S3 Integration With FTP Server Part-02",
          "Salesforce Integration With FTP Server Part-01",
          "Salesforce Integration With FTP Server Part-02"
        ],
        "Dell Boomi AtomSphere Shapes Explanation": [
          "Cleanse Shape and Return Document Shape Example",
          "Add to Cache Shape Example 1",
          "Add to Cache Shape Example 2",
          "Properties in Dell Boomi Part-01",
          "Properties in Dell Boomi Part-02",
          "Decision and Exception Shape Example",
          "Process Route Shape Example",
          "Business Rule Shape Example"
        ],
        "Web Services(SOAP & REST)": [
          "Consume SOAP Service",
          "Consume REST Service",
          "Build SOAP Service in Dell Boomi",
          "Build Rest Service in Dell Boomi"
        ]
      },
      "requirements": [
        "Very Basic Database/SQL concepts are required",
        "Tools: MYSQL Database",
        "All you need a Computer machine; windows, Mac, and Linux users are all welcome"
      ],
      "description": "What is Dell Boomi AtomSphere?\nDell Boomi is a cloud integration platform which is used to integrate different applications.\nWhat are we learning the in the course?\nDisk connector\nDatabase connector\nHTTP connector\nWebservice SOAP client connector\nWeb service Server Connector\nSet properties and much more\nAfter this Course:\nOnce your are done with the course, you will have maximum knowledge of Dell Boomi Cloud Integration platform and can easily apply concepts to create multiple different integrations.\nCareer Perspective:\nIf you want to  pursue a career in the field of Data Integration Engineer, Please enroll in course. In this course, we will make your foundation by starting from very basic and then later in the course, we'll cover some advance topics , so, in-case you are very new or have no knowledge of Cloud integration platforms ,by the end this course, you will attain maximum knowledge of this tool.\nAnother aspect of this course is that not only I am going through all the concepts but also give the practical demonstration. The pace of this course is very slow, means, I will emphasize ample time on the subject and will try to cover everything that I can.\nBoomi was founded in 2000, beginning with \"configuration-based\" integration. Its technology allows users to build and deploy integration processes using a visual interface and a drag and drop technique.\nHave a Great Learning..!!!",
      "target_audience": [
        "Professionals who want to learn Cloud Integration tool"
      ]
    },
    {
      "title": "Statistics for Data Analysis Using R",
      "url": "https://www.udemy.com/course/statistics-using-r/",
      "bio": "Learn Programming in R & R Studio • Descriptive, Inferential Statistics • Plots for Data Visualization • Data Science",
      "objectives": [
        "Learn R programming from the ground up, starting from the basics and progressing to advanced data analysis techniques.",
        "Learn the basic statistical concepts first, followed by practical application using R Studio, combining theory and practice for effective learning.",
        "Master descriptive statistics, including mean, mode, median, skewness, and kurtosis, and how to apply these concepts to your data analysis.",
        "Understand and perform inferential statistics such as one and two-sample z-tests, t-tests, Chi-Square tests, F-tests, ANOVA, and TukeyHSD, and more.",
        "Explore probability distributions, including normal, binomial, and Poisson, and their applications in data analysis.",
        "Develop the skills to perform data manipulation, visualization, and statistical analysis using R.",
        "Apply statistical concepts in real-world scenarios, enhancing your problem-solving abilities and decision-making skills.",
        "Boost your career prospects with a strong foundation in statistics and R programming, valuable skills in today’s data-driven job market.",
        "Equip yourself with the tools to handle large datasets and perform complex statistical analyses with confidence.",
        "Enhance your ability to make data-driven decisions by mastering the use of R for statistical analysis."
      ],
      "course_content": {
        "1. Getting Started with R and R Studio": [
          "Introduction - Section 1",
          "Installing R and R Studio (Windows)",
          "The First Look of R and R Studio. R you ready?",
          "The First Look at the Functions in R",
          "Saving the R Script File",
          "Data Types in R",
          "Simple Mathematical Operations",
          "Download - Section 1 Notes and Codes",
          "Section 1 - Practice Assignment"
        ],
        "2. Bonus Section: Descriptive Statistics Theory (lessons from my other course)": [
          "Introduction - Section 2",
          "Understanding Basic Statistical Terms (Theory)",
          "Descriptive Statistics (Theory)",
          "Measurement of Central Tendency (Theory)",
          "Measurement of Variation (Theory)",
          "Download - Section 2 Slides"
        ],
        "3. Descriptive Statistics Using R": [
          "Introduction - Section 3",
          "Getting Help",
          "Measurement of Central Tendency - Mean (Using R)",
          "Measurement of Central Tendency - Median and Mode (Using R)",
          "Measurement of Variation - Range, IQR and Standard Deviation (Using R)",
          "Download - Section 3 Notes and Codes",
          "Section 3 - Practice Assignment"
        ],
        "4. Vectors, Factors, Lists, Matrix and Data Frames in R": [
          "Introduction - Section 4",
          "Introduction",
          "Vectors Explained",
          "Factors Explained",
          "Lists Explained",
          "Matrix Explained",
          "Data Frames Explained",
          "Download - Section 4 Notes and Codes",
          "Section 4 - Practice Assignment"
        ],
        "5. Data Visualization": [
          "Introduction - Section 5",
          "Your first plot in R",
          "*** Scatter Plot ***",
          "Add the Plot Main and Axis Lebel Text",
          "Let's Draw Some Lines on the Plot",
          "Change the Plot Characters (pch) from Circles to Plus Signs",
          "Let's Look at Filtered Data",
          "One is not enough, I want more plots on a single page!",
          "Add text to the plot",
          "Make plot colorful, and text bigger and bold",
          "Multiple pairs of scatter diagrams - when one plot is not enough!",
          "Time Series Plot",
          "*** Histogram ***",
          "*** Box and Whisker Plot ***",
          "Download - Section 5 Notes and Codes",
          "Section 5 - Practice Assignment"
        ],
        "6. Descriptive Statistics Re-visited": [
          "Introduction - Section 6",
          "Descriptive Statistics Using psych Package",
          "Download - Section 6 Notes and Codes"
        ],
        "7. Bonus Section: Basic Probability Theory (lessons from my other course)": [
          "Introduction - Section 7",
          "Probability Definition",
          "Probability - Union and Intersection",
          "Probability - The Law of Addition, Multiplication and Conditional Probability",
          "Factorial, Permutations and Combinations",
          "Download - Section 7 Slides"
        ],
        "8. Probability Distributions": [
          "Introduction - Section 8",
          "Central Limit Theorem (Theory)",
          "Central Limit Theorem Demonstration Using R",
          "*** Normal Probability Distribution (Theory) ***",
          "R Functions for Normal Distribution - rnorm, pnorm, qnorm and dnorm",
          "Plotting Normal Distribution Using R Functions",
          "Introducting \"visualize\" Package",
          "*** Binomial Probability Distribution (Theory) ***",
          "R Functions for Binomial Distribution - rbinom, pbinom, qbinom and dbinom",
          "Plotting Binomial Distribution Using R Functions",
          "Binomial Distribution using Visualize Package",
          "*** Poisson Distribution (Theory) ***",
          "R Functions for Poisson Distribution - rpois, ppois, qpois and dpois",
          "Plotting Poisson Distribution Using R Functions",
          "Poisson Distribution using Visualize Package",
          "Download - Section 8 Notes and Codes"
        ],
        "9. Inferential Statistics - Hypothesis Tests": [
          "Introduction - Section 9",
          "Types of Mean and Variance Tests",
          "Hypothesis Testing - Types of Errors (Theory)",
          "What is p value? (Theory)",
          "*** Hypothesis Testing - One Sample Z Test (Theory) ***",
          "One Sample z Test Using R",
          "One Sample z Test using BSDA Package",
          "*** One Sample t Test (Theory) ***",
          "One Sample t Test Using R",
          "Visualizing One Sample t Test Results using Visualize Package",
          "*** One Sample Variance Test - Chi Square Test (Theory) ***",
          "One Sample Variance Test Using Envstats Package",
          "Chi Square Distribution for One Sample Variance Test",
          "*** Two Sample Z Test (Theory) ***",
          "Two Sample Z Test Using R",
          "Visualizing Two Sample Z Test Using Visualize Package",
          "Two Sample Z Test for Populations with Different Means",
          "*** Two Sample t Test (Theory) ***",
          "Two Sample t Test (Equal Variance) Using R",
          "Two Sample t Test (Unequal Variance) Using R",
          "*** Paired t Test (Theory) ***",
          "Paired t Test Using R",
          "*** Two Sample Variance Test Using F Test (Theory) ***",
          "Two Sample Variance Test (F Distribution) Using R",
          "Visualizing Two Sample Variance Test Results using Visualize Package",
          "*** ANOVA Introduction (Theory) ***",
          "Understanding the concept behind ANOVA without doing any calculation.",
          "Formulas and calculations in ANOVA (Theory)",
          "ANOVA Example Using Manual Calculations (Theory)",
          "Analysis of Variance (ANOVA) Using R",
          "Post-hoc Test - TukeyHSD",
          "*** Goodness of Fit Test (Theory) ***",
          "Goodness of Fit Test Using R - Example 1",
          "Goodness of Fit Test Using R - Example 2",
          "*** Contingency Tables (Theory) ***",
          "Contingency Table Using R - Example 1",
          "Contingency Table Using R - Example 2",
          "Download - Section 9 Notes and Codes"
        ],
        "Bonus Section": [
          "BONUS LECTURE"
        ]
      },
      "requirements": [
        "Basic school level mathematics will be helpful.",
        "You will need to download and install R and R Studio on your PC or laptop. Both R and R Studio are for Free Software."
      ],
      "description": "Perform simple or complex statistical calculations using R Programming! - You don't need to be a programmer for this :)\nLearn statistics, and apply these concepts in your workplace using R.\nThe course will teach you the basic concepts related to Statistics and Data Analysis,  and help you in applying these concepts. Various examples and data sets are used to explain the application.\nI will explain the basic theory first, and then I will show you how to use R to perform these calculations.\nThe following areas of statistics are covered:\nDescriptive Statistics - Mean, Mode, Median, Quartile, Range, Inter Quartile Range, Standard Deviation. (Using base R function and the psych package)\nData Visualization - 3 commonly used charts: Histogram, Box and Whisker Plot and Scatter Plot (using base R commands)\nProbability - Basic Concepts, Permutations, Combinations (Basic theory only)\nPopulation and Sampling - Basic concepts (theory only)\nProbability Distributions - Normal, Binomial  and Poisson Distributions (Base R functions and the visualize package)\nHypothesis Testing - One Sample and Two Samples - z Test, t-Test, F Test, Chi-Square Test\nANOVA - Perform Analysis of Variance (ANOVA) step by step doing the manual calculation and by using R.\n\n\nWhat are other students saying about this course?\nThis course is a perfect mix of theory and practice. I highly recommend it for those who want to not only get good with R, but to also become proficient in statistics. (5 stars by Aaron Verive)\nYou get both the “how” and “why” for both the statistics and R programming. I’m really happy with this course. (5 stars by Elizabeth Crook)\nSandeep has such a clear approach, pedagogic and explains everything he does. Perfect for a novice like myself. (5 stars by Hashim Al-Haboobi)\nVery clear explanation. Coming from a non-technical background, it is immensely helpful that Prof. Sandeep Kumar is explaining all the minor details to prevent any scope for confusion. (5 stars by Ann Mary Biju)\nI had a limited background in R and statistics going into this course. I feel like this gave me the perfect foundation to progress to more complex topics in both of those areas. I'm very happy I took this course. (5 stars by Thach Phan)\nDr. Kumar is a fantastic teacher who takes you step by step. Can't say enough about his approach. Detailed. Not only clear descriptions of statistics but you will learn many details that make R easier to use and understand. (5 stars by James Reynolds)\nThis is a wonderful course, I do recommend it. The best Udemy course I took. (5 stars by Joao Alberto Arantes Do Amaral)\nThe course exceeded my expectations and i would like to thank the instructor Mr Sandeep Kumar for creating such an amazing course. The best thing about this course is the Theory incorporated that helps you understand what you are going to code in R. I have really learnt a lot. If you a looking for the best course for R then look no further because this is the best there can be. (5 stars by Kipchumba Brian)\n\n\nWhat are you waiting for?\nThis course comes with Udemy's 30 days money-back guarantee. If you are not satisfied with the course, get your money back.\nI hope to see you in the course.",
      "target_audience": [
        "Anyone who want to use statistics to make fact based decisions.",
        "Anyone who wants to learn R and R Studio for career in data science.",
        "Anyone who thinks Statistics is confusing and wants to learn it in plain and simple language."
      ]
    },
    {
      "title": "Machine Learning for Absolute Beginners - Level 2",
      "url": "https://www.udemy.com/course/machine-learning-for-absolute-beginners-level-2/",
      "bio": "Learn the Python Fundamentals and Pandas Library for Data Science Projects",
      "objectives": [
        "Python for Data Science and Machine Learning Projects",
        "Learn to use the Pandas Data Science Library",
        "JupyterLab Development tool",
        "Develop Jupyter Notebooks",
        "Loading and Analysing Tabular Datasets",
        "Selecting, Filtering, and Cleaning Data",
        "Grouping, Sorting, and Exporting Data"
      ],
      "course_content": {
        "Getting Started with Level 2!": [
          "Welcome!",
          "Anaconda Installation",
          "JupyterLab Overview",
          "Working with a Jupyter Notebook"
        ],
        "Python Fundamentals for Data Science": [
          "Overview",
          "Variables and Data Types",
          "Strings",
          "Lists",
          "IF and For-Loop Statements",
          "Functions",
          "Dictionaries",
          "Classes, Objects, Attributes, and Methods",
          "Importing Modules",
          "Libraries for Data Science Projects",
          "Exercise #1 - Python Fundamentals"
        ],
        "Introduction to the Pandas Library": [
          "Overview",
          "Series Data Structure (1D)",
          "DataFrame Data Structure (2D)",
          "Data Selection in a DataFrame",
          "Exercise #2 – Pandas Series and DataFrame"
        ],
        "Loading Data into a DataFrame": [
          "Overview",
          "Kaggle and the Titanic Dataset",
          "Loading a Tabular Data File",
          "Adjusting the Loading Parameters",
          "Preview the DataFrame",
          "Using Summary Statistics",
          "The Concept of Methods Chaining",
          "Sorting and Ranking",
          "Filtering",
          "Grouping",
          "Exercise #3 – Data Loading and Analysis"
        ],
        "Data Cleaning and Transformation": [
          "Overview",
          "Removing Columns or Rows",
          "Removing Duplicate Rows",
          "Renaming Column Labels",
          "Dropping Missing Values",
          "Filling-in Missing Values",
          "Creating Dummy Variables",
          "Exporting Data into Files",
          "Exercise #4 – Data Cleaning and Transformation"
        ],
        "Course Summary": [
          "Let's Recap and Thank You!",
          "*** BONUS ***"
        ]
      },
      "requirements": [
        "There are no specific prerequisites for starting this training as it is designed for absolute beginners.",
        "It is recommended to start with \"Machine Learning for Absolute Beginners - Level 1\""
      ],
      "description": "********* Feedback from Students ************\nThere's not much I can say. This instructor knows how to teach people, especially beginners. One of the best I've come across so far! Irlon T.\nExcellent course, allows you to go step by step and get to know the bookcase, direct to the point thank you for such good teaching Jesus David\nI am currently new in data science, I knew Python language and Competitive programming after that I was waiting to learn data science by myself. This course really very helpful for me to learn new things and data science and machine learning. I am really waiting for this new part (Level 3). Animesh K.\nA fantastic course for getting the knowledge of pandas in depth along with ML applications. This is what I am searching for. It had better if it has some more quiz questions to test our skills and understanding. Enjoyed a lot. Anshika Verma\n***********************************************\nUnleash the Power of ML\nMachine Learning is one of the most exciting fields in the hi-tech industry, gaining momentum in various applications. Companies are looking for data scientists, data engineers, and ML experts to develop products, features, and projects that will help them unleash the power of machine learning. As a result, a data scientist is one of the top ten wanted jobs worldwide!\nMachine Learning for Absolute Beginners\nThe “Machine Learning for Absolute Beginners” training program is designed for beginners looking to understand the theoretical side of machine learning and to enter the practical side of data science. The training is divided into multiple levels, and each level is covering a group of related topics for continuous step-by-step learning.\nLevel 2 - Python and Pandas\nThe second course, as part of the training program, aims to help you start your practical journey. You will learn the Python fundamentals and the amazing Pandas data science library, including:\nPython syntax for developing data science projects\nUsing JupyterLab tool for Jupiter notebooks\nLoading large datasets from files using Pandas\nPerform data analysis and exploration\nPerform data cleaning and transformation as a pre-processing step before moving into machine learning algorithms.\nEach section has a summary exercise as well as a complete solution to practice new knowledge.\nThe Game just Started!\nEnroll in the training program and start your journey to become a data scientist!",
      "target_audience": [
        "Developers curious about data science projects",
        "Beginner Data Scientists",
        "AI Product Managers",
        "ML Engineers",
        "AI/ML Consultants"
      ]
    },
    {
      "title": "Scala and Spark for Big Data and Machine Learning",
      "url": "https://www.udemy.com/course/scala-and-spark-for-big-data-and-machine-learning/",
      "bio": "Learn the latest Big Data technology - Spark and Scala, including Spark 2.0 DataFrames!",
      "objectives": [
        "Use Scala for Programming",
        "Use Spark 2.0 DataFrames to read and manipulate data",
        "Use Spark to Process Large Datasets",
        "Understand hot to use Spark on AWS and DataBricks"
      ],
      "course_content": {
        "Course Introduction": [
          "Introduction",
          "Course FAQs",
          "Scala and Spark Overview"
        ],
        "Scala IDE Options and Overview": [
          "ScalaIDE Overview",
          "Computer Set-up Time!"
        ],
        "Windows Scala and Spark Set-up and Installation": [
          "Windows Introduction",
          "Quick note about Windows Installation.",
          "Windows Scala and Spark Installation",
          "Atom Windows Installation",
          "Terminal Exericse"
        ],
        "Mac OS Setup and Installation": [
          "Mac OS Installation and Setup"
        ],
        "Linux (Ubuntu) Setup and Installation": [
          "Installing Scala and Spark on Linux (Ubuntu)"
        ],
        "Scala Programming: Level One": [
          "Arithmetic and Numbers",
          "Values and Variables",
          "Booleans and Comparison Operators",
          "Strings and Basic Regex",
          "Tuples",
          "Scala Basics - Assessment Test Exercises",
          "Scala Basics Assessment Test Questions",
          "Scala Basics - Assessment Test Solutions"
        ],
        "Collections": [
          "Intro to Collections",
          "Lists",
          "Arrays",
          "Sets",
          "Maps",
          "Collections - Assessment Test Exercise",
          "Scala Collections Assessment Test",
          "Collections Assessment Test - Solutions"
        ],
        "Scala Programming: Level Two": [
          "Flow Control",
          "For Loops",
          "While Loops",
          "Functions",
          "Scala Programming Exercises",
          "Scala Programming Exercises - Solutions"
        ],
        "Spark DataFrames with Scala": [
          "Quick Note for Windows Users!",
          "Introduction to Spark DataFrames",
          "DataFrames Overview",
          "Spark DataFrame Operations",
          "GroupBy and Aggregate Functions",
          "Missing data",
          "Date and Timestamps",
          "Quick Note on DataFrame Project",
          "DataFrame Project Exercises",
          "DataFrame Project - Solutions"
        ],
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning",
          "Machine Learning with Spark",
          "IntelliJ IDEA Installation Overview"
        ]
      },
      "requirements": [
        "Basic Programming Knowledge in some language",
        "Basic Math Skills",
        "English Language"
      ],
      "description": "Learn how to utilize some of the most valuable tech skills on the market today, Scala and Spark! In this course we will show you how to use Scala and Spark to analyze Big Data.\n\nScala and Spark are two of the most in demand skills right now, and with this course you can learn them quickly and easily! This course comes packed with content:\nCrash Course in Scala Programming\nSpark and Big Data Ecosystem Overview\nUsing Spark's MLlib for Machine Learning\nScale up Spark jobs using Amazon Web Services\nLearn how to use Databrick's Big Data Platform\nand much more!\nThis course comes with full projects for you including topics such as analyzing financial data or using machine learning to classify Ecommerce customer behavior! We teach the latest methodologies of Spark 2.0 so you can learn how to use SparkSQL, Spark DataFrames, and Spark's MLlib!\nAfter completing this course you will feel comfortable putting Scala and Spark on your resume!\nThanks and I will see you inside the course!",
      "target_audience": [
        "Someone who already knows how to program and is interested in learning Big Data Technologies",
        "Interested in using Spark with Scala for Machine Learning with Large Data Sets"
      ]
    },
    {
      "title": "TensorFlow for Deep Learning Bootcamp",
      "url": "https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/",
      "bio": "Learn TensorFlow by Google. Become an AI, Machine Learning, and Deep Learning expert!",
      "objectives": [
        "Build TensorFlow models using Computer Vision, Convolutional Neural Networks and Natural Language Processing",
        "Complete access to ALL interactive notebooks and ALL course slides as downloadable guides",
        "Increase your skills in Machine Learning, Artificial Intelligence, and Deep Learning",
        "Understand how to integrate Machine Learning into tools and applications",
        "Learn to build all types of Machine Learning Models using the latest TensorFlow 2",
        "Build image recognition, text recognition algorithms with deep neural networks and convolutional neural networks",
        "Using real world images to visualize the journey of an image through convolutions to understand how a computer “sees” information, plot loss and accuracy",
        "Applying Deep Learning for Time Series Forecasting",
        "Gain the skills you need to become a TensorFlow Developer",
        "Be recognized as a top candidate for recruiters seeking TensorFlow developers"
      ],
      "course_content": {
        "Introduction": [
          "Course Outline",
          "Join Our Online Classroom!",
          "Exercise: Meet Your Classmates & Instructor",
          "All Course Resources + Asking Questions + Getting Help",
          "ZTM Resources"
        ],
        "Deep Learning and TensorFlow Fundamentals": [
          "What is deep learning?",
          "Why use deep learning?",
          "What are neural networks?",
          "Python + Machine Learning Monthly",
          "What is deep learning already being used for?",
          "What is and why use TensorFlow?",
          "What is a Tensor?",
          "What we're going to cover throughout the course",
          "How to approach this course",
          "Need A Refresher?",
          "Creating your first tensors with TensorFlow and tf.constant()",
          "Creating tensors with TensorFlow and tf.Variable()",
          "Creating random tensors with TensorFlow",
          "Shuffling the order of tensors",
          "Creating tensors from NumPy arrays",
          "Getting information from your tensors (tensor attributes)",
          "Indexing and expanding tensors",
          "Manipulating tensors with basic operations",
          "Matrix multiplication with tensors part 1",
          "Matrix multiplication with tensors part 2",
          "Matrix multiplication with tensors part 3",
          "Changing the datatype of tensors",
          "Tensor aggregation (finding the min, max, mean & more)",
          "Tensor troubleshooting example (updating tensor datatypes)",
          "Finding the positional minimum and maximum of a tensor (argmin and argmax)",
          "Squeezing a tensor (removing all 1-dimension axes)",
          "One-hot encoding tensors",
          "Trying out more tensor math operations",
          "Exploring TensorFlow and NumPy's compatibility",
          "Making sure our tensor operations run really fast on GPUs",
          "TensorFlow Fundamentals challenge, exercises & extra-curriculum",
          "Monthly Coding Challenges, Free Resources and Guides",
          "LinkedIn Endorsements"
        ],
        "Neural network regression with TensorFlow": [
          "Introduction to Neural Network Regression with TensorFlow",
          "Inputs and outputs of a neural network regression model",
          "Anatomy and architecture of a neural network regression model",
          "Creating sample regression data (so we can model it)",
          "Note: Code update for upcoming lecture(s) for TensorFlow 2.7.0+ fix",
          "The major steps in modelling with TensorFlow",
          "Steps in improving a model with TensorFlow part 1",
          "Steps in improving a model with TensorFlow part 2",
          "Steps in improving a model with TensorFlow part 3",
          "Evaluating a TensorFlow model part 1 (\"visualise, visualise, visualise\")",
          "Evaluating a TensorFlow model part 2 (the three datasets)",
          "Evaluating a TensorFlow model part 3 (getting a model summary)",
          "Evaluating a TensorFlow model part 4 (visualising a model's layers)",
          "Evaluating a TensorFlow model part 5 (visualising a model's predictions)",
          "Evaluating a TensorFlow model part 6 (common regression evaluation metrics)",
          "Evaluating a TensorFlow regression model part 7 (mean absolute error)",
          "Evaluating a TensorFlow regression model part 7 (mean square error)",
          "Setting up TensorFlow modelling experiments part 1 (start with a simple model)",
          "Setting up TensorFlow modelling experiments part 2 (increasing complexity)",
          "Comparing and tracking your TensorFlow modelling experiments",
          "How to save a TensorFlow model",
          "How to load and use a saved TensorFlow model",
          "(Optional) How to save and download files from Google Colab",
          "Putting together what we've learned part 1 (preparing a dataset)",
          "Putting together what we've learned part 2 (building a regression model)",
          "Putting together what we've learned part 3 (improving our regression model)",
          "Preprocessing data with feature scaling part 1 (what is feature scaling?)",
          "Preprocessing data with feature scaling part 2 (normalising our data)",
          "Preprocessing data with feature scaling part 3 (fitting a model on scaled data)",
          "TensorFlow Regression challenge, exercises & extra-curriculum",
          "Learning Guideline"
        ],
        "Neural network classification in TensorFlow": [
          "Introduction to neural network classification in TensorFlow",
          "Example classification problems (and their inputs and outputs)",
          "Input and output tensors of classification problems",
          "Typical architecture of neural network classification models with TensorFlow",
          "Creating and viewing classification data to model",
          "Checking the input and output shapes of our classification data",
          "Building a not very good classification model with TensorFlow",
          "Trying to improve our not very good classification model",
          "Creating a function to view our model's not so good predictions",
          "Note: Updates for TensorFlow 2.7.0",
          "Make our poor classification model work for a regression dataset",
          "Non-linearity part 1: Straight lines and non-straight lines",
          "Non-linearity part 2: Building our first neural network with non-linearity",
          "Non-linearity part 3: Upgrading our non-linear model with more layers",
          "Non-linearity part 4: Modelling our non-linear data once and for all",
          "Non-linearity part 5: Replicating non-linear activation functions from scratch",
          "Getting great results in less time by tweaking the learning rate",
          "Using the TensorFlow History object to plot a model's loss curves",
          "Using callbacks to find a model's ideal learning rate",
          "Training and evaluating a model with an ideal learning rate",
          "Introducing more classification evaluation methods",
          "Finding the accuracy of our classification model",
          "Creating our first confusion matrix (to see where our model is getting confused)",
          "Making our confusion matrix prettier",
          "Putting things together with multi-class classification part 1: Getting the data",
          "Multi-class classification part 2: Becoming one with the data",
          "Multi-class classification part 3: Building a multi-class classification model",
          "Multi-class classification part 4: Improving performance with normalisation",
          "Multi-class classification part 5: Comparing normalised and non-normalised data",
          "Multi-class classification part 6: Finding the ideal learning rate",
          "Multi-class classification part 7: Evaluating our model",
          "Multi-class classification part 8: Creating a confusion matrix",
          "Multi-class classification part 9: Visualising random model predictions",
          "What \"patterns\" is our model learning?",
          "TensorFlow classification challenge, exercises & extra-curriculum"
        ],
        "Computer Vision and Convolutional Neural Networks in TensorFlow": [
          "Introduction to Computer Vision with TensorFlow",
          "Introduction to Convolutional Neural Networks (CNNs) with TensorFlow",
          "Downloading an image dataset for our first Food Vision model",
          "Becoming One With Data",
          "Becoming One With Data Part 2",
          "Becoming One With Data Part 3",
          "Building an end to end CNN Model",
          "Using a GPU to run our CNN model 5x faster",
          "Trying a non-CNN model on our image data",
          "Improving our non-CNN model by adding more layers",
          "Breaking our CNN model down part 1: Becoming one with the data",
          "Breaking our CNN model down part 2: Preparing to load our data",
          "Breaking our CNN model down part 3: Loading our data with ImageDataGenerator",
          "Breaking our CNN model down part 4: Building a baseline CNN model",
          "Breaking our CNN model down part 5: Looking inside a Conv2D layer",
          "Breaking our CNN model down part 6: Compiling and fitting our baseline CNN",
          "Breaking our CNN model down part 7: Evaluating our CNN's training curves",
          "Breaking our CNN model down part 8: Reducing overfitting with Max Pooling",
          "Breaking our CNN model down part 9: Reducing overfitting with data augmentation",
          "Breaking our CNN model down part 10: Visualizing our augmented data",
          "Breaking our CNN model down part 11: Training a CNN model on augmented data",
          "Breaking our CNN model down part 12: Discovering the power of shuffling data",
          "Breaking our CNN model down part 13: Exploring options to improve our model",
          "Downloading a custom image to make predictions on",
          "Writing a helper function to load and preprocessing custom images",
          "Making a prediction on a custom image with our trained CNN",
          "Multi-class CNN's part 1: Becoming one with the data",
          "Multi-class CNN's part 2: Preparing our data (turning it into tensors)",
          "Multi-class CNN's part 3: Building a multi-class CNN model",
          "Multi-class CNN's part 4: Fitting a multi-class CNN model to the data",
          "Multi-class CNN's part 5: Evaluating our multi-class CNN model",
          "Multi-class CNN's part 6: Trying to fix overfitting by removing layers",
          "Multi-class CNN's part 7: Trying to fix overfitting with data augmentation",
          "Multi-class CNN's part 8: Things you could do to improve your CNN model",
          "Multi-class CNN's part 9: Making predictions with our model on custom images",
          "Saving and loading our trained CNN model",
          "TensorFlow computer vision and CNNs challenge, exercises & extra-curriculum"
        ],
        "Transfer Learning in TensorFlow Part 1: Feature extraction": [
          "What is and why use transfer learning?",
          "Downloading and preparing data for our first transfer learning model",
          "Introducing Callbacks in TensorFlow and making a callback to track our models",
          "Exploring the TensorFlow Hub website for pretrained models",
          "Building and compiling a TensorFlow Hub feature extraction model",
          "Blowing our previous models out of the water with transfer learning",
          "Plotting the loss curves of our ResNet feature extraction model",
          "Building and training a pre-trained EfficientNet model on our data",
          "Different Types of Transfer Learning",
          "Comparing Our Model's Results",
          "TensorFlow Transfer Learning Part 1 challenge, exercises & extra-curriculum",
          "Exercise: Imposter Syndrome"
        ],
        "Transfer Learning in TensorFlow Part 2: Fine tuning": [
          "Introduction to Transfer Learning in TensorFlow Part 2: Fine-tuning",
          "Importing a script full of helper functions (and saving lots of space)",
          "Downloading and turning our images into a TensorFlow BatchDataset",
          "Discussing the four (actually five) modelling experiments we're running",
          "Comparing the TensorFlow Keras Sequential API versus the Functional API",
          "Note: Fixes for EfficientNetB0 model creation + weight loading",
          "Creating our first model with the TensorFlow Keras Functional API",
          "Compiling and fitting our first Functional API model",
          "Getting a feature vector from our trained model",
          "Drilling into the concept of a feature vector (a learned representation)",
          "Downloading and preparing the data for Model 1 (1 percent of training data)",
          "Building a data augmentation layer to use inside our model",
          "Note: Small fix for next video, for images not augmenting",
          "Visualizing what happens when images pass through our data augmentation layer",
          "Building Model 1 (with a data augmentation layer and 1% of training data)",
          "Building Model 2 (with a data augmentation layer and 10% of training data)",
          "Creating a ModelCheckpoint to save our model's weights during training",
          "Fitting and evaluating Model 2 (and saving its weights using ModelCheckpoint)",
          "Loading and comparing saved weights to our existing trained Model 2",
          "Preparing Model 3 (our first fine-tuned model)",
          "Fitting and evaluating Model 3 (our first fine-tuned model)",
          "Comparing our model's results before and after fine-tuning",
          "Downloading and preparing data for our biggest experiment yet (Model 4)",
          "Preparing our final modelling experiment (Model 4)",
          "Fine-tuning Model 4 on 100% of the training data and evaluating its results",
          "Comparing our modelling experiment results in TensorBoard",
          "How to view and delete previous TensorBoard experiments",
          "Transfer Learning in TensorFlow Part 2 challenge, exercises and extra-curriculum"
        ],
        "Transfer Learning with TensorFlow Part 3: Scaling Up": [
          "Introduction to Transfer Learning Part 3: Scaling Up",
          "Getting helper functions ready and downloading data to model",
          "Outlining the model we're going to build and building a ModelCheckpoint callback",
          "Creating a data augmentation layer to use with our model",
          "Creating a headless EfficientNetB0 model with data augmentation built in",
          "Fitting and evaluating our biggest transfer learning model yet",
          "Unfreezing some layers in our base model to prepare for fine-tuning",
          "Fine-tuning our feature extraction model and evaluating its performance",
          "Saving and loading our trained model",
          "Downloading a pretrained model to make and evaluate predictions with",
          "Making predictions with our trained model on 25,250 test samples",
          "Unravelling our test dataset for comparing ground truth labels to predictions",
          "Confirming our model's predictions are in the same order as the test labels",
          "Creating a confusion matrix for our model's 101 different classes",
          "Evaluating every individual class in our dataset",
          "Plotting our model's F1-scores for each separate class",
          "Creating a function to load and prepare images for making predictions",
          "Making predictions on our test images and evaluating them",
          "Discussing the benefits of finding your model's most wrong predictions",
          "Writing code to uncover our model's most wrong predictions",
          "Plotting and visualising the samples our model got most wrong",
          "Making predictions on and plotting our own custom images",
          "Transfer Learning in TensorFlow Part 3 challenge, exercises and extra-curriculum"
        ],
        "Milestone Project 1: Food Vision Big™": [
          "Introduction to Milestone Project 1: Food Vision Big™",
          "Making sure we have access to the right GPU for mixed precision training",
          "Getting helper functions ready",
          "Introduction to TensorFlow Datasets (TFDS)",
          "Exploring and becoming one with the data (Food101 from TensorFlow Datasets)",
          "Creating a preprocessing function to prepare our data for modelling",
          "Batching and preparing our datasets (to make them run fast)",
          "Exploring what happens when we batch and prefetch our data",
          "Creating modelling callbacks for our feature extraction model",
          "Note: Mixed Precision producing errors for TensorFlow 2.5+",
          "Turning on mixed precision training with TensorFlow",
          "Creating a feature extraction model capable of using mixed precision training",
          "Checking to see if our model is using mixed precision training layer by layer",
          "Training and evaluating a feature extraction model (Food Vision Big™)",
          "Introducing your Milestone Project 1 challenge: build a model to beat DeepFood",
          "Milestone Project 1: Food Vision Big™, exercises and extra-curriculum"
        ],
        "NLP Fundamentals in TensorFlow": [
          "Welcome to natural language processing with TensorFlow!",
          "Introduction to Natural Language Processing (NLP) and Sequence Problems",
          "Example NLP inputs and outputs",
          "The typical architecture of a Recurrent Neural Network (RNN)",
          "Preparing a notebook for our first NLP with TensorFlow project",
          "Becoming one with the data and visualising a text dataset",
          "Splitting data into training and validation sets",
          "Converting text data to numbers using tokenisation and embeddings (overview)",
          "Setting up a TensorFlow TextVectorization layer to convert text to numbers",
          "Mapping the TextVectorization layer to text data and turning it into numbers",
          "Creating an Embedding layer to turn tokenised text into embedding vectors",
          "Discussing the various modelling experiments we're going to run",
          "Model 0: Building a baseline model to try and improve upon",
          "Creating a function to track and evaluate our model's results",
          "Model 1: Building, fitting and evaluating our first deep model on text data",
          "Visualising our model's learned word embeddings with TensorFlow's projector tool",
          "High-level overview of Recurrent Neural Networks (RNNs) + where to learn more",
          "Model 2: Building, fitting and evaluating our first TensorFlow RNN model (LSTM)",
          "Model 3: Building, fitting and evaluating a GRU-cell powered RNN",
          "Model 4: Building, fitting and evaluating a bidirectional RNN model",
          "Discussing the intuition behind Conv1D neural networks for text and sequences",
          "Model 5: Building, fitting and evaluating a 1D CNN for text",
          "Using TensorFlow Hub for pretrained word embeddings (transfer learning for NLP)",
          "Model 6: Building, training and evaluating a transfer learning model for NLP",
          "Preparing subsets of data for model 7 (same as model 6 but 10% of data)",
          "Model 7: Building, training and evaluating a transfer learning model on 10% data",
          "Fixing our data leakage issue with model 7 and retraining it",
          "Comparing all our modelling experiments evaluation metrics",
          "Uploading our model's training logs to TensorBoard and comparing them",
          "Saving and loading in a trained NLP model with TensorFlow",
          "Downloading a pretrained model and preparing data to investigate predictions",
          "Visualising our model's most wrong predictions",
          "Making and visualising predictions on the test dataset",
          "Understanding the concept of the speed/score tradeoff",
          "NLP Fundamentals in TensorFlow challenge, exercises and extra-curriculum"
        ]
      },
      "requirements": [
        "Mac / Windows / Linux - all operating systems work with this course!",
        "No previous TensorFlow knowledge required. Basic understanding of Machine Learning is helpful"
      ],
      "description": "Just launched with all modern best practices for building neural networks with TensorFlow and becoming a TensorFlow & Deep Learning Expert!\nJoin a live online community of over 900,000+ students and a course taught by a TensorFlow expert. This course will take you from absolute beginner with TensorFlow, to creating state-of-the-art deep learning neural networks.\n\n\nTensorFlow experts earn up to $204,000 USD a year, with the average salary hovering around $148,000 USD. By taking this course you will be joining the growing Machine Learning industry and becoming a top paid TensorFlow Developer!\n\nHere is a full course breakdown of everything we will teach (yes, it's very comprehensive, but don't be intimidated, as we will teach you everything from scratch!):\n\nThe goal of this course is to teach you all the skills necessary for you to become a top 10% TensorFlow Developer.\n\nThis course will be very hands on and project based. You won't just be staring at us teach, but you will actually get to experiment, do exercises, and build machine learning models and projects to mimic real life scenarios. By the end of it all, you will develop skillsets needed to develop modern deep learning solutions that big tech companies encounter.\n\n\n0 — TensorFlow Fundamentals\nIntroduction to tensors (creating tensors)\nGetting information from tensors (tensor attributes)\nManipulating tensors (tensor operations)\nTensors and NumPy\nUsing @tf.function (a way to speed up your regular Python functions)\nUsing GPUs with TensorFlow\n\n\n\n1 — Neural Network Regression with TensorFlow\nBuild TensorFlow sequential models with multiple layers\nPrepare data for use with a machine learning model\nLearn the different components which make up a deep learning model (loss function, architecture, optimization function)\nLearn how to diagnose a regression problem (predicting a number) and build a neural network for it\n\n\n2 — Neural Network Classification with TensorFlow\nLearn how to diagnose a classification problem (predicting whether something is one thing or another)\nBuild, compile & train machine learning classification models using TensorFlow\nBuild and train models for binary and multi-class classification\nPlot modelling performance metrics against each other\nMatch input (training data shape) and output shapes (prediction data target)\n\n\n\n3 — Computer Vision and Convolutional Neural Networks with TensorFlow\nBuild convolutional neural networks with Conv2D and pooling layers\nLearn how to diagnose different kinds of computer vision problems\nLearn to how to build computer vision neural networks\nLearn how to use real-world images with your computer vision models\n\n\n\n4 — Transfer Learning with TensorFlow Part 1: Feature Extraction\nLearn how to use pre-trained models to extract features from your own data\nLearn how to use TensorFlow Hub for pre-trained models\nLearn how to use TensorBoard to compare the performance of several different models\n\n\n\n5 — Transfer Learning with TensorFlow Part 2: Fine-tuning\nLearn how to setup and run several machine learning experiments\nLearn how to use data augmentation to increase the diversity of your training data\nLearn how to fine-tune a pre-trained model to your own custom problem\nLearn how to use Callbacks to add functionality to your model during training\n\n\n6 — Transfer Learning with TensorFlow Part 3: Scaling Up (Food Vision mini)\nLearn how to scale up an existing model\nLearn to how evaluate your machine learning models by finding the most wrong predictions\nBeat the original Food101 paper using only 10% of the data\n\n\n7 — Milestone Project 1: Food Vision\nCombine everything you've learned in the previous 6 notebooks to build Food Vision: a computer vision model able to classify 101 different kinds of foods. Our model well and truly beats the original Food101 paper.\n\n\n\n8 — NLP Fundamentals in TensorFlow\nLearn to:\nPreprocess natural language text to be used with a neural network\nCreate word embeddings (numerical representations of text) with TensorFlow\nBuild neural networks capable of binary and multi-class classification using:\nRNNs (recurrent neural networks)\nLSTMs (long short-term memory cells)\nGRUs (gated recurrent units)\nCNNs\nLearn how to evaluate your NLP models\n\n\n\n9 — Milestone Project 2: SkimLit\nReplicate a the model which powers the PubMed 200k paper to classify different sequences in PubMed medical abstracts (which can help researchers read through medical abstracts faster)\n\n\n\n10 — Time Series fundamentals in TensorFlow\nLearn how to diagnose a time series problem (building a model to make predictions based on data across time, e.g. predicting the stock price of AAPL tomorrow)\nPrepare data for time series neural networks (features and labels)\nUnderstanding and using different time series evaluation methods\nMAE — mean absolute error\nBuild time series forecasting models with TensorFlow\nRNNs (recurrent neural networks)\nCNNs (convolutional neural networks)\n\n\n\n11 — Milestone Project 3: (Surprise)\nIf you've read this far, you are probably interested in the course. This last project will be good... we promise you, so see you inside the course ;)\n\n\nTensorFlow is growing in popularity and more and more job openings are appearing for this specialized knowledge. As a matter of fact, TensorFlow is outgrowing other popular ML tools like PyTorch in job market. Google, Airbnb, Uber, DeepMind, Intel, IBM, Twitter, and many others are currently powered by TensorFlow. There is a reason these big tech companies are using this technology and you will find out all about the power that TensorFlow gives developers.\n\n\nWe guarantee you this is the most comprehensive online course on TensorFlow. So why wait? Make yourself stand out by becoming a TensorFlow Expert and advance your career.\n\n\nSee you inside the course!",
      "target_audience": [
        "Anyone who wants to become a top 10% TensorFlow Developer and be at the forefront of Artificial Intelligence, Machine Learning, and Deep Learning",
        "Students, developers, and data scientists who want to demonstrate practical machine learning skills through the building and training of models using TensorFlow",
        "Anyone looking to expand their knowledge when it comes to AI, Machine Learning and Deep Learning",
        "Anyone looking to master building ML models with the latest version of TensorFlow"
      ]
    },
    {
      "title": "Advanced Web Scraping with Python using Scrapy & Splash",
      "url": "https://www.udemy.com/course/advanced-web-scraping-with-python-using-scrapy-splash/",
      "bio": "The most advanced web scraping & crawling course using Scrapy & Splash! Take your web scraping skills to the next level.",
      "objectives": [
        "Advanced web scraping techniques",
        "Best techniques to analyse a website before scraping it",
        "Write clean spiders",
        "Optimize Splash scripts",
        "Bypass 504 HTTP errors",
        "Build Splash Cluster",
        "Bypass Google ReCaptcha (not solving it)",
        "Build Desktop apps for Scrapy Spiders (Tkinter)",
        "ScrapyRT",
        "Showcase scraped data using ScrapyRT & Flask",
        "Heavy data processing",
        "Input & Output processors"
      ],
      "course_content": {
        "Introduction": [
          "Development Environment (Walkthrough)",
          "Installing Splash(Windows Pro/Enterprise edition & Mac OS)",
          "Installing Splash(Windows Home Edition)",
          "Installing Splash (Linux)",
          "Udemy 101",
          "Asking questions"
        ],
        "Centris Canada": [
          "Project Intro",
          "Understanding the API",
          "Consuming the API PART 1",
          "Code update (Handle 555 & 403 HTTP responses)",
          "Consuming the API PART 2",
          "XHR Pagination",
          "Summary Page",
          "Bypass 504 HTTP Error (Method 1)",
          "Bypass 504 HTTP Error (Method 2)",
          "Bypass 504 HTTP Error (Method 3)",
          "Project source code"
        ],
        "Steam Store": [
          "Project Intro",
          "Extracting data PART 1",
          "Extracting data PART 2",
          "Extracting data PART 3",
          "Extracting data PART 4",
          "Pagintion",
          "ItemLoader",
          "Data processing PART 1",
          "Data processing PART 2",
          "Data processing PART 3",
          "Project source code"
        ],
        "Build Web App (ScrapyRT + Flask)": [
          "ScrapyRT",
          "Using Flask with ScrapyRT",
          "Flask templates PART 1",
          "Flask templates PART 2",
          "Flask templates PART 3",
          "Project source code"
        ],
        "Zillow": [
          "Locate the API",
          "ReCaptcha Response",
          "Testing the API",
          "Spoofing Cookie header + Custom Cookie parser",
          "Parsing JSON Objects",
          "Advanced pagination",
          "Media Pipelines PART 1",
          "Media Pipelines PART 2",
          "Project source code"
        ],
        "Scrapy & Tkinter for Desktop Apps": [
          "Desktop APP PART 1",
          "Desktop APP PART 2",
          "Desktop APP PART 3",
          "Desktop APP PART 4(Threading)",
          "Project source code"
        ]
      },
      "requirements": [
        "PC or Mac with internet access.",
        "Have done a couple of projects using SCRAPY & SPLASH is extremely REQUIRED.",
        "Basics of elements selection using XPATH is also extremely REQUIRED."
      ],
      "description": "Hi there & welcome to the most advanced online resource on Web Scraping with Python using Scrapy & Splash. This course is fully project-based means pretty much on each section we gonna scrape a different website & tackle a different web scraping dilemma also rather than focusing on the basics of Scrapy & Splash we gonna dive straight forward into real-world projects, this also means that this course is absolutely not suitable for beginners with no background on web scraping, Scrapy, Splash & XPath expressions.\n---This courses covers a variety of topics such as:---\nRequests chaining, like how the requests must be sent in a certain order otherwise they won't be fulfilled at all.\nHow to analyze a website before scraping it, this is an important step to do since it helps a lot in choosing the right tools to scrape a website & it literally has a huge impact on the performance of your final product.\nHow to optimize Splash scripts by reducing/aborting all the unnecessary requests that have nothing to do with the data points you're going to scrape, this is an important thing to do if you care about the performance of Splash as it is the key to bypass 504 Gateway Timeout HTTP errors in Splash.\nWe gonna also cover how to build a Cluster of Splash instances with a load balancer(HAProxy) rather than having one fully overloaded Splash instance this also helps in bypassing 504 Gateway Timeout errors.\nHeavy data processing, you'll understand how Input & Output processors work so you'll be able to use them in order to clean the scraped data points as this will ensure the quality of your feeds.\nWe'll use ScrapyRT (Scrapy RealTime) to build spiders that can fetch data in real-time.\nShowcase the scraped data points in a minimalist web app using ScrapyRT & Flask, this is extremely helpful for web scraping freelancers.\nBypass Google ReCaptcha, please don't get me wrong on this point, I don't mean that we will solve it using Scrapy, instead, I'm gonna show you a technique that I use frequently to fool websites and let them think that the request is sent using a browser & was performed by a human being!\nBuild clean & well-structured spiders\nFinally, we gonna build a Desktop app using Tkinter, the app will fetch & execute all the available spiders in your Scrapy project, you can also choose the feed type, feed location & name, this is also extremely helpful & important if you're a web scraping freelancer, it is always a good idea to deliver to your client a desktop app rather than installing Scrapy on his machine & stuff like that.\n\n\nThis course is straight to the point, there's no \"foobar\" or \"quotes to toscrape dot com\" as other courses do so make sure you have a good level of focus & lot of determination & motivation.\nBy the end of this course, you'll sharpen your skills in web scraping using Scrapy & Splash, you'll be able to write clean & high performing spiders that differentiate you from others, this also means if you're a web scraping freelancer you'll get more offers since you can deliver \"User-Friendly\" spiders with a Graphical User Interface(GUI) or web apps that fetch data in real-time.\nSo join me on this course & let's harvest the web together!",
      "target_audience": [
        "Anyone wants to learn advanced web scraping techniques",
        "Anyone wants to learn how to turn Scrapy projects into Desktop/web apps",
        "Web scraping freelancers"
      ]
    },
    {
      "title": "Python for Data Science and Machine Learning Bootcamp",
      "url": "https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/",
      "bio": "Learn how to use NumPy, Pandas, Seaborn , Matplotlib , Plotly , Scikit-Learn , Machine Learning, Tensorflow , and more!",
      "objectives": [
        "Use Python for Data Science and Machine Learning",
        "Use Spark for Big Data Analysis",
        "Implement Machine Learning Algorithms",
        "Learn to use NumPy for Numerical Data",
        "Learn to use Pandas for Data Analysis",
        "Learn to use Matplotlib for Python Plotting",
        "Learn to use Seaborn for statistical plots",
        "Use Plotly for interactive dynamic visualizations",
        "Use SciKit-Learn for Machine Learning Tasks",
        "K-Means Clustering",
        "Logistic Regression",
        "Linear Regression",
        "Random Forest and Decision Trees",
        "Natural Language Processing and Spam Filters",
        "Neural Networks",
        "Support Vector Machines"
      ],
      "course_content": {
        "Course Introduction": [
          "Introduction to the Course",
          "Course Help and Welcome",
          "Course FAQs"
        ],
        "Environment Set-Up": [
          "Python Environment Setup"
        ],
        "Jupyter Overview": [
          "Updates to Notebook Zip",
          "Jupyter Notebooks",
          "Optional: Virtual Environments"
        ],
        "Python Crash Course": [
          "Welcome to the Python Crash Course Section!",
          "Introduction to Python Crash Course",
          "Python Crash Course - Part 1",
          "Python Crash Course - Part 2",
          "Python Crash Course - Part 3",
          "Python Crash Course - Part 4",
          "Python Crash Course Exercises - Overview",
          "Python Crash Course Exercises - Solutions"
        ],
        "Python for Data Analysis - NumPy": [
          "Welcome to the NumPy Section!",
          "Introduction to Numpy",
          "Numpy Arrays",
          "Quick Note on Array Indexing",
          "Numpy Array Indexing",
          "Numpy Operations",
          "Numpy Exercises Overview",
          "Numpy Exercises Solutions"
        ],
        "Python for Data Analysis - Pandas": [
          "Welcome to the Pandas Section!",
          "Introduction to Pandas",
          "Series",
          "DataFrames - Part 1",
          "DataFrames - Part 2",
          "DataFrames - Part 3",
          "Missing Data",
          "Groupby",
          "Merging Joining and Concatenating",
          "Operations",
          "Data Input and Output"
        ],
        "Python for Data Analysis - Pandas Exercises": [
          "Note on SF Salary Exercise",
          "SF Salaries Exercise Overview",
          "SF Salaries Solutions",
          "Ecommerce Purchases Exercise Overview",
          "Ecommerce Purchases Exercise Solutions"
        ],
        "Python for Data Visualization - Matplotlib": [
          "Welcome to the Data Visualization Section!",
          "Introduction to Matplotlib",
          "Matplotlib Part 1",
          "Matplotlib Part 2",
          "Matplotlib Part 3",
          "Matplotlib Exercises Overview",
          "Matplotlib Exercises - Solutions"
        ],
        "Python for Data Visualization - Seaborn": [
          "Introduction to Seaborn",
          "Distribution Plots",
          "Categorical Plots",
          "Matrix Plots",
          "Grids",
          "Regression Plots",
          "Style and Color",
          "Seaborn Exercise Overview",
          "Seaborn Exercise Solutions"
        ],
        "Python for Data Visualization - Pandas Built-in Data Visualization": [
          "Pandas Built-in Data Visualization",
          "Pandas Data Visualization Exercise",
          "Pandas Data Visualization Exercise- Solutions"
        ]
      },
      "requirements": [
        "Some programming experience",
        "Admin permissions to download files"
      ],
      "description": "Are you ready to start your path to becoming a Data Scientist!\nThis comprehensive course will be your guide to learning how to use the power of Python to analyze data, create beautiful visualizations, and use powerful machine learning algorithms!\nData Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems!\nThis course is designed for both beginners with some programming experience or experienced developers looking to make the jump to Data Science!\nThis comprehensive course is comparable to other Data Science bootcamps that usually cost thousands of dollars, but now you can learn all that information at a fraction of the cost! With over 100 HD video lectures and detailed code notebooks for every lecture this is one of the most comprehensive course for data science and machine learning on Udemy!\nWe'll teach you how to program with Python, how to create amazing data visualizations, and how to use Machine Learning with Python! Here a just a few of the topics we will be learning:\nProgramming with Python\nNumPy with Python\nUsing pandas Data Frames to solve complex tasks\nUse pandas to handle Excel Files\nWeb scraping with python\nConnect Python to SQL\nUse matplotlib and seaborn for data visualizations\nUse plotly for interactive visualizations\nMachine Learning with SciKit Learn, including:\nLinear Regression\nK Nearest Neighbors\nK Means Clustering\nDecision Trees\nRandom Forests\nNatural Language Processing\nNeural Nets and Deep Learning\nSupport Vector Machines\nand much, much more!\nEnroll in the course and become a data scientist today!",
      "target_audience": [
        "This course is meant for people with at least some programming experience"
      ]
    },
    {
      "title": "NumPy, SciPy, Matplotlib & Pandas A-Z: Machine Learning",
      "url": "https://www.udemy.com/course/numpy-scipy-matplotlib-pandas-a-z-machine-learning/",
      "bio": "NumPy | SciPy | Matplotlib | Pandas | Machine Learning | Data Science | Deep Learning | Pre-Machine Learning Analysis",
      "objectives": [
        "Solid foundation in Python programming, data types, loops, conditionals, functions and more",
        "Create and analyze projects via Python NumPy, SciPy, Matplotlib & Pandas",
        "Clean data with pandas Series and DataFrames",
        "Master data visualization",
        "Understanding the NumPy library to efficiently work with arrays, matrices, and perform mathematical operations.",
        "Go from absolute beginner to become a confident Python NumPy, Pandas and Matplotlib user"
      ],
      "course_content": {
        "Introduction": [
          "Introduction of Python Numpy",
          "Introduction of Numpy Random",
          "Introduction of NumPy ufunc",
          "Introduction of Pyton Pandas"
        ],
        "Python Numpy": [
          "Numpy Creating Arrays",
          "Numpy Array Indexing",
          "Numpy Array Slicing",
          "Numpy Data Types",
          "Numpy Array Shape",
          "Numpy Array Reshaping",
          "Numpy Arrays"
        ],
        "NumPy Random": [
          "Numpy Random Data Distribution",
          "Numpy Random Permutations",
          "Numpy Seaborn",
          "Numpy Normal Distribution",
          "Numpy Binomial Distribution",
          "Numpy Poisson Distribution",
          "Numpy Uniform Distribution"
        ],
        "NumPy ufunc": [
          "NumPy ufunc Create",
          "NumPy ufunc Simple Arithmetic",
          "NumPy ufunc Rounding Decimals",
          "NumPy ufunc Logs",
          "NumPy ufunc Summations",
          "NumPy ufunc Products"
        ],
        "Python Pandas": [
          "Pandas Series",
          "Pandas DataFrames",
          "Pandas Read CSV",
          "Pandas Read JSON",
          "Pandas Analyzing DataFrames"
        ],
        "Matplotlib": [
          "Introduction of Matplotlib",
          "Plotting",
          "Markers",
          "Line",
          "Labels",
          "Grid",
          "Subplot",
          "Scatter",
          "Bars",
          "Histograms",
          "Pie Charts"
        ],
        "SciPy": [
          "Introduction of Python SciPy",
          "SciPy Constants",
          "SciPy Optimizers",
          "SciPy Sparse Data",
          "SciPy Graphs",
          "SciPy Spatial Data",
          "SciPy Matlab Arrays",
          "SciPy Statistical Significance Tests"
        ]
      },
      "requirements": [
        "No specific knowledge needed"
      ],
      "description": "Are you eager to dive into the core libraries that form the backbone of data manipulation, scientific computing, visualization, and machine learning in Python? Welcome to \"NumPy, SciPy, Matplotlib & Pandas A-Z: Machine Learning,\" your comprehensive guide to mastering these essential libraries for data science and machine learning.\n\n\nNumPy, SciPy, Matplotlib, and Pandas are the cornerstone libraries in Python for performing data analysis, scientific computing, and visualizing data. Whether you're a data enthusiast, aspiring data scientist, or machine learning practitioner, this course will equip you with the skills needed to harness the full potential of these libraries for your data-driven projects.\n\n\nKey Learning Objectives:\n\n\nLearn NumPy's fundamentals, including arrays, array operations, and broadcasting for efficient numerical computations.\n\n\nExplore SciPy's capabilities for mathematics, statistics, optimization, and more, enhancing your scientific computing skills.\n\n\nMaster Pandas for data manipulation, data analysis, and transforming datasets to extract valuable insights.\n\n\nDive into Matplotlib to create stunning visualizations, including line plots, scatter plots, histograms, and more to effectively communicate data.\n\n\nUnderstand how these libraries integrate with machine learning algorithms to preprocess, analyze, and visualize data for predictive modeling.\n\n\nApply these libraries to real-world projects, from data cleaning and exploration to building machine learning models.\n\n\nLearn techniques to optimize code and make efficient use of these libraries for large datasets and complex computations.\n\n\nGain insights into best practices, tips, and tricks for maximizing your productivity while working with these libraries.\n\n\nWhy Choose This Course?\n\n\nThis course offers a deep dive into NumPy, SciPy, Matplotlib, and Pandas, ensuring you grasp their core functionalities for data science and machine learning.\n\n\nPractice your skills with coding exercises, projects, and practical examples that simulate real-world data analysis scenarios.\n\n\nBenefit from the guidance of experienced instructors who are passionate about data science and eager to share their knowledge.\n\n\nEnroll once and enjoy lifetime access to the course materials, enabling you to learn at your own pace and revisit concepts whenever necessary.\n\n\nMastery of these libraries is crucial for anyone pursuing a career in data science, machine learning, or scientific computing.\n\n\nUnlock the power of NumPy, SciPy, Matplotlib, and Pandas for data analysis and machine learning. Enroll today in \"NumPy, SciPy, Matplotlib & Pandas A-Z: Machine Learning\" and elevate your data science skills. Don't miss this opportunity to become proficient in these fundamental libraries and enhance your data-driven projects!",
      "target_audience": [
        "All levels of students",
        "Anyone who want to explore the world of Python",
        "This course is for you, if you want a great career"
      ]
    },
    {
      "title": "2 Real World Azure Data Engineer Project End to End",
      "url": "https://www.udemy.com/course/two-real-world-azure-data-engineer-projects-end-to-end/",
      "bio": "Engineering Project Building from scratch including designing, architecting, implementing solution and overall testing.",
      "objectives": [
        "You will learn how to Architect, Design and build a real-world enterprise level data platform solution including multiple services.",
        "You will learn design solution using ADF, Azure Function, Databricks, pyspark, Azure Data lake storage Gen 2 (ADLS), Azure SQL Server",
        "You will learn how to build a real-world data pipeline in Azure Data Factory (ADF). This course has been taught using 2 real world use case scenarios.",
        "You will learn how to transform data using Databricks Notebook Activity in Azure Data Factory (ADF) and load into Azure Data Lake Storage Gen2",
        "You will learn how to build production ready pipelines and good practices and naming standards",
        "You will learn how to integrate Databricks with ADF and send the response back from Databricks to ADF",
        "You will learn how to develop the triggered based Azure Function to validate files.",
        "You will learn how to create Azure Key vault and use it to store secret credentials and SAS token",
        "You will learn how to connect the Azure SQL Database and Databricks cluster using the Key Vault",
        "You will learn how to mount he Azure Storage Account in the Databricks to access the files and preform transformation on it.",
        "You will learn how to transform the data in the Azure Databricks using the pyspark."
      ],
      "course_content": {},
      "requirements": [
        "Basic understanding about cloud computing will be useful, but not necessary.",
        "Experience in Azure is not required, I will take you through everything necessary to learn this course and build the project"
      ],
      "description": "This course will help you in preparing and mastering your Azure Data engineering Concepts.\nIt is not like any random project like covid, or twitter analysis. These project is real world projects on which I personally worked and developed it for big clients.\nHighlights of the Course:\n\n\nDesigned to keep only précised information no beating around the bush. (To save your time).\nReal time implementation, no dummy use case.\nCan be added as part of your resume.\nIt will help you to showcase your experience in interviews and discussion.\nInvolve complex architecture solution which is aligned with industry best practices.\nSingle projects involve various component integration like ADF, ADLS, Databricks, Azure SQL DB, Key Vault.\nSolves the problem of real time experience for new Data engineers.\n\n\nThis course has been developed in mind to keep all the best practices followed in the Industry as an data engineering project and solution.\n\n\nTechnologies involved:\n\n\nAzure Data Lake Storage Gen 2\nAzure Data Factory\nData Factory Pipeline\nAzure Functions\nAzure Key Vault\nAzure SQL DB\nSSMS\nAWS S3 Bucket\nConnect ADF to Databricks\nConnect Databricks to SQL Server\nConnect Databricks to ADLS\nConnect S3 to Azure Cloud\nTriggers\nSAS token\nCreate Secrets scope in Databricks\nStore secretes in Key Vault and access them\nWhat you will learn after this course:\n\n\nHow to think, design and develop the solution in the data engineering world.\nHow to create the architecture diagram for data engineering  projects.\nHow to Create Azure Data Factory Account\nHow to Create Azure Data Lake Storage Gen 2 account.\nHow to Create Azure Databricks Workspace.\nHow to create S3 storage account.\nHow to create Azure Function.\nHow to implement logic in the Databricks notebook using pyspark.\nHow to connect ADF to Databricks.\nHow to chain the multiple pieces together in project.\nHow to create Azure SQL Server.\nHow to load the data from file to Azure SQL server.\nHow to connect Databricks notebook with Azure SQL Server.\nHow to Store secrets in the Azure Key Vault.",
      "target_audience": [
        "Aspiring Data engineer who are searching for project to add in resume",
        "Someone who is looking for Real World uses cases to implement as Data engineering Solution",
        "University students looking for a career in Data Engineering",
        "IT developers working on other disciplines trying to move to Data Engineering",
        "Data Engineers/ Data Warehouse Developers currently working on on-premises technologies, or other cloud platforms such as AWS or GCP who want to learn Azure Technologies",
        "Data Architects looking to gain an understanding about Azure Data Engineering stack",
        "Data Scientists who want extend their knowledge into data engineering"
      ]
    },
    {
      "title": "Azure Machine Learning & MLOps : Beginner to Advance",
      "url": "https://www.udemy.com/course/azure-machine-learning-mlops-mg/",
      "bio": "The most compressive course covering MLOps and machine learning on Azure | Zero to Hero",
      "objectives": [
        "How to use Azure Machine Learning from Development to Production",
        "How to Use Azure DevOps for Continuous Integration, continuous deployment on Machine Learning",
        "How to automate End 2 End machine Learning Solution on Azure",
        "How to Deploy Machine learning Models on Azure (Azure Container Instances, Azure Kubernetese Services, managed endpoints)",
        "Run an end-to-end CI/CD MLOps pipeline using Azure DevOps & Azure Machine learning",
        "Bests practices and highly demanded capabilities of machine learning on Azure Cloud"
      ],
      "course_content": {
        "Azure MLOps": [
          "Complete Intro to Azure Machine Learning Service",
          "Intro to Azure DevOps",
          "Setting up Azure DevOps Configurations",
          "Create & Deploy Infrastructure as Code Pipeline",
          "CI Pipeline ( Continuous Integration) for ML",
          "CI Pipeline ( Continuous Integration) for ML",
          "Automated Training with CI Pipeline",
          "CD Pipeline (Continuous Deployment) for Staging",
          "CD Pipeline(Continuous Deployment)for Production",
          "Testing End to End MLOps Pipelines",
          "Extra 1 : Azure MLOps with GitHub Actions & Azure Machine Learning",
          "Extra 2: Databricks MLOps With GitHub Actions & MLflow"
        ],
        "Azure ML SDK V2": [
          "Azure MLOps V2 Accelerator"
        ],
        "Azure Machine Learning | Highly Demanded Capabilities": [
          "Responsible AI in Action - Part 1",
          "Responsible AI in Action - Part 2",
          "Azure Machine Learning Pipeline",
          "Feature Store (Feast) in Azure (Machine Learning)",
          "Ray and Dask in Azure Machine Learning",
          "Auto ML for Computer Vision in Azure Machine Learning",
          "Differential Privacy For Machine Learning In Action (Sensitive Data)",
          "Integrate Azure Databricks with Azure Machine Learning",
          "AutoML for Natural Language Processing (NLP) in Azure Machine Learning",
          "Train and Score Thousands of Machine Learning Models in Parallel ( Many Models i",
          "Why do Machine learning models fail? (Data Drift Monitoring in Azure)",
          "Integrating Azure Synapse with Azure Machine Learning",
          "Deploy Multi Model Endpoint in Azure Machine Learning",
          "Rollout (Blue-Green) Real-Time Machine Learning Model Deployment in Azure",
          "Consume Azure Machine Learning models in Power BI",
          "Use PowerApps to Create Apps for Deployed Azure Machine Learning Models",
          "Attach Your OnPrem Kubernetes to Azure Machine Learning Using Azure Arc",
          "Components in Azure Machine Learning Pipelines",
          "Being Platform Agnostic with MLflow in Azure Machine Learning",
          "Deploy Spark ML model as Web Service Using Azure Machine Learning",
          "Azure Machine Learning Environments",
          "Scalable & Managed Batch Prediction with Azure Machine Learning",
          "Execute Azure Machine Learning pipelines in Azure Data Factory or Synapse",
          "Extra: Train Machine learning model once and deploy it anywhere with ONNX",
          "Extra: Azure Machine Learning Networking (Hands-on)",
          "Extra: AutoML Comparison in Databricks VS Azure Machine Learning",
          "Extra: Machine Learning Lineage with Microsoft Purview",
          "Extra: Azure + Open AI = The Future is Here"
        ]
      },
      "requirements": [
        "Nice to have familiarity with basics of Machine Learning",
        "If you want to practice the contents, free or paid subscription to Microsoft Azure is required"
      ],
      "description": "A course instructed by me and my digital twin if:\nYou are looking for a comprehensive, engaging, and fun course for mastering Azure Machine learning ( up to even advanced industry-required topics) plus fully hands-on end-to-end implantation of MLOps ( DevOps for Machine learning on Azure). If yes, then this is the right and very unique course for you!\nMachine Learning Operations (MLOps) is a rapidly growing culture nad highly demanded in the industry with a set of principles, and guidelines defined in the machine learning world to deploy a machine learning model into production.\nAzure Machine Learning is a cloud service for accelerating and managing the machine learning project lifecycle. Machine learning professionals, data scientists, and engineers can use it in their day-to-day workflows: Train and deploy models, and manage MLOps.\nYou can create a model in Azure Machine Learning or use a model built from an open-source platform, such as Pytorch, TensorFlow, or scikit-learn. MLOps tools help you monitor, retrain, and redeploy models.\n\n\nKey points about this course\n\n\nVery detailed in-depth and comprehensive coverage\nThis course will help you prepare for entry into this hot career path of Machine Learning and MLOps\nThe course is regularly updated with recent features\nBest practices and impactful features of Azure ML (e.g, Explainable AI)  with its tricks are all covered\nContains some extra videos relevant to Azure Machine Learning and Databricks (Apache Spark)",
      "target_audience": [
        "Anyone who wants to learn more about Data Science and Machine Learning specifically on Cloud",
        "Data scientists who want to earn DP-100 Certification",
        "Developers who want to enter AI Cloud Solution Architect or Machine Learning Engineer career path",
        "Anyone who wants to start a career in or wants to learn about the Machine Learning and MLOPs on Cloud"
      ]
    },
    {
      "title": "TensorFlow and the Google Cloud ML Engine for Deep Learning",
      "url": "https://www.udemy.com/course/from-0-to-1-tensorflow-for-deep-learning/",
      "bio": "CNNs, RNNs and other neural networks for unsupervised and supervised deep learning",
      "objectives": [
        "Build and execute machine learning models on TensorFlow",
        "Implement Deep Neural Networks, Convolutional Neural Networks and Recurrent Neural Networks",
        "Understand and implement unsupervised learning models such as Clustering and Autoencoders"
      ],
      "course_content": {
        "Introduction": [
          "You, This Course and Us",
          "Source Code and PDFs",
          "Datasets for all Labs"
        ],
        "Installation": [
          "Install TensorFlow",
          "Install Jupyter Notebook",
          "Running on the GCP vs. Running on your local machine",
          "Lab: Setting Up A GCP Account",
          "Lab: Using The Cloud Shell",
          "Datalab ~ Jupyter",
          "Lab: Creating And Working On A Datalab Instance"
        ],
        "TensorFlow and Machine Learning": [
          "Introducing Machine Learning",
          "Representation Learning",
          "Neural Networks Introduced",
          "Introducing TensorFlow",
          "Running on the GCP vs. Running on your local machine",
          "Lab: Simple Math Operations",
          "Computation Graph",
          "Tensors",
          "Lab: Tensors",
          "Linear Regression Intro",
          "Placeholders and Variables",
          "Lab: Placeholders",
          "Lab: Variables",
          "Lab: Linear Regression with Made-up Data",
          "TensorFlow Basics"
        ],
        "Working with Images": [
          "Image Processing",
          "Images As Tensors",
          "Lab: Reading and Working with Images",
          "Lab: Image Transformations",
          "Images"
        ],
        "K-Nearest-Neighbors with TensorFlow": [
          "Introducing MNIST",
          "K-Nearest Neigbors",
          "One-hot Notation and L1 Distance",
          "Steps in the K-Nearest-Neighbors Implementation",
          "Lab: K-Nearest-Neighbors",
          "MNIST with K-Nearest Neighbors"
        ],
        "Linear Regression with a Single Neuron": [
          "Learning Algorithm",
          "Individual Neuron",
          "Learning Regression",
          "Learning XOR",
          "XOR Trained"
        ],
        "Linear Regression in TensorFlow": [
          "Lab: Access Data from Yahoo Finance",
          "Non TensorFlow Regression",
          "Lab: Linear Regression - Setting Up a Baseline",
          "Gradient Descent",
          "Lab: Linear Regression",
          "Lab: Multiple Regression in TensorFlow",
          "Linear Regression"
        ],
        "Logistic Regression in TensorFlow": [
          "Logistic Regression Introduced",
          "Linear Classification",
          "Lab: Logistic Regression - Setting Up a Baseline",
          "Logit",
          "Softmax",
          "Argmax",
          "Lab: Logistic Regression",
          "Logistic Regression"
        ],
        "The Estimator API": [
          "Estimators",
          "Lab: Linear Regression using Estimators",
          "Lab: Logistic Regression using Estimators",
          "Estimators"
        ],
        "Neural Networks and Deep Learning": [
          "Traditional Machine Learning",
          "Deep Learning",
          "Operation of a Single Neuron",
          "The Activation Function",
          "Training a Neural Network: Back Propagation",
          "Lab: Automobile Price Prediction - Exploring the Dataset",
          "Lab: Automobile Price Prediction - Using TensorFlow for Prediction",
          "Hyperparameters",
          "Vanishing and Exploding Gradients",
          "The Bias-Variance Trade-off",
          "Preventing Overfitting",
          "Lab: Iris Flower Classification",
          "Neural Networks and Deep Learning"
        ]
      },
      "requirements": [
        "Basic proficiency at programming in Python",
        "Basic understanding of machine learning models is useful but not required"
      ],
      "description": "TensorFlow is quickly becoming the technology of choice for deep learning, because of how easy TF makes it to build powerful and sophisticated neural networks. The Google Cloud Platform is a great place to run TF models at scale, and perform distributed training and prediction.\nThis is a comprehensive, from-the-basics course on TensorFlow and building neural networks. It assumes no prior knowledge of Tensorflow, all you need to know is basic Python programming.\n\nWhat's covered:\nDeep learning basics: What a neuron is; how neural networks connect neurons to 'learn' complex functions; how TF makes it easy to build neural network models\nUsing Deep Learning for the famous ML problems: regression, classification, clustering and autoencoding\nCNNs - Convolutional Neural Networks: Kernel functions, feature maps, CNNs v DNNs\nRNNs - Recurrent Neural Networks: LSTMs, Back-propagation through time and dealing with vanishing/exploding gradients\nUnsupervised learning techniques - Autoencoding, K-means clustering, PCA as autoencoding\nWorking with images\nWorking with documents and word embeddings\nGoogle Cloud ML Engine: Distributed training and prediction of TF models on the cloud\nWorking with TensorFlow estimators",
      "target_audience": [
        "Developers who want to understand and build ML and deep learning models in TensorFlow",
        "Data scientists who want to learn cutting edge TensorFlow technology"
      ]
    },
    {
      "title": "No Nonsense Python: learn Python basics and start coding",
      "url": "https://www.udemy.com/course/no-nonsense-python/",
      "bio": "Because nobody becomes an expert developer by taking a 30 hour course.",
      "objectives": [
        "Python fundamentals. Everything you need to know to get started and nothing you don't!"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Our Python Environment"
        ],
        "Core concepts": [
          "The theory of variables",
          "Getting to grips with variables",
          "Arithmetic and comparison operators",
          "Variable reassignment & arithmetic with variables",
          "Creating and interacting with lists",
          "Getting started with tuples",
          "Using dictionaries to drive your program",
          "Commenting your code"
        ],
        "Adding some logic!": [
          "The theory of if statements",
          "If, then, else statements",
          "EXERCISE: If statements",
          "SOLUTION: If statements",
          "The theory of loops",
          "Dive into while loops",
          "EXERCISE: while loops",
          "SOLUTION: while loops",
          "Getting to grips with for loops",
          "EXERCISE: for loops",
          "SOLUTION: for loops",
          "Code structure & indentation",
          "String Functions"
        ],
        "Structure": [
          "The theory of functions",
          "Creating our first function"
        ],
        "What next?": [
          "Resources to continue your learning",
          "What do I do now?"
        ]
      },
      "requirements": [
        "None"
      ],
      "description": "Embark on a Python Journey Like No Other!\n\n\nAre you tired of slogging through endless hours of irrelevant Python content? Do you yearn for a course that hones in on what truly matters, without overwhelming you with obscure, seldom-used features? Look no further, for your Python programming voyage is about to become a thrilling adventure.\n\n\nIntroducing our exceptional course:\n\n\nNo Nonsense Python\nIn this meticulously crafted program, I have streamlined the Python learning process to empower you with the core essentials, sparing you the unnecessary detours. Here's a glimpse of what awaits you:\n\n\nMaster the Basics: Dive deep into the fundamentals of Python, gaining an unwavering grasp of essential concepts.\n\n\nPractical Skills: Learn how to create and manipulate variables, perform arithmetic wizardry, and use comparison operators like a pro.\n\n\nString Sorcery: Unleash the magic of string functions, unlocking the power to manipulate text with precision.\n\n\nData Structures: Delve into the world of lists, tuples, and dictionaries, and understand how these structures can reshape your Python prowess.\n\n\nDecision-Making: Harness the art of if statements, making informed choices within your code.\n\n\nLooping Mastery: Tame the for and while loops to iterate through data like a coding maestro.\n\n\nFunction Fundamentals: Discover the elegance of defining and calling functions, a cornerstone for creating efficient, reusable code.\n\n\nUnleash Your Potential: You'll be amazed at how fast you can transform from a Python novice to a confident coder.\n\n\nNo Experience Required: This course is tailor-made for beginners with zero prior programming knowledge. I'm here to guide you from square one, igniting your passion for Python.\n\n\nJoin me on this thrilling voyage into the heart of Python, where I strip away the fluff and deliver knowledge that truly empowers. Say goodbye to unnecessary complexities and hello to a new era of Python programming.\n\n\nReady to embark on a Python adventure like never before? Enroll now, and let's unlock the magic of Python together!",
      "target_audience": [
        "People that would love to get hands on with Python",
        "Beginner Python Developers",
        "Python Newbies"
      ]
    },
    {
      "title": "Anyword AI: The Best Generative Artificial Intelligence Tool",
      "url": "https://www.udemy.com/course/anyword-ai-the-best-generative-artificial-intelligence-tool/",
      "bio": "Unlock the Power of AI Content Creation with AnyWord – The Ultimate Solution for Your Marketing Needs!",
      "objectives": [
        "Using Anyword for Generative AI & Copywriting",
        "Quickly generate outstanding copy for every channel and use case",
        "Improve copy based on your goals (sign ups, purchases, reach...) in just a click",
        "Use data-driven performance scores to find which copy will perform best.",
        "Generate winning article headlines based on your brand, audience and goals",
        "Leverage Anyword’s powerful AI copywriting to craft accurate, engaging, human-sounding blog content in every section of your post.",
        "Create 100% original content using Anyword’s built-in plagiarism checker",
        "Create Social Media Posts, Blogs, Company Bios, Product Descriptions, Landing Pages, SMS and a lot more"
      ],
      "course_content": {
        "Introduction": [
          "Welcome",
          "Introduction to Anyword",
          "Signing up for Anyword"
        ],
        "Getting Started with Anyword and generating results": [
          "Writing our first prompt for generating an 'Instagram Caption'",
          "Anyword Homepage Walkthrough"
        ],
        "Exploring 'Data Driven Editor' templates in Anyword": [
          "Working with 'Prompt' Template in Anyword",
          "Working with 'Content Improver' in Anyword",
          "Generating copy for 'Ads' using Anyword",
          "Creating and improving Landing Page content using Anyword",
          "Using the 'Social' templates for engaging content",
          "Generating professional 'Email' content using Anyword",
          "Generate description, reviews, features etc using 'Products' templates",
          "'General Writing' templates in Anyword",
          "Exploring 'More Features' & Copywriting Frameworks - AIDA, PAS, BAB",
          "Setting up 'Target Audience' and using it while generating copy ideas"
        ],
        "Exploring the Blog Wizard in Anyword": [
          "Using 'Blog Wizard' to create a complete blog from scratch in Anyword within"
        ],
        "End Notes": [
          "Congratulations and Thankyou."
        ]
      },
      "requirements": [
        "No experience needed, we will learn everything from basics and gradually move to advanced topics"
      ],
      "description": "Are you tired of constantly struggling to create quality content that engages your audience and drives results? Say goodbye to those long hours spent brainstorming and writing – and say hello to Anyword!\nThis revolutionary AI platform leverages natural language processing and machine learning to generate exceptional content for all your marketing channels. From social media posts and blogs to product descriptions, company bios, landing pages, SMS, and more – Anyword has got you covered!\nBy enrolling in our Udemy course, you'll learn how to unleash the full potential of this powerful tool. I will guide you through the process of using Anyword, helping you create content that resonates with your target audience and drives conversions. We will cover all use cases, and take live examples of each feature of this portal.\nPlus, every student who completes the course will receive a certificate of completion, demonstrating their proficiency in using Anyword for content creation.\nTop Reasons why you should choose this Course :\nThis course is designed keeping in mind the students from all backgrounds - hence we cover everything from basics, and gradually progress towards advanced topics.\nActual Case Studies and Live Examples of all use cases and prompts\nAll Doubts will be answered.\nNew content added regularly and useful educational emails are sent to all students.\nDon't wait any longer to revolutionize your marketing strategy – enroll in our Anyword course on Udemy today and unleash the power of AI content creation!",
      "target_audience": [
        "Copywriters",
        "Content Creators",
        "Artificial Intelligence Enthusiasts",
        "Digital Marketing professionals, SEO experts, Bloggers and article writers"
      ]
    },
    {
      "title": "Deep Learning with Python and Keras",
      "url": "https://www.udemy.com/course/deep-learning-with-python-and-keras/",
      "bio": "Understand and build Deep Learning models for images, text and more using Python and Keras",
      "objectives": [
        "To describe what Deep Learning is in a simple yet accurate way",
        "To explain how deep learning can be used to build predictive models",
        "To distinguish which practical applications can benefit from deep learning",
        "To install and use Python and Keras to build deep learning models",
        "To apply deep learning to solve supervised and unsupervised learning problems involving images, text, sound, time series and tabular data.",
        "To build, train and use fully connected, convolutional and recurrent neural networks",
        "To look at the internals of a deep learning model without intimidation and with the ability to tweak its parameters",
        "To train and run models in the cloud using a GPU",
        "To estimate training costs for large models",
        "To re-use pre-trained models to shortcut training time and cost (transfer learning)"
      ],
      "course_content": {
        "Welcome to the course!": [
          "Welcome to the course!",
          "Introduction",
          "Real world applications of deep learning",
          "Download and install Anaconda",
          "Installation Video Guide",
          "Obtain the code for the course",
          "Course Folder Walkthrough",
          "Your first deep learning model"
        ],
        "Data": [
          "Section 2 Intro",
          "Tabular data",
          "Data exploration with Pandas code along",
          "Visual data Exploration",
          "Plotting with Matplotlib",
          "Unstructured Data",
          "Images and Sound in Jupyter",
          "Feature Engineering",
          "Exercise 1 Presentation",
          "Exercise 1 Solution",
          "Exercise 2 Presentation",
          "Exercise 2 Solution",
          "Exercise 3 Presentation",
          "Exercise 3 Solution",
          "Exercise 4 Presentation",
          "Exercise 4 Solution",
          "Exercise 5 Presentation",
          "Exercise 5 Solution"
        ],
        "Machine Learning": [
          "Section 3 Intro",
          "Machine Learning Problems",
          "Supervised Learning",
          "Linear Regression",
          "Cost Function",
          "Cost Function code along",
          "Finding the best model",
          "Linear Regression code along",
          "Evaluating Performance",
          "Evaluating Performance code along",
          "Classification",
          "Classification code along",
          "Overfitting",
          "Cross Validation",
          "Cross Validation code along",
          "Confusion matrix",
          "Confusion Matrix code along",
          "Feature Preprocessing code along",
          "Exercise 1 Presentation",
          "Exercise 1 solution",
          "Exercise 2 Presentation",
          "Exercise 2 solution"
        ],
        "Deep Learning Intro": [
          "Section 4 Intro",
          "Deep Learning successes",
          "Neural Networks",
          "Deeper Networks",
          "Neural Networks code along",
          "Multiple Outputs",
          "Multiclass classification code along",
          "Activation Functions",
          "Feed forward",
          "Exercise 1 Presentation",
          "Exercise 1 Solution",
          "Exercise 2 Presentation",
          "Exercise 2 Solution",
          "Exercise 3 Presentation",
          "Exercise 3 Solution",
          "Exercise 4 Presentation",
          "Exercise 4 Solution"
        ],
        "Gradient Descent": [
          "Section 5 Intro",
          "Derivatives and Gradient",
          "Backpropagation intuition",
          "Chain Rule",
          "Derivative Calculation",
          "Fully Connected Backpropagation",
          "Matrix Notation",
          "Numpy Arrays code along",
          "Learning Rate",
          "Learning Rate code along",
          "Gradient Descent",
          "Gradient Descent code along",
          "EWMA",
          "Optimizers",
          "Optimizers code along",
          "Initialization code along",
          "Inner Layers Visualization code along",
          "Exercise 1 Presentation",
          "Exercise 1 Solution",
          "Exercise 2 Presentation",
          "Exercise 2 Solution",
          "Exercise 3 Presentation",
          "Exercise 3 Solution",
          "Exercise 4 Presentation",
          "Exercise 4 Solution",
          "Tensorboard"
        ],
        "Convolutional Neural Networks": [
          "Section 6 Intro",
          "Features from Pixels",
          "MNIST Classification",
          "MNIST Classification code along",
          "Beyond Pixels",
          "Images as Tensors",
          "Tensor Math code along",
          "Convolution in 1 D",
          "Convolution in 1 D code along",
          "Convolution in 2 D",
          "Image Filters code along",
          "Convolutional Layers",
          "Convolutional Layers code along",
          "Pooling Layers",
          "Pooling Layers code along",
          "Convolutional Neural Networks",
          "Convolutional Neural Networks code along",
          "Weights in CNNs",
          "Beyond Images",
          "Exercise 1 Presentation",
          "Exercise 1 Solution",
          "Exercise 2 Presentation",
          "Exercise 2 Solution"
        ],
        "Cloud GPUs": [
          "Google Colaboratory GPU notebook setup",
          "Floyd GPU notebook setup"
        ],
        "Recurrent Neural Networks": [
          "Section 8 Intro",
          "Time Series",
          "Sequence problems",
          "Vanilla RNN",
          "LSTM and GRU",
          "Time Series Forecasting code along",
          "Time Series Forecasting with LSTM code along",
          "Rolling Windows",
          "Rolling Windows code along",
          "Exercise 1 Presentation",
          "Exercise 1 Solution",
          "Exercise 2 Presentation",
          "Exercise 2 Solution"
        ],
        "Improving performance": [
          "Section 9 Intro",
          "Learning curves",
          "Learning curves code along",
          "Batch Normalization",
          "Batch Normalization code along",
          "Dropout",
          "Dropout and Regularization code along",
          "Data Augmentation",
          "Continuous Learning",
          "Image Generator code along",
          "Hyperparameter search",
          "Embeddings",
          "Embeddings code along",
          "Movies Reviews Sentiment Analysis code along",
          "Exercise 1 Presentation",
          "Exercise 1 Solution",
          "Exercise 2 Presentation",
          "Exercise 2 Solution",
          "Exercise 3 Presentation"
        ]
      },
      "requirements": [
        "Knowledge of Python, familiarity with control flow (if/else, for loops) and pythonic constructs (functions, classes, iterables, generators)",
        "Use of bash shell (or equivalent command prompt) and basic commands to copy and move files",
        "Basic knowledge of linear algebra (what is a vector, what is a matrix, how to calculate dot product)",
        "Use of ssh to connect to a cloud computer"
      ],
      "description": "This course is designed to provide a complete introduction to Deep Learning. It is aimed at beginners and intermediate programmers and data scientists who are familiar with Python and want to understand and apply Deep Learning techniques to a variety of problems.\nWe start with a review of Deep Learning applications and a recap of Machine Learning tools and techniques. Then we introduce Artificial Neural Networks and explain how they are trained to solve Regression and Classification problems.\nOver the rest of the course we introduce and explain several architectures including Fully Connected, Convolutional and Recurrent Neural Networks, and for each of these we explain both the theory and give plenty of example applications.\nThis course is a good balance between theory and practice. We don't shy away from explaining mathematical details and at the same time we provide exercises and sample code to apply what you've just learned.\nThe goal is to provide students with a strong foundation, not just theory, not just scripting, but both. At the end of the course you'll be able to recognize which problems can be solved with Deep Learning, you'll be able to design and train a variety of Neural Network models and you'll be able to use cloud computing to speed up training and improve your model's performance.",
      "target_audience": [
        "Software engineers who are curious about data science and about the Deep Learning buzz and want to get a better understanding of it",
        "Data scientists who are familiar with Machine Learning and want to develop a strong foundational knowledge of deep learning"
      ]
    },
    {
      "title": "U&P AI - Natural Language Processing (NLP) with Python",
      "url": "https://www.udemy.com/course/understand-and-practice-ai-natural-language-processing-in-python/",
      "bio": "Become an NLP Engineer by creating real projects using Python, semantic search, text mining and search engines!",
      "objectives": [
        "Understand every detail and build real stuff in NLP",
        "(NEW)Learn how some plugins use semantic search to generate source code",
        "(NEW)Building your vocabulary for any NLP model",
        "(NEW)Reducing Dimensions of your Vocabulary for Machine Learning Models",
        "(NEW)Feature Engineering and convert text to numerical values for machine learning models",
        "(NEW) Keyword search VS Semantic search",
        "(NEW)Similarity between documents",
        "(NEW)Dealing with WordNet",
        "(NEW)Search engines under the hood",
        "Tokenizing text data",
        "Converting words to their base forms using stemming",
        "Converting words to their base forms using lemmatization",
        "Dividing text data into chunks",
        "Dealing with corpuses",
        "Extracting document term matrix using the Bag of Words model",
        "Building a category predictor",
        "Constructing a gender identifier",
        "Building a sentiment analyzer",
        "Topic modeling using Latent Dirichlet Allocation"
      ],
      "course_content": {
        "Getting an Idea of NLP and its Applications": [
          "Note!",
          "Introduction to NLP",
          "By The End Of This Section",
          "Installation",
          "Tips",
          "U - Tokenization",
          "P - Tokenization",
          "U - Stemming",
          "P - Stemming",
          "U - Lemmatization",
          "P - Lemmatization",
          "U - Chunks",
          "P - Chunks",
          "U - Bag Of Words",
          "P - Bag Of Words",
          "U - Category Predictor",
          "P - Category Predictor",
          "U - Gender Identifier",
          "P - Gender Identifier",
          "U - Sentiment Analyzer",
          "P - Sentiment Analyzer",
          "U - Topic Modeling",
          "P - Topic Modeling",
          "Summary"
        ],
        "Feature Engineering": [
          "Using Google Colab",
          "Introduction",
          "One Hot Encoding",
          "Count Vectorizer",
          "N-grams",
          "Hash Vectorizing",
          "Word Embedding",
          "FastText"
        ],
        "Dealing with corpus and WordNet": [
          "Introduction",
          "In-built corpora",
          "External Corpora",
          "Corpuses & Frequency Distribution",
          "Frequency Distribution",
          "WordNet",
          "Wordnet with Hyponyms and Hypernyms",
          "The Average according to WordNet"
        ],
        "Create your Vocabulary for any NLP Model": [
          "Putting the previous knowledge together",
          "Introduction and Challenges",
          "1 - Building your Vocabulary",
          "2 - Building your Vocabulary",
          "3 - Building your Vocabulary",
          "4 - Building your Vocabulary",
          "5 - Building your Vocabulary",
          "Dot Product",
          "Similarity using Dot Product",
          "Reducing Dimensions of your Vocabulary using token improvement",
          "Reducing Dimensions of your Vocabulary using n-grams",
          "Reducing Dimensions of your Vocabulary using normalizing",
          "Reducing Dimensions of your Vocabulary using case normalization",
          "When to use stemming and lemmatization?",
          "Sentiment Analysis Overview",
          "Two approaches for sentiment analysis",
          "Sentiment Analysis using rule-based",
          "Sentiment Analysis using machine learning - 1",
          "Sentiment Analysis using machine learning - 2",
          "Summary"
        ],
        "Word2Vec in Detail and what is going on under the hood": [
          "Introduction",
          "Bag of words in detail",
          "Vectorizing",
          "Vectorizing and Cosine Similarity",
          "Topic modeling in Detail",
          "Make your Vectors will more reflect the Meaning, or Topic, of the Document",
          "Sklearn in a short way",
          "Summary"
        ],
        "Find and Represent the Meaning or Topic of Natural Language Text": [
          "Note!",
          "Keyword Search VS Semantic Search",
          "Problems in TI-IDF leads to Semantic Search",
          "Transform TF-IDF Vectors to Topic Vectors under the hood"
        ]
      },
      "requirements": [
        "A little bit of python"
      ],
      "description": "-- UPDATED -- (NEW LESSONS ARE NOT IN THE PROMO VIDEO)\nTHIS COURSE IS FOR BEGINERS OR INTERMEDIATES, IT IS NOT FOR EXPERTS\nThis course is a part of a series of courses specialized in artificial intelligence :\nUnderstand and Practice AI - (NLP)\nThis course is focusing on the NLP:\nLearn key NLP concepts and intuition training to get you quickly up to speed with all things NLP.\nI will give you the information in an optimal way, I will explain in the first video for example what is the concept, and why is it important, what is the problem that led to thinking about this concept and how can I use it (Understand the concept). In the next video, you will go to practice in a real-world project or in a simple problem using python (Practice).\nThe first thing you will see in the video is the input and the output of the practical section so you can understand everything and you can get a clear picture!\nYou will have all the resources at the end of this course, the full code, and some other useful links and articles.\nIn this course, we are going to learn about natural language processing. We will discuss various concepts such as tokenization, stemming, and lemmatization to process text. We will then discuss how to build a Bag of Words model and use it to classify text. We will see how to use machine learning to analyze the sentiment of a given sentence. We will then discuss topic modeling and implement a system to identify topics in a given document. We will start with simple problems in NLP such as Tokenization Text, Stemming, Lemmatization, Chunks, Bag of Words model. and we will build some real stuff such as :\nLearning How to Represent the Meaning of Natural Language Text\nBuilding a category predictor to predict the category of a given text document.\nConstructing a gender identifier based on the name.\nBuilding a sentiment analyzer used to determine whether a movie review is positive or negative.\nTopic modeling using Latent Dirichlet Allocation\nFeature Engineering\nDealing with corpora and WordNet\nDealing With your Vocabulary for any NLP and ML model\nTIPS (for getting through the course):\nTake handwritten notes. This will drastically increase your ability to retain the information.\nAsk lots of questions on the discussion board. The more the better!\nRealize that most exercises will take you days or weeks to complete.\nWrite code yourself, don’t just sit there and look at my code.\nYou don't know anything about NLP? let's break it down!\nI am always available to answer your questions and help you along your data science journey. See you in class!\nNOTICE that This course will be modified and I will add new content and new concepts from one time to another, so stay informed! :)",
      "target_audience": [
        "Anyone who wants to understand NLP concepts and build some projects",
        "Beginner python developers curios about NLP, this course is not for experienced data scientists"
      ]
    },
    {
      "title": "Statistical Arbitrage Bot Build in Crypto with Python (A-Z)",
      "url": "https://www.udemy.com/course/statistical-arbitrage-bot-build-in-crypto-with-python-a-z/",
      "bio": "Build a Pairs Trade bot like a boss on the ByBit Crypto exchange with a statistical arbitrage edge in Python.",
      "objectives": [
        "Gain hands-on experience in developing a Statistical Arbitrage pairs trading crypto bot",
        "Automate and filter searches for all possible co-integrated pairs on a given exchange",
        "Learn what actually moves price in the markets",
        "Understand the use of metrics including hedge ratio, p-value, t-value and c-value",
        "Learn fundamental trading principles and gain a proven statistical edge",
        "Learn the optimal position size",
        "Get clarity on entry and exit signals",
        "Develop an automated bot that can place and manage limit and market orders automatically",
        "Learn backtesting techniques for optimal pair selection",
        "Learn how to calculate spread and Zscore for optimal entry and exit signals"
      ],
      "course_content": {
        "Introduction": [
          "Setting Expectations",
          "The Elephant in The Room: Why Share This Information?",
          "Why You Will Succeed Where Most Do Not",
          "Course Structure and What to Expect"
        ],
        "Resources": [
          "Download Code, Files and Resource Here",
          "UPDATED CODE II - Latest Code for ByBit"
        ],
        "Intuition - Fundamental Principles In Trading": [
          "The Biggest Illusion in Trading",
          "Probability - That Math Does Not Lie",
          "Edge #1 - Strategy and Signal",
          "Edge #2 - Optimal Position Sizing",
          "Edge #3 - Becoming the Casino",
          "Edge #4 - Profiting in Up, Down and Ranging markets",
          "Edge #5 - Managing Exchange and Volatility Risk",
          "Check Your Knowledge"
        ],
        "Intuition - Strategy": [
          "Gameplan - Project Overview",
          "What is Statistical Arbitrage and Cointegration",
          "About cointegration metrics",
          "Statistical Arbitrage - Benefits, Challenges and Risks",
          "Test Your Knowledge"
        ],
        "Intuition - Manual Trading": [
          "Manually Trading with ByBit - Overview",
          "What ACTUALLY Moves Price",
          "Other Important Orderbook Information",
          "IMPORTANT: Check Your Leverage Settings",
          "Placing Both MARKET and LIMIT Orders",
          "All About Shorting Crypto (Optional)",
          "Placing Our First Short Trade",
          "Candlestick Charts (Optional, Recommended for Newbies)",
          "About Conditional Orders and Closing Comments",
          "Test Your Knowledge"
        ],
        "Preparation - Exchange and Coding Environment": [
          "Creating Your TestNet Account (on ByBit)",
          "Create Your API Keys",
          "API Documentation Overview",
          "Choosing Our Python IDE",
          "Creating Our PyCharm Environment"
        ],
        "Troubleshooting and Updated Code": [
          "Discord and Q&A",
          "PyBIT ByBIT API New Walkthrough (Please Watch)",
          "NEW: ByBit Code Repository Update"
        ],
        "Execution - Strategy": [
          "Strategy - Coding Plan Review",
          "Strategy - Getting Started",
          "Creating Input Variables (Config File)",
          "WebSockets and Rate Limits",
          "Getting Tradeable Symbols",
          "Getting and Storing Price Information - Part I",
          "Getting and Storing Price Information - Part II",
          "Checkpoint: Review Price History File",
          "Structuring Close Prices",
          "Calculating Cointegration",
          "Saving and Ranking Pairs",
          "Checkpoint II: Progress and Recap",
          "Writing our Backtesting and Charting Scripts",
          "Plotting and Analysing Charts",
          "Backtesting - Performance Review",
          "UPDATE - Automated Backtesting with PDF Output"
        ],
        "Execution - Function Building Blocks": [
          "Reviewing Our Selected Trading Strategy Plan",
          "Executing Coding Plan Review",
          "Configuration Variables and API Connections",
          "Establishing Our WebSocket Connection",
          "Creating Calculation Functions",
          "Testing Calculation Trade Details Function",
          "Function for Position Side and Size",
          "Closing Positions and Active Orders",
          "Setting Leverage",
          "Building Our Place Order Function",
          "Initialise Order Execution",
          "Price Calls - Part I",
          "Price Calls - Part II",
          "Price Calls - Getting Liquidity",
          "Statistical Functions with Z-Score",
          "Getting Latest Z-Score Signal",
          "Checking for Open and Active Positions",
          "Retrieving Price and Quantity for Open and Active Positions",
          "Query Existing Orders",
          "Checking Limit Order Status",
          "Final Functions and Preparation"
        ],
        "Execution - Bot Building": [
          "Starting Main Execution and Checking Positions",
          "HOT Qualification and Liquidity Check",
          "Determining Long, Short and Initial Capital",
          "Placing Limit Orders",
          "Placing BTC and ETH Orders (Liquidity Test)",
          "Handling Limit Positions and Returning Kill Switch",
          "Testing Our Bot",
          "BUG FIX - Kill Switch",
          "Restoring Bot to Intended Config",
          "StatBot I Review"
        ]
      },
      "requirements": [
        "Basic knowledge of Python preferred but not required per appendix tutorials",
        "Very basic knowledge of cryptocurrencies required"
      ],
      "description": "As requested by the Crypto Wizards community, this course provides you with:\n\n\nAn intuitive understanding of trading principles in crypto (and other) markets\nOptimal calculations for risk, position sizing and entry/exit signals\nEverything you need to know to practically get started in Statistical Arbitrage\nHow to find edge in multiple places and stack as many odds in your favour as possible\nPairs trading concepts which can profit in upwards, sideways and downwards (all) market conditions\nAn understand of Statistical Arbitrage and associated metrics\nAn understand on how trading works on a Crypto Exchange\nHow to tap into exchange price information at lightening speed via WebSockets and REST API\nPython code and walkthrough (line-by-line) for finding your own co-integrated statistical arbitrage trading pairs\nPython code and walkthrough (line-by-line) for developing your own trading bot\nMost retail traders never learn some of what you will come across here, either because those who understand the concepts have not taken the time to break this down so that anyone can follow, or because there is so much nonsense existing today that filtering through the noise can be challenging.\n\n\nIn this course, we aim to break down barriers so that absolutely ANYONE can understand and tap into the advantages that these techniques can provide. The lecturer, Shaun McDonogh, himself admits that he is not a math wiz, nor needs to be. Once you understand these principles, you can apply them anywhere.\n\n\nWe will be using the ByBit exchange (taking advantage of one major benefit offered by the exchange) to build and test our bot. At no point do we use real money for testing. Rather, we use the testate funds provided by the exchange for ensuring forward testing in a \"live\" testate environment.\n\n\nIMPORTANT: This course is for educational purposes only. Nothing you learn in this course is promising favourable or adverse results. You will be learning known methods for calculating statistical arbitrage and building trading bots. How you test and implement this knowledge is up to you.",
      "target_audience": [
        "Retail traders looking to gain a statistical edge in the crypto markets",
        "Beginner programmers looking for interesting Python projects related to trading",
        "Advanced traders looking for a proven strategy to add to their arsenal"
      ]
    },
    {
      "title": "The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero",
      "url": "https://www.udemy.com/course/complete-dbt-data-build-tool-bootcamp-zero-to-hero-learn-dbt/",
      "bio": "Become a dbt professional with this ALL-IN-ONE COURSE covering both theory and practice through a real-world project!",
      "objectives": [
        "Learn to use the dbt platform professionally through the creation of an exhaustive, real-world, hands-on dbt - Airbnb project covering both Theory and Practice",
        "Set up the complete development environment on Mac & Windows, Connect to Snowflake and BI, Configure dbt profile, extend the IDE with dbt tools",
        "Learn core dbt concepts such as Models, Materialization, Sources, Seeds, Snapshots, Packages, Hooks, Exposures, Analyses, write complex SQL queries",
        "Understand the dbt project structure and learn about dbt tips & tricks, advanced techniques and best practices, extend dbt with your own / third-party macros",
        "Implement singular and generic dbt tests, work with additional arguments and default config values, customize dbt built-in tests",
        "Document your models and pipeline, customize the dbt docs page, Explore and analyse dependencies between transformation steps",
        "Understand how dbt fits into the modern data stack, learn about the stages of the Data-Maturity Model, and well functioning Data Architectures",
        "Master ETL/ELT procedures, Data Transformations, Modern Data Stack, Slowly Changing Dimensions, Common Table Expressions and Analytics Engineering",
        "Understand what is a Data Warehouse, Data Lake, or Data Lakehouse and when to use which, handle Data Collection, Data Wrangling and Data Integrations",
        "See how advanced testing works using dbt-expectations, a Great Expectations inspired testing framework",
        "Test your knowledge with certification preparation question",
        "Listen to real-world use-cases from industry professionals",
        "Learn dbt Orchestration Best Practices Hands On"
      ],
      "course_content": {
        "Course Introduction": [
          "Instructors Introduction",
          "Welcome",
          "Course Structure Overview",
          "Course Resources"
        ],
        "Theory - The Data Maturity Model": [
          "Introduction - Maslow's Pyramid of Data",
          "The Data Maturity Model",
          "ETL and ELT",
          "The Data Maturity Model"
        ],
        "Theory - Data Warehouses, Data Lakes and Lakehouses": [
          "Data Warehousing - a short introduction",
          "External Tables and Cloud Data Warehouses",
          "Data Lakes",
          "Data Lakehouse",
          "Data Warehouses, Data Lakes and Lakehouses"
        ],
        "Theory - The Modern Data Stack": [
          "The Modern Data Stack",
          "The Modern Data Stack"
        ],
        "Theory - Slowly Changing Dimension (SCD)": [
          "The Basics of Slowly Changing Dimensions",
          "Type 0 - Retain Original",
          "Type 1 - Overwrite",
          "Type 2 / SCD2 - Add New Row",
          "Type 3 - Add New Attribute",
          "Slowly Changing Dimension (SCD)"
        ],
        "Intro to the practical sessions: dbt and the Airbnb use-case": [
          "dbt Overview",
          "Use-case and Input Data Model Overview"
        ],
        "Practice - Setup": [
          "ESSENTIAL README: How to access course's resources and solution project",
          "Snowflake Registration",
          "Fast Track - Setting up Snowflake with a Click of the Button",
          "A note on the Snowflake data import",
          "Importing Airbnb Data into Snowflake",
          "READ ME! Setup instructions and Prerequisites",
          "OPTIONAL - WINDOWS - Installing Python and pip",
          "WINDOWS - Setting up a Python Virtualenv",
          "MAC - Setting up Python and a Virtualenv",
          "dbt Installation",
          "READ ME: Resolving Snowflake Connection Issues",
          "Creating a dbt project and connecting it to Snowflake using dbt init",
          "Overview of the dbt Project Structure",
          "Setting up flags for dbt 1.10",
          "A note on the DEV schema",
          "Datasets and Data Flow Overview"
        ],
        "Models": [
          "Learning Objectives - Models",
          "Models Overview",
          "Theory: CTE - Common Table Expressions",
          "Creating our first model: Airbnb listings",
          "Models Quiz",
          "Create the src_hosts model"
        ],
        "Materializations": [
          "Learning Objectives - Materializations",
          "Materializations Overview",
          "Model Dependencies and dbt's ref tag",
          "Table type materialization & Project-level Materialization config",
          "Incremental materialization",
          "Ephemeral materialization",
          "Quiz - Materializations"
        ],
        "Seeds and Sources": [
          "Learning Objectives - Seeds and Sources",
          "Seeds and Sources Overview",
          "Seeds",
          "Sources",
          "Source Freshness"
        ]
      },
      "requirements": [
        "Basic SQL experience",
        "No previous programming language experience required",
        "Working computer (Mac/Windows/Linux)",
        "Network access whitelist to snowflake(.com) and GitHub if you work behind a firewall or VPN",
        "Git and Python (We are linking to the installation instructions of these tools in the course)"
      ],
      "description": "Become a dbt professional from scratch with this single course, solving a real-world problem step by step! We cover both theory and hands-on practice! Delivered by an instructor with 20+ years of Data Engineering experience. This is the MOST COMPLETE, CONTINUOUSLY UPDATED independent dbt (Data Build Tool) software course in the world - as of 2025!\nThis course is the TOP RATED and the BESTSELLER dbt course on Udemy!\n\n\n\"Fantastic course. Well-chosen examples perfectly illustrate the many features that are covered. The pacing is spot on and it is easy to replicate the examples.\"\n\"Excellent course! Edit: I managed to pass the dbt certification exam. I couldn't have done it without your help! Again, it's an awesome course!\"\n\"I love how you're explaining everything at just the right level!\"\n\n\nThank you for joining us for The Complete dbt (Data Build Tool) Bootcamp: Zero to Hero - we are super excited to have you in the course!\nThe structure of the course is designed to have a top-down approach. It starts with the Analytics Engineering Theory - all you need to know to put dbt (Data Build Tool) in context and to have an understanding of how it fits into the modern data stack. We start with the big picture; then, we go deeper and deeper. Once you learn about the pieces, we will shift to the technicalities - a practical section - which will focus on putting together the dbt “puzzle”. The practical section will cover each and every single dbt feature present today through the construction of a complete, real-world project; Airbnb. This presents an opportunity for us to show you which features should be used at what stage in a given project, and you will see how dbt is used in the industry.\n\n\nTHEORETICAL SECTION:\nAmong several other topics, the theoretical section puts special emphasis on transferring knowledge in the following areas;\nData-Maturity Model\nWell-functioning Data Architectures\nData Warehouses, Data Lakes, and Data Lakehouses\nETL and ELT procedures and Data Transformations\nFundamentals of dbt (Data Build Tool)\nAnalytics Engineering\nModern Data Stack\nSlowly Changing Dimensions\nCTEs\nOnce we understand the theoretical layer and how dbt fits into the picture, we will start building out a dbt project from scratch, just as you would do in the real world.\n\n\nPRACTICAL SECTION:\nThe practical section will go through a real-world Airbnb project where you will master the ins and outs of dbt! We put special focus on getting everyone up and ready before the technical deep dive; hence we will start off by setting up our Development Environment:\nMAC Development Environment Setup\nWINDOWS Development Environment Setup\nIDE dbt Extension Installation\nCreation and Activation of Virtual Environments\nSetting up Snowflake\nUsing the dbt Power User Visual Studio extension\nOnce we are ready - among several other technical topics, the following features will be covered;\ndbt Models\ndbt Materializations\ndbt Tests\ndbt Documentation\ndbt Sources, Seeds, Snapshots\ndbt Hooks and Operations\nJinja and Macros\ndbt Packages\nAnalyses, Exposures\ndbt Seeds\nData Visualization (Preset)\nWorking with Great Expectations (dbt-expectations)\nDebugging tests in dbt\ndbt Orchestration\nOnce the theory and the practical stages are finished, we will dive into the best practices and more advanced topics. The course is continuously updated; whenever dbt publishes an update, we adjust the course accordingly, so you always be up to date!\nWho is this course for?\nData Engineers\nData Analysts\nData Scientists\nBI Developers\nBI Analyst\n... and anyone who interacts with data lake/data warehouse/data lakehouse or uses SQL!\nCourse Level Explained (Zero > Hero)\nThe course has no expectations about your abilities and starts education from zero. Every exercise is an unavoidable step in your studies. In the same way, don't start an exercise of a superior level without completing the preceding ones: you will be in difficulty if you do so. Practice is the only way to learn, and it cannot be taken lightly. We will be next to you along the journey and you have our absolute support!\nWhen the Airbnb project is presented to you, you must do it entirely, without omitting any guidelines, and by understanding the objective. A project \"almost completely\" done is often a project \"totally incomplete\" for us. Give special attention to detail. Your only reliable source of information regarding the instructions is the pedagogical team, don't trust the \"I've heard\".\nBy the time you complete the course, you will be equipped with both a very solid theoretical understanding and practical expertise with dbt. All the fundamentals, dbt features, best practices, advanced techniques and more will be covered in our course, which will make you become a master in dbt. Are you ready? ;)\nHow to get help?\nWe just published our initial round of Discussions on Udemy which is the easiest and most efficient way for you to post questions, receive answers, and peruse questions from other students. If you have questions or feedback, please reach out to us!\n\n\nThat wraps it up for us for now!\n\nOnce again, thank you for being a part of this course.\n\n\nWe can't wait to get started with you soon!\nAll the best,\nZoltan C. Toth\n\n\ndbt Mark and the dbt logo are trademarks of dbt Labs, Inc.",
      "target_audience": [
        "Analytics Engineers",
        "Data Analysts",
        "BI Analysts",
        "Data Scientists",
        "Data Engineers"
      ]
    },
    {
      "title": "Complete Course on A/B Testing with Interview Guide",
      "url": "https://www.udemy.com/course/product-experimentation-ab-testing-in-r-with-real-examples/",
      "bio": "AB Testing, Multivariate Testing, Multi-armed Bandit, Mock Interview Questions, R Coding, Statistics, Hypothesis Testing",
      "objectives": [
        "How companies like Google, Facebook, Amazon use Experimentation to launch successful products",
        "Ace Experimentation & A/B Testing interviews",
        "A/B Testing, Multivariate Testing & Multi-armed Bandit Testing",
        "Hypothesis testing, including inferential statistics, significance level, type I and II errors, p-values, statistical significance and statistical power",
        "End to end process from hypothesis generation & design to implementation & analysis",
        "Real world examples from Amazon, AirBnb, Square, Uber",
        "Relevance of statistics and how each statistical concept fits in the big picture of A/B testing",
        "Sample size calculation and test results analysis using R",
        "Use experimentation to optimize websites and app",
        "Sample size calculation using online calculators",
        "Use experimentation to increase conversion on landing pages or in-app campaigns",
        "Templates & Cheatsheet to generate, prioritize and analyze A/B tests"
      ],
      "course_content": {
        "Welcome to the course on Product Experimentation!": [
          "Overview of the course"
        ],
        "Product Experimentation Overview": [
          "Welcome to the section!",
          "Introduction",
          "Quiz!",
          "Types of Experiments",
          "Multi-armed Bandit Testing",
          "A/B Testing vs Multi-armed Bandit Testing",
          "Quiz!",
          "Process of Testing",
          "Hypothesis Generation"
        ],
        "Statistics & A/B Testing": [
          "Welcome to the section!",
          "Importance of Statistics in A/B Testing",
          "Sampling Error",
          "Confidence Intervals",
          "Null Hypothesis & Statistical Significance",
          "Quiz!",
          "Type 1 & Type 2 Errors",
          "Statistical Significance & p-value",
          "Statistical Power",
          "AB Testing Statistics Cheat Sheet"
        ],
        "Sample Size & Duration": [
          "Welcome to the section!",
          "Sample Size & Duration",
          "Other Considerations",
          "Calculate Sample Size"
        ],
        "Start to Finish Example in R and Interview Preparation Guide": [
          "Welcome to the section!",
          "Resources for this section",
          "Decco - Part 1",
          "Decco - Part 2",
          "A/B Testing Interview Questions",
          "Mock A/B Testing Interview - Robinhood",
          "Mock A/B Testing Interview - Doordash",
          "Course Completion"
        ]
      },
      "requirements": [
        "You should have basic knowledge of statistical concepts such as mean, standard deviation and variance"
      ],
      "description": "Have you always wondered how companies like Google, Facebook, Amazon use experimentation and AB Testing to launch successful products?\nDo you want to apply online experimentation at your start-up or your current role?\nOr maybe you are interviewing for a role in big Tech and wondering how to succeed in those interviews?\n\n\nWith the rise of smartphones, online controlled testing has really come to the forefront. If you do a google search for Experimentation or A/B testing, you will come across thousands of blogs and articles that discuss this topic. Unfortunately, most of them are either full of inaccuracies and misinterpretation of mathematical concepts Or they are too difficult to understand. This is not surprising. A/B testing is a deep area - there are many nuances involved throughout the process from conceptualization & design all the way to implementation & analysis. This course addresses this. I have designed this course to go deep into important statistical concepts but in a way that is easy to understand using everyday examples.\n\n\nIn just two hours, you will learn -\nWhat product experimentation is and how to do it right\nWhat is AB Testing, Multivariate Testing and Multi-armed Bandit Testing\nWhat is the relevance of statistics in AB testing\nWhat do statistical concepts such as confidence intervals, Type 1, Type 2 errors, p-value, statistical significance and statistical power mean And how do they fit in the big picture\nAnd how to calculate sample size and duration for a successful AB test\nHow to excel in AB testing interviews through real interview questions\n\n\nAll these concepts will be reinforced with real world examples from companies such as Amazon, AirBnb, Square and Uber. I will also provide you with templates and cheat sheets that have really helped me in my career. In 2 hours, you can master product experimentation and immediately start applying it in your job or interviews . See you in the course!",
      "target_audience": [
        "Analytics or Data Science practitioners who want to master Product Experimentation or A/B Testing",
        "Product Managers interested in learning A/B Testing",
        "If you work in Product Development as a Product Manager, Engineer, Data Scientist or Analyst",
        "If you are looking to be hired in big tech",
        "If you are trying to break into data science",
        "If you are looking to make a career switch into a role in Product Experimentation"
      ]
    },
    {
      "title": "Full stack web development and AI with Python (Django)",
      "url": "https://www.udemy.com/course/unaicorn/",
      "bio": "HTML, CSS, JavaScript, Python, Django, Pandas, Sklearn, Keras, Git, Linux - Full stack web development /data science/ AI",
      "objectives": [
        "Full stack web development through learning:",
        "HTML to create websites",
        "CSS and Bootstrap to style your websites",
        "JavaScript - one of the most in demand coding languages in the market for web development",
        "jQuery - a simplified way of applying Javascript",
        "Python, an extremely valuable, versatile and powerful coding language",
        "Django - the python framework for creating dynamic websites that can even integrate machine learning and AI",
        "Create dynamic websites using the Model-View-Controller software design pattern",
        "Data science - the ability to handle, clean, visualise and analyse big data. Some of the biggest salaries and investments go into Data Scientists (NumPy, Pandas, Sklearn, Matplotlib, Seaborn)",
        "Full training in entry mathematics and statistics with a heavy emphasis on machine learning",
        "How to develop machine learning from scratch - training algorithms using big data that can then be used in production for making predictions",
        "Deep learning / AI - learn to create your own AI solutions, such as image classifiers, AI capable of creating art, and much more",
        "Create a range of cutting edge neural network architectures",
        "Document your code at a UK industry standard",
        "Use AWS tools such as EC2 to host your websites",
        "Integrate web server tools such as Nginx and Gunicorn",
        "Master essential developer tools such as GIT, Jupyter notebook, Google Colab, GPUs, Putty, Browser Developer Tools",
        "Gain experience in digital security - the DOs and DONTs of developing and scaling online websites and services",
        "Harness the power of Linux",
        "Create Application Programming Interfaces (APIs) in Python",
        "Gain the ability to access machines (e.g. computers) remotely using SSH",
        "Professional training in developing problem solving skills",
        "Develop a broad portfolio of projects you can showcase to any employer",
        "Gain the ability to adapt to any coding language with the concepts of Python",
        "Where to find machine learning computing power for free",
        "Master intermediate Python concepts such as object oriented programming and functional programming",
        "Create, maintain and post a range of databases within your websites"
      ],
      "course_content": {
        "Introduction": [
          "Welcome",
          "How to optimise your learning",
          "Your first website!"
        ],
        "HTML - the building block of all websites": [
          "In this section: HTML",
          "Introduction to HTML",
          "Getting set up",
          "Your first website in more detail",
          "Paragraphs and Headers",
          "Formatting text - bold, italic",
          "Introduction to practicals - the importance of step by step",
          "Practical - Portfolio website",
          "Solutions",
          "Practical - Tutorial site for beginners",
          "Solutions",
          "Links and URLs",
          "Images",
          "Lists",
          "Tables",
          "Practical Website for a Zoo",
          "Solutions",
          "Solutions part 2",
          "Line breaks",
          "Forms",
          "Practical - Update your Portfolio website",
          "Solutions",
          "Solutions part 2",
          "Adding comments",
          "Developer console",
          "iFrames",
          "HTML5 - Semantics",
          "Defined code - Div and span",
          "Getting your site live on the internet",
          "Bonus: free hosting for testing",
          "How to find the < > that you need",
          "Moblie responsive - viewport",
          "HTML Quiz",
          "In the next section - CSS"
        ],
        "CSS - universal styling of websites": [
          "In this section: CSS",
          "CSS - What is it and why is it so useful?",
          "Integrating CSS - best practise",
          "Colours - text, backgrounds, borders",
          "Practical - Beginners styling",
          "Solutions",
          "Picking colours",
          "Text styling",
          "Practical - Startup product page",
          "Solutions",
          "Practical - Portfolio page styled",
          "Solutions",
          "IDs and classes",
          "Divs and spans revisited",
          "Position",
          "Practical - Movie poster, coming soon",
          "Solutions",
          "Float",
          "Height and width",
          "Practical - Colour boxes",
          "Solutions",
          "Margins and padding",
          "Practical - Art gallery",
          "Solutions",
          "Decorating links <a>",
          "Practical - Creating navigation bars",
          "Solutions",
          "Tables",
          "Child, parent",
          "Display",
          "Practical - Art gallery ++",
          "Art gallery ++ solutions",
          "Keep up the good work!",
          "CSS Quiz",
          "Up next - Bootstrap"
        ],
        "Bootstrap - styling, made quick and easy": [
          "In this section: Bootstrap",
          "Adding Bootstrap to your site",
          "Using Bootstrap - jumbotrons, buttons, nav bars and more",
          "Customising bootstrap classes",
          "Practical - Lets have some fun!",
          "Solutions",
          "Grid",
          "Practical - Creating a product page",
          "Solutions",
          "Bootstrap Quiz",
          "In the next section - JavaScript"
        ],
        "JavaScript - making your websites interactive": [
          "In this section: JS",
          "Introduction to JavaScript",
          "Adding JavaScript to your HTML",
          "Functions - On click",
          "Practical - Date / time generator",
          "Solutions",
          "innerHTML",
          "Style - JavaScript changing CSS",
          "Other events - onmouseover",
          "Practical - Colour changer",
          "Solutions",
          "Variables + Functions",
          "Data types - strings, numbers, arrays",
          "Developer console - Error handling",
          "Strings and Numbers",
          "Operators",
          "Comparisons",
          "Logical operators",
          "User interaction with variables",
          "Practical - Counter",
          "Solutions",
          "Arrays",
          "Arrays in more detail",
          "Practical - Counter with background color",
          "Solutions",
          "Objects",
          "If / else",
          "Else if",
          "Practical - Welcome message",
          "Solutions",
          "Getting values from HTML forms into JS",
          "Using forms for a basic calculator",
          "Practical - Beginners Bank",
          "Solutions",
          "For loops",
          "Practical - Car dealership",
          "Solutions",
          "While loops",
          "Comments",
          "Practical - Tip calculator",
          "Solutions",
          "Functions - return",
          "Practical - Reaction time",
          "Solutions part 1",
          "Solutions part 2",
          "Basics of jQuery (optional)",
          ".css and .html",
          "Event handlers",
          "Animate",
          "Bonus: ES6 - Features and syntax",
          "Let",
          "Const",
          "Exponentiation **",
          "Arrow functions",
          "Default parameter values",
          "JavaScript Quiz",
          "In the next section - Python"
        ],
        "Python - coding for more than just websites": [
          "In this section: Python",
          "Introduction to Python",
          "Setting up python, pip and jupyter notebook",
          "Using jupyter notebook",
          "Print and input",
          "Practical - Welcome program",
          "Solutions",
          "Common data types",
          "Operators and comparisons",
          ".format",
          "Practical - Basic calculator",
          "Solutions",
          "Practical - Weight converter",
          "Solutions",
          "Using comments",
          "Importing modules and using methods",
          "Investigating errors",
          "Practical - Dice rolling simulator",
          "Solutions",
          "If / else / elif",
          "Practical - Number guessing challenge",
          "Solutions",
          "Lists",
          "Practical - Twister spinner",
          "Solutions",
          "A few list methods",
          "List indexes",
          "List slicing",
          "Practical - list within a list, within a list",
          "Solutions",
          "For loops",
          "Continue, break",
          "List comprehension",
          "Tuples and sets",
          "Dictionaries",
          "Practical - Currency converter",
          "Solutions",
          "While loops",
          "While loops in games",
          "Practical - Number guessing revisited",
          "Solutions",
          "Practical - Five lives",
          "Practical - Five lives: solutions",
          "Functions",
          "Function parameters",
          "Recursive functions",
          "Practical - Tic tac toe",
          "Solutions #1",
          "Solutions #2",
          "Solutions #3",
          "Requests library",
          "Practical - Real time currency conversion",
          "Solutions",
          "Practical - Live bitcoin values",
          "Note for bitcoin solutions",
          "Solutions",
          "Reading and writing files",
          "Creating your own modules to import",
          "Object oriented programming (OOP)",
          "Creating a class",
          "methods in a class",
          "__init__",
          "Objects - using attributes in a method",
          "Changing variables in a class object",
          "Practical - PayFriend, your own online bank",
          "Solutions",
          "Practical - Adventure journey",
          "Solutions",
          "Solutions part 2",
          "Functional programming",
          "Lambda",
          "Map",
          "Filter",
          "Generators",
          "Python Quiz",
          "Up next - Linux"
        ],
        "Linux - navigating your computer system": [
          "In this section - Linux",
          "Introduction to Linux",
          "Getting set up",
          "Navigating - pwd, ls, cd",
          "Make directory - mkdir",
          "Move file - mv",
          "Delete - rm",
          "Copy file - cp",
          "Tail",
          "Vim - writing and editing scripts in Linux",
          "Run a python script",
          "To keep in mind - SSH",
          "Practical - Quick challenge",
          "Quick challenge solutions",
          "Linux Quiz"
        ],
        "Django - python web framework for machine learning and AI": [
          "In this section - Django",
          "Introduction to Django",
          "Setting up Django, virtualenv and Atom",
          "Creating your first project",
          "Model View Controller",
          "URLs, Views",
          "Templates, Settings",
          "Practical - Your first Django website",
          "Your first Django website solutions",
          "Multi-page site",
          "Static files - CSS, JS, images",
          "Forms - Input / output",
          "Custom python scripts - import",
          "What you have learned with Django so far",
          "Beginner Django Quiz",
          "Up next - Git"
        ],
        "Git - industry standard tool for version control": [
          "In this section: Git",
          "Introduction to Git",
          "Setting up Git",
          "Setting up Github",
          "Init",
          "Add, Push, Commit",
          "Pull",
          "Practical - uploading one of your websites",
          "Uploading one of your websites - solutions",
          "Git Quiz",
          "Up next - Data science"
        ],
        "Beginner's Data science - load, clean, visualise and analyse big data": [
          "What is to come - DS, ML, DL, AI",
          "In this section: Data science",
          "Introduction to data science",
          "Getting set up for data science",
          "Accessing our first data set",
          "Loading our data - Pandas",
          "Basic exploration of the Dataframe",
          "Accessing columns",
          "Basic visualisation - crosstab, countplot, factorplot",
          "Variable types",
          "The 4 Cs of data cleaning",
          "Correcting",
          "Completing",
          "Creating",
          "Converting",
          "Titanic data set - data science recap",
          "Data science Quiz",
          "Up next - Machine learning"
        ]
      },
      "requirements": [
        "You need to have absolutely no prior knowledge of coding or website development - we start right from the basics and quickly get you up to speed",
        "A basic laptop and an internet connection"
      ],
      "description": "MASTERCLASS, WORLD CLASS COURSE - FULL STACK WEB DEVELOPMENT, MACHINE LEARNING + AI INTEGRATIONS\nMaster practical and theoretical concepts\nThis full stack web development, Django and AI combination course leads you through a complete range of software skills and languages, skilling you up to be an incredibly on-demand developer. The combination of being able to create full-stack websites AND machine learning and AI models is very rare - something referred to as a unAIcorn. This is exactly what you will be able to do by the end of this course.\nWhy you need this course\nWhether you're looking to get into a high paying job in tech, aspiring to build a portfolio so that you can land remote contracts and work from the beach, or you're looking to grow your own tech start-up, this course will be essential to set you up with the skills and knowledge to develop you into a unAIcorn.\nIt won't matter if you're a complete beginner to software or a seasoned veteran. This course will fill all the gaps in between. I will be there with you through your complete learning experience.\nWhat you will get out of this course\nI will give you straightforward examples, instructions, advice, insights and resources for you to take simple steps to start coding your own programs, solving problems that inspire you and instilling the 'developer's mindset' of problem solving into you.\nI don't just throw you in at the deep end - I provide you with the resources to learn and develop what you need at a pace that works for you and then help you stroll through to the finish line. Studies have shown that to learn effectively from online courses tutorials should last around ten minutes each. Therefore to maximise your learning experience all of the lectures in this course have been created around this amount of time or less.\nMy course integrates all of the aspects required to get you on the road becoming a successful web, software and machine learning developer. I teach and I preach, with live, practical exercises and walkthroughs throughout each of the sections.\n\n\nBy paying a small cost for this course I believe you will get your value back, with a lot more by the time you have completed it.\nAsk yourself - how much is mastering a full spectrum of skills in some of of the most exciting areas of software worth to you?\nHow long will it take?\nAlthough everyone is different, on average it has taken existing students between 1 - 6 months to complete the course, whilst developing their skills and knowledge along the way. It's best not to speed through the content, and instead go through a handful of lectures, try out the concepts by coding, yourself, and move on once you feel you've grasped the basics of those lectures.\nWho this is not for\nThis course is not for anyone looking for a one-click fix. Although I provide you with a path walked enough times that it can be a smooth journey it still requires time and effort from you to make it happen. If you're not interested in putting in your energy to truly better yours skills then this may not be the right course for you.\nWhat materials are included?\nThe majority of my lectures I have chosen to be as video so that you can hear me and see my workings when we're going through each and every area of the course. I include a vast array of practical projects that you can then use in the future to showcase your skills as you develop them, along with introductory clips and quizzes in each section to ensure that you're grasping the concepts effectively.\nI will be consistently adding more content and resources to the course as time goes by. Keep checking back here if you're not sure right now and feel free to send me a message with any questions or requests you may have.\nSo go ahead and click the 'Buy now' button when you feel ready on your screen.\nI look forward to seeing you in the course.",
      "target_audience": [
        "Complete beginners looking to learn from zero",
        "Seasoned developers looking to enhance their skills and diversify their portfolio",
        "Anyone looking to develop their technical skills"
      ]
    },
    {
      "title": "Data Science with Python (beginner to expert)",
      "url": "https://www.udemy.com/course/data-science-with-python-certification-training/",
      "bio": "Start your career as Data Scientist from scratch. Learn Data Science with Python. Predict trends with advanced analytics",
      "objectives": [
        "End-to-end knowledge of Data Science",
        "Prepare for a career path as Data Scientist / Consultant",
        "Overview of Python programming and its application in Data Science",
        "Detailed level programming in Python - Loops, Tuples, Dictionary, List, Functions & Modules, etc.",
        "Decision-making and Regular Expressions",
        "Introduction to Data Science Libraries",
        "Components of Python Ecosystem",
        "Analysing Data using Numpy and Pandas",
        "Data Visualisation with Matplotlib",
        "Three-Dimensional Plotting with Matplotlib",
        "Data Visualisation with Seaborn",
        "Introduction to Statistical Analysis - Math and Statistics",
        "Terminologies & Categories of Statistics, Correlation, Mean, Median, Mode, Quartile",
        "Data Science Methodology - From Problem to Approach, From Requirements to Collection, From Understanding to Preparation",
        "Data Science Methodology - From Modeling to Evaluation, From Deployment to Feedback",
        "Introduction to Machine Learning",
        "Types of Machine Learning - Supervised, Unsupervised, Reinforcement",
        "Regression Analysis - Linear Regression, Multiple Linear Regression, Polynomial Regression",
        "Implementing Linear Regression, Multiple Linear Regression, Polynomial Regression",
        "Classification, Classification algorithms, Logistic Regression",
        "Decision Tree, Implementing Decision Tree, Support Vector Machine (SVM), Implementing SVM",
        "Clustering, Clustering Algorithms, K-Means Clustering, Hierarchical Clustering",
        "Agglomerative & Divisive Hierarchical clustering",
        "Implementation of Agglomerative Hierarchical Clustering",
        "Association Rule Learning",
        "Apriori algorithm - working and implementation"
      ],
      "course_content": {
        "Introduction to Data Science": [
          "Introduction to Data Science"
        ],
        "Introduction to Python Programming": [
          "Introduction to Python Programming"
        ],
        "Variables and Data Types": [
          "Variables and Data Types - part 1",
          "Variables and Data Types - part 2"
        ],
        "Input-Output, Keywords, Identifiers": [
          "Input-Output, Keywords, Identifiers - part 1",
          "Input-Output, Keywords, Identifiers - part 2"
        ],
        "Operators and Types of Operators": [
          "Operators and Types of Operators - part 1",
          "Operators and Types of Operators - part 2"
        ],
        "Decision-Making": [
          "Decision-Making"
        ],
        "Loops in Python": [
          "Loops in Python - part 1",
          "Loops in Python - part 2",
          "Loops in Python - part 3"
        ],
        "List in Python": [
          "List in Python - part 1",
          "List in Python - part 2"
        ],
        "Tuples in Dictionary": [
          "Tuples in Dictionary - part 1",
          "Tuples in Dictionary - part 2"
        ],
        "Functions and Modules": [
          "Functions and Modules - part 1",
          "Functions and Modules - part 2",
          "Functions and Modules - part 3"
        ]
      },
      "requirements": [
        "Enthusiasm and determination to make your mark on the world!"
      ],
      "description": "A warm welcome to the Data Science with Python course by Uplatz.\n\n\nData Science with Python involves not only using Python language to clean, analyze and visualize data, but also applying Python programming skills to predict and identify trends useful for decision-making.\n\n\nWhy Python for Data Science?\nSince data revolution has made data as the new oil for organizations, today's decisions are driven by multidisciplinary approach of using data, mathematical models, statistics, graphs, databases for various business needs such as forecasting weather, customer segmentation, studying protein structures in biology, designing a marketing campaign, opening a new store, and the like. The modern data-powered technology systems are driven by identifying, integrating, storing and analyzing data for useful business decisions. Scientific logic backed with data provides solid understanding of the business and its analysis. Hence there is a need for a programming language that can cater to all these diverse needs of data science, machine learning, data analysis & visualization, and that can be applied to practical scenarios with efficiency. Python is a programming language that perfectly fits the bill here and shines bright as one such language due to its immense power, rich libraries and built in features that make it easy to tackle the various facets of Data Science.\n\n\nThis Data Science with Python course by Uplatz will take your journey from the fundamentals of Python to exploring simple and complex datasets and finally to predictive analysis & models development. In this Data Science using Python course, you will learn how to prepare data for analysis, perform complex statistical analyses, create meaningful data visualizations, predict future trends from data, develop machine learning & deep learning models, and more.\nThe Python programming part of the course will gradually take you from scratch to advanced programming in Python. You'll be able to write your own Python scripts and perform basic hands-on data analysis. If you aspire to become a data scientist and want to expand your horizons, then this is the perfect course for you. The primary goal of this course is to provide you a comprehensive learning framework to use Python for data science.\nIn the Data Science with Python training you will gain new insights into your data and will learn to apply data science methods and techniques, along with acquiring analytics skills. With understanding of the basic python taught in the initial part of this course, you will move on to understand the data science concepts, and eventually will gain skills to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular Python toolkits such as pandas, NumPy, matplotlib, scikit-learn, and so on.\nThe Data Science with Python training will help you learn and appreciate the fact that how this versatile language (Python) allows you to perform rich operations starting from import, cleansing, manipulation of data, to form a data lake or structured data sets, to finally visualize data - thus combining all integral skills for any aspiring data scientist, analyst, consultant, or researcher. In this Data Science using Python training, you will also work with real-world datasets and learn the statistical & machine learning techniques you need to train the decision trees and/or use natural language processing (NLP). Simply grow your Python skills, understand the concepts of data science, and begin your journey to becoming a top data scientist.\n\n\nData Science with Python Programming - Course Syllabus\n\n\n1. Introduction to Data Science\nIntroduction to Data Science\nPython in Data Science\nWhy is Data Science so Important?\nApplication of Data Science\nWhat will you learn in this course?\n\n\n2. Introduction to Python Programming\nWhat is Python Programming?\nHistory of Python Programming\nFeatures of Python Programming\nApplication of Python Programming\nSetup of Python Programming\nGetting started with the first Python program\n\n\n3. Variables and Data Types\nWhat is a variable?\nDeclaration of variable\nVariable assignment\nData types in Python\nChecking Data type\nData types Conversion\nPython programs for Variables and Data types\n\n\n4. Python Identifiers, Keywords, Reading Input, Output Formatting\nWhat is an Identifier?\nKeywords\nReading Input\nTaking multiple inputs from user\nOutput Formatting\nPython end parameter\n\n\n5. Operators in Python\nOperators and types of operators\n- Arithmetic Operators\n- Relational Operators\n- Assignment Operators\n- Logical Operators\n- Membership Operators\n- Identity Operators\n- Bitwise Operators\nPython programs for all types of operators\n\n\n6. Decision Making\nIntroduction to Decision making\nTypes of decision making statements\nIntroduction, syntax, flowchart and programs for\n- if statement\n- if…else statement\n- nested if\nelif statement\n\n\n7. Loops\nIntroduction to Loops\nTypes of loops\n- for loop\n- while loop\n- nested loop\nLoop Control Statements\nBreak, continue and pass statement\nPython programs for all types of loops\n\n\n8. Lists\nPython Lists\nAccessing Values in Lists\nUpdating Lists\nDeleting List Elements\nBasic List Operations\nBuilt-in List Functions and Methods for list\n\n\n9. Tuples and Dictionary\nPython Tuple\nAccessing, Deleting Tuple Elements\nBasic Tuples Operations\nBuilt-in Tuple Functions & methods\nDifference between List and Tuple\nPython Dictionary\nAccessing, Updating, Deleting Dictionary Elements\nBuilt-in Functions and Methods for Dictionary\n\n\n10. Functions and Modules\nWhat is a Function?\nDefining a Function and Calling a Function\nWays to write a function\nTypes of functions\nAnonymous Functions\nRecursive function\nWhat is a module?\nCreating a module\nimport Statement\nLocating modules\n\n\n11. Working with Files\nOpening and Closing Files\nThe open Function\nThe file Object Attributes\nThe close() Method\nReading and Writing Files\nMore Operations on Files\n\n\n12. Regular Expression\nWhat is a Regular Expression?\nMetacharacters\nmatch() function\nsearch() function\nre match() vs re search()\nfindall() function\nsplit() function\nsub() function\n\n\n13. Introduction to Python Data Science Libraries\nData Science Libraries\nLibraries for Data Processing and Modeling\n- Pandas\n- Numpy\n- SciPy\n- Scikit-learn\nLibraries for Data Visualization\n- Matplotlib\n- Seaborn\n- Plotly\n\n\n14. Components of Python Ecosystem\nComponents of Python Ecosystem\nUsing Pre-packaged Python Distribution: Anaconda\nJupyter Notebook\n\n\n15. Analysing Data using Numpy and Pandas\nAnalysing Data using Numpy & Pandas\nWhat is numpy? Why use numpy?\nInstallation of numpy\nExamples of numpy\nWhat is ‘pandas’?\nKey features of pandas\nPython Pandas - Environment Setup\nPandas – Data Structure with example\nData Analysis using Pandas\n\n\n16. Data Visualisation with Matplotlib\nData Visualisation with Matplotlib\n- What is Data Visualisation?\n- Introduction to Matplotlib\n- Installation of Matplotlib\nTypes of data visualization charts/plots\n- Line chart, Scatter plot\n- Bar chart, Histogram\n- Area Plot, Pie chart\n- Boxplot, Contour plot\n\n\n17. Three-Dimensional Plotting with Matplotlib\nThree-Dimensional Plotting with Matplotlib\n- 3D Line Plot\n- 3D Scatter Plot\n- 3D Contour Plot\n- 3D Surface Plot\n\n\n18. Data Visualisation with Seaborn\nIntroduction to seaborn\nSeaborn Functionalities\nInstalling seaborn\nDifferent categories of plot in Seaborn\nExploring Seaborn Plots\n\n\n19. Introduction to Statistical Analysis\nWhat is Statistical Analysis?\nIntroduction to Math and Statistics for Data Science\nTerminologies in Statistics – Statistics for Data Science\nCategories in Statistics\nCorrelation\nMean, Median, and Mode\nQuartile\n\n\n20. Data Science Methodology (Part-1)\nModule 1: From Problem to Approach\nBusiness Understanding\nAnalytic Approach\nModule 2: From Requirements to Collection\nData Requirements\nData Collection\nModule 3: From Understanding to Preparation\nData Understanding\nData Preparation\n\n\n21. Data Science Methodology (Part-2)\nModule 4: From Modeling to Evaluation\nModeling\nEvaluation\nModule 5: From Deployment to Feedback\nDeployment\nFeedback\nSummary\n\n\n22. Introduction to Machine Learning and its Types\nWhat is a Machine Learning?\nNeed for Machine Learning\nApplication of Machine Learning\nTypes of Machine Learning\n- Supervised learning\n- Unsupervised learning\n- Reinforcement learning\n\n\n23. Regression Analysis\nRegression Analysis\nLinear Regression\nImplementing Linear Regression\nMultiple Linear Regression\nImplementing Multiple Linear Regression\nPolynomial Regression\nImplementing Polynomial Regression\n\n\n24. Classification\nWhat is Classification?\nClassification algorithms\nLogistic Regression\nImplementing Logistic Regression\nDecision Tree\nImplementing Decision Tree\nSupport Vector Machine (SVM)\nImplementing SVM\n\n\n25. Clustering\nWhat is Clustering?\nClustering Algorithms\nK-Means Clustering\nHow does K-Means Clustering work?\nImplementing K-Means Clustering\nHierarchical Clustering\nAgglomerative Hierarchical clustering\nHow does Agglomerative Hierarchical clustering Work?\nDivisive Hierarchical Clustering\nImplementation of Agglomerative Hierarchical Clustering\n\n\n26. Association Rule Learning\nAssociation Rule Learning\nApriori algorithm\nWorking of Apriori algorithm\nImplementation of Apriori algorithm",
      "target_audience": [
        "Data Scientists",
        "Data Analysts / Data Consultants",
        "Senior Data Scientists / Data Analytics Consultants",
        "Newbies and beginners aspiring for a career in Data Science",
        "Data Engineers",
        "Machine Learning Engineers",
        "Software Engineers and Programmers",
        "Python Developers",
        "Data Science Managers",
        "Machine Learning / Data Science SMEs",
        "Digital Data Analysts",
        "Anyone interested in Data Science, Data Analytics, Data Engineering"
      ]
    },
    {
      "title": "Machine Learning : Complete Maths for Machine Learning",
      "url": "https://www.udemy.com/course/machine-learning-2020-complete-maths-for-machine-learning/",
      "bio": "Learn Math for Machine Learning, Math for Data Science, Linear Algebra, Calculus, Vectors & Matrices, Probability & more",
      "objectives": [
        "Understand every mathematical concept very intuitively and you will surely overcome the fear of mathematics, if you have any.",
        "Master various concepts of Calculus and understand how they are used for optimization of Machine Learning Algorithms.",
        "Learn Algebraic Equations and how they are used in the Machine Learning",
        "Understand Linear Algebra including Vectors and Matrices",
        "You will learn how the matrix transformation is used for various data transformations.",
        "Get all the basics covered for probability including conditional probability and probability distribution"
      ],
      "course_content": {},
      "requirements": [
        "No pre-requisites. I will teach you right from basics of Algebra.",
        "Passion to learn something new."
      ],
      "description": "Congratulations if you are reading this. That simply means, you have understood the importance of mathematics to truly understand and learn Data Science and Machine Learning.\nIn this course, we will cover right from the foundations of Algebraic Equations, Linear Algebra, Calculus including Gradient using Single and Double order derivatives, Vectors, Matrices, Probability and much more.\nMathematics form the basis of almost all the Machine Learning algorithms. Without maths, there is no Machine Learning. Machine Learning uses mathematical implementation of the algorithms and without understanding the math behind it is like driving a car without knowing what kind of engine powers it.\nYou may have studied all these math topics during school or universities and may want to freshen it up. However, many of these topics, you may have studied in a different context without understanding why you were learning them. They may not have been taught intuitively or though you may know majority of the topics, you can not correlate them with Machine Learning.\nThis course of Math For Machine Learning, aims to bridge that gap. We will get you upto speed in the mathematics required for Machine Learning and Data Science. We will go through all the relevant concepts in great detail, derive various formulas and equations intuitively.\nThis course is divided into following sections,\nAlgebra Foundations\nIn this section, we will lay the very foundation of Algebraic Equations including Linear Equations and how to plot them. We will understand what are Exponents, Logs, Polynomial and quadratic equations. Almost all the Machine Learning algorithms use various functions for loss measurement or optimization. We will go through the basics of functions, how to represent them and what are continuous and non-continuous functions.\nCalculus\nIt is said that without calculus and differential equations, Machine Learning would have never been possible. The Gradient Descent using derivatives is essence of minimizing errors for a Machine Learning algorithm. We will understand various terms of Rate of Change, Limits, What is Derivative, including Single, Double and Partial Derivatives. I will also explain with an example, how machine learning algorithms use calculus for optimization.\nLinear Algebra\nLinear Algebra is the mathematics of the 21st Century. Every record of data is bound by some form of algebraic equation. However, it's nearly impossible for humans to create such an equation from a dataset of thousands of records. That's where the ability of vectors and matrices to crunch those numerical equations and create meaningful insights in the form of linear equations help us. We will see, right from the foundations of Vectors, Vector Arithmetic, Matrices and various arithmetic operations on them. We will also see, how the vectors and matrices together can be used for various data transformations in Machine Learning and Data Science.\nProbability\nProbability plays an important role during classification type of machine learning problems. It is also the most important technique to understand the statistical distribution of the data. Conditional probability also helps in classification of the dependent variable or prediction of a class.\nWith all of that covered, you will start getting every mathematical term that is taught in any of the machine learning and data science class.\nMathematics has been my favorite subject since the childhood and you will see my passion in teaching maths as you go through the course.\nI firmly believe in what Einstein said, \"If you can not explain it simple enough, You have not understood it enough.\". I hope I can live upto this statement.\nI am super excited to see you inside the class. So hit the ENROLL button and I will see you inside the course.\nYou will truly enjoy Mathematics For Machine Learning....",
      "target_audience": [
        "Beginners who want to learn Data Science and Machine Learning",
        "Practitioners and experts who want to get a refresher of the maths for machine learning"
      ]
    },
    {
      "title": "PyTorch for Deep Learning and Computer Vision",
      "url": "https://www.udemy.com/course/pytorch-for-deep-learning-and-computer-vision/",
      "bio": "Build Highly Sophisticated Deep Learning and Computer Vision Applications with PyTorch",
      "objectives": [
        "Implement Machine and Deep Learning applications with PyTorch",
        "Build Neural Networks from scratch",
        "Build complex models through the applied theme of Advanced Imagery and Computer Vision",
        "Solve complex problems in Computer Vision by harnessing highly sophisticated pre-trained models",
        "Use style transfer to build sophisticated AI applications"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Additional FREE Content",
          "Your Instructor"
        ],
        "Getting Started": [
          "Finding the codes (Github)",
          "A Look at the Projects"
        ],
        "Intro to Tensors - PyTorch": [
          "Intro",
          "1 Dimensional Tensors",
          "Vector Operations",
          "2 Dimensional Tensors",
          "Slicing 3D Tensors",
          "Matrix Multiplication",
          "Gradient with PyTorch",
          "Outro"
        ],
        "Linear Regression - PyTorch": [
          "Intro",
          "Making Predictions",
          "Linear Class",
          "Custom Modules",
          "Creating Dataset",
          "Loss Function",
          "Gradient Descent",
          "Mean Squared Error",
          "Training - Code Implementation",
          "Getting Weird Results?",
          "Outro",
          "Summary"
        ],
        "Perceptrons - PyTorch": [
          "Intro",
          "What is Deep Learning",
          "Creating Dataset",
          "Perceptron Model",
          "Model Setup",
          "Model Training",
          "Model Testing",
          "Outro"
        ],
        "Deep Neural Networks - PyTorch": [
          "Intro",
          "Non-Linear Boundaries",
          "Architecture",
          "Feedforward Process",
          "Error Function",
          "Backpropagation",
          "Code Implementation",
          "Testing Model",
          "Outro"
        ],
        "Image Recognition - PyTorch": [
          "Intro",
          "MNIST Dataset",
          "Training and Test Datasets",
          "Important Update - Bug fix for next lesson",
          "Image Transforms",
          "Neural Network Implementation",
          "Neural Network Validation",
          "Test Links",
          "Final Tests",
          "A note on adjusting batch size",
          "Outro"
        ],
        "Convolutional Neural Networks - PyTorch": [
          "Convolutions and MNIST",
          "Convolutional Layer",
          "Convolutions II",
          "Pooling",
          "Fully Connected Network",
          "Neural Network Implementation with PyTorch",
          "Model Training with PyTorch"
        ],
        "CIFAR 10 Classification - PyTorch": [
          "The CIFAR 10 Dataset",
          "Testing LeNet",
          "Hyperparameter Tuning",
          "Data Augmentation"
        ],
        "Transfer Learning - PyTorch": [
          "Pre-trained Sophisticated Models",
          "Github Link for Dataset",
          "AlexNet and VGG16"
        ]
      },
      "requirements": [
        "No experience is required"
      ],
      "description": "PyTorch has rapidly become one of the most transformative frameworks in the field of Deep Learning. Since its release, PyTorch has completely changed the landscape in the field of deep learning due to its flexibility, and how easy it is to use when building Deep Learning models.\nDeep Learning jobs command some of the highest salaries in the development world. This course is meant to take you from the complete basics, to building state-of-the art Deep Learning and Computer Vision applications with PyTorch.\nLearn & Master Deep Learning with PyTorch in this fun and exciting course with top instructor Rayan Slim. With over 44000 students, Rayan is a highly rated and experienced instructor who has followed a \"learn by doing\" style to create this amazing course.\nYou'll go from beginner to Deep Learning expert and your instructor will complete each task with you step by step on screen.\nBy the end of the course, you will have built state-of-the art Deep Learning and Computer Vision applications with PyTorch. The projects built in this course will impress even the most senior developers and ensure you have hands on skills that you can bring to any project or company.\nThis course will show you to:\nLearn how to work with the tensor data structure\nImplement Machine and Deep Learning applications with PyTorch\nBuild neural networks from scratch\nBuild complex models through the applied theme of advanced imagery and Computer Vision\nLearn to solve complex problems in Computer Vision by harnessing highly sophisticated pre-trained models\nUse style transfer to build sophisticated AI applications that are able to seamlessly recompose images in the style of other images.\nNo experience required. This course is designed to take students with no programming/mathematics experience to accomplished Deep Learning developers.\nThis course also comes with all the source code and friendly support in the Q&A area.\nWho this course is for:\nAnyone with an interest in Deep Learning and Computer Vision\nAnyone (no matter the skill level) who wants to transition into the field of Artificial Intelligence\nEntrepreneurs with an interest in working on some of the most cutting edge technologies\nAll skill levels are welcome!",
      "target_audience": [
        "Anyone with an interest in Deep Learning and Computer Vision",
        "Anyone (no matter the skill level) who wants to transition into the field of Artificial Intelligence",
        "Entrepreneurs with an interest in working on some of the most cutting edge technologies",
        "All skill levels are welcome!"
      ]
    },
    {
      "title": "YOLO: Custom Object Detection & Web App in Python",
      "url": "https://www.udemy.com/course/yolo-custom-object-detection/",
      "bio": "Learn to train custom object detection model using Python, OpenCV. Develop web app with Streamlit",
      "objectives": [
        "Python based YOLO Object Detection using Custom Trained Dataset Models.",
        "YOLO Custom Training",
        "YOLO V5 Object Detection",
        "Train multiple objects",
        "Essential concepts of Streamlit",
        "Develop Web App with Python"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Understanding Object Detection",
          "Install Python",
          "Install Virtual Environment",
          "Install Python Packages",
          "Do's & Don'ts in Data Collection and Labeling",
          "Download Complete Python & Data"
        ],
        "Data Preparation": [
          "What we will do in this section",
          "Collect Data",
          "Labeling",
          "Get List of XML files in Python",
          "Read & Extract Labels Data from XML files",
          "Read & Extract Labels Data from XML files part2",
          "Convert Labels information into Pandas Dataframe",
          "Labels for YOLO model",
          "Create Labes for YOLO model in Python",
          "Split Data/Images into train and test sets",
          "LABEL ENCODING TO OBJECTS",
          "Create Train & Test Folder",
          "Create Function and Move Train image, and label text in train folder",
          "Move Test images, and label text in test folder"
        ],
        "Training YOLO Model": [
          "Create YAML file",
          "Google Drive Resources",
          "Setting Up Google Colab",
          "Get YOLO v5 repository",
          "Training YOLO v5 model",
          "New Update in Training Command",
          "Save YOLO model",
          "New update in export model",
          "How to Resume training",
          "Results & Evaluation"
        ],
        "Prediction from YOLO Model": [
          "Download Resources",
          "What we will do",
          "Step-1, Load data.yaml file",
          "Step-2: Load YOLO model with OpenCV",
          "Step-3: Get detection from YOLO model",
          "Understand YOLO model output detections",
          "Non Maximum Suppression - part 1",
          "Non Maximum Suppression - part 2",
          "Draw Bounding Box",
          "Create YOLO Predictions Module",
          "Final Object Detection from Image with YOLO",
          "Real Time Object Detection with YOLO",
          "Error !!!"
        ],
        "Next Steps": [
          "Web App"
        ],
        "Streamlit Crash Course for Web App": [
          "Streamlit Crash Course",
          "Install Visual Studio Code",
          "Activate Virtual Environment in VS Code",
          "Your First Streamlit App",
          "Streamlit Basics part - 1",
          "Streamlit Basics part - 2",
          "Streamlit Basics part - 3",
          "Streamlit Layouts - page configuration",
          "Streamlit Layouts - Side Bar",
          "Streamlit Layouts - Columns",
          "Streamlit Layouts - Tabs",
          "Streamlit Widgets",
          "Streamlit Widgets - Radio Button",
          "Streamlit Widget - Selectbox",
          "Streamlit Widget - Slider",
          "Streamlit Widgets - Text Inputs",
          "Streamlit Widgets - File Upload",
          "Streamlit Widgets - File Upload part - 2"
        ],
        "Project: Develop Web App for YOLO Model": [
          "Download Resources",
          "Setting Up App",
          "Home Page",
          "YOLO App - Load YOLO model into app",
          "YOLO App - part - 2",
          "YOLO App - part -3",
          "YOLO App - part - 4",
          "YOLO App - part - 5",
          "YOLO App - part - 6",
          "YOLO for Images App",
          "YOLO for Video/Real Time Object Detection Part - 1",
          "YOLO for Video/Real Time Object Detection Part - 1"
        ],
        "BONUS": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "A decent configuration computer (preferably Windows) and an enthusiasm to dive into the world Image and Object Recognition using Python",
        "Machine Learning Python Knowledge",
        "Basics of OpenCV"
      ],
      "description": "Welcome to 'YOLO: Custom Object Detection & Web App in Python'\nObject Detection is the most used applications of Computer Vision, where computer/machine can able to locate and classify the object in an image.\n\n\nIn this course we specifically using YOLO (You Only Look Once) and powerful and popular unified object detection model. YOLO uses neural networks to provide real-time object detection. This algorithm is popular because of its speed and accuracy. It has been used in various applications to detect traffic signals, people, parking meters, and animals.\n\n\nThis course is divided into two halves. The first half deals with object detection with custom dataset where we will locate 20 classes of objects. And in second half we will create an web app and give the Graphical User Interphase experience to the use. Not only that we will also deploy our model in Cloud platform.\n\n\nNow let us see the topics in the course\n\n\nIntroductory theory session about YOLO Object Detection\nHere in this section I will explain history of  Object Detection\nObject Detection Metrics like IoU (Intersection Over Union), Precision, mean Average Precision (mAP) etc.\nThen we will see the mathematical concept behind YOLO\nAlso I will cover how YOLO improved from each version\n\n\nAfter that, we are ready to proceed with preparing our computer for Python coding by downloading and installing the Python package and will check and see if everything is installed fine.\n2.  Data Preparation for YOLO model\nIn this section we will put every we learn in to practice. This section is completely hands-on where we will do python code and use pandas dataframes to prepare the data.\na.   Thumb rules to follow in Collect Data\nb.   Label image for  object detection: Here we will use LabelImg tool which is an open source tool to label the label.\nc.   Parse data from XML files and extract information like filename, size, bounding box info like (xmin, xmax, ymin, ymax)\nd.   Process the data from XML in pandas dataframe. And then split the image and save the respective label information                         information in train and test.\n\n\n3.  Train YOLO v5 Model\n4.   Develop Web App in Python\n\n\nThat's all about the topics which are currently included in this quick course. The code, images and weights used in this course has been uploaded and shared in a folder. I will include the link to download them in the last session or the resource section of this course. You are free to use the code in your projects with no questions asked.\n\n\nAlso after completing this course, you will be provided with a course completion certificate which will add value to your portfolio.",
      "target_audience": [
        "Beginner and Professional who want to develop custom object detection model form scratch."
      ]
    },
    {
      "title": "Modern Natural Language Processing in Python",
      "url": "https://www.udemy.com/course/modern-nlp/",
      "bio": "Solve Seq2Seq and Classification NLP tasks with Transformer and CNN using Tensorflow 2 in Google Colab",
      "objectives": [
        "Build a Transformer, new model created by Google, for any sequence to sequence task (e.g. a translator)",
        "Build a CNN specialized in NLP for any classification task (e.g. sentimental analysis)",
        "Write a custom training process for more advanced training methods in NLP",
        "Create customs layers and models in TF 2.0 for specific NLP tasks",
        "Use Google Colab and Tensorflow 2.0 for your AI implementations",
        "Pick the best model for each NLP task",
        "Understand how we get computers to give meaning to the human language",
        "Create datasets for AI from those data",
        "Clean text data",
        "Understand why and how each of those models work",
        "Understand everything about the attention mechanism, lying behind the newest and most powerful NLP algorithms"
      ],
      "course_content": {
        "Introduction": [
          "Welcome! NLP introduction & course overview",
          "Course curriculum, Colab toolkit and data links",
          "EXTRA: Learning Path"
        ],
        "CNN for NLP - Intuition": [
          "Introduction",
          "Basic CNN",
          "Image to text",
          "Image for word embedding from the session 7",
          "CNN for NLP"
        ],
        "CNN for NLP - Application (sentimental analysis)": [
          "Introduction",
          "Dependencies",
          "Loading Files",
          "Cleaning",
          "Inputs creation",
          "Model",
          "Configuration",
          "Training",
          "Evaluation"
        ],
        "Transformer - Intuition": [
          "Introduction",
          "Old fashioned",
          "General understanding",
          "Attention",
          "Positional encoding",
          "Last details"
        ],
        "Transformer - Application": [
          "Introduction",
          "Dependencies",
          "Loading Files",
          "Cleaning",
          "Tokenizing",
          "Max Length",
          "Inputs Creation",
          "Positional encoding",
          "Scaled dot product attention",
          "Multi head attention",
          "Encoder",
          "Decoder",
          "Transformer",
          "Training",
          "Evaluation"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "PC with Internet connection",
        "Python Programming Skills",
        "Recommended: Experience with TF2.0",
        "Recommended: Google Collab"
      ],
      "description": "Modern Natural Language Processing course is designed for anyone who wants to grow or start a new career and gain a strong background in NLP.\n\n\nNowadays, the industry is becoming more and more in need of NLP solutions. Chatbots and online automation, language modeling, event extraction, fraud detection on huge contracts are only a few examples of what is demanded today. Learning NLP is key to bring real solutions to the present and future needs.\n\n\nThroughout this course, we will leverage the huge amount of speech and text data available online, and we will explore the main 3 and most powerful NLP applications, that will give you the power to successfully approach any real-world challenge.\n\n\nFirst, we will dive into CNNs to create a sentimental analysis application.\nThen we will go for Transformers, replacing RNNs, to create a language translation system.\n\n\nThe course is user-friendly and efficient: Modern NL leverages the latest technologies—Tensorflow 2.0 and Google Colab—assuring you that you won’t have any local machine/software version/compatibility issues and that you are using the most up-to-date tools.",
      "target_audience": [
        "AI amateurs that are eager to learn how we process language nowadays",
        "AI students that need to have a deeper and wider knowledge about NLP",
        "Business driven people that are eager to know how NLP can be applied to their field to leverage any text data",
        "Anyone who wants to start a new career and get a strong background in NLP, adding efficient cases to their portfolio"
      ]
    },
    {
      "title": "LLMs Mastery: Complete Guide to Transformers & Generative AI",
      "url": "https://www.udemy.com/course/llms-mastery-complete-guide-to-transformers-generative-ai/",
      "bio": "Generative AI, r1, LLMs, ChatGPT, GPT4, o1, Llama3, Decoders, T5, BERT, LoRA, FSDP, 4bit, Machine Learning, Data Science",
      "objectives": [
        "Grasp NLP Fundamentals: Understand the evolution from rule-based systems to advanced LLMs like Llama3, Gemma2, Phi3, and Mistral.",
        "Master Transformers & LLMs: Learn the architecture and application of Transformers in depth. Including tokenization, embeddings, pre-training & fine-tunning.",
        "Understand Generative AI Principles: Develop skills in building and fine-tuning generative models for real-world applications using RLHF and Chat-Templates",
        "Use Transformer Models: Overview LLMs and Encoder-Decoder models like BERT, GPT, T5, Llama and more in many different NLP tasks: Personal assistant, Reviews, QA",
        "Specialised Techniques: Implement 8-bit and 4-bit training, and use tools like DeepSpeed and FSDP, along with PeFT, LoRA, FlashAttention and more."
      ],
      "course_content": {
        "1.1 Introduction: Course Overview + What You'll Learn": [
          "Introduction to Part 1 of the Course: Transformers Fundamentals",
          "Introduction to Part 2 of The Course: LLMs"
        ],
        "1.2 Getting Started: How to Make the Best Use of this Course": [
          "Course Structure: How to get the Most out of this Course",
          "Environment Setup: Prepare and Use the Resource of this Course Right"
        ],
        "1.3 Overview of Natural Language Processing: Bring Transformers into Perspective": [
          "Rule-Based Systems Era",
          "Statistical Era",
          "Machine Learning Era",
          "Embeddings Era",
          "Embeddings Era Quiz"
        ],
        "1.4 Transformers Introduction: Important Concepts and Use-cases": [
          "Encoders, Decoders and The Attention Mechanism",
          "Understanding the Transformer Architecture",
          "Pre-training & Fine-tunning",
          "Tokenisation & Embeddings",
          "Pre-training and Fine-tuning Transformers"
        ],
        "1.5 Popular Transformers Models: Choose the Best Model for the Job": [
          "BERT",
          "GPT",
          "T5",
          "Exploring T5: The Text-to-Text Transfer Transformer"
        ],
        "1.6 Using Transformers: Building Blocks and Hidden Gems (Practical)": [
          "Building Blocks",
          "Tokenizers",
          "Understanding Tokenizers for Transformers",
          "Word Embeddings",
          "Masked Language Modeling (MLM)",
          "Masked Language Modeling Quizz",
          "Semantic Search Index"
        ],
        "1.7 Mastering Real-World Scenarios with Transformers and LLMs (Practical)": [
          "BERT (Encoder-model) for Extractive Question Answering",
          "GPT (Decoder-model) for Instruction Following",
          "Understanding GPT Instruct",
          "T5 (Encoder-Decoder-model) for Writing Product Reviews",
          "NLP Evolution and Fundamentals"
        ],
        "2.1 Introduction to Large Language Models": [
          "Decoding Large Language Models: An Introduction",
          "Introduction to Large Language Models (LLMs)",
          "RLHF: Teaching LLMs to Communicate Effectively",
          "Understanding Input/Output in Language Models",
          "Chat Templates: Hands On Overview",
          "Decision Frameworks for LLM Selection",
          "Generation: An Interactive Guide"
        ],
        "2.2 Preparing for LLM Training (Practical)": [
          "Comprehensive Dive into Sequence Length",
          "Token Counts: Practical Intuition & Impact",
          "Precision Matters: Numerical Precision in Training",
          "Sequences and Tokens",
          "Navigating GPU Selection: A Guide to Hardware Platforms",
          "Practice Fundamentals: Most Basic Form of Training LLMs",
          "Practice Fundamentals: Most Basic Form of Training LLMs - Part 2",
          "Practice Fundamentals: Most Basic Form of Training LLMs - Part 3"
        ],
        "2.3 Advanced LLM Training Techniques (Practical)": [
          "Understanding Practical Limitations",
          "Boosting Efficiency: PeFT and LoRa in Depth",
          "Managing Data Memory: Batch Size & Sequence Length",
          "Advanced Solutions: Gradient Accumulation & Checkpointing",
          "Fitting Giants: Practical Introduction to LoRA for Large Models",
          "Expanding LoRA: Adapter Merging and Effective Evaluations",
          "Advanced Techniques"
        ]
      },
      "requirements": [
        "Basic understanding of Python programming, some basic familiarity with machine learning concepts, and a strong eagerness to delve into LLMs and Natural Language Processing."
      ],
      "description": "Welcome to \"LLMs Mastery: Complete Guide to Generative AI & Transformers\"!\nThis practical course is designed to equip you with the knowledge and skills to build efficient, production-ready Large Language Models using cutting-edge technologies.\n\n\nKey Topics Covered:\nGenerative AI: Understand the principles and applications of Generative AI in creating new data instances.\nChatGPT & GPT4: Dive into the workings of advanced AI models like ChatGPT and GPT4.\nLLMs: Start with the basics of LLMs, learning how they decode, process inputs and outputs, and how they are taught to communicate effectively.\nEncoder-Decoders: Master the concept of encoder-decoder models in the context of Transformers.\nT5, GPT2, BERT: Get hands-on experience with popular Transformer models such as T5, GPT2, and BERT.\nMachine Learning & Data: Understand the role of machine learning and data in training robust AI models.\nAdvanced Techniques: Sophisticated training strategies like PeFT, LoRa, managing data memory and merging adapters.\nSpecialised Skills:  Cutting-edge training techniques, including 8-bit, 4-bit training and Flash-Attention.\nScalable Solutions: Master the use of advanced tools like DeepSpeed and FSDP to efficiently scale model training.\n\n\nCourse Benefits:\n• Career Enhancement: Position yourself as a valuable asset in tech teams, capable of tackling significant AI challenges and projects.\n• Practical Application: Learn by doing—build projects that demonstrate your ability to apply advanced LLM techniques in real-world situations.\n• Innovative Approach: Stay at the forefront of AI technology by mastering techniques that are shaping the future of machine learning.\n\n\n\n\nWhat You Will Learn:\nNatural Language Processing Basics\n• Journey Through NLP Evolution: From rule-based systems to advanced embeddings.\n• Foundation in NLP: Set the stage for advanced learning in natural language processing.\n\n\nIntroduction to Transformers\n• Transformer Architecture: Learn about encoders, decoders, and attention mechanisms.\n• Model Strategies: Understand pre-training, fine-tuning, tokenization, and embeddings.\n\n\nPopular Transformer Models\n• Explore Key Models: Dive into BERT, GPT, and T5 and their unique capabilities.\n• Deepen Model Insights: Uncover the potential and versatility of Transformer technology.\n\n\nUsing Transformers (Practical)\n• Hands-On Experience: Apply Transformers in real-world scenarios.\n• Advanced Techniques: Master tokenization, embeddings, and MLMs.\n• Project Implementation: Build a Semantic Search Index.\n\n\nNLP Tasks and Applications (Practical)\n• Real-World Applications: Use BERT for question answering, GPT for personal assistants, and T5 for writing reviews.\n• Practical NLP Skills: Experience the direct application of NLP tasks.\n\n\nFoundations of Large Language Models\n• Introduction to LLMs: Understand basic architecture and functionalities.\n• Communication Techniques: Enhance model responsiveness with RLHF.\n• Input/Output Processes: Explore how LLMs handle data for AI interactions.\n\n\nAdvanced Configuration and Optimization\n• Chat Template Design: Practical experience in structuring LLM interactions.\n• Model Selection Frameworks: Strategic decision-making for choosing LLMs.\n• Generation Techniques: Tailor LLM outputs through interactive learning.\n\n\nSpecialized Training Techniques\n• Advanced Model Training: Focus on sequence length, token counts, and numerical precision.\n• Efficiency Methods: Learn 8-bit and 4-bit training to adapt models to constraints.\n• Scaling Tools: Implement DeepSpeed and FSDP for efficient model scaling.\n\n\nPractical Applications of LLMs\n• Application in Contexts: Apply LLM skills in simulated real-world projects.\n• Task-Specific Training: Optimize models for specific tasks like memory management and efficiency.\n\n\n\n\n\n\nWho This Course Is For:\nTech Professionals: Enhance your skills and knowledge in cutting-edge AI technologies.\nAspiring AI Practitioners: Get a comprehensive education in LLMs from basic principles to advanced applications.\nResearchers and Students: Gain a deep understanding of the latest developments and how they can be applied to solve complex problems.\n\nReady to dive into the world of Generative AI and Transformers?\nEnroll today and start your journey to mastery!",
      "target_audience": [
        "Those who wish to understand the new world of LLMs, how chatGPT, GPT4 and Llama work, and how to build their own powerful language models."
      ]
    },
    {
      "title": "Master Generative AI: Automate Content Effortlessly with AI",
      "url": "https://www.udemy.com/course/ai-content-generation/",
      "bio": "Automate Text, Image, Audio, and Video Generation: ChatGPT, Stable Diffusion XL, DALLE-3, Midjourney, GPT-4, Perplexity!",
      "objectives": [
        "Use ChatGPT, DALLE-2, Stable Diffusion, Whisper, Synthesia, MAKE-A-VIDEO, IMAGEN to Automate Content Creation",
        "Master Prompt Engineering across: Text to text & Text to Image Generation for best results on BARD, GPT-4, DALLE-2, GEN-1, Stable Diffusion",
        "Stay up-to-date on Generative AI News & Concepts in practical ways: GANs, GAI, LMMs, Transformers, Stable Diffusion, AI Content Generation,",
        "Unleash Your Content Potential! Generate 100s of Ideas for Text, Image, Audio, & Video with the Power of Generative AI.",
        "Learn how to use industry-leading tools for text, image, audio & video generation",
        "Unleash Your AI Creativity with Avatars! Learn to Generate AI-Powered Avatars without Writing Any Code in Stable Diffusion",
        "Stay Ahead of the Game with the Latest Generative AI News. Get Access to Cutting-Edge Updates and Advance Your Skills Today!",
        "Unlock Your Content Creation Potential! Get Hands-On Experience and Gain a Competitive Edge with Generative AI.",
        "Learn how to leverage Perplexity AI for Research and Writing"
      ],
      "course_content": {
        "Your Outline to be a Generative AI Pro by end of the course": [
          "Introduction",
          "Course outline",
          "Watch it at your own pace: 1.25x, 1.5x",
          "Join Discord for updates",
          "Claim Free AI Newsletter spot"
        ],
        "AI Text Generation (ChatGPT, Google's BARD, Bing GPT4, Poe, Lambda, etc.)": [
          "(Update) New ChatGPT UI",
          "ChatGPT Walkthrough (GPT 3, 3.5 and GPT-4)",
          "ChatGPT Web-browsing",
          "GPT-3 / 3.5 vs GPT-4",
          "ChatGPT Code Interpreter Quick Guide",
          "AI Shorts Generation: Quick Guide",
          "Olly Social (AI Chrome Extension for Social Media Engagement)",
          "GPTs - Building a Knowledge base Chatbot with ChatGPT",
          "DALLE-3 Inside GPT-4",
          "GPT-4 Vision Guide",
          "Prompt Engineering: What are text prompts?",
          "Prompt Engineering (Part 2)",
          "Learn how to leverage Perplexity AI for Research and Writing",
          "2025 Perplexity AI Update",
          "AI Content Detection: How to bypass?",
          "Resource for 200+ Prompts",
          "Claude - New Feature (Visual PDFs)"
        ],
        "[NEW 2024] Prompt Engineering": [
          "Prompt Library",
          "Strategy 1: Clear Instructions",
          "Strategy 2: Reference Text",
          "Strategy 3: Splitting complex text",
          "Strategy 4: Let Model Think",
          "Strategy 5: Use External Tools",
          "Strategy 6: Test Changes Systematically"
        ],
        "AI Image Generation (no code method)": [
          "Overview of the section",
          "Massive GPT-4o Image Generation Update (2025)",
          "Midjourney Mastery: Create Visually Stunning AI Art",
          "The Best Open source AI Model // FREE ALTERNATIVE",
          "Quick note",
          "AI Avatar Generation",
          "YouTube Thumbnails with DALLE-3",
          "/Describe feature in Midjourney",
          "Midjourney v5.2: infinite unzoom & /shorten updates",
          "20+ Prompts for Hyper-realism",
          "Adobe Firefly: Quick Guide",
          "What are prompts?",
          "Filters in prompts",
          "How to write prompts?",
          "Introduction to DALLE-2",
          "Let's try DALLE-2"
        ],
        "AI Video Generation (MAKE-A-VIDEO by Meta, GEN-1, IMAGEN by Google, Synthesia)": [
          "How to use GEN-1?",
          "100 Short Videos in 10 minutes",
          "Generating real-human videos",
          "Make a Video Avatar of yourself!",
          "Short Films using AI (GEN-2)",
          "AI Video Editing",
          "Gen-3 Update",
          "Gen-3 Alpha Image to Video RunwayML"
        ],
        "Autonomous AI: AutoGPT + All new AI Updates": [
          "AutoGPT Setup",
          "Open source autogpt setup",
          "Generative Agents",
          "Generative Agents (2)",
          "JARVIS",
          "AgentGPT",
          "BabyAGI",
          "Devin, AI Software Engineer, by Cognitive Labs",
          "Llama-3 (Replace Github co-pilot with Llama-3)",
          "DeepSeek-r1, o3-mini, o1-mini, and Gemini 2.0 Comparison"
        ],
        "AI Work Automation": [
          "Google Workspace AI Updates"
        ],
        "AI Audio Generation (Whisper by OpenAI, Google's MusicLM and more!)": [
          "IMP: Generating Audio using AI",
          "MusicLM by Google: Generating Music using AI!",
          "Speech to Text: Whisper AI by Runway",
          "Summary"
        ],
        "AI Code Generation": [
          "GPT-Engineer"
        ],
        "AI Image Generation (Stable Diffusion, DALLE-2, Google's IMAGEN) Low code": [
          "Introduction to stable diffusion",
          "Two ways to work in Stable Diffusion",
          "Offline Setup: Stable Diffusion",
          "Offline: Stable Diffusion Walkthrough",
          "Offline: Image to Image Generation",
          "Offline: Inpainting + Outpainting"
        ]
      },
      "requirements": [
        "No Programming experience is necessary",
        "Access to a computer with internet",
        "Optionally: Computer with a decent graphics setup"
      ],
      "description": "Welcome to Best Generative AI Course in the Market. This comprehensive helps you master tools like ChatGPT, Midjourney, BARD, GPT-4, DALLE-2, DALLE-3 Stable Diffusion, and GEN-1. You will learn how to use these tools to generate text, image, audio, and video content with minimal effort.\nThe course is designed with hands-on learning in mind, so you will get plenty of practice with each of the technologies covered. You will learn about prompt engineering techniques and how to achieve the best results with text-to-text and text-to-image generation. Additionally, you will learn how to use AI tools such as ChatGPT, DALLE-2, Stable Diffusion, Whisper, Synthesia, MAKE-A-VIDEO, and IMAGEN to automate your content creation process, saving time and effort.\nIn addition to the technical aspects of Generative AI, this \"Best Generative AI Course\" also covers the latest news and concepts in the field. You will learn about GANs, GAI, LMMs, Transformers, Stable Diffusion, and AI content generation. This will give you a well-rounded understanding of the field and help you stay ahead of the curve.\nThis \"Best Generative AI Course\" is suitable for both beginners and experienced professionals. Whether you're just starting out in the field or looking to take your skills to the next level, this course will provide you with the knowledge and skills you need to succeed. Don't miss this opportunity to revolutionize the way you create content. Enroll now and become an expert in Generative AI!\nIn conclusion, this \"Best Generative AI Course\" offers a comprehensive and hands-on approach to learning about the latest and greatest in Generative AI. With a focus on the most innovative technologies and techniques, you will be able to generate content with ease and stay ahead of the curve. Don't hesitate, enroll now and take your skills to the next level!",
      "target_audience": [
        "Content creators and marketers looking to improve their workflow and efficiency",
        "Entrepreneurs and small business owners looking to create unique and engaging content",
        "Developers and engineers who want to integrate generative AI into their projects",
        "Students and professionals who are interested in learning about the latest trends and developments in generative AI",
        "Anyone who is passionate about creating unique and engaging content using the latest technology."
      ]
    },
    {
      "title": "Generative AI & ChatGPT models (UPDATED 2024)",
      "url": "https://www.udemy.com/course/gpt-3-open-ai-api-dalle-api-codex-quickstart/",
      "bio": "Guides and projects to guide you through all the steps required to get started with OpenAI, and DALL.e 3 APIs",
      "objectives": [
        "OpenAI API : Learn to interact with the models and perform a wide variety of natural language tasks with GPT-3",
        "OpenAI Codex : translate natural language to code, perform code continuation",
        "Experiment with code examples, the OpenAI playground and the API models",
        "DALL·E API : generate images from scratch based on a text input",
        "Learn to apply text completion and various other language-processing tasks",
        "[PROJECT] : Build a simple application with the text completion endpoint",
        "[PROJECT] : Generate new images from scratch with the DALL.e model",
        "[PROJECT] : Translate natural language to code with the OpenAI Codex"
      ],
      "course_content": {
        "Introduction & Presentation": [
          "Welcome into AI !",
          "What you should know ?"
        ],
        "Discovering the beta of Open AI": [
          "Introduction to the OpenAI API",
          "GPT-3 : what it is ?",
          "GPT-3 : what it is ?",
          "Models, endpoints and usage",
          "Playground : testing Open AI's API and GPT-3 models",
          "Playground : testing Open AI's API and GPT-3 models",
          "Documentation, guidelines and examples"
        ],
        "OPEN AI : The API Reference, Installation and Quickstart example": [
          "Introduction + starter project (download)",
          "Download starter project",
          "OpenAI Node.js Library : Installation and usage 1/2",
          "OpenAI Node.js Library : Installation and usage 2/2",
          "API KEY and Authentication",
          "Making Requests : async function, POST request and completion output",
          "Making Requests { source code}",
          "Adjust settings",
          "2024 UPDATE - Text Generation (Google Colab)"
        ],
        "Completion model : Learn how to generate content - (Project : FACTS SEARCH)": [
          "Introduction and starter project",
          "Download starter project",
          "API Key + OpenAI API Configuration",
          "Create completion with davinci model",
          "design the prompt and adjust the settings",
          "GUIDE - How to create a good prompt ?",
          "Completion - {source code}",
          "CONCLUSION + download final project"
        ],
        "DALL.e model : learn to generate new images from text": [
          "Introduction and starter project",
          "Download the starter project",
          "Open AI API configuration",
          "Open AI API Configuration (demo)",
          "Design the prompt and adjust settings",
          "Prompt design {source code}",
          "Making requests to the DALL.e API",
          "Open AI API Configuration {source code}",
          "Final steps : generate new images with a text input",
          "2024 UPDATE - Image Generation (Google Colab)"
        ],
        "CODEX API (deprecated) : translate natural language to code": [
          "Introduction and starter project",
          "Download the starter project",
          "Quickstart and best practices",
          "code completion - {source code}",
          "design prompt",
          "Design Prompt",
          "[READ] - Code completion (CODEX API) deprecated",
          "Generate code",
          "2024 UPDATE - Text Generation (Google Colab)"
        ]
      },
      "requirements": [
        "Basics of programming language and web development : HTML & CSS",
        "Javascript fundamentals + knowledge of ECMAScript 2015",
        "Basic command line operations",
        "Online Editor or IDE (recommanded : Visual code studio)",
        "The latest version of Nodejs",
        "Modern Browser (Chrome, Safari, Brave) and the React Developer Tools"
      ],
      "description": "The OpenAI API is one of the most exciting advancements in the world of natural language and code processing.\nIts powerful models and flexible endpoints offer a wealth of possibilities for web developers looking to take their skills to the next level.\nIn this comprehensive course, you will gain a deep understanding of the OpenAI API and its capabilities, along with hands-on experience in building your applications using Node JS, ReactJS, and NextJS.\nWhether you are a seasoned web developer or just starting, this course has something for everyone.\n\n\n# What You Will Learn :\nThe Completions Endpoint: at the heart of the OpenAI API, the completions endpoint is flexible enough to solve a wide range of language processing tasks, including content generation, summarization, semantic search, and sentiment analysis\nModels: explore the different models available through the OpenAI API, including the cutting-edge GPT-3 language model, and discover how they can be used to solve unique use cases\nPrompt Design and Settings: master the art of prompt design and settings and learn how they can impact the API's output\nTokens: acquire a comprehensive understanding of tokens and how to use them to control the API's output\n\n\n# Quick Start Tutorial :\nIn this course, you'll dive into the world of OpenAI and GPT-3 language models. Our focus will be on the completions endpoint and how it can be applied to text completion and various other language-processing tasks\nYou'll learn how to use the OpenAI playground to experiment with code examples and understand the concepts of prompt design and settings, tokens, and models.\nThe course will also include a Quick Start Tutorial, where you'll get hands-on experience with the API, building your text completion application from scratch. Get ready to unlock the power of OpenAI and take your language processing skills to the next level!\nYou will learn how to build a simple sample node application. Along the way, you’ll learn key concepts and techniques that are fundamental to using the API for any task, including authentication with the API key, prompt, settings, completion\nBy the end, you will have built applications with techniques that you can apply to your use cases\n\n\n# Cutting-Edge Tech Stacks\nTo ensure that you have the skills to tackle real-world web development challenges, this course focuses on the latest tech stacks, including Node JS, ReactJS, and NextJS. With these skills in your arsenal, you will be equipped to build cutting-edge applications that are both functional and visually stunning.\nWhether you're a seasoned web developer with years of experience or just starting, this course is designed to help you take your skills to the next level. No prior programming experience is required, and the course is suitable for developers of all levels, from beginners to advanced.\nTake advantage of this opportunity to take your web development skills to the next level and join the revolution in natural language and code processing with the OpenAI API. With this comprehensive course, you will gain the skills and knowledge needed to build cutting-edge applications that solve real-world challenges.\n\n\n# Projects :\nFacts Search: Learn to guide the completion model towards factual answering by showing it how to respond to questions that fall outside its knowledge base\nImage generator: Learn to create images from text with the DALL.e model\nCode completion: We build a sample application using the Codex API to translate natural language instructions into code instructions\n# Target and intended audience\nFor Web Developers of All Levels\nBasics of Node JS, ReactJS, and NextJS : a plus\n# Requirements and prerequisites for the course :\nBasics of web development: HTML & CSS\nThe javascript fundamentals (all demos are in Javascript)\nSome basic programming language\n\n\nGet Started Now and Unleash Your Potential!\nLearn to master the OpenAI API and revolutionize your Web Development Skills!\nSo why wait? Enroll now and start your journey to becoming a master of the OpenAI API!",
      "target_audience": [
        "Javascript developers",
        "Front and backend developers",
        "Students in Web development and software engineering",
        "Anyone interested in building apps with the OpenAI API and the GPT-3 language"
      ]
    },
    {
      "title": "Ultimate RAG Bootcamp Using Langchain,LangGraph & Langsmith",
      "url": "https://www.udemy.com/course/ultimate-rag-bootcamp-using-langchainlanggraph-langsmith/",
      "bio": "Build powerful RAG pipelines: Traditional, Advanced, Multimodal & Agentic AI with LangChain,LangGraph and Langsmith",
      "objectives": [
        "Build traditional RAG pipelines for accurate and efficient information retrieval.",
        "Implement advanced retrieval methods like hybrid search, multimodal RAG, and persistent memory.",
        "Design multi-agent and autonomous RAG systems using LangGraph for collaborative AI reasoning.",
        "Use LangSmith for tracking, debugging, and optimizing RAG workflows in real-world projects.",
        "Integrate LangSmith for tracking, debugging, and optimizing RAG performance.",
        "Use vector databases like FAISS, Pinecone, and Weaviate efficiently.",
        "Build domain-specific knowledge chatbots with hybrid search.",
        "Develop multimodal AI assistants that process both text and images."
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Introduction To RAG": [
          "Introduction To RAG",
          "Some Examples And Advantages Of Using RAG",
          "Business Uecases Impact With RAG",
          "Prompt Engineering Vs FineTuning Vs RAG"
        ],
        "Core Components In RAG": [
          "Data Ingestion And Preprocessing",
          "Query Processing And Output Generation Phase"
        ],
        "VS Code And Anaconda Installation": [
          "Anaconda Installation And VS Code Installation"
        ],
        "Data Ingestion And Data Parsing Techniques": [
          "Project Structure And Environment Set Up With UV Package",
          "Document Structure In LangChain",
          "Ingesting And Parsing Text Data Using Document Loaders",
          "Text Splitting Techniques",
          "Ingestion And Parsing PDF Documents",
          "Handling Common PDF Issues",
          "Ingestion And Data Parsing Word Documents",
          "Parsing CSV And Excel Files",
          "Json files Parsing And Processing",
          "SQL Databases Parsing And Processing"
        ],
        "Vector Embedding And Vector Databases": [
          "Introduction To Embeddings And Vector Databases",
          "Visualization Of Embedding And Consine Similarity",
          "Creating Your First Embeddings With HuggingFace Embedding Models",
          "Getting Started With OPENAI Embeddings",
          "Sematic And Similarity Search Uing Open AI Embedding Models"
        ],
        "Vector Stores And Vector Databases": [
          "Vector Stores Vs Vector Databases",
          "Building Tradition RAG With ChromaDb Vector Store-Part 1",
          "Building Traditional RAG With ChromaDB Vector Store-Part 2",
          "Building Traditional RAG With ChromaDB Vector Store- Part 3",
          "Building RAG Pipeline Using LCEL(Langchain Expression Language)",
          "Adding New Documents To Existing Vector Store",
          "Advanced RAG Techniques-Conversational Memory",
          "How To Use GROQ LLM",
          "Building a RAG System with LangChain and FAISS- Part 1",
          "Building a RAG System with LangChain and FAISS- Part 2",
          "InMemory Vector Store",
          "Working With Full Fledged DataStax Astra VectorDB",
          "Working With PineCone VectorStore DB"
        ],
        "Advanced Chunking And Preprocessing Techniques": [
          "Semantic Chunking With RAG",
          "Semantic Chunking With Python",
          "Building RAG Pipeline With Semantic Chunker",
          "Semantic Chunking With Langchain"
        ],
        "Hybrid Search Strategies": [
          "Combining Dense And Sparse Matrix",
          "Combining Dense And Sparse Retriever With Langchain",
          "Benefits Of Combining Dense And Sparse Search",
          "Reranking Hybrid Search Statergy",
          "Reranking Hybrid Statergy Implementation",
          "Maximal Marginal Relevance Theoretical Explanation",
          "MMR Retriever Implementation",
          "When To And When Not To Use MMR"
        ],
        "Query Enhancement": [
          "Query Expansion Technique",
          "Query Expansion Technique Implementation",
          "Query Decomposition Understanding And Implementation",
          "HyDE Technique And Implementation"
        ]
      },
      "requirements": [
        "Basic understanding of Python programming (variables, loops, functions).",
        "No prior knowledge of RAG is required — everything will be taught from scratch.",
        "Familiarity with AI concepts like LLMs is helpful",
        "Basic Knowledge Of Langchain"
      ],
      "description": "Unlock the Power of Retrieval-Augmented Generation (RAG) – From Traditional to Advanced Agentic AI Systems\nIn today’s AI-driven world, Retrieval-Augmented Generation (RAG) is one of the most impactful and in-demand techniques, powering everything from intelligent chatbots and personal assistants to automated research agents and enterprise AI systems.\nThe Ultimate RAG Bootcamp is your complete, step-by-step guide to mastering RAG using the latest and most powerful tools — LangChain, LangGraph, and LangSmith. Whether you’re an AI beginner or an experienced developer, this course takes you from the fundamentals of RAG pipelines all the way to advanced Agentic RAG architectures used in production by leading companies.\nWhy This Course?\nUnlike other courses that only touch on basic RAG concepts, this bootcamp goes deeper. You will:\nLearn traditional RAG step-by-step.\nMaster advanced retrieval strategies like hybrid search, vector optimization, and multimodal RAG.\nImplement multi-agent, autonomous AI pipelines that can think, plan, and act collaboratively.\nUse LangSmith for experiment tracking, debugging, and performance optimization.\nBuild real-world, deployable AI applications from start to finish.\nBy the end, you won’t just understand RAG — you’ll be able to design, optimize, and deploy advanced AI systems for real-world scenarios.\nWhat You’ll Learn\n1. RAG Foundations\nWhat RAG is and why it matters.\nTraditional RAG architecture: data ingestion, parsing, embeddings, and retrieval.\nChoosing and using vector databases effectively.\nBuilding retrieval + generation workflows with LangChain.\n2. Advanced RAG Techniques\nAdvanced chunking strategies for precision retrieval.\nHybrid search: combining vector and keyword search.\nMultimodal RAG for text, images, and more.\nPersistent memory for context retention.\nSelf-RAG for improving retrieval quality.\nAdaptive & Corrective RAG for dynamic and error-resistant pipelines.\n3. Agentic RAG Pipelines\nMulti-agent architectures with LangGraph.\nDesigning agents for research, summarization, and decision-making.\nAutonomous RAG with minimal human intervention.\nCollaborative AI reasoning with specialized agents.\n4. LangSmith for RAG Evaluation & Optimization\nTracking and managing RAG experiments.\nDebugging retrieval pipelines and fixing bottlenecks.\nRunning evaluation metrics to boost accuracy.\n5. Real-World RAG Projects\nChatbot with domain-specific knowledge.\nMulti-agent research assistant for automated reports.\nMultimodal AI assistant with text and image retrieval.\nDeploying RAG applications to the cloud.\nWho This Course Is For\nAI developers & machine learning engineers.\nData scientists integrating retrieval systems.\nSoftware developers building intelligent assistants.\nResearchers exploring advanced RAG workflows.\nAnyone aiming to master RAG from scratch to production-ready deployment.\nTools & Frameworks You’ll Master\nLangChain – Build modular RAG pipelines.\nLangGraph – Create advanced agent-based workflows with memory.\nLangSmith – Track, debug, and evaluate RAG systems.\nVector Databases – FAISS, Pinecone, Weaviate, and more.\nCloud Deployment – Take AI apps from development to production.\nYour Learning Journey\nUnderstand RAG fundamentals.\nBuild real-world retrieval pipelines.\nAdvance to agentic and autonomous AI systems.\nDeploy and monitor in production.\nOptimize for continuous improvement.\nRAG is more than just an AI trend — it’s the foundation of intelligent, context-aware applications.\nBy the end of this bootcamp, you’ll have hands-on, production-ready skills to build and deploy cutting-edge RAG pipelines with LangChain, LangGraph, and LangSmith.\nJoin the Ultimate RAG Bootcamp today — and start building AI systems that truly understand, reason, and deliver results.",
      "target_audience": [
        "AI developers & ML engineers who want to master RAG from basics to advanced agentic systems.",
        "Data scientists aiming to integrate retrieval systems into AI workflows.",
        "Software developers building intelligent assistants, chatbots, or research tools.",
        "Researchers exploring advanced RAG workflows and multi-agent AI pipelines.",
        "AI enthusiasts & beginners who want a hands-on, step-by-step approach to RAG without prior experience."
      ]
    },
    {
      "title": "NotebookLM : Generative AI Interacting with Your Own Data",
      "url": "https://www.udemy.com/course/notebook-lm/",
      "bio": "NotebookLM is an interactive AI playground for data analysis and machine learning from the Google Gemini team",
      "objectives": [
        "Understand the fundamentals of NotebookLM",
        "Master basic NotebookLM operations",
        "Apply NotebookLM for data analysis",
        "Leverage advanced NotebookLM features"
      ],
      "course_content": {
        "Introduction to NotebookLM": [
          "What is NotebookLM?",
          "Demo of NotebookLM",
          "What this Course is About"
        ],
        "Google NotebookLM in Action": [
          "Analyzing GitHub Code with NotebookLM",
          "Analyzing Youtube Videos with NotebookLM",
          "Analyzing Stock Investments with NotebookLM",
          "Analyzing CSV Files with NotebookLM",
          "Analyzing Sentiment with NotebookLM",
          "Creating a Podcast using NotebookLM",
          "Using NotebookLM to Study for an Exam",
          "Customize Audio Overviews using NotebookLM",
          "NotebookLM Interactive Audio Overviews",
          "*NEW* Use NotebookLM to Generate Mind Maps",
          "*NEW* NotebookLM Pro / Google AI Pro Plan"
        ],
        "Thank you!": [
          "Thank you and next steps!"
        ]
      },
      "requirements": [
        "Google account",
        "Google Workspace account (optional)"
      ],
      "description": "Unlock the power of artificial intelligence with NotebookLM, a powerful and intuitive tool designed to make data analysis and machine learning accessible to everyone. In this comprehensive course, you'll learn how to use NotebookLM to explore data using natural language processing and gain valuable insights.\nThis course is a set of videos that demonstrate the capabilities of NotebookLM.\nYou'll see how it can:\nInterpret the contents of a CSV file\nAnalyze a YouTube Video using its transcript\nCreate mind maps\nUnderstand the sentiment of hundreds of reviews\nAnalyze a company's 10Q and related investor call\nDetermine the purpose of Javascript, python, and other code\nGain insights from PDF research papers\nConsolidate several sources (up to 50) into a single chat-like interface\nDisclaimer: This course is primarily designed to demonstrate the power of NotebookLM. You should be able to follow along using your account if you wish. Links to the data sources used are provided.\nNotebookLM is like using a Generative AI such as ChatGPT or Gemini on your own data. Gain insights from things that are important to you.\nWhether you're a data scientist, researcher, student, or simply curious about AI, this course will equip you with the skills and knowledge you need to succeed. Join us on this exciting journey and discover the endless possibilities of NotebookLM!",
      "target_audience": [
        "Data scientists and analysts: Learn how to use NotebookLM to streamline data analysis tasks and build more sophisticated models.",
        "Researchers: Explore new ways to use AI for research and experimentation.",
        "Students: Gain practical experience with AI tools and develop valuable skills for the future.",
        "Professionals in various fields: Learn how AI can be applied to their specific domains, such as marketing, finance, healthcare, or education.",
        "Individuals with a general interest in AI: Get a solid foundation in AI concepts and tools."
      ]
    },
    {
      "title": "Complete & Practical SAS, Statistics & Data Analysis Course",
      "url": "https://www.udemy.com/course/complete-practical-sas-statistics-data-analysis-course/",
      "bio": "A complete guide and use cases study for job seekers and beginners -- start career in SAS, Statistics and Data science",
      "objectives": [
        "Be equipped with a powerful tool for the most sexy data analytics career path!",
        "Read and write various types of raw data with different formats and options",
        "Create and modify various professional and statistical reports",
        "Be aware of statistical analysis and concepts such as non parametric test, interaction, correlation..",
        "Master the most complete SAS graphics tool such GTL and statistical plots",
        "Learn comprehensive SAS Macro programming knowledge -- variables and user defined functions",
        "Perform many real world case studies -- retail banks, credit bureau, marketing firms and clinical trials",
        "Apply powerful data manipulation -- SQL, subsetting, slicing, filtering, transformation, ranking, sorting..",
        "Understand data management and data piping",
        "Use SAS ODS -- help deliver many useful objects such as charts, tables between different systems",
        "Hundreds of SAS sample codes to explain arrays, functions and business cases"
      ],
      "course_content": {
        "Introduction": [
          "What are you going to learn from this course"
        ],
        "SAS environment and basic concepts": [
          "Introduce SAS environment",
          "SAS library",
          "Try SAS codes",
          "Home work and data installation",
          "SAS Cloud Version",
          "Introduction to SAS Studio Installation on University Edition"
        ],
        "Get started SAS programming": [
          "Create data sets from external files",
          "SAS program - create data sets",
          "Valid names and comments",
          "SAS program -- Valid names and comments",
          "Data type and format",
          "SAS program -- data type and format",
          "SAS program - date and format",
          "Mechanism of SAS data set",
          "Summarize SAS operations"
        ],
        "Input and output raw data": [
          "List input (1)",
          "List input (2)",
          "SAS program -- list input",
          "Read data with fixed layout",
          "Read data with modified list input",
          "Input data with other features",
          "Write data using data step",
          "SAS program --write data",
          "Import and export data"
        ],
        "Manipulate data by data step programming": [
          "Duplicate data sets",
          "SAS program -- duplicate data sets",
          "Modify variables",
          "SAS program -- modify variables",
          "Variables selection.",
          "Rename variables",
          "SAS program -- rename variables",
          "Assign labels to variables",
          "SAS program -- assign labels to variables",
          "Subsetting data sets",
          "SAS program -- Subsetting data sets"
        ],
        "Control flow in SAS": [
          "Structured programming (I)",
          "SAS program -- Structured programming (I)",
          "Structured programming (II)",
          "SAS program -- Structured programming (II)"
        ],
        "SAS data step functions": [
          "Data step character functions",
          "SAS program --data step character functions",
          "Data step numeric functions",
          "SAS program -- numeric functions",
          "Data step special functions",
          "Data step special functions",
          "User defined format",
          "SAS program -- User defined format"
        ],
        "Use cases study (1)": [
          "Case study 001(read employee data)",
          "SAS program -- read employee data",
          "Case study 002(read chronic disease data)",
          "SAS program -- Case study 002(read chronic disease data)",
          "Case study 003 ( read business account data)",
          "SAS program -- Case study 003 ( read business account data)",
          "Case study 004 (process stock data)",
          "SAS program -- Case study 004 (process stock data)"
        ],
        "Other SAS features in data step programming": [
          "Automatic Variables(_N_ and _ERROR_)",
          "Output statement",
          "Return statement",
          "pinpoint the first and last record",
          "SAS program -- Pinpoint the first and last record",
          "Retain statement",
          "SAS program -- Retain statement",
          "Data step array (1)",
          "Data step array (2)"
        ],
        "Use cases study (2)": [
          "Case study 005(create new KPI features)",
          "Case study 006(students grades)"
        ]
      },
      "requirements": [
        "Basic computer operational skills.",
        "Basic math skills.",
        "Data intuition"
      ],
      "description": "You should take this course!\n• If you need a complete and comprehensive package that covers SAS programming, intuitive statistics interpretation, data analysis, and predictive modeling, and\n• If you would like to learn by doing various practical use cases fitting in the positions in different business portfolios, and\n• Whether you are a job seeker or beginner intending to start a data science career\nThen this around 18 hours course is right for you!\nThis complete SAS course includes more than 150 lectures and contains 11 real world case studies/projects in different applied areas such as banking and marketing. After this intensive training, you will be equipped with a powerful tool for the most sexy data analytics career path!",
      "target_audience": [
        "Beginners or job seekers interested in learning SAS Programming, statistical and data analysis in industry fields.",
        "People who wish to enter data science/analytics field."
      ]
    },
    {
      "title": "ChatGPT: GPT-3, GPT-4 Turbo: Unleash the Power of LLM's",
      "url": "https://www.udemy.com/course/open-ais-generative-pre-trained-transformer-3-gpt3/",
      "bio": "Your Gateway to AI-Powered Creativity and Innovation",
      "objectives": [
        "Build next-gen apps with OpenAI's powerful models",
        "Access GPT-3, which performs a variety of natural language tasks, Codex, which translates natural language to code, and DALL·E, which creates and edits images",
        "Start building with a simple API call in Python",
        "Perform a wide variety of natural language tasks with GPT-3.",
        "Translate natural language to code with Codex.",
        "Understand how to query GPT3 chat",
        "Understand what is GPT3 Playground",
        "Application Integration with Streamlit",
        "Voice Conversations"
      ],
      "course_content": {
        "What are Transformers?": [
          "RNN & Transformers"
        ],
        "What is ChatGPT ?": [
          "What is ChatGPT?",
          "ChatGPT Quiz"
        ],
        "How You Can Use ChatGPT": [
          "Create your Account with chat GPT3",
          "Getting Started with chatGPT and principles of ethical AI"
        ],
        "Prompt Engineering": [
          "Best Practices for Prompt Engineering"
        ],
        "Open AI Models": [
          "GPT3 - Codex - Content Filter",
          "OPEN AI Model Quiz"
        ],
        "Parameters in Playground": [
          "Temperature",
          "Max Tokens",
          "Stop Sequence",
          "top p",
          "Injecting Start & Restart Text",
          "Frequency and Presence Penalty",
          "Best of",
          "Show Probabilities"
        ],
        "How is ChatGPT Trained?": [
          "How is ChatGPT Trained ?"
        ],
        "ChatGPT- AI Take Over ?": [
          "ChatGPT- AI Take Over ?"
        ],
        "Money making ideas with ChatGPT": [
          "fiverr example 1: WRITING & TRANSLATION",
          "fiverr example 2: Social Media Marketing",
          "fiverr example 3: Articles & Blog Posts",
          "problogger example 4: Articles & Blog Posts",
          "esty example 5: Generate and Sell Digital Images via DALL·E 2",
          "fiverr/youtube example6: Write video scripts and generate video with pictory",
          "fiverr/youtube example6: Video generated from pictory",
          "fiverr example 7: Adding English subtitles to any vernacular video",
          "Kindle / Startup: Write a Book or SAAS product for bed time stories"
        ],
        "Installing Anaconda and creating virtual env": [
          "Installing Anaconda , creating venv , installing openai streamlit libraries"
        ]
      },
      "requirements": [
        "Basic Knowledge of Python",
        "Interest to be more productive at work or school with use of AI"
      ],
      "description": "Note: Have opened many videos for preview  please only enroll if you follow the preview video's , Any suggestions or modifications ping me in Q&A will respond within 3 business days in most times worst case 7 days if i am travelling\nCurrent Topics\nWhat are Transformers?  (Technical)\nConcept of RNN\n3 main concepts in Transformers\nPositional Encoding\nAttention\nSelf-Attention\nWhat is ChatGPT?  (Technical)\nHow You Can Use ChatGPT (Non-Technical)\nCreating your Generative Pre-trained Transformer 3 (GPT-3) account\nBasic Querying Generative Pre-trained Transformer 3 (GPT-3)  and how ethical principles are upheld\nPrompt Engineering  (Technical)\nBest Practices for  Prompt Engineering\nOpen AI Models (Technical)\nWe will explore when to use OPENAI models\nGPT3\nCodex\nContent Filter\nParameters in Playground (Technical)\nTemperature\nMax Tokens\nStop Sequence\nTop-P\nInjecting Start & Restart Text\nFrequency and Presence Penalty\nBest of\nShow Probabilities\n\n\nHow is ChatGPT Trained?   (Technical)\nWhat is Generative Pre-trained Transformer 3 (GPT-3)  and how is it different from rest of transformers\nChatGPT- AI Take Over ?  (Non-Technical)\nMoney making ideas with ChatGPT   (Non-Technical)\nfiverr example\nWRITING & TRANSLATION\nSocial Media Marketing\nArticles & Blog Posts\nESTY\nGenerate and Sell Digital Images via DALL·E 2\nWrite video scripts and generate video with pictory\nDemo of video generated from pictory\nMore From fiverr:\nGenerate a Script with ChatGPT and use the script to convert to video pictory\nAdd English Subtitles to any vernacular video\nOwn Startup or Business Ideas\nWrite a Book or SAAS product for bed time stories\nInstalling anaconda    (Technical)\nOpen AI With Google Docs  (Non-Technical)\nCreating  a AI blog Idea generator and a paragraph to worth 100 lines with Google Docs\nOpen AI With Google slides (Non-Technical)\nIntegrating Open AI with Google Sheets with Magic Slides\nOpen AI with Google Collab  (Technical)\nInstalling  codesquire ai Chrome Plugin\nCode Auto Completion Example\nExplain Code function\nOpen AI with Jupyter Notebooks  (Technical)\nOpen AI with Google Sheets   (Non-Technical)\nExample of native integration of  ChatGPT with Google Sheets\nGoogle Sheets with GPT for sheets plugin\nPandas AI - The AI-Powered Python Library for Supercharged Data Analysis\nProjects   (Technical - requires basic knowledge of python)\nBuild a auto reply to any customer review with GPT3 and Streamlit\nBuild auto Leetcode solution generator with Streamlit + chatgpt3\nCan ChatGPT solve your Leetcode problems ?\nchat GPT + Streamlit with python solution for any Leetcode problem\nchat GPT + Streamlit with any programing solution for any l:etcode problem\nBuild Image Generation Application using  DALL·E 2\nGenerating YouTube Video Transcript with Whisper\nBuild a ChatGPT clone with Gradio in Python\nUnified Dalle-2 and ChatGPT Interface with Panel\nChatGPT4\nHow to Access ChatGPT4\nBenchmarking Chat GPT 3.5 vs ChatGPT 4\nPrivacy and ChatGPT Business Plan\nContent Creation with ChatGPT4\nShared Links\nWhat are Custom Instructions?\nChatGPT UI Gets a Facelift: New Features and Improvements\nChatGPT's new Voice Conversations feature\nChatGPT4-Vision\nDALL-E in ChatGPT  Beta Plugin\nDALL-E in ChatGPT Alpha Access Form - You will only be able to use DALL-E in ChatGPT if you have ChatGPT Plus.\nChatGPT DALL-E3 Beta Plugin - First Prompt and order of 4 images\nChatGPT4: Create on-brand logos with DALL-E 3\nCreate an Animated QR Code with DALL-E 3\nChatGPT Plugins\nGetting started with ChatGPT plugins\nHow to get access to ChatGPT Plugin\nThird Party Plugins\nAsk Your PDF\nAmbition for Job Search\nSpeechKi adding  audio generation powers to ChatGPT\nOne Word Domain : Finding Your Online Identity\nShow Me : visualize the output of ChatGPT in different ways\nVoxScript : Youtube Transcript and Summary\nPortfolio Pilot : AI Investing Guide\nCompetitor PPC Ads: Spy on your competition remix and borrow their successful advertising campaigns\nAI playlist for Spotify with Playlist AI\nCareer Copilot ChatGPT Plugin to Find Job Opportunities as a Software Developer\nOptions Pro The Ultimate Tool for Options Traders\nTasty Recipes A Kitchen Essential for Home Cooks, Meal Planners, and Food Enthusiasts\nJoPilot for job seekers\nCloudflare Radar  Get Insights into How People Use the Internet\nRoboAd ChatGPT Plugin: The AI-Powered Ad Assistant\nTutory  Find the Best Tutor for Your Needs\nCreate a QR code\nHow to Use KeyMate AI Search with ChatGPT to Enhance Your Knowledge\nHow to Use the Space Photo Explorer Plugin to Explore NASA Photos\nWolfram Alpha: The Future of Knowledge Discovery\nScholar AI\nThe Notable ChatGPT Plugin: A Powerful Tool for Data Analysis and Visualization\nZillow Your Real Estate Assistant\nGet Transcripts and Insights Into Audience Engagement\nHow to Use the Indeed Plugin for ChatGPT to Find Jobs\nUnleashing the Power of AI: World Bank Data and Smart Slides Plugins in ChatGPT\nCanva ChatGPT Plugin: The Future of Visual Content Creation?\nUnlocking the Earth ChatGPT Plugin: A Guide to Customizing Maps\n\n\nLangChain\nwhat is LangChain?\nLangChain Modules\nModels\nPrompts\nChain\nTools and Agents\nMemory\nDocument Loaders and Indexes\nProject :\nBuilding a Chatbot on your custom csv data\nLLM Stack\nAutoGPT\nWhat is AutoGPT?\nInstall and Execute Auto GPT Locally\nAuto GPT with Agent  GPT in a Browser with AgentGPT\nUnlock Power of Automation with  God Mode: The AI Tool that Automates Complex\nGPTs\nWhat are GPTs and Benefits of Building GPTs\nHow To Create Your GPT  without external data\nHow To Create Your GPT  with external data - knowledge base (PDF, Word)\nHow to Extend GPT with Custom Actions Using the Quotable API with No Authentication\nHow to Extend GPT with Custom Actions with Auth - Basic  ( Trending Movies GPT)\nHow to Extend GPT with Custom Actions Using the Quotable API with  OAUTH coming soon)\nGPT Store: A new way to share your ChatGPT creations\nGPTs vs. ChatGPT Plugins\nBing Image Creator & DALLE 3: The Future of Digital Art Creation with 20+ prompts (Non-Technical)\nMicrosoft Azure OpenAI Service    (Non-Technical)\nLearn about how you can use Open AI in Azure\nExamples of Microsoft products using Open AI\nPower BI\nPower platform\nPrompt Marketplace    (Non-Technical)\nPrompt Base\nPointE-3D\nDALLE Training\nCLIP &  Diffusion\nLimitations of DALLE2\nReference to Research Paper Hierarchical Text-Conditional Image Generation with CLIP Latent ( Self Study)\nHow to Use ChatGPT4?\nBenchmarking ChatGPT 3.5 vs  ChatGPT4\n\n\nMoney making ideas with ChatGPT ( Technical)\nOpen AI Bug Bounty Program\n\n\nCo-Pilot with Office 365\nIntroduction to Co-Pilot\nMore Creative in Word\nMore Analytical in Excel ( Sales Data)\nMore Expressive in Power Point (convert word to power point and power point to word)\nMore Productive in Outlook\nMore Collaborative in Teams ( Notes from Conflicting meetings)\nSemantic Kernel  - Build your own  Python Copilot with Azure AI or Open AI (coming soon)\nGPT Index  (coming soon)\nDo we have something like ChatGPT which returns real time results ?   (Non-Technical)\nComparing You vs ChatGPT",
      "target_audience": [
        "Anyone who believes AI will help you to be productive",
        "Idea of AI is helpful but not necessary"
      ]
    },
    {
      "title": "Learn Machine Learning & Data Mining in Python",
      "url": "https://www.udemy.com/course/implement-machine-learning-in-data-mining-using-python/",
      "bio": "Learn Building Machine Learning & Deep Learning Models in Python, and use the Results in Data Mining Analyses",
      "objectives": [
        "Learn everything about Data Mining and its applications",
        "Understand Machine Learning and its connection with Data Mining",
        "Learn all Machine Learning algorithms, their types, and their usage in business",
        "Learn how to implement Machine Learning algorithms in different business scenarios",
        "Learn how to install and use Python programming language to create machine learning algorithms in a simple way",
        "Learn how to import your data sets into Python and make required cleaning before creating the algorithms",
        "Learn how to interpret the results of each algorithms and compare them with each other to choose the optimum one",
        "Learn how to create graphs in Pythons, such as scattered and regression graphs and use them in your analyses",
        "Learn data analysis in PySpark"
      ],
      "course_content": {},
      "requirements": [
        "Basic knowledge in Statistics and operating systems"
      ],
      "description": "If you seek to learn how to create machine learning models and use them in data mining process, this course is for you. You will understand in this course what is data mining process and how to implement machine learning algorithms in data mining. Moreover, you will learn in details how deep learning does work and how to build a deep learning model to solve a business problem. In the beginning of the course, you will understand the basic concepts of data mining and learn about the business fields where data mining is implemented.\nAfter that you will learn how to create machine learning models in Python using several data science libraries developed especially for this purpose. NumPy, Pandas, and Matplotlib are some examples of these models that you will learn how to import and use to create machine learning algorithms in Python. You will learn typing codes in Python from scratch without the need to have a pervious knowledge in coding. You will be familiar with the essential code needed to build machine learning models. This course is designed to provide you with the knowledge you need in a simple and straightforward way to smooth the learning process. You will build your knowledge step by step until you become familiar with the most used Machine Learning algorithms.",
      "target_audience": [
        "Anyone who need to use machine learning algorithms in data mining for business implementation.",
        "Anyone wants to learn Machine Learning in Python.",
        "Anyone wants to learn data analysis in PySpark."
      ]
    },
    {
      "title": "Intro to Big Data, Data Science and Artificial Intelligence",
      "url": "https://www.udemy.com/course/introduction-to-big-data-data-science/",
      "bio": "Big Data Technology & Tools for Non-Technical Leaders. Industry expert insights on IoT, AI and Machine Learning for all.",
      "objectives": [
        "Examples of Big Data and Data Science in Practice (Healthcare, Logistics & Transportation, Manufacturing, and Real Estate & Property Management industries)",
        "Big Data Definition and Data Sources. Why we need to be data and technology savvy.",
        "Introduction to Data Science and Skillset required for working with Big Data",
        "Technological Breakthroughs which Enable Big Data Solutions (Connectivity, Cloud, Open Source, Hadoop and NoSQL)",
        "Big Data Technology Architecture and most popular technology tools used for each Architecture Layer",
        "Beginner's Introduction to Data Analysis, Artificial Intelligence and Machine Learning",
        "Simplified Overview of Machine Learning Algorithms and Neural Networks"
      ],
      "course_content": {
        "Course overview and Introduction to big data": [
          "Course Introduction",
          "Guest Speakers",
          "BEFORE YOU START",
          "Why learn about big data?",
          "Big data definition and Sources of data",
          "Big Data Definition",
          "New Sources of Data"
        ],
        "Big Data in Practice - LOGISTICS & TRANSPORTATION": [
          "Section introduction",
          "Logistics & Transportation: Social Impact of Artificial Intelligence & IoT",
          "Logistics & Transportation: Predictive & Prescriptive Maintenance",
          "Logistics & Transportation: Prepositioning of Goods and Just in Time inventory",
          "Logistics & Transportation: Route Optimisation",
          "Logistics & Transportation: Warehouse Optimisation and order picking",
          "Logistics & Transportation: The Future of the industry",
          "Logistics and Transportation Quiz",
          "Google Maps News"
        ],
        "Big Data in Practice - PREDICTIVE MAINTENANCE IN MANUFACTURING": [
          "Predictive Maintenance in Manufacturing - Case Study SIBUR",
          "Predictive maintenance"
        ],
        "Big Data in Practice: REAL ESTATE & PROPERTY MANAGEMENT": [
          "Real Estate: Introduction to big data in real estate",
          "Real Estate: Business Drivers for Using Big Data",
          "Real Estate & Property Management: Technological Enablers",
          "Real Estate: Building Asset Management and Building Information Modelling",
          "Real Estate: Big Data and IoT in Building Maintenance and Management - examples",
          "Real Estate: Smart Buildings",
          "Additional Resources to Lecture on Smart Buildings",
          "Real Estate: Smart Cities (examples - Los Angeles and Hudson Yards in New York)",
          "Additional resources on Smart Cities",
          "Real Estate: Smart Technologies Cost and Government Subsidies (example - Norway)",
          "Real Estate: Data Driven Future",
          "Real Estate and Property Management",
          "Operational Efficiencies and Sustainability"
        ],
        "Big Data in Practice: HEALTHCARE": [
          "Healthcare: Data Challenges in Healthcare Industry",
          "Healthcare: Transforming Role of AI and Data Measurement Technologies",
          "Healthcare: Artificial Intelligence in Disease Prevention",
          "Healthcare: Artificial Intelligence in Anti-Ageing",
          "Healthcare: AI in Clinical Decision Making and Cancer Treatment",
          "Healthcare: Clash of AI and Traditional Healthcare Science",
          "Healthcare: Final Remarks - Value of Artificial Intellegence to Consumers",
          "BIG DATA IN PRACTICE: SECTION WRAP-UP",
          "Healthcare",
          "AI in Medical Research"
        ],
        "Data Science and Required Skillset": [
          "Data Science Definition and Required Skillset",
          "Guest Speakers importance of working in teams & understanding business objective",
          "Data Science Skillset: Section Wrap-Up",
          "Handouts",
          "Data Science Skills",
          "Data Science and Business Skills"
        ],
        "Introduction to Big Data Technologies": [
          "Key Technological Advances and Enablers",
          "Wide Adoption of Cloud Computing",
          "Data Management Technological Breakthroughs (e.g. NoSQL, Hadoop)",
          "Open Source and Open APIs",
          "Big Data Enablers",
          "Additional Resources and Handouts",
          "Big Data Technology Architecture (including examples of popular technologies)",
          "Big data technology architecture",
          "Additional Resources and Handouts",
          "Technology Architecture"
        ],
        "Introduction to data analysis, Artificial Intelligence and Machine Learning": [
          "Why to be data and tech savvy",
          "Big Data Analytics and Artificial Intelligence Definitions",
          "Machine Learning Workflow and Training a Model",
          "Model Accuracy and Ability to Generalise",
          "Machine Learning Components: DATA",
          "Machine Learning Components: FEATURES",
          "Machine Learning Components: ALGORITHMS",
          "Additional Resources and Handouts",
          "Introduction to AI quiz"
        ],
        "Simplified Overview of Machine Learning Algorithms": [
          "Classical Machine Learning: Supervised and Unsupervised Learning",
          "SUPERVISED LEARNING: Classification",
          "Classification: Naive Bayes",
          "Classification: Decision Trees",
          "Classification: Support Vector Machines (SVM)",
          "Classification: Logistic Regression",
          "Classification: K Nearest Neighbour",
          "Classification: Anomaly Detection",
          "SUPERVISED LEARNING: Regression",
          "Classical Machine Learning: Unsupervised Learning",
          "UNSUPERVISED LEARNING: Clustering",
          "Clustering: K-Means",
          "Clustering: Mean-Shift",
          "Clustering: DBSCAN",
          "Clustering: Anomaly Detection",
          "UNSUPERVISED LEARNING: Dimensionality Reduction",
          "UNSUPERVISED LEARNING: Association Rule",
          "CLASSICAL MACHINE LEARNING - Section Wrap Up",
          "REINFORCEMENT LEARNING",
          "ENSEMBLES",
          "Machine Learning Quiz"
        ],
        "Introduction to Deep Learning and Neural Networks": [
          "DEEP LEARNING AND NEURAL NETWORKS",
          "NEURAL NETWORKS: Convolutional Neural Network",
          "NEURAL NETWORKS: Recurrent Neural Network",
          "NEURAL NETWORKS: Generative Adversarial Network (GAN)",
          "Additional Resources",
          "Neural Networks Quiz"
        ]
      },
      "requirements": [
        "Curiosity about business and technology",
        "There are no special requirements or prerequisites. Anyone can learn from this course."
      ],
      "description": "This course is designed for anyone who is new to big data projects, and would like to get better understanding what machine learning and artificial intelligence mean in practice. It is not a technical course, it does not involve coding, but it will make you feel confident when working in teams with data scientists and programmers. It will bring you up to speed with the data science, ML and AI terminology.\nThe course is also designed for people who are generally interested in modern technologies and their applications - we have included case studies covering oil&gas predictive maintenance, use of AI in healthcare, application of sensor and other digital technologies  in buildings and construction, the role of machine learning in transport and logistics and many more.\nYou will learn about big data, Internet of Things (IoT), data science, big data technologies, artificial intelligence (AI), machine learning (ML) algorithms, neural networks, and why this could be relevant to you even if you don't have technology or data science background. Please note that this is NOT TECHNICAL TRAINING and it does NOT teach Coding/Development or Statistics, but it is suitable for technical professionals.  I am proud to say that this course was purchased by a large oil&gas company in Asia to educate their field engineers about machine learning as part of their digitalisation strategy.\nThe course includes the interviews with industry experts that cover  big data developments in Real Estate, Logistics & Transportation and Healthcare industries.  You will learn how machine learning is used to predict engine failures, how artificial intelligence is used in anti-ageing, cancer treatment and clinical diagnosis, you will find out what technology is used in managing smart buildings and smart cities including Hudson Yards in New York.  We have got fantastic guest speakers who are the experts in their areas:\n- WAEL ELRIFAI - Global VP of Solution Engineering - Big Data, IoT & AI at Hitachi Vantara with over 15 years of experience in the field of machine learning and IoT. Wael is also a Co-Authour of the book \"The Future of IoT\".\n- ED GODBER - Healthcare Strategist with over 20 years of experience in Healthcare, Pharmaceuticals and start-ups specialising in Artificial Intelligence.\n- YULIA PAK - Real Estate and Portfolio Strategy Consultant with over 12 years of experience in Commercial Real Estate advisory, currently working with clients who deploy IoT technologies to improve management of their real estate portfolio.\nHope you will enjoy the course and let me know  in the comments of each section how I can improve the course!  Please follow me on social media (Shortlisted Productions) - you can find the links on my profile page - just click on my name at the bottom of the page just before the reviews.  And please check out my other courses on Climate Change.",
      "target_audience": [
        "Non-technical leaders and managers",
        "Anyone who is interested in big data, machine learning and artificial intelligence",
        "Professionals considering career switch",
        "People with technical background who want to gain insights in real life applications of data science skills",
        "Anyone who works with coders, data engineers and data scientists and wants to learn basics about big data technology and tools",
        "People without maths or computer science background, but who want to understand how Machine Learning algorithms work"
      ]
    },
    {
      "title": "Apache Spark In-Depth (Spark with Scala)",
      "url": "https://www.udemy.com/course/apache-spark-in-depth-spark-with-scala/",
      "bio": "Apache Spark In-Depth (Spark with Scala)",
      "objectives": [
        "Apache Spark from scratch to in-depth, starting from simple word count program to Batch Processing to Spark Structure Streaming, Performance Tuning, Optimization, Application Development and Deployment.",
        "Completing this course will also make you ready for most interview questions",
        "Includes Optional Project and path to success"
      ],
      "course_content": {
        "Apache Spark In-Depth (With Scala)": [
          "Introduction to Data Engineering Career Path",
          "Day 1 - Introduction to Spark",
          "Day 2 - Introduction to Spark",
          "Day 3 - Spark Installation on Linux VM",
          "Day 4 - RDD Day 1",
          "Day 5 - RDD Day 2",
          "Day 6 - RDD Day 3",
          "Day 7 - RDD Day 4",
          "Day 8 - RDD Day 5",
          "Day 9 - Dataframe Day 1",
          "Day 10 - Dataframe Day 2",
          "Day 11 - Dataframe Day 3",
          "Day 12 - Dataframe Day 4",
          "Day 13 - Dataframe Day 5",
          "Day 14 - Dataframes Day 6",
          "Day 15 - Dataframes - Spark SQL",
          "Day 16 - Datasets",
          "Day 17 - Spark Application Development and Deployment",
          "Day 18 - Spark Application Development and Deployment",
          "Day 19 - Performance Tuning and Optimization",
          "Day 20 - Common Errors and Debugging",
          "Day 21 - Spark Streaming D 1",
          "Day 22 - Spark Streaming D 2",
          "Day 23 - Spark Streaming D 3",
          "Day 24 - Project",
          "Day 25 - What Next, Job Assistance and How to Prepare for Interview",
          "Career Guidance"
        ]
      },
      "requirements": [
        "No Pre-requisite required. Curiosity to learn new technology.",
        "Good to know: Hadoop Basics and Scala Basics.",
        "Excellent if you have completed my below 2 data engineering courses: \"Big Data Hadoop and Spark with Scala\" and \"Scala Programming In-Depth\""
      ],
      "description": "Learn Apache Spark From Scratch To In-Depth\n\n\nFrom the instructor of successful Data Engineering courses on \"Big Data Hadoop and Spark with Scala\" and \"Scala Programming In-Depth\"\n\n\nFrom Simple program on word count to Batch Processing to Spark Structure Streaming.\nFrom Developing and Deploying Spark application to debugging.\nFrom Performance tuning, Optimization to Troubleshooting\n\n\nContents all you need for in-depth study of Apache Spark and to clear Spark interviews.\n\n\nTaught in very simple English language so any one can follow the course very easily.\n\n\nNo Prerequisites, Good to know basics about Hadoop and Scala\n\n\nPerfect place to start learning Apache Spark\n\n\nApache Spark is a unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing.\n\n\nSpeed\nRun workloads 100x faster.\nApache Spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine.\n\n\nEase of Use\nWrite applications quickly in Java, Scala, Python, R, and SQL.\nSpark offers over 80 high-level operators that make it easy to build parallel apps. And you can use it interactively from the Scala, Python, R, and SQL shells.\n\n\nGenerality\nCombine SQL, streaming, and complex analytics.\nSpark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application.\n\n\nRuns Everywhere\nSpark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. It can access diverse data sources.",
      "target_audience": [
        "People looking to advance their career in Data Engineering, Big Data, Hadoop, Spark",
        "Already working on Big Data Hadoop/ Spark and want to clear the concepts"
      ]
    },
    {
      "title": "Neural Networks in Python from Scratch: Complete guide",
      "url": "https://www.udemy.com/course/neural-networks-in-python-a-guide-for-beginners/",
      "bio": "Learn the fundamentals of Deep Learning of neural networks in Python both in theory and practice!",
      "objectives": [
        "Learn step by step all the mathematical calculations involving artificial neural networks",
        "Implement neural networks in Python and Numpy from scratch",
        "Understand concepts like perceptron, activation functions, backpropagation, gradient descent, learning rate, and others",
        "Build neural networks applied to classification and regression tasks",
        "Implement neural networks using libraries, such as: Pybrain, sklearn, TensorFlow, and PyTorch"
      ],
      "course_content": {
        "Introduction": [
          "Introduction and course content",
          "Get the materials"
        ],
        "Single layer perceptron": [
          "Plan of attack",
          "Applications of artificial neural networks",
          "Biological fundamentals",
          "Artificial neuron",
          "Perceptron",
          "Perceptron implementation 1",
          "Perceptron implementation 2",
          "Weight update 1",
          "Weight update 2",
          "Perceptron implementation 3",
          "Perceptron implementation 4",
          "Perceptron implementation 5",
          "Additional reading",
          "Single layer perceptron",
          "Homework instruction",
          "Homework solution"
        ],
        "Multilayer perceptron": [
          "Plan of attack",
          "Introduction to multilayer neural networks",
          "Activation functions",
          "Sigmoid function implementation",
          "Hidden layer activation 1",
          "Hidden layer activation 2",
          "Multilayer perceptron implementation 1",
          "Multilayer perceptron implementation 2",
          "Output layer activation",
          "Multilayer perceptron implementation 3",
          "Error calculation (loss function)",
          "Multilayer perceptron implementation 4",
          "Basic algorithm",
          "Gradient descent and derivative",
          "Multilayer perceptron implementation 5",
          "Output layer delta",
          "Multilayer perceptron implementation 6",
          "Hidden layer delta",
          "Multilayer perceptron implementation 7",
          "Backpropagation and learning rate",
          "Weight update with backprogation 1",
          "Multilayer perceptron implementation 8",
          "Weight update with backprogation 2",
          "Multilayer perceptron implementation 9",
          "Multilayer perceptron implementation 10",
          "Iris dataset",
          "Bias, error and multiple outputs",
          "Hidden layers",
          "Output layer with categorical data",
          "Stochastic gradient descent",
          "Deep learning",
          "Additional reading",
          "Multi-layer perceptron",
          "Homework instruction",
          "Homework solution"
        ],
        "Libraries for neural networks": [
          "Plan of attack",
          "Pybrain 1",
          "Pybrain 2",
          "Homework instruction: iris dataset",
          "Homework solution",
          "Sklearn for classification 1",
          "Sklearn for classification 2",
          "Sklearn for classification 3",
          "Sklearn for regression",
          "Homework instruction: wine classification",
          "Homework solution",
          "TensorFlow for image classification 1",
          "TensorFlow for imagem classification 2",
          "TensorFlow for image classification 3",
          "Homework instruction: fashion mnist classification",
          "Homework solution",
          "PyTorch for classification 1",
          "PyTorch for classification 2",
          "PyTorch for classification 3",
          "Homework instruction: diabetes classification",
          "Homework solution",
          "Final remarks"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Programming logic (if, while and for statements)",
        "Basic Python programming",
        "No prior knowledge about Artificial Neural Networks or Artificial Intelligence"
      ],
      "description": "Artificial neural networks are considered to be the most efficient Machine Learning techniques nowadays, with companies the likes of Google, IBM and Microsoft applying them in a myriad of ways. You’ve probably heard about self-driving cars or applications that create new songs, poems, images and even entire movie scripts! The interesting thing about this is that most of these were built using neural networks. Neural networks have been used for a while, but with the rise of Deep Learning, they came back stronger than ever and now are seen as the most advanced technology for data analysis.\nOne of the biggest problems that I’ve seen in students that start learning about neural networks is the lack of easily understandable content. This is due to the fact that the majority of the materials that are available are very technical and apply a lot of mathematical formulas, which simply makes the learning process incredibly difficult for whomever wishes to take their first steps in this field. With this in mind, the main objective of this course is to present the theoretical and mathematical concepts of neural networks in a simple yet thorough way, so even if you know nothing about neural networks, you’ll understand all the processes. We’ll cover concepts such as perceptrons, activation functions, multilayer networks, gradient descent and backpropagation algorithms, which form the foundations through which you will understand fully how a neural network is made. We’ll also cover the implementations on a step-by-step basis using Python, which is one of the most popular programming languages in the field of Data Science. It’s important to highlight that the step-by-step implementations will be done without using Machine Learning-specific Python libraries, because the idea behind this course is for you to understand how to do all the calculations necessary in order to build a neural network from scratch.\nTo sum it all up, if you wish to take your first steps in Deep Learning, this course will give you everything you need. It’s also important to note that this course is for students who are getting started with neural networks, therefore the explanations will deliberately be slow and cover each step thoroughly in order for you to learn the content in the best way possible. On the other hand, if you already know your way around neural networks, this course will be very useful for you to revise and review some important concepts.\nAre you ready to take the next step in your professional career? I’ll see you in the course!",
      "target_audience": [
        "Beginners who are starting to learn about Artificial Neural Networks or Deep Learning",
        "People interested in the theory of Artificial Neural Networks",
        "Undergraduate students who are studying subjects related to Artificial Intelligence",
        "Anyone interested in Artificial Intelligence or Artificial Neural Networks"
      ]
    },
    {
      "title": "Python for Data Analytics - Beginner to Advanced",
      "url": "https://www.udemy.com/course/python-for-data-analytics/",
      "bio": "Learn Python for Data Analytics. Learn how to analyze and visualize different data types and do projects with them.",
      "objectives": [
        "Learn how to analyze data",
        "Learn how to do a data analysis project",
        "Learn how to visualize data",
        "Learn (or repeat) the basics of statistics and python",
        "Learn the analysis of time series data"
      ],
      "course_content": {
        "Helpful statistics concepts (optional)": [
          "General concepts in statistics",
          "Mean-Mode-Median",
          "Mean-Mode-Median Calculation Exercise",
          "Probability Introduction",
          "Inferential Statistics Introduction",
          "Standard Deviation - Variance Calculation Exercise",
          "Confidence Interval",
          "Confidence Interval Practice"
        ],
        "Introduction to Coding": [
          "Installing Python and Code Editor",
          "Python Files"
        ],
        "Pandas for Data Analysis": [
          "Pandas Part 1: Introduction - Series - Dataframes - Missing Data Handling -",
          "Pandas Part 2: Data Manipulation - Sorting and Ranking - Merge - Data Cleaning",
          "Pandas Part 3: Group by - Aggregating Data - Data Visualization - Multi Indexes"
        ],
        "Time Series Analysis": [
          "Data set for pandas for time series analysis",
          "Pandas for time series analysis",
          "Seasonality",
          "Dickey-Fuller test for stationarity",
          "Autocorrelation",
          "Decomposition"
        ],
        "Numpy": [
          "Numpy - Introduction to Arrays",
          "Array Indexing",
          "Array Slicing and Array Iterating"
        ],
        "Matplotlib": [
          "Matplotlib Introduction",
          "Matplotlib Coding",
          "Matplotlib Quiz"
        ],
        "Seaborn": [
          "Visualization of distributions",
          "Visualization of statistical relationships",
          "Plotting Categorical Data",
          "Seaborn Quiz"
        ],
        "Project 1": [
          "Project Data",
          "Project 1"
        ],
        "Project 2": [
          "Project Data",
          "Project 2"
        ],
        "Project 3": [
          "Project Data",
          "Project 3"
        ]
      },
      "requirements": [
        "No requirements. This course includes Python & Statistics fundamentals."
      ],
      "description": "This is a data analysis course which we use Python and its libraries in order to clean, analyze and visualize our data. This course is for anyone who is interested in data analytics. You don't need to have any knowledge about python or statistics since we will be repeating these two at the beginning of the course. We will cover python libraries which is designed for data manipulation, data analysis, data visualization. Topics we are going to be covering:\n-Fundamentals of Statistics\n-Pandas ( a Python Library designed for data cleaning, data analysis and data manipulation)\n-Time Series Analysis\n-Matplotlib (a Python Library designed for data visualization)\n-Seaborn (a Python Library designed for data visualization)\n-Data Analysis Projects\nwill be covered in the course. After this course, you can create and share data analysis projects, start learning about machine learning in order to becoming a data scientist or you can learn a business intelligence tool like Microsoft Power BI or Tableau in order to start your career in business analytics. General concepts and codes and their returns will be covered in this course. In all course process and finishing it i would love to answering your questions about data analysis, data science and other concepts. Feel free to contact to me via courses Q&A Section .",
      "target_audience": [
        "People who is interested in data related roles, especially data analytics.",
        "People who wants to learn data analysis",
        "People who wants to become a data analyst",
        "People who wants to become a data scientist"
      ]
    },
    {
      "title": "From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase",
      "url": "https://www.udemy.com/course/from-0-1-machine-learning/",
      "bio": "A down-to-earth, shy but confident take on machine learning techniques that you can put to work today",
      "objectives": [
        "Identify situations that call for the use of Machine Learning",
        "Understand which type of Machine learning problem you are solving and choose the appropriate solution",
        "Use Machine Learning and Natural Language processing to solve problems like text classification, text summarization in Python"
      ],
      "course_content": {
        "Introduction": [
          "You, This Course and Us",
          "Source Code and PDFs",
          "A sneak peek at what's coming up"
        ],
        "Jump right in : Machine learning for Spam detection": [
          "Solving problems with computers",
          "Machine Learning: Why should you jump on the bandwagon?",
          "Plunging In - Machine Learning Approaches to Spam Detection",
          "Spam Detection with Machine Learning Continued",
          "Get the Lay of the Land : Types of Machine Learning Problems"
        ],
        "Solving Classification Problems": [
          "Solving Classification Problems",
          "Random Variables",
          "Bayes Theorem",
          "Naive Bayes Classifier",
          "Naive Bayes Classifier : An example",
          "K-Nearest Neighbors",
          "K-Nearest Neighbors : A few wrinkles",
          "Support Vector Machines Introduced",
          "Support Vector Machines : Maximum Margin Hyperplane and Kernel Trick",
          "Artificial Neural Networks:Perceptrons Introduced"
        ],
        "Clustering as a form of Unsupervised learning": [
          "Clustering : Introduction",
          "Clustering : K-Means and DBSCAN"
        ],
        "Association Detection": [
          "Association Rules Learning"
        ],
        "Dimensionality Reduction": [
          "Dimensionality Reduction",
          "Principal Component Analysis"
        ],
        "Regression as a form of supervised learning": [
          "Regression Introduced : Linear and Logistic Regression",
          "Bias Variance Trade-off"
        ],
        "Natural Language Processing and Python": [
          "Applying ML to Natural Language Processing",
          "Installing Python - Anaconda and Pip",
          "Natural Language Processing with NLTK",
          "Natural Language Processing with NLTK - See it in action",
          "Web Scraping with BeautifulSoup",
          "A Serious NLP Application : Text Auto Summarization using Python",
          "Python Drill : Autosummarize News Articles I",
          "Python Drill : Autosummarize News Articles II",
          "Python Drill : Autosummarize News Articles III",
          "Put it to work : News Article Classification using K-Nearest Neighbors",
          "Put it to work : News Article Classification using Naive Bayes Classifier",
          "Python Drill : Scraping News Websites",
          "Python Drill : Feature Extraction with NLTK",
          "Python Drill : Classification with KNN",
          "Python Drill : Classification with Naive Bayes",
          "Document Distance using TF-IDF",
          "Put it to work : News Article Clustering with K-Means and TF-IDF",
          "Python Drill : Clustering with K Means"
        ],
        "Sentiment Analysis": [
          "Solve Sentiment Analysis using Machine Learning",
          "Sentiment Analysis - What's all the fuss about?",
          "ML Solutions for Sentiment Analysis - the devil is in the details",
          "Sentiment Lexicons ( with an introduction to WordNet and SentiWordNet)",
          "Regular Expressions",
          "Regular Expressions in Python",
          "Put it to work : Twitter Sentiment Analysis",
          "Twitter Sentiment Analysis - Work the API",
          "Twitter Sentiment Analysis - Regular Expressions for Preprocessing",
          "Twitter Sentiment Analysis - Naive Bayes, SVM and Sentiwordnet"
        ],
        "Decision Trees": [
          "Using Tree Based Models for Classification",
          "Planting the seed - What are Decision Trees?",
          "Growing the Tree - Decision Tree Learning",
          "Branching out - Information Gain",
          "Decision Tree Algorithms",
          "Titanic : Decision Trees predict Survival (Kaggle) - I",
          "Titanic : Decision Trees predict Survival (Kaggle) - II",
          "Titanic : Decision Trees predict Survival (Kaggle) - III"
        ]
      },
      "requirements": [
        "No prerequisites, knowledge of some undergraduate level mathematics would help but is not mandatory. Working knowledge of Python would be helpful if you want to run the source code that is provided."
      ],
      "description": "Prerequisites: No prerequisites, knowledge of some undergraduate level mathematics would help but is not mandatory. Working knowledge of Python would be helpful if you want to run the source code that is provided.\nTaught by a Stanford-educated, ex-Googler and an IIT, IIM - educated ex-Flipkart lead analyst. This team has decades of practical experience in quant trading, analytics and e-commerce.\n\nThis course is a down-to-earth, shy but confident take on machine learning techniques that you can put to work today\n\nLet’s parse that.\n\nThe course is down-to-earth : it makes everything as simple as possible - but not simpler\nThe course is shy but confident : It is authoritative, drawn from decades of practical experience -but shies away from needlessly complicating stuff.\nYou can put ML to work today : If Machine Learning is a car, this car will have you driving today. It won't tell you what the carburetor is.\nThe course is very visual : most of the techniques are explained with the help of animations to help you understand better.\nThis course is practical as well : There are hundreds of lines of source code with comments that can be used directly to implement natural language processing and machine learning for text summarization, text classification in Python.\n\nThe course is also quirky. The examples are irreverent. Lots of little touches: repetition, zooming out so we remember the big picture, active learning with plenty of quizzes. There’s also a peppy soundtrack, and art - all shown by studies to improve cognition and recall.\nWhat's Covered:\nMachine Learning:\n\nSupervised/Unsupervised learning, Classification, Clustering, Association Detection, Anomaly Detection, Dimensionality Reduction, Regression.\nNaive Bayes, K-nearest neighbours, Support Vector Machines, Artificial Neural Networks, K-means, Hierarchical clustering, Principal Components Analysis, Linear regression, Logistics regression, Random variables, Bayes theorem, Bias-variance tradeoff\nNatural Language Processing with Python:\n\nCorpora, stopwords, sentence and word parsing, auto-summarization, sentiment analysis (as a special case of classification), TF-IDF, Document Distance, Text summarization, Text classification with Naive Bayes and K-Nearest Neighbours and Clustering with K-Means\nSentiment Analysis:\nWhy it's useful, Approaches to solving - Rule-Based , ML-Based , Training , Feature Extraction, Sentiment Lexicons, Regular Expressions, Twitter API, Sentiment Analysis of Tweets with Python\nMitigating Overfitting with Ensemble Learning:\n\nDecision trees and decision tree learning, Overfitting in decision trees, Techniques to mitigate overfitting (cross validation, regularization), Ensemble learning and Random forests\nRecommendations:  Content based filtering, Collaborative filtering and Association Rules learning\n\nGet started with Deep learning: Apply Multi-layer perceptrons to the MNIST Digit recognition problem\n\nA Note on Python: The code-alongs in this class all use Python 2.7. Source code (with copious amounts of comments) is attached as a resource with all the code-alongs. The source code has been provided for both Python 2 and Python 3 wherever possible.",
      "target_audience": [
        "Yep! Analytics professionals, modelers, big data professionals who haven't had exposure to machine learning",
        "Yep! Engineers who want to understand or learn machine learning and apply it to problems they are solving",
        "Yep! Product managers who want to have intelligent conversations with data scientists and engineers about machine learning",
        "Yep! Tech executives and investors who are interested in big data, machine learning or natural language processing",
        "Yep! MBA graduates or business professionals who are looking to move to a heavily quantitative role"
      ]
    },
    {
      "title": "Linear Regression and Logistic Regression using R Studio",
      "url": "https://www.udemy.com/course/linear-regression-and-logistic-regression-r-studio-starttech/",
      "bio": "Linear Regression and Logistic Regression for beginners. Understand the difference between Regression & Classification",
      "objectives": [
        "Learn how to solve real life problem using the Linear and Logistic Regression technique",
        "Preliminary analysis of data using Univariate and Bivariate analysis before running regression analysis",
        "Graphically representing data in R before and after analysis",
        "How to do basic statistical operations in R",
        "Understand how to interpret the result of Linear and Logistic Regression model and translate them into actionable insight",
        "Indepth knowledge of data collection and data preprocessing for Linear and Logistic Regression problem"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course Resources"
        ],
        "Basics of Statistics": [
          "Types of Data",
          "This is a Milestone!",
          "Types of Statistics",
          "Describing the data graphically",
          "Measures of Centers",
          "Measures of Dispersion",
          "Quiz"
        ],
        "Getting started with R and R studio": [
          "Installing R and R studio",
          "Basics of R and R studio",
          "Packages in R",
          "Inputting data part 1: Inbuilt datasets of R",
          "Inputting data part 2: Manual data entry",
          "Inputting data part 3: Importing from CSV or Text files",
          "Creating Barplots in R",
          "Creating Histograms in R",
          "Quiz"
        ],
        "Data Preprocessing before building Linear Regression Model": [
          "Gathering Business Knowledge",
          "Data Exploration",
          "The Data and the Data Dictionary",
          "Importing the dataset into R",
          "Univariate Analysis and EDD",
          "EDD in R",
          "Outlier Treatment",
          "Outlier Treatment in R",
          "Missing Value imputation",
          "Missing Value imputation in R",
          "Seasonality in Data",
          "Bi-variate Analysis and Variable Transformation",
          "Variable transformation in R",
          "Non Usable Variables",
          "Dummy variable creation: Handling qualitative data",
          "Dummy variable creation in R",
          "Correlation Matrix and cause-effect relationship",
          "Correlation Matrix in R",
          "Quiz"
        ],
        "Linear Regression Model": [
          "The problem statement",
          "Basic equations and Ordinary Least Squared (OLS) method",
          "Assessing Accuracy of predicted coefficients",
          "Assessing Model Accuracy - RSE and R squared",
          "Simple Linear Regression in R",
          "Multiple Linear Regression",
          "The F - statistic",
          "Interpreting result for categorical Variable",
          "Multiple Linear Regression in R",
          "Test-Train split",
          "Bias Variance trade-off",
          "More about test-train split",
          "Test-Train Split in R",
          "Practice Assignment"
        ],
        "Introduction to the classification Models": [
          "Three classification models and Data set",
          "Importing the data into R",
          "The problem statements",
          "Why can't we use Linear Regression?"
        ],
        "Building a Logistic Regression Model": [
          "Logistic Regression",
          "Training a Simple Logistic model in R",
          "Results of Simple Logistic Regression",
          "Logistic with multiple predictors",
          "Training multiple predictor Logistic model in R",
          "Confusion Matrix",
          "Evaluating Model performance",
          "Predicting probabilities, assigning classes and making Confusion Matrix",
          "Quiz"
        ],
        "Test-Train Split": [
          "Test-Train Split",
          "Test-Train Split in R",
          "Quiz",
          "The final milestone!"
        ],
        "Congratulations & about your certificate": [
          "Congratulations & About your certificate",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "This course starts from basics and you do not even need coding background to build these models in R Studio",
        "Students will need to install R and R studio software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Linear Regression and Logistic Regression course that teaches you everything you need to create a Linear or Logistic Regression model in R Studio, right?\nYou've found the right Linear Regression course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using linear and logistic regression technique of Machine Learning.\nCreate a linear regression and logistic regression model in R Studio and analyze its result.\nConfidently practice, discuss and understand Machine Learning concepts\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nHow this course will help you?\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you the most popular technique of machine learning, which is Linear Regression\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear regression.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Linear Regression model, which is the most popular Machine Learning model, to solve business problems.\nBelow are the course contents of this course on Linear Regression:\nSection 1 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics\nthen graphical representations to describe the data and then a lecture on measures of center like mean\nmedian and mode and lastly measures of dispersion like range and standard deviation\nSection 2 - Python basic\nThis section gets you started with Python.\nThis section will help you set up the python and Jupyter environment on your system and it'll teach\nyou how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nSection 3 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\nSection 4 - Data Preprocessing\nIn this section you will learn what actions you need to take a step by step to get the data and then\nprepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment, missing value imputation, variable transformation and correlation.\nSection 5 - Regression Model\nThis section starts with simple linear regression and then covers multiple linear regression.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you\nunderstand where the concept is coming from and how it is important. But even if you don't understand\nit,  it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models accuracy, what is the meaning of F statistic, how categorical variables in the independent variables dataset are interpreted in the results, what are other variations to the ordinary least squared method and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a regression model in Python will soar. You'll have a thorough understanding of how to use regression modelling to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhat is the Linear regression technique of Machine learning?\nLinear Regression is a simple machine learning model for regression problems, i.e., when the target variable is a real value.\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\nWhen there is a single input variable (x), the method is referred to as simple linear regression.\nWhen there are multiple input variables, the method is known as multiple linear regression.\nWhy learn Linear regression technique of Machine learning?\nThere are four reasons to learn Linear regression technique of Machine learning:\n1. Linear Regression is the most popular machine learning technique\n2. Linear Regression has fairly good prediction accuracy\n3. Linear Regression is simple to implement and easy to interpret\n4. It gives you a firm base to start learning other advanced techniques of Machine Learning\nHow much time does it take to learn Linear regression technique of machine learning?\nLinear Regression is easy but no one can determine the learning time it takes. It totally depends on you. The method we adopted to help you learn Linear regression starts from the basics and takes you to advanced level within hours. You can follow the same, but remember you can learn nothing without practicing it. Practice is the only way to remember whatever you have learnt. Therefore, we have also provided you with another data set to work on as a separate project of Linear regression.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 4 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of Linear Regression modelling - Having a good knowledge of Linear Regression gives you a solid understanding of how machine learning works. Even though Linear regression is the simplest technique of Machine learning, it is still the most popular one with fairly good prediction ability. Fifth and sixth section cover Linear regression topic end-to-end and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.\nWhy use Python for data Machine Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Machine Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nMachine Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master Linear and Logistic Regression from beginner to advanced level in a short span of time"
      ]
    },
    {
      "title": "PyTorch Ultimate: From Basics to Cutting-Edge",
      "url": "https://www.udemy.com/course/pytorch-ultimate/",
      "bio": "Become an expert applying the most popular Deep Learning framework PyTorch",
      "objectives": [
        "learn all relevant aspects of PyTorch from simple models to state-of-the-art models",
        "deploy your model on-premise and to Cloud",
        "Transformers",
        "Natural Language Processing (NLP), e.g. Word Embeddings, Zero-Shot Classification, Similarity Scores",
        "CNNs (Image-, Audio-Classification; Object Detection)",
        "Style Transfer",
        "Recurrent Neural Networks",
        "Autoencoders",
        "Generative Adversarial Networks",
        "Recommender Systems",
        "adapt top-notch algorithms like Transformers to custom datasets",
        "develop CNN models for image classification, object detection, Style Transfer",
        "develop RNN models, Autoencoders, Generative Adversarial Networks",
        "learn about new frameworks (e.g. PyTorch Lightning) and new models like OpenAI ChatGPT",
        "use Transfer Learning"
      ],
      "course_content": {
        "Course Overview & System Setup": [
          "Course Overview",
          "PyTorch Introduction",
          "System Setup",
          "How to Get the Course Material",
          "Additional Information for Mac-Users",
          "Setting up the conda environment",
          "General Environment Setup Error Handling",
          "How to work with the course"
        ],
        "Machine Learning": [
          "Artificial Intelligence (101)",
          "Machine Learning (101)",
          "Machine Learning Models (101)"
        ],
        "Deep Learning Introduction": [
          "Deep Learning General Overview",
          "Deep Learning Modeling 101",
          "Performance",
          "From Perceptron to Neural Network",
          "Layer Types",
          "Activation Functions",
          "Loss Functions",
          "Optimizers"
        ],
        "Model Evaluation": [
          "Underfitting Overfitting (101)",
          "Train Test Split (101)",
          "Resampling Techniques (101)"
        ],
        "Neural Network from Scratch (opt. but highly recommended)": [
          "Section Overview",
          "NN from Scratch (101)",
          "Calculating the dot-product (Coding)",
          "NN from Scratch (Data Prep)",
          "NN from Scratch Modeling __init__ function",
          "NN from Scratch Modeling Helper Functions",
          "NN from Scratch Modeling forward function",
          "NN from Scratch Modeling backward function",
          "NN from Scratch Modeling optimizer function",
          "NN from Scratch Modeling train function",
          "NN from Scratch Model Training",
          "NN from Scratch Model Evaluation"
        ],
        "Tensors": [
          "Section Overview",
          "From Tensors to Computational Graphs (101)",
          "Tensor (Coding)"
        ],
        "PyTorch Modeling Introduction": [
          "Section Overview",
          "Model Training (101)",
          "Linear Regression from Scratch (Coding, Model Training)",
          "Linear Regression from Scratch (Coding, Model Evaluation)",
          "Model Class (Coding)",
          "Exercise: Learning Rate and Number of Epochs",
          "Solution: Learning Rate and Number of Epochs",
          "Batches (101)",
          "Batches (Coding)",
          "Datasets and Dataloaders (101)",
          "Datasets and Dataloaders (Coding)",
          "Saving and Loading Models (101)",
          "Saving and Loading Models (Coding)",
          "Hyperparameter Tuning (101)",
          "Hyperparameter Tuning (Coding)"
        ],
        "Classification Models": [
          "Section Overview",
          "Classification Types (101)",
          "Confusion Matrix (101)",
          "ROC curve (101)",
          "Multi-Class 1: Data Prep",
          "Multi-Class 2: Dataset class (Exercise)",
          "Multi-Class 3: Dataset class (Solution)",
          "Multi-Class 4: Network Class (Exercise)",
          "Multi-Class 5: Network Class (Solution)",
          "Multi-Class 6: Loss, Optimizer, and Hyper Parameters",
          "Multi-Class 7: Training Loop",
          "Multi-Class 8: Model Evaluation",
          "Multi-Class 9: Naive Classifier",
          "Multi-Class 10: Summary",
          "Multi-Label (Exercise)",
          "Multi-Label (Solution)"
        ],
        "CNN: Image Classification": [
          "Section Overview",
          "CNNs (101)",
          "CNN (Interactive)",
          "Image Preprocessing (101)",
          "Image Preprocessing (Coding)",
          "Binary Image Classification (101)",
          "Binary Image Classification (Coding)",
          "MultiClass Image Classification (Exercise)",
          "MultiClass Image Classification (Solution)",
          "Layer Calculations (101)",
          "Layer Calculations (Coding)"
        ],
        "CNN: Audio Classification": [
          "Audio Classification (101)",
          "Audio Classification (Exercise)",
          "Audio Classification (Exploratory Data Analysis)",
          "Audio Classification (Data Prep-Solution)",
          "Audio Classification (Model-Solution)"
        ]
      },
      "requirements": [
        "basic Python knowledge"
      ],
      "description": "PyTorch is a Python framework developed by Facebook to develop and deploy Deep Learning models. It is one of the most popular Deep Learning frameworks nowadays.\n\n\nIn this course you will learn everything that is needed for developing and applying Deep Learning models to your own data. All relevant fields like Regression, Classification, CNNs, RNNs, GANs, NLP, Recommender Systems, and many more are covered. Furthermore, state of the art models and architectures  like Transformers, YOLOv7, or ChatGPT are presented.\nIt is important to me that you learn the underlying concepts as well as how to implement the techniques. You will be challenged to tackle problems on your own, before I present you my solution.\n\n\nIn my course I will teach you:\nIntroduction to Deep Learning\nhigh level understanding\nperceptrons\nlayers\nactivation functions\nloss functions\noptimizers\nTensor handling\ncreation and specific features of tensors\nautomatic gradient calculation (autograd)\nModeling introduction, incl.\nLinear Regression from scratch\nunderstanding PyTorch model training\nBatches\nDatasets and Dataloaders\nHyperparameter Tuning\nsaving and loading models\nClassification models\nmultilabel classification\nmulticlass classification\nConvolutional Neural Networks\nCNN theory\ndevelop an image classification model\nlayer dimension calculation\nimage transformations\nAudio Classification with torchaudio and spectrograms\nObject Detection\nobject detection theory\ndevelop an object detection model\nYOLO v7, YOLO v8\nFaster RCNN\nStyle Transfer\nStyle transfer theory\ndeveloping your own style transfer model\nPretrained Models and Transfer Learning\nRecurrent Neural Networks\nRecurrent Neural Network theory\ndeveloping LSTM models\nRecommender Systems with Matrix Factorization\nAutoencoders\nTransformers\nUnderstand Transformers, including Vision Transformers (ViT)\nadapt ViT to a custom dataset\nGenerative Adversarial Networks\nSemi-Supervised Learning\nNatural Language Processing (NLP)\nWord Embeddings Introduction\nWord Embeddings with Neural Networks\nDeveloping a Sentiment Analysis Model based on One-Hot Encoding, and GloVe\nApplication of Pre-Trained NLP models\nModel Debugging\nHooks\nModel Deployment\ndeployment strategies\ndeployment to on-premise and cloud, specifically Google Cloud\nMiscellanious Topics\nChatGPT\nResNet\nExtreme Learning Machine (ELM)\n\n\nEnroll right now to learn some of the coolest techniques and boost your career with your new skills.\n\n\nBest regards,\nBert",
      "target_audience": [
        "Python developers willing to learn one of the most interesting and in-demand techniques"
      ]
    },
    {
      "title": "End-to-End Machine Learning: From Idea to Implementation",
      "url": "https://www.udemy.com/course/sustainable-and-scalable-machine-learning-project-development/",
      "bio": "Build, Manage, and Deploy Machine Learning (AI) Projects with Python and MLOps",
      "objectives": [
        "How To Efficiently Build Sustainable And Scalable Machine Learning Projects Using The Best Practices",
        "Data Versioning",
        "Distributed Data Processing",
        "Feature Extraction",
        "Distributed Model Training",
        "Model Evaluation",
        "Experiment Tracking",
        "Error analysis",
        "Model Inference",
        "Creating An Application Using The Model We Train",
        "Metadata management",
        "Reproducibility",
        "MLOps",
        "MLOps principals",
        "Machine Learning Operations",
        "Machine Learning",
        "Deep Learning",
        "Artificial Intelligence",
        "AI"
      ],
      "course_content": {
        "Introduction": [
          "Why This Course?",
          "Why Too Many Companies Fail?",
          "Why Too Many Companies Fail - Resources",
          "Tips To Improve Your Course Taking Experience",
          "Discord Server",
          "Where to start?",
          "Lecture Slides",
          "A Note For Windows Users"
        ],
        "Git and Github Quickstart": [
          "Git and Github Quickstart section introduction",
          "Git and Github - What are they?",
          "Git Installation - Linux",
          "Git Installation - Windows",
          "Git Installation - MacOS",
          "Github - Account creation",
          "Adding an SSH key pair to GitHub account - Linux",
          "Adding an SSH key pair to GitHub Account - MacOS",
          "Adding an SSH key pair to GitHub account - Windows",
          "Git and GitHub - Basic workflow",
          "Reverting Your Changes Back",
          "Commit History",
          "Aliases",
          "Reverting Back to a Previous Commit",
          "Git Diff",
          "Branching and Merging",
          "Pull Request and Code Review",
          "Rebase",
          "Stashing",
          "Tagging",
          "Cherry Pick",
          "Git and GitHub - Final Words"
        ],
        "Docker Quickstart": [
          "Docker Quickstart section introduction",
          "What Is Docker and Why Do We Use It?",
          "Installation - Linux",
          "Installation - Windows",
          "Installation - MacOS",
          "A Note For NVIDIA GPU Users",
          "Docker Containers",
          "Docker Containers - Hands On",
          "Why Docker Is So Good?",
          "Docker Images",
          "Dockerfile",
          "More about Dockerfile",
          "Persistent Data In Docker",
          "Persistent Data In Docker - Volumes - Hands On",
          "Persistent Data in Docker - Bind Mounting - Hands On",
          "Docker Compose",
          "Dockerfile Best Practices"
        ],
        "DVC": [
          "DVC - Section Introduciton",
          "Data Versioning",
          "Accessing Your Data",
          "Pipelines - Part 1",
          "Pipelines - Part 2",
          "Pipelines - Part 3",
          "Metrics And Experiments"
        ],
        "Hydra": [
          "Hydra - Section Introduction",
          "How to Use Hydra From Command-Line?",
          "Specifying A Config File",
          "More About OmegaConf",
          "Grouping Config Files",
          "Selecting Default Configs",
          "Multirun",
          "Output And Working Directory",
          "Logging",
          "Debugging",
          "Instantiate",
          "Packages",
          "A Small Project To See \"The Big Picture\"",
          "Small Project - Assignment",
          "Small Project - Assignment Solution",
          "Tab Completion",
          "Structured Configs",
          "Structured Configs Basic Usage",
          "Hierarchical Static Configuration",
          "Config Groups in Structured Configs - Part 1",
          "Config Groups in Structured Configs - Part 2",
          "Defaults List in Structured Configs",
          "Structured Config Schema",
          "Validating Config Parameters Using Pydantic",
          "Extending The Small Project With Structured Configs",
          "Extending The Small Project With Structured Configs - Course Assignment",
          "Extending The Small Project With Structured Configs - Assignment Solution"
        ],
        "Google Cloud Platform Quickstart": [
          "Google Cloud Platform - Section Introduction",
          "How to Create An Account?",
          "How to Create a Project?",
          "\"gsutils\" and \"gcloud\" commands",
          "A Note About \"gsutils\" and \"gcloud\" commands",
          "Google Cloud Storage (GCS) - Bucket Creation",
          "Google Cloud Storage (GCS) - Bucket Usage",
          "Section Checkpoint",
          "Google Compute Engine (GCE)",
          "Google Compute Engine (GCE) - Quotas",
          "Artifact Registry",
          "Firewall Rules",
          "Instance Groups"
        ],
        "MLFlow": [
          "Prerequisites",
          "MLFlow - Section introduction",
          "A Note About The Lecture Resources",
          "MLFlow Tracking",
          "MLFlow Tracking - Hands-on",
          "Storage Types",
          "Projects",
          "Projects - Hands-on",
          "Models",
          "Models - Hands-on",
          "Model Registry",
          "Model Registry - Hands-on",
          "Local MLFlow Setup",
          "Prerequisites",
          "General Overview of the Production MLFlow Architecture",
          "Artifact Store Creation on GCP",
          "Backend Store Creation on GCP",
          "Tracking Server Deployment on GCP",
          "Testing the Deployed Tracking Server"
        ],
        "Dask": [
          "Dask - Section Introduction",
          "Dask DataFrame",
          "Getting Started with Dask",
          "Creating and Storing Dask DataFrames",
          "Dask DataFrame - Best Practices",
          "Shuffling for GroupBy and Join",
          "Delayed",
          "Futures",
          "Scheduling",
          "Deploying Clusters - Command Line",
          "Deploying Clusters - Python API"
        ],
        "Launching Jobs on Google Cloud Platform for Distributed Model Training": [
          "Prerequisites",
          "Introduction",
          "Python API",
          "Instance Template Creation",
          "Instance Group Creation",
          "Startup Script Creation",
          "Creating Configs",
          "Launching Jobs",
          "Other Tools You Might Use For Distributed Model Training",
          "Usage Examples"
        ],
        "FastAPI": [
          "Introduction",
          "Installation And Our First App",
          "Path Parameters",
          "Query Parameters",
          "Request Body"
        ]
      },
      "requirements": [
        "Basic Understanding Of Machine Learning",
        "Python Programming Language",
        "You Will Learn The Rest In The Course"
      ],
      "description": "Embark on a hands-on journey to mastering Machine Learning project development with Python and MLOps. This course is meticulously crafted to equip you with the essential skills required to build, manage, and deploy real-world Machine Learning projects.\n\n\nWith a focus on practical application, you'll dive into the core of MLOps (Machine Learning Operations) to understand how to streamline the lifecycle of Machine Learning projects from ideation to deployment. Discover the power of Python as the driving force behind the efficient management and operationalization of Machine Learning models.\n\n\nEngage with a comprehensive curriculum that covers data versioning, distributed data processing, feature extraction, model training, evaluation, and much more. The course also introduces you to essential MLOps tools and practices that ensure the sustainability and scalability of Machine Learning projects.\n\n\nWork on a capstone project that encapsulates all the crucial elements learned throughout the course, providing you with a tangible showcase of your newfound skills. Receive constructive feedback and guidance from an experienced instructor dedicated to helping you succeed.\n\n\nJoin a vibrant community of like-minded learners and professionals through our interactive platform, and kickstart a rewarding journey into the dynamic world of Machine Learning projects powered by Python and MLOps. By the end of this course, you'll have a solid foundation, practical skills, and a powerful project in your portfolio that demonstrates your capability to lead Machine Learning projects to success.\n\n\nEnroll today and take a significant step towards becoming proficient in developing and deploying Machine Learning projects using Python and MLOps. Your adventure into the practical world of Machine Learning awaits!",
      "target_audience": [
        "Students who are interested in pursuing a career in machine learning project development and want to gain expertise in sustainable and scalable development practices",
        "Machine learning engineers who are interested in developing machine learning solutions that are scalable and sustainable in the long run",
        "Data scientists who are looking to expand their skill set to include machine learning project development that is scalable and sustainable",
        "Researchers who are interested in developing machine learning models more efficiently",
        "Software developers who want to gain expertise in developing sustainable and scalable machine learning projects",
        "Start-up founders who want to develop machine learning projects that can be scaled up to meet future demands while also being sustainable",
        "Technical project managers who want to learn how to effectively manage and oversee sustainable and scalable machine learning projects",
        "Professionals in the technology industry who want to stay up-to-date with the latest trends and advancements in machine learning project development",
        "Companies and organizations that want to implement sustainable and scalable machine learning projects to improve their operations, efficiency, and profitability"
      ]
    },
    {
      "title": "Advanced SQL Bootcamp",
      "url": "https://www.udemy.com/course/advanced-sql-bootcamp/",
      "bio": "Take your SQL skills to the next level with our Advanced SQL Bootcamp",
      "objectives": [
        "Learn the power of Subqueries in SQL",
        "Discover how to use Window Functions and Partitions in SQL",
        "Understand advanced JOIN commands in SQL",
        "Learn about SQL Set Operations",
        "Discover Grouping Sets",
        "Building table relationships and schema structures in SQL",
        "Use SQL Transactions with Update and Set calls",
        "Understand Table Inheritance with SQL",
        "Create Views in SQL",
        "Learn how to use Stored Procedures",
        "Discover how to use Triggers across SQL Tables",
        "Understand general useful methods and commands in Advanced SQL"
      ],
      "course_content": {
        "Introduction": [
          "Course FAQs",
          "Course Introduction",
          "Course Curriculum",
          "Database Set-up"
        ],
        "Subqueries and CTEs": [
          "Subqueries Overview",
          "Subqueries in FROM Statements",
          "Common Table Expressions",
          "Subqueries for Comparisons",
          "IN and NOT IN Statements",
          "ANY and ALL statements",
          "EXISTS clause in Subqueries",
          "Recursive CTEs",
          "Subqueries - Exercises",
          "Subqueries - Exercise Solutions"
        ],
        "Window Functions": [
          "Window Functions Overview",
          "OVER, PARTITION and ORDER BY",
          "WINDOW and Window Functions",
          "Window Functions Exercise",
          "Window Functions Exercise - Solutions"
        ],
        "Advanced JOIN Operations": [
          "Self-Joins",
          "Cross-Join",
          "Full Join",
          "USING Keyword and Natural JOINs",
          "Advanced JOINs Exercise",
          "Advanced JOINs Exercise Solutions"
        ],
        "Set Operations": [
          "UNION",
          "INTERSECT",
          "EXCEPT",
          "Set Operations - Exercise",
          "Set Operations - Exercise Solutions"
        ],
        "Grouping Sets": [
          "Grouping Sets Overview",
          "CUBE",
          "ROLLUP",
          "Grouping Sets Exercises",
          "Grouping Sets - Exercise Solutions"
        ],
        "Schema Structures and Table Relationships": [
          "Information Schema",
          "COMMENT",
          "Adding and Dropping Constraints",
          "Adding and Dropping Foreign Keys",
          "Schema Structure Exercises",
          "Schema Structure - Exercise Solutions"
        ],
        "Transactions": [
          "Transactions Overview",
          "UPDATE and SET",
          "Beginning and Ending Transactions",
          "SAVEPOINT",
          "Database Locks",
          "Transactions Exercises",
          "Transactions Exercise - Solutions"
        ],
        "Table Inheritance and Partioning": [
          "Range Partitioning",
          "List Partitioning",
          "Hash Partitioning",
          "Table Inheritance",
          "Table Partitioning Exercises",
          "Table Partitioning Exercises - Solutions"
        ],
        "Views": [
          "Creating Views",
          "Modifying and Deleting Views",
          "Updatable Views",
          "Materialized Views",
          "Recursive Views",
          "Views Exercise",
          "Views Exercise - Solutions"
        ]
      },
      "requirements": [
        "You should know basic SQL already before taking this course, like the concepts covered in our Complete SQL Bootcamp"
      ],
      "description": "Welcome to the best online course to take your SQL skills to the next level!\nThis course is designed to take you from basic SQL knowledge to a SQL Developer Professional. After completing this course you will have a better understanding of how tables and databases work as well as advanced capabilities in querying the information that is stored in a SQL database effectively.\nSQL is a critical skill for the modern workforce and we've created an online course specifically designed to take your current SQL knowledge to an advanced level. We'll begin the course with a deep dive into understanding how to construct subqueries and common table expressions, afterwards we'll move on to discussing window functions and advanced Join operations. Then you'll learn about the power of sets, including set operations and grouping sets. We will continue by learning about larger scope topics such as schema structures, table relationships, table inheritance, and views. We'll also teach you how to create easy to call stored procedures and automatic triggers across your database. After each section we will test your skills with a set of exercise questions.\nIn this course, you'll learn everything you need to be an expert SQL developer, we cover the topics that other basics courses don't! In this course you'll learn about:\nSubqueries\nCommon Table Expressions\nWindow Functions\nAdvanced Join Operations\nSet Operations\nGrouping Sets\nSchema Structures and Table Relationships\nTable Transactions\nViews\nTable Inheritance\nStored Procedures\nTriggers\nUseful Advanced Methods\nand much more!\nNot only does this course come with great technical learning content, but we also provide support in our Q&A Forums inside the course as well as access to our exclusive student community discord server where you can connect with other students. All of this material comes with a 30-day money back guarantee , so you can try the course completely risk free.\nEnroll today and we'll see you inside the course!",
      "target_audience": [
        "SQL Developers looking to upskill to the next level"
      ]
    },
    {
      "title": "Python & Machine Learning for Financial Analysis",
      "url": "https://www.udemy.com/course/ml-and-python-in-finance-real-cases-and-practical-solutions/",
      "bio": "Master Python Programming Fundamentals and Harness the Power of ML to Solve Real-World Practical Applications in Finance",
      "objectives": [
        "Master Python 3 programming fundamentals for Data Science and Machine Learning with focus on Finance.",
        "Understand how to leverage the power of Python to apply key financial concepts such as calculating daily portfolio returns, risk and Sharpe ratio.",
        "Understand the theory and intuition behind Capital Asset Pricing Model (CAPM)",
        "Understand how to use Jupyter Notebooks for developing, presenting and sharing Data Science projects.",
        "key Python Libraries such as NumPy for scientific computing, Pandas for Data Analysis, Matplotlib/Seaborn for data plotting/visualization",
        "Master SciKit-Learn library to build, train and tune machine learning models using real-world datasets.",
        "Apply machine and deep learning models to solve real-world problems in the banking and finance sectors",
        "Understand the theory and intuition behind several machine learning algorithms for regression, classification and clustering",
        "Assess the performance of trained machine learning regression models using various KPI (Key Performance indicators)",
        "Assess the performance of trained machine learning classifiers using various KPIs such as accuracy, precision, recall, and F1-score.",
        "Understand the underlying theory, intuition behind Artificial Neural Networks (ANNs), Recurrent Neural Networks (RNNs) & Long Short Term Memory Networks (LSTM).",
        "Train ANNs using back propagation and gradient descent algorithms.",
        "Optimize ANNs hyper parameters such as number of hidden layers and neurons to enhance network performance.",
        "Master feature engineering and data cleaning strategies for machine learning and data science applications."
      ],
      "course_content": {},
      "requirements": [
        "No prior experience required."
      ],
      "description": "Are you ready to learn python programming fundamentals and directly apply them to solve real world applications in Finance and Banking?\nIf the answer is yes, then welcome to the “The Complete Python and Machine Learning for Financial Analysis” course in which you will learn everything you need to develop practical real-world finance/banking applications in Python!\nSo why Python?\nPython is ranked as the number one programming language to learn in 2020, here are 6 reasons you need to learn Python right now!\n1. #1 language for AI & Machine Learning: Python is the #1 programming language for machine learning and artificial intelligence.\n2. Easy to learn: Python is one of the easiest programming language to learn especially of you have not done any coding in the past.\n3. Jobs: high demand and low supply of python developers make it the ideal programming language to learn now.\n4. High salary: Average salary of Python programmers in the US is around $116 thousand dollars a year.\n5. Scalability: Python is extremely powerful and scalable and therefore real-world apps such as Google, Instagram, YouTube, and Spotify are all built on Python.\n6. Versatility: Python is the most versatile programming language in the world, you can use it for data science, financial analysis, machine learning, computer vision, data analysis and visualization, web development, gaming and robotics applications.\n\n\nThis course is unique in many ways:\n1. The course is divided into 3 main parts covering python programming fundamentals, financial analysis in Python and AI/ML application in Finance/Banking Industry. A detailed overview is shown below:\na) Part #1 – Python Programming Fundamentals: Beginner’s Python programming fundamentals covering concepts such as: data types, variables assignments, loops, conditional statements, functions, and Files operations. In addition, this section will cover key Python libraries for data science such as Numpy and Pandas. Furthermore, this section covers data visualization tools such as Matplotlib, Seaborn, Plotly, and Bokeh.\nb) Part #2 – Financial Analysis in Python: This part covers Python for financial analysis. We will cover key financial concepts such as calculating daily portfolio returns, risk and Sharpe ratio. In addition, we will cover Capital Asset Pricing Model (CAPM), Markowitz portfolio optimization, and efficient frontier. We will also cover trading strategies such as momentum-based and moving average trading.\nc) Part #3 – AI/Ml in Finance/Banking: This section covers practical projects on AI/ML applications in Finance. We will cover application of Deep Neural Networks such as Long Short Term Memory (LSTM) networks to perform stock price predictions. In addition, we will cover unsupervised machine learning strategies such as K-Means Clustering and Principal Components Analysis to perform Baking Customer Segmentation or Clustering. Furthermore, we will cover the basics of Natural Language Processing (NLP) and apply it to perform stocks sentiment analysis.\n2. There are several mini challenges and exercises throughout the course and you will learn by doing. The course contains mini challenges and coding exercises in almost every video so you will learn in a practical and easy way.\n3. The Project-based learning approach: you will build more than 6 full practical projects that you can add to your portfolio of projects to showcase your future employer during job interviews.\n\n\nSo who is this course for?\nThis course is geared towards the following:\nFinancial analysts who want to harness the power of Data science and AI to optimize business processes, maximize revenue, reduce costs.\nPython programmer beginners and data scientists wanting to gain a fundamental understanding of Python and Data Science applications in Finance/Banking sectors.\nInvestment bankers and financial analysts wanting to advance their careers, build their data science portfolio, and gain real-world practical experience.\nThere is no prior experience required, Even if you have never used python or any programming language before, don’t worry! You will have a clear video explanation for each of the topics we will be covering. We will start from the basics and gradually build up your knowledge.\nIn this course, (1) you will have a true practical project-based learning experience, we will build more than 6 projects together (2) You will have access to all the codes and slides, (3) You will get a certificate of completion that you can post on your LinkedIn profile to showcase your skills in python programming to employers. (4) All of this comes with a 30 day money back guarantee so you can give a course a try risk free! Check out the preview videos and the outline to get an idea of the projects we will be covering.\n\n\nEnroll today and I look forward to seeing you inside!",
      "target_audience": [
        "Financial analysts who want to harness the power of Data science and AI to optimize business processes, maximize revenue, reduce costs.",
        "Python programmer beginners and data scientists wanting to gain a fundamental understanding of Python and Data Science applications in Finance/Banking sectors.",
        "Investment bankers and financial analysts wanting to advance their careers, build their data science portfolio, and gain real-world practical experience.",
        "There is no prior experience required, Even if you have never used python or any programming language before, don’t worry! You will have a clear video explanation for each of the topics we will be covering. We will start from the basics and gradually build up your knowledge."
      ]
    },
    {
      "title": "Artificial Neural Networks (ANN) with Keras in Python and R",
      "url": "https://www.udemy.com/course/deep-learning-with-keras-and-tensorflow-in-python-and-r/",
      "bio": "Understand Deep Learning and build Neural Networks using TensorFlow 2.0 and Keras in Python and R",
      "objectives": [
        "Get a solid understanding of Artificial Neural Networks (ANN) and Deep Learning",
        "Learn usage of Keras and Tensorflow libraries",
        "Understand the business scenarios where Artificial Neural Networks (ANN) is applicable",
        "Building a Artificial Neural Networks (ANN) in Python and R",
        "Use Artificial Neural Networks (ANN) to make predictions"
      ],
      "course_content": {},
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Course on Deep Learning using Keras and Tensorflow that teaches you everything you need to create a Neural Network model in Python and R, right?\nYou've found the right Neural Networks course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using Neural network Models.\nHave a clear understanding of Advanced Neural network concepts such as Gradient Descent, forward and Backward Propagation etc.\nCreate Neural network models in Python and R using Keras and Tensorflow libraries and analyze their results.\nConfidently practice, discuss and understand Deep Learning concepts\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Neural networks course.\nIf you are a business Analyst or an executive, or a student who wants to learn and apply Deep learning in Real world problems of business, this course will give you a solid base for that by teaching you some of the most advanced concepts of Neural networks and their implementation in Python without getting too Mathematical.\nWhy should you choose this course?\nThis course covers all the steps that one should take to create a predictive model using Neural Networks.\nMost courses only focus on teaching how to run the analysis but we believe that having a strong theoretical understanding of the concepts enables us to create a good model . And after running the analysis, one should be able to judge how good the model is and interpret the results to actually be able to help the business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using Deep learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 250,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Practice test, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take practice test to check your understanding of concepts. There is a final practical assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Neural network based model i.e. a Deep Learning model, to solve business problems.\nBelow are the course contents of this course on ANN:\nPart 1 - Python and R basics\nThis part gets you started with Python.\nThis part will help you set up the python and Jupyter environment on your system and it'll teach you how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nPart 2 - Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nPart 3 - Creating Regression and Classification ANN model in Python and R\nIn this part you will learn how to create ANN models in Python.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. We also solve a regression problem in which we try to predict house prices in a location. We will also cover how to create complex ANN architectures using functional API. Lastly we learn how to save and restore models.\nWe also understand the importance of libraries such as Keras and TensorFlow in this part.\nPart 4 - Data Preprocessing\nIn this part you will learn what actions you need to take to prepare Data for the analysis, these steps are very important for creating a meaningful.\nIn this section, we will start with the basic theory of decision tree then we cover data pre-processing topics like  missing value imputation, variable transformation and Test-Train split.\nBy the end of this course, your confidence in creating a Neural Network model in Python will soar. You'll have a thorough understanding of how to use ANN to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow are some popular FAQs of students who want to start their Deep learning journey-\n\n\nWhy use Python for Deep Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Deep Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nDeep Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Anyone curious to master ANN from Beginner level in short span of time"
      ]
    },
    {
      "title": "Azure Data Factory | Data Engineering on Azure and Fabric",
      "url": "https://www.udemy.com/course/build-data-engineering-pipelines-with-azure-data-factory/",
      "bio": "Data Factory on Azure and Microsoft Fabric, Azure DevOps (CI/CD), Azure Data Lake, Azure SQL and more (DP-203, DP-600)",
      "objectives": [
        "Azure Data Factory",
        "Microsoft Fabric Data Factory",
        "Azure Storage Accounts",
        "Azure SQL Databases and Upsert Operations",
        "Overview and Background of Azure Big Data Analytics Solutions",
        "Integrating Data From Various File Formats",
        "Authentication and Access: Service Principals, Managed Identities and Azure Key Vault",
        "Data Pipelines, Copy Activity, Data Flows, Control Flow and Transformation Activities",
        "Implementing a Modern Data Lake",
        "Control Flow, Parameters and Variables",
        "Real World Application of Azure Data Factory",
        "Azure DevOps Source Control with Data Factory",
        "Continuous Integration and Continuous Deployment (CI/CD)"
      ],
      "course_content": {},
      "requirements": [
        "Microsoft Azure Subscription",
        "Basic SQL"
      ],
      "description": "I am pleased to present this course on one of the most in demand data engineering tools around... Azure Data Factory!\n\n\nAs the demand for cloud-based data integration services continues to skyrocket, there is a huge demand for professionals with knowledge of services like Azure Data Factory and Microsoft Fabric. By learning these applications, users can enhance their skills and increase their job prospects in the field of data engineering and analytics.\n\n\nIn this course you will primarily be using Azure Data Factory on Microsoft Azure and Microsoft Fabric in addition to other services such as Azure Blob Storage, Azure Data Lake Storage Gen 2 and Azure SQL Database.\n\n\nThe course is packed with lectures, code-along videos and a dedicated course project. As an added benefit you will also have lifetime access to all of the lectures…\n\n\nThis course will cover the following topics:\nAzure Storage Solutions such as Azure Blob Storage and Azure Data Lake Gen2 Storage\nThe basics of Azure Data Factory including the core components such as Linked Services, Datasets, Activities, Data Flows, Pipelines and Integration Runtimes\nIntegrating data from various file formats such as CSV, JSON and Parquet\nThe Copy Activity in Azure Data Factory\nData Flows, Control Flow and Transformation Activities in Azure Data Factory\nOrchestrating Data Integration Workflows\nHow to create Schedules and Triggers to execute your pipelines\nHow to use Parameters and Variables with your Linked Services, Datasets and Pipelines\nHow to use Azure Data Factory with SQL Databases\nAuthentication and Access including Managed Identities, Service Principals and Azure Key Vault\nThe Data Factory Experience in Microsoft Fabric\nAzure DevOps Source Control\nContinuous Integration and Continuous Deployment",
      "target_audience": [
        "Anyone interested in Azure, Microsoft Fabric or Cloud Data Engineering",
        "Anyone interested in working with Big Data",
        "Anyone working with cloud data"
      ]
    },
    {
      "title": "The AI Ethics Course 2025: Work with AI Responsibly",
      "url": "https://www.udemy.com/course/the-ai-ethics-course-2025-work-with-ai-responsibly/",
      "bio": "Master AI Ethics from Data Collection to Deployment: A Complete Guide to Ethical AI",
      "objectives": [
        "Understand the ethical challenges surrounding AI",
        "Examine ethical principles like transparency, fairness, privacy, and accountability",
        "Explore the intersection of AI ethics and laws, including GDPR and the EU AI Act",
        "Analyze real-world ethical dilemmas in AI applications",
        "Review responsible AI development strategies",
        "Discover the risks of AI deployment, such as misinformation, hallucinations, and intellectual property concerns",
        "Learn about ChatGPT’s ethical implications"
      ],
      "course_content": {},
      "requirements": [
        "No prior experience is required in this course. We'll start from the basics and build your understanding step by step."
      ],
      "description": "The AI Ethics Course: Preparing You for the Real-World Challenges of AI—Today and Tomorrow\nDo you want to gain critical AI skills while ensuring ethical and responsible use in your organization?\nIf so, you’re in the right place!\nAs AI grows more powerful and integrated into everyday life, building and using it responsibly has never been more important. This AI Ethics course offers an in-depth understanding of artificial intelligence's ethical challenges.\nUnlike other AI ethics courses, this one teaches you to identify and manage ethical risks throughout the entire AI lifecycle—from data collection and model development to deployment. Whether building language models, deploying generative AI, or using tools like ChatGPT, our resources ensure your work upholds key ethical principles: privacy, fairness, transparency, and accountability.\nWhy Take This Course?\nComprehensive Curriculum: Follow a structured journey—from AI basics to advanced ethical topics like regulatory compliance and digital equity.\nPractical + Philosophical: This isn’t just theory. You’ll explore real-life examples, apply ethical thinking to hands-on AI scenarios, and walk away with actionable strategies.\nCurrent and Global: Stay updated with the most recent developments in AI and data governance, including the EU AI Act, GDPR, HIPAA, and the future of global frameworks.\nChatGPT Case Studies: Learn to apply ethical thinking to large language models use cases (LLMs). Explore such issues as data privacy, misinformation, inconsistency, and more.\nThis AI Ethics course simplifies complex ethical issues from user, business, and developer perspectives. You'll learn to distinguish proprietary, public, and web-scraped data and understand their unique challenges. Key topics—including data bias, supervised fine-tuning, AI hallucinations, deepfakes, plagiarism, and risk management—are explored thoroughly. A key course component is the ChatGPT section—one of today's leading AI tools—highlighting real-world issues and engaging conversational examples.\nAre you ready to stand out as a professional who understands the latest AI trends and the ethical responsibilities that come with them?\nToday, nearly everyone interacts with AI. It’s essential to prepare your organization for responsible engagement. Don't fear emerging technology—lead by understanding, questioning, and using it ethically.\nThe demand for AI ethics specialists is skyrocketing. Isn’t it time for you to become one?\nWe’re so confident you'll love this course—we're offering a FULL 30-day money-back guarantee!\nSign up today—ZERO risk, ALL reward!\nClick the 'Buy Now' button and embrace the AI-driven future of tomorrow.",
      "target_audience": [
        "Data scientists and AI engineers aiming to build fair, transparent, and privacy-conscious AI",
        "AI enthusiasts interested in AI ethics and its societal impact",
        "AI Users looking to understand ethical AI interactions and responsible usage",
        "Developers and tech professionals ensuring ethical AI development and deployment",
        "Business leaders and decision-makers integrating AI while managing ethical risks"
      ]
    },
    {
      "title": "Machine Learning with Javascript",
      "url": "https://www.udemy.com/course/machine-learning-with-javascript/",
      "bio": "Master Machine Learning from scratch using Javascript and TensorflowJS with hands-on projects.",
      "objectives": [
        "Assemble machine learning algorithms from scratch!",
        "Build interesting applications using Javascript and ML techniques",
        "Understand how ML works without relying on mysterious libraries",
        "Optimize your algorithms with advanced performance and memory usage profiling",
        "Use the low-level features of Tensorflow JS to supercharge your algorithms",
        "Grow a strong intuition of ML best practices"
      ],
      "course_content": {},
      "requirements": [
        "Basic understanding of terminal and command line usage",
        "Ability to read basic math equations"
      ],
      "description": "If you're here, you already know the truth: Machine Learning is the future of everything.\nIn the coming years, there won't be a single industry in the world untouched by Machine Learning.  A transformative force, you can either choose to understand it now, or lose out on a wave of incredible change.  You probably already use apps many times each day that rely upon Machine Learning techniques.  So why stay in the dark any longer?\nThere are many courses on Machine Learning already available.  I built this course to be the best introduction to the topic.  No subject is left untouched, and we never leave any area in the dark.  If you take this course, you will be prepared to enter and understand any sub-discipline in the world of Machine Learning.\n\n\nA common question - Why Javascript?  I thought ML was all about Python and R?\nThe answer is simple - ML with Javascript is just plain easier to learn than with Python.  Although it is immensely popular, Python is an 'expressive' language, which is a code-word that means 'a confusing language'.  A single line of Python can contain a tremendous amount of functionality; this is great when you understand the language and the subject matter, but not so much when you're trying to learn a brand new topic.\nBesides Javascript making ML easier to understand, it also opens new horizons for apps that you can build.  Rather than being limited to deploying Python code on the server for running your ML code, you can build single-page apps, or even browser extensions that run interesting algorithms, which can give you the possibility of developing a completely novel use case!\n\n\nDoes this course focus on algorithms, or math, or Tensorflow, or what?!?!\nLet's be honest - the vast majority of ML courses available online dance around the confusing topics.  They encourage you to use pre-build algorithms and functions that do all the heavy lifting for you.  Although this can lead you to quick successes, in the end it will hamper your ability to understand ML.  You can only understand how to apply ML techniques if you understand the underlying algorithms.\nThat's the goal of this course - I want you to understand the exact math and programming techniques that are used in the most common ML algorithms.  Once you have this knowledge, you can easily pick up new algorithms on the fly, and build far more interesting projects and applications than other engineers who only understand how to hand data to a magic library.\nDon't have a background in math?  That's OK! I take special care to make sure that no lecture gets too far into 'mathy' topics without giving a proper introduction to what is going on.\n\n\nA short list of what you will learn:\nAdvanced memory profiling to enhance the performance of your algorithms\nBuild apps powered by the powerful Tensorflow JS library\nDevelop programs that work either in the browser or with Node JS\nWrite clean, easy to understand ML code, no one-name variables or confusing functions\nPick up the basics of Linear Algebra so you can dramatically speed up your code with matrix-based operations. (Don't worry, I'll make the math easy!)\nComprehend how to twist common algorithms to fit your unique use cases\nPlot the results of your analysis using a custom-build graphing library\nLearn performance-enhancing strategies that can be applied to any type of Javascript code\nData loading techniques, both in the browser and Node JS environments",
      "target_audience": [
        "Javascript developers interested in Machine Learning"
      ]
    },
    {
      "title": "Open-source LLMs: Uncensored & secure AI locally with RAG",
      "url": "https://www.udemy.com/course/open-source-llms-uncensored-secure-ai-locally-with-rag/",
      "bio": "Private ChatGPT Alternatives: Llama3, Mistral a. more with Function Calling, RAG, Vector Databases, LangChain, AI-Agents",
      "objectives": [
        "Why Open-Source LLMs? Differences, Advantages, and Disadvantages of Open-Source and Closed-Source LLMs",
        "What are LLMs like ChatGPT, Llama, Mistral, Phi3, Qwen2-72B-Instruct, Grok, Gemma, etc.",
        "Which LLMs are available and what should I use? Finding \"The Best LLMs\"",
        "Requirements for Using Open-Source LLMs Locally",
        "Installation and Usage of LM Studio, Anything LLM, Ollama, and Alternative Methods for Operating LLMs",
        "Censored vs. Uncensored LLMs",
        "Finetuning an Open-Source Model with Huggingface or Google Colab",
        "Vision (Image Recognition) with Open-Source LLMs: Llama3, Llava & Phi3 Vision",
        "Hardware Details: GPU Offload, CPU, RAM, and VRAM",
        "All About HuggingChat: An Interface for Using Open-Source LLMs",
        "System Prompts in Prompt Engineering + Function Calling",
        "Prompt Engineering Basics: Semantic Association, Structured & Role Prompts",
        "Groq: Using Open-Source LLMs with a Fast LPU Chip Instead of a GPU",
        "Vector Databases, Embedding Models & Retrieval-Augmented Generation (RAG)",
        "Creating a Local RAG Chatbot with Anything LLM & LM Studio",
        "Linking Ollama & Llama 3, and Using Function Calling with Llama 3 & Anything LLM",
        "Function Calling for Summarizing Data, Storing, and Creating Charts with Python",
        "Using Other Features of Anything LLM and External APIs",
        "Tips for Better RAG Apps with Firecrawl for Website Data, More Efficient RAG with LlamaIndex & LlamaParse for PDFs and CSVs",
        "Definition and Available Tools for AI Agents, Installation and Usage of Flowise Locally with Node (Easier Than Langchain and LangGraph)",
        "Creating an AI Agent that Generates Python Code and Documentation, and Using AI Agents with Function Calling, Internet Access, and Three Experts",
        "Hosting and Usage: Which AI Agent Should You Build and External Hosting, Text-to-Speech (TTS) with Google Colab",
        "Finetuning Open-Source LLMs with Google Colab (Alpaca + Llama-3 8b, Unsloth)",
        "Renting GPUs with Runpod or Massed Compute",
        "Security Aspects: Jailbreaks and Security Risks from Attacks on LLMs with Jailbreaks, Prompt Injections, and Data Poisoning",
        "Data Privacy and Security of Your Data, as well as Policies for Commercial Use and Selling Generated Content"
      ],
      "course_content": {
        "Introduction and Overview": [
          "Welcome",
          "Course Overview",
          "My Goal and Some Tips",
          "Explanation of the Links",
          "Important Links",
          "Instructor Introduction: Arnold Oberleiter (Arnie)"
        ],
        "Why Open-Source LLMs? Differences, Advantages, and Disadvantages": [
          "What is this Section about?",
          "What are LLMs like ChatGPT, Llama, Mistral, etc.",
          "Which LLMs are available and what should I use: Finding \"The Best LLMs\"",
          "Disadvantages of Closed-Source LLMs like ChatGPT, Gemini, and Claude",
          "Advantages and Disadvantages of Open-Source LLMs like Llama3, Mistral & more",
          "OpenSoure LLMs get betther! DeepSeek R1 Infos",
          "Recap: Don't Forget This!"
        ],
        "The Easiest Way to Run Open-Source LLMs Locally & What You Need": [
          "Requirements for Using Open-Source LLMs Locally: GPU, CPU & Quantization",
          "Installing LM Studio and Alternative Methods for Running LLMs",
          "Using Open-Source Models in LM Studio: Llama 3, Mistral, Phi-3 & more",
          "4 Censored vs. Uncensored LLMs: Llama3 with Dolphin Finetuning",
          "The Use Cases of classic LLMs like Phi-3 Llama and more",
          "Vision (Image Recognition) with Open-Source LLMs: Llama3, Llava & Phi3 Vision",
          "Some Examples of Image Recognition (Vision)",
          "More Details on Hardware: GPU Offload, CPU, RAM, and VRAM",
          "Summary of What You Learned & an Outlook to Lokal Servers & Prompt Engineering"
        ],
        "Prompt Engineering for Open-Source LLMs and Their Use in the Cloud": [
          "HuggingChat: An Interface for Using Open-Source LLMs",
          "System Prompts: An Important Part of Prompt Engineering",
          "Why is Prompt Engineering Important? [A example]",
          "Semantic Association: The most Importnant Concept you need to understand",
          "The structured Prompt: Copy my Prompts",
          "Instruction Prompting and some Cool Tricks",
          "Role Prompting for LLMs",
          "Shot Prompting: Zero-Shot, One-Shot & Few-Shot Prompts",
          "Reverse Prompt Engineering and the \"OK\" Trick",
          "Chain of Thought Prompting: Let`s think Step by Step",
          "Tree of Thoughts (ToT) Prompting in LLMs",
          "The Combination of Prompting Concepts",
          "Creating Your Own Assistants in HuggingChat",
          "Groq: Using Open-Source LLMs with a Fast LPU Chip Instead of a GPU",
          "Recap: What You Should Remember"
        ],
        "Function Calling, RAG, and Vector Databases with Open-Source LLMs": [
          "What Will Be Covered in This Section?",
          "What is Function Calling in LLMs",
          "Vector Databases, Embedding Models & Retrieval-Augmented Generation (RAG)",
          "Installing Anything LLM and Setting Up a Local Server for a RAG Pipeline",
          "Local RAG Chatbot with Anything LLM & LM Studio",
          "Function Calling with Llama 3 & Anything LLM (Searching the Internet)",
          "Function Calling, Summarizing Data, Storing & Creating Charts with Python",
          "Other Features of Anything LLM: TTS and External APIs",
          "Downloading Ollama & Llama 3, Creating & Linking a Local Server",
          "Recap Don't Forget This!"
        ],
        "Optimizing RAG Apps: Tips for Data Preparation": [
          "What Will Be Covered in This Section: Better RAG, Data & Chunking",
          "Tips for Better RAG Apps: Firecrawl for Your Data from Websites",
          "More Efficient RAG with LlamaIndex & LlamaParse: Data Preparation for PDFs &more",
          "LlamaIndex Update: LlamaParse made easy!",
          "Chunk Size and Chunk Overlap for a Better RAG Application",
          "Recap: What You Learned in This Section"
        ],
        "Local AI Agents with Open-Source LLMs": [
          "What Will Be Covered in This Section on AI Agents",
          "AI Agents: Definition & Available Tools for Creating Opensource AI-Agents",
          "We use Langchain with Flowise, Locally with Node.js",
          "Installing Flowise with Node.js (JavaScript Runtime Environment)",
          "Problems with Flowise installation",
          "How to Fix Problems on the Installation with Node",
          "The Flowise Interface for AI-Agents and RAG ChatBots",
          "Local RAG Chatbot with Flowise, LLama3 & Ollama: A Local Langchain App",
          "Our First AI Agent: Python Code & Documentation with Superwicer and 2 Worker",
          "AI Agents with Function Calling, Internet and Three Experts for Social Media",
          "Which AI Agent Should You Build & External Hosting with Render",
          "Chatbot with Open-Source Models from Huggingface & Embeddings in HTML (Mixtral)",
          "Insanely fast inference with the Groq API",
          "How to use DeepSeek R1: Locally, in Browser and the API",
          "Recap What You Should Remember"
        ],
        "Finetuning, Renting GPUs, Open-Source TTS, Finding the BEST LLM & More Tips": [
          "What Is This Section About?",
          "Text-to-Speech (TTS) with Google Colab",
          "Moshi Talk to an Open-Source AI",
          "Finetuning an Open-Source Model with Huggingface or Google Colab",
          "Finetuning Open-Source LLMs with Google Colab, Alpaca + Llama-3 8b from Unsloth",
          "What is the Best Open-Source LLM I Should Use?",
          "Llama 3.1 Infos and What Models should you use",
          "Grok from xAI",
          "Renting a GPU with Runpod or Massed Compute if Your Local PC Isn't Enough",
          "Recap: What You Should Remember!"
        ],
        "Data Privacy, Security, and What Comes Next?": [
          "THE LAST SECTION: What is This About?",
          "Jailbreaks: Security Risks from Attacks on LLMs with Prompts",
          "Prompt Injections: Security Problem of LLMs",
          "Data Poisoning and Backdoor Attacks",
          "Data Privacy and Security: Is Your Data at Risk?",
          "Commercial Use and Selling of AI-Generated Content",
          "My Thanks and What's Next?",
          "Bonus"
        ]
      },
      "requirements": [
        "No prior knowledge is required; everything will be shown step by step.",
        "It is advantageous to have a PC with a good graphics card, 16 GB RAM, and 6 GB VRAM (the Apple M series, Nvidia, and AMD are ideal), but this is not mandatory."
      ],
      "description": "ChatGPT is useful, but have you noticed that there are many censored topics, you are pushed in certain political directions, some harmless questions go unanswered, and our data might not be secure with OpenAI? This is where open-source LLMs like Llama3, Mistral, Grok, Falkon, Phi3, and Command R+ can help!\nAre you ready to master the nuances of open-source LLMs and harness their full potential for various applications, from data analysis to creating chatbots and AI agents? Then this course is for you!\nIntroduction to Open-Source LLMs\nThis course provides a comprehensive introduction to the world of open-source LLMs. You'll learn about the differences between open-source and closed-source models and discover why open-source LLMs are an attractive alternative. Topics such as ChatGPT, Llama, and Mistral will be covered in detail. Additionally, you’ll learn about the available LLMs and how to choose the best models for your needs. The course places special emphasis on the disadvantages of closed-source LLMs and the pros and cons of open-source LLMs like Llama3 and Mistral.\nPractical Application of Open-Source LLMs\nThe course guides you through the simplest way to run open-source LLMs locally and what you need for this setup. You will learn about the prerequisites, the installation of LM Studio, and alternative methods for operating LLMs. Furthermore, you will learn how to use open-source models in LM Studio, understand the difference between censored and uncensored LLMs, and explore various use cases. The course also covers finetuning an open-source model with Huggingface or Google Colab and using vision models for image recognition.\nPrompt Engineering and Cloud Deployment\nAn important part of the course is prompt engineering for open-source LLMs. You will learn how to use HuggingChat as an interface, utilize system prompts in prompt engineering, and apply both basic and advanced prompt engineering techniques. The course also provides insights into creating your own assistants in HuggingChat and using open-source LLMs with fast LPU chips instead of GPUs.\nFunction Calling, RAG, and Vector Databases\nLearn what function calling is in LLMs and how to implement vector databases, embedding models, and retrieval-augmented generation (RAG). The course shows you how to install Anything LLM, set up a local server, and create a RAG chatbot with Anything LLM and LM Studio. You will also learn to perform function calling with Llama 3 and Anything LLM, summarize data, store it, and visualize it with Python.\nOptimization and AI Agents\nFor optimizing your RAG apps, you will receive tips on data preparation and efficient use of tools like LlamaIndex and LlamaParse. Additionally, you will be introduced to the world of AI agents. You will learn what AI agents are, what tools are available, and how to install and use Flowise locally with Node.js. The course also offers practical insights into creating an AI agent that generates Python code and documentation, as well as using function calling and internet access.\nAdditional Applications and Tips\nFinally, the course introduces text-to-speech (TTS) with Google Colab and finetuning open-source LLMs with Google Colab. You will learn how to rent GPUs from providers like Runpod or Massed Compute if your local PC isn’t sufficient. Additionally, you will explore innovative tools like Microsoft Autogen and CrewAI and how to use LangChain for developing AI agents.\nHarness the transformative power of open-source LLM technology to develop innovative solutions and expand your understanding of their diverse applications. Sign up today and start your journey to becoming an expert in the world of large language models!",
      "target_audience": [
        "To everyone who wants to learn something new and dive deep into open-source LLMs with RAG, Function Calling and AI-Agents",
        "To entrepreneurs who want to become more efficient and save money",
        "To developers, programmers, and tech enthusiasts",
        "To anyone who doesn't want the restrictions of big tech companies and wants to use uncensored AI"
      ]
    },
    {
      "title": "Modern Reinforcement Learning: Deep Q Agents (PyTorch & TF2)",
      "url": "https://www.udemy.com/course/deep-q-learning-from-paper-to-code/",
      "bio": "How to Turn Deep Reinforcement Learning Research Papers Into Agents That Beat Classic Atari Games",
      "objectives": [
        "How to read and implement deep reinforcement learning papers",
        "How to code Deep Q learning agents",
        "How to Code Double Deep Q Learning Agents",
        "How to Code Dueling Deep Q and Dueling Double Deep Q Learning Agents",
        "How to write modular and extensible deep reinforcement learning software",
        "How to automate hyperparameter tuning with command line arguments"
      ],
      "course_content": {
        "Introduction": [
          "What You Will Learn In This Course",
          "Required Background, software, and hardware",
          "How to Succeed in this Course"
        ],
        "Fundamentals of Reinforcement Learning": [
          "Agents, Environments, and Actions",
          "Markov Decision Processes",
          "Value Functions, Action Value Functions, and the Bellman Equation",
          "Model Free vs. Model Based Learning",
          "The Explore-Exploit Dilemma",
          "Temporal Difference Learning"
        ],
        "Deep Learning Crash Course": [
          "Dealing with Continuous State Spaces with Deep Neural Networks",
          "Naive Deep Q Learning in Code: Step 1 - Coding the Deep Q Network",
          "Naive Deep Q Learning in Code: Step 2 - Coding the Agent Class",
          "Naive Deep Q Learning in Code: Step 3 - Coding the Main Loop and Learning",
          "Naive Deep Q Learning in Code: Step 4 - Verifying the Functionality of Our Code",
          "Naive Deep Q Learning in Code: Step 5 - Analyzing Our Agent's Performance",
          "Dealing with Screen Images with Convolutional Neural Networks"
        ],
        "Human Level Control Through Deep Reinforcement Learning: From Paper to Code": [
          "How to Read Deep Learning Papers",
          "Analyzing the Paper",
          "How to Modify the OpenAI Gym Atari Environments",
          "How to Preprocess the OpenAI Gym Atari Screen Images",
          "How to Stack the Preprocessed Atari Screen Images",
          "How to Combine All the Changes",
          "How to Add Reward Clipping, Fire First, and No Ops",
          "How to Code the Agent's Memory",
          "How to Code the Deep Q Network",
          "Coding the Deep Q Agent: Step 1 - Coding the Constructor",
          "Coding the Deep Q Agent: Step 2 - Epsilon-Greedy Action Selection",
          "Coding the Deep Q Agent: Step 3 - Memory, Model Saving and Network Copying",
          "Coding the Deep Q Agent: Step 4 - The Agent's Learn Function",
          "Coding the Deep Q Agent: Step 5 - The Main Loop and Analyzing the Performance"
        ],
        "Deep Reinforcement Learning with Double Q Learning": [
          "Analyzing the Paper",
          "Coding the Double Q Learning Agent and Analyzing Performance"
        ],
        "Dueling Network Architectures for Deep Reinforcement Learning": [
          "Analyzing the Paper",
          "Coding the Dueling Deep Q Network",
          "Coding the Dueling Deep Q Learning Agent and Analyzing Performance",
          "Coding the Dueling Double Deep Q Learning Agent and Analyzing Performance"
        ],
        "Improving On Our Solutions": [
          "Implementing a Command Line Interface for Rapid Model Testing",
          "Consolidating Our Code Base for Maximum Extensability",
          "How to Test Our Agent and Watch it Play the Game in Real Time"
        ],
        "Conclusion": [
          "Summarizing What We've Learned"
        ],
        "Bonus Lecture": [
          "Bonus Video: Where to Go From Here"
        ],
        "Tensorflow 2 Implementations": [
          "Differences Between Tensorflow 2 and PyTorch",
          "Coding the Deep Q Network Class in Tensorflow 2",
          "Coding the Deep Q Learning Agent in Tensorflow 2",
          "Testing the Tensorflow 2 Deep Q Learning Agent",
          "Coding the Tensorflow 2 Double Q Learning Agent",
          "Coding the Dueling Network and Agent in Tensorflow 2",
          "Coding the Dueling Double DQN Agent in Tensorflow 2"
        ]
      },
      "requirements": [
        "Some College Calculus",
        "Exposure To Deep Learning",
        "Comfortable with Python"
      ],
      "description": "In this complete deep reinforcement learning course you will learn a repeatable framework for reading and implementing deep reinforcement learning research papers. You will read the original papers that introduced the Deep Q learning, Double Deep Q learning, and Dueling Deep Q learning algorithms. You will then learn how to implement these in pythonic and concise PyTorch and Tensorflow 2 code, that can be extended to include any future deep Q learning algorithms. These algorithms will be used to solve a variety of environments from the Open AI gym's Atari library, including Pong, Breakout, and Bankheist.\n\n\nYou will learn the key to making these Deep Q Learning algorithms work, which is how to modify the Open AI Gym's Atari library to meet the specifications of the original Deep Q Learning papers. You will learn how to:\nRepeat actions to reduce computational overhead\nRescale the Atari screen images to increase efficiency\nStack frames to give the Deep Q agent a sense of motion\nEvaluate the Deep Q agent's performance with random no-ops to deal with model over training\nClip rewards to enable the Deep Q learning agent to generalize across Atari games with different score scales\n\n\nIf you do not have prior experience in reinforcement or deep reinforcement learning, that's no problem. Included in the course is a complete and concise course on the fundamentals of reinforcement learning. The introductory course in reinforcement learning will be taught in the context of solving the Frozen Lake environment from the Open AI Gym.\nWe will cover:\nMarkov decision processes\nTemporal difference learning\nThe original Q learning algorithm\nHow to solve the Bellman equation\nValue functions and action value functions\nModel free vs. model based reinforcement learning\nSolutions to the explore-exploit dilemma, including optimistic initial values and epsilon-greedy action selection\nAlso included is a mini course in deep learning using the PyTorch framework. This is geared for students who are familiar with the basic concepts of deep learning, but not the specifics, or those who are comfortable with deep learning in another framework, such as Tensorflow or Keras. You will learn how to code a deep neural network in Pytorch as well as how convolutional neural networks function. This will be put to use in implementing a naive Deep Q learning agent to solve the Cartpole problem from the Open AI gym.",
      "target_audience": [
        "Python developers eager to learn about cutting edge deep reinforcement learning"
      ]
    },
    {
      "title": "Time Series Analysis, Forecasting, and Machine Learning",
      "url": "https://www.udemy.com/course/time-series-analysis/",
      "bio": "Python for LSTMs, ARIMA, Deep Learning, AI, Support Vector Regression, +More Applied to Time Series Forecasting",
      "objectives": [
        "ETS and Exponential Smoothing Models",
        "Holt's Linear Trend Model and Holt-Winters",
        "Autoregressive and Moving Average Models (ARIMA)",
        "Seasonal ARIMA (SARIMA), and SARIMAX",
        "Auto ARIMA",
        "The statsmodels Python library",
        "The pmdarima Python library",
        "Machine learning for time series forecasting",
        "Deep learning (ANNs, CNNs, RNNs, and LSTMs) for time series forecasting",
        "Tensorflow 2 for predicting stock prices and returns",
        "Vector autoregression (VAR) and vector moving average (VMA) models (VARMA)",
        "AWS Forecast (Amazon's time series forecasting service)",
        "FB Prophet (Facebook's time series library)",
        "Modeling and forecasting financial time series",
        "GARCH (volatility modeling)"
      ],
      "course_content": {
        "Welcome": [
          "Introduction and Outline",
          "Warmup (Optional)"
        ],
        "Getting Set Up": [
          "Where to get the code, notebooks, and data",
          "How to Succeed in This Course",
          "Temporary 403 Errors"
        ],
        "Time Series Basics": [
          "Time Series Basics Section Introduction",
          "What is a Time Series?",
          "Modeling vs. Predicting",
          "Why Do We Care About Shapes?",
          "Types of Tasks",
          "Power, Log, and Box-Cox Transformations",
          "Power, Log, and Box-Cox Transformations in Code",
          "Forecasting Metrics",
          "Financial Time Series Primer",
          "Price Simulations in Code",
          "Random Walks and the Random Walk Hypothesis",
          "The Naive Forecast and the Importance of Baselines",
          "Naive Forecast and Forecasting Metrics in Code",
          "Time Series Basics Section Summary",
          "Suggestion Box"
        ],
        "Exponential Smoothing and ETS Methods": [
          "Exponential Smoothing Section Introduction",
          "Exponential Smoothing Intuition for Beginners",
          "SMA Theory",
          "SMA Code",
          "EWMA Theory",
          "EWMA Code",
          "SES Theory",
          "SES Code",
          "Holt's Linear Trend Model (Theory)",
          "Holt's Linear Trend Model (Code)",
          "Holt-Winters (Theory)",
          "Holt-Winters (Code)",
          "Walk-Forward Validation",
          "Walk-Forward Validation in Code",
          "Application: Sales Data",
          "Application: Stock Predictions",
          "SMA Application: COVID-19 Counting",
          "SMA Application: Algorithmic Trading",
          "Exponential Smoothing Section Summary",
          "(Optional) More About State-Space Models"
        ],
        "ARIMA": [
          "ARIMA Section Introduction",
          "Autoregressive Models - AR(p)",
          "Moving Average Models - MA(q)",
          "ARIMA",
          "ARIMA in Code",
          "Stationarity",
          "Stationarity in Code",
          "ACF (Autocorrelation Function)",
          "PACF (Partial Autocorrelation Function)",
          "ACF and PACF in Code (pt 1)",
          "ACF and PACF in Code (pt 2)",
          "Auto ARIMA and SARIMAX",
          "Model Selection, AIC and BIC",
          "Auto ARIMA in Code",
          "Auto ARIMA in Code (Stocks)",
          "ACF and PACF for Stock Returns",
          "Auto ARIMA in Code (Sales Data)",
          "How to Forecast with ARIMA",
          "Forecasting Out-Of-Sample",
          "ARIMA Section Summary"
        ],
        "Vector Autoregression (VAR, VMA, VARMA)": [
          "Vector Autoregression Section Introduction",
          "VAR and VARMA Theory",
          "VARMA Code (pt 1)",
          "VARMA Code (pt 2)",
          "VARMA Code (pt 3)",
          "VARMA Econometrics Code (pt 1)",
          "VARMA Econometrics Code (pt 2)",
          "Granger Causality",
          "Granger Causality Code",
          "Converting Between Models (Optional)",
          "Vector Autoregression Section Summary"
        ],
        "Machine Learning Methods": [
          "Machine Learning Section Introduction",
          "Supervised Machine Learning: Classification and Regression",
          "Autoregressive Machine Learning Models",
          "Machine Learning Algorithms: Linear Regression",
          "Machine Learning Algorithms: Logistic Regression",
          "Machine Learning Algorithms: Support Vector Machines",
          "Machine Learning Algorithms: Random Forest",
          "Extrapolation and Stock Prices",
          "Machine Learning for Time Series Forecasting in Code (pt 1)",
          "Forecasting with Differencing",
          "Machine Learning for Time Series Forecasting in Code (pt 2)",
          "Application: Sales Data",
          "Application: Predicting Stock Prices and Returns",
          "Application: Predicting Stock Movements",
          "Machine Learning Section Summary"
        ],
        "Deep Learning: Artificial Neural Networks (ANN)": [
          "Artificial Neural Networks: Section Introduction",
          "The Neuron",
          "Forward Propagation",
          "The Geometrical Picture",
          "Activation Functions",
          "Multiclass Classification",
          "ANN Code Preparation",
          "Feedforward ANN for Time Series Forecasting Code",
          "Feedforward ANN for Stock Return and Price Predictions Code",
          "Human Activity Recognition Dataset",
          "Human Activity Recognition: Code Preparation",
          "Human Activity Recognition: Data Exploration",
          "Human Activity Recognition: Multi-Input ANN",
          "Human Activity Recognition: Feature-Based Model",
          "Human Activity Recognition: Combined Model",
          "How Does a Neural Network \"Learn\"?",
          "Artificial Neural Networks: Section Summary"
        ],
        "Deep Learning: Convolutional Neural Networks (CNN)": [
          "CNN Section Introduction",
          "What is Convolution?",
          "What is Convolution? (Pattern-Matching)",
          "What is Convolution? (Weight Sharing)",
          "Convolution on Color Images",
          "Convolution for Time Series and ARIMA",
          "CNN Architecture",
          "CNN Code Preparation",
          "CNN for Time Series Forecasting in Code",
          "CNN for Human Activity Recognition",
          "CNN Section Summary"
        ],
        "Deep Learning: Recurrent Neural Networks (RNN)": [
          "RNN Section Introduction",
          "Simple RNN / Elman Unit (pt 1)",
          "Simple RNN / Elman Unit (pt 2)",
          "Aside: State Space Models vs. RNNs",
          "RNN Code Preparation",
          "RNNs: Understanding by Implementing (Paying Attention to Shapes)",
          "GRU and LSTM (pt 1)",
          "GRU and LSTM (pt 2)",
          "LSTMs for Time Series Forecasting in Code",
          "LSTMs for Time Series Classification in Code",
          "The Unreasonable Ineffectiveness of Recurrent Neural Networks",
          "RNN Section Summary"
        ]
      },
      "requirements": [
        "Decent Python coding skills",
        "Numpy, Matplotlib, Pandas, and Scipy (I teach this for free! My gift to the community)",
        "Matrix arithmetic",
        "Probability"
      ],
      "description": "Hello friends!\nWelcome to Time Series Analysis, Forecasting, and Machine Learning in Python.\nTime Series Analysis has become an especially important field in recent years.\nWith inflation on the rise, many are turning to the stock market and cryptocurrencies in order to ensure their savings do not lose their value.\nCOVID-19 has shown us how forecasting is an essential tool for driving public health decisions.\nBusinesses are becoming increasingly efficient, forecasting inventory and operational needs ahead of time.\n\n\nLet me cut to the chase. This is not your average Time Series Analysis course. This course covers modern developments such as deep learning, time series classification (which can drive user insights from smartphone data, or read your thoughts from electrical activity in the brain), and more.\nWe will cover techniques such as:\nETS and Exponential Smoothing\nHolt's Linear Trend Model\nHolt-Winters Model\nARIMA, SARIMA, SARIMAX, and Auto ARIMA\nACF and PACF\nVector Autoregression and Moving Average Models (VAR, VMA, VARMA)\nMachine Learning Models (including Logistic Regression, Support Vector Machines, and Random Forests)\nDeep Learning Models (Artificial Neural Networks, Convolutional Neural Networks, and Recurrent Neural Networks)\nGRUs and LSTMs for Time Series Forecasting\nWe will cover applications such as:\nTime series forecasting of sales data\nTime series forecasting of stock prices and stock returns\nTime series classification of smartphone data to predict user behavior\nThe VIP version of the course will cover even more exciting topics, such as:\nAWS Forecast (Amazon's state-of-the-art low-code forecasting API)\nGARCH (financial volatility modeling)\nFB Prophet (Facebook's time series library)\nSo what are you waiting for? Signup now to get lifetime access, a certificate of completion you can show off on your LinkedIn profile, and the skills to use the latest time series analysis techniques that you cannot learn anywhere else.\nThanks for reading, and I'll see you in class!\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Anyone who loves or wants to learn about time series analysis",
        "Students and professionals who want to advance their career in finance, time series analysis, or data science"
      ]
    },
    {
      "title": "Ensemble Machine Learning in Python: Random Forest, AdaBoost",
      "url": "https://www.udemy.com/course/machine-learning-in-python-random-forest-adaboost/",
      "bio": "Ensemble Methods: Boosting, Bagging, Boostrap, and Statistical Machine Learning for Data Science in Python",
      "objectives": [
        "Understand and derive the bias-variance decomposition",
        "Understand the bootstrap method and its application to bagging",
        "Understand why bagging improves classification and regression performance",
        "Understand and implement Random Forest",
        "Understand and implement AdaBoost"
      ],
      "course_content": {
        "Get Started": [
          "Outline and Motivation",
          "Where to get the Code and Data",
          "All Data is the Same",
          "Plug-and-Play",
          "How to Succeed in This Course"
        ],
        "Bias-Variance Trade-Off": [
          "Bias-Variance Key Terms",
          "Bias-Variance Trade-Off",
          "Bias-Variance Decomposition",
          "Polynomial Regression Demo",
          "K-Nearest Neighbor and Decision Tree Demo",
          "Cross-Validation as a Method for Optimizing Model Complexity",
          "Suggestion Box"
        ],
        "Bootstrap Estimates and Bagging": [
          "Bootstrap Estimation",
          "Bootstrap Demo",
          "Bagging",
          "Bagging Regression Trees",
          "Bagging Classification Trees",
          "Stacking"
        ],
        "Random Forest": [
          "Random Forest Algorithm",
          "Random Forest Regressor",
          "Random Forest Classifier",
          "Random Forest vs Bagging Trees",
          "Implementing a \"Not as Random\" Forest",
          "Connection to Deep Learning: Dropout"
        ],
        "AdaBoost": [
          "AdaBoost Algorithm",
          "Additive Modeling",
          "AdaBoost Loss Function: Exponential Loss",
          "AdaBoost Implementation",
          "Comparison to Stacking",
          "Connection to Deep Learning",
          "Summary and What's Next"
        ],
        "Background Review": [
          "Confidence Intervals"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow"
        ],
        "Extra Help With Python Coding for Beginners (FAQ by Student Request)": [
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Python 2 vs Python 3"
        ],
        "Effective Learning Strategies for Machine Learning (FAQ by Student Request)": [
          "How to Succeed in this Course (Long Version)",
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "Machine Learning and AI Prerequisite Roadmap (pt 1)",
          "Machine Learning and AI Prerequisite Roadmap (pt 2)"
        ]
      },
      "requirements": [
        "Calculus (derivatives)",
        "Numpy, Matplotlib, Sci-Kit Learn",
        "K-Nearest Neighbors, Decision Trees",
        "Probability and Statistics (undergraduate level)",
        "Linear Regression, Logistic Regresion"
      ],
      "description": "In recent years, we've seen a resurgence in AI, or artificial intelligence, and machine learning.\nMachine learning has led to some amazing results, like being able to analyze medical images and predict diseases on-par with human experts.\nGoogle's AlphaGo program was able to beat a world champion in the strategy game go using deep reinforcement learning.\nMachine learning is even being used to program self driving cars, which is going to change the automotive industry forever. Imagine a world with drastically reduced car accidents, simply by removing the element of human error.\nGoogle famously announced that they are now \"machine learning first\", and companies like NVIDIA and Amazon have followed suit, and this is what's going to drive innovation in the coming years.\nMachine learning is embedded into all sorts of different products, and it's used in many industries, like finance, online advertising, medicine, and robotics.\nIt is a widely applicable tool that will benefit you no matter what industry you're in, and it will also open up a ton of career opportunities once you get good.\nMachine learning also raises some philosophical questions. Are we building a machine that can think? What does it mean to be conscious? Will computers one day take over the world?\nThis course is all about ensemble methods.\nWe've already learned some classic machine learning models like k-nearest neighbor and decision tree. We've studied their limitations and drawbacks.\nBut what if we could combine these models to eliminate those limitations and produce a much more powerful classifier or regressor?\nIn this course you'll study ways to combine models like decision trees and logistic regression to build models that can reach much higher accuracies than the base models they are made of.\nIn particular, we will study the Random Forest and AdaBoost algorithms in detail.\nTo motivate our discussion, we will learn about an important topic in statistical learning, the bias-variance trade-off. We will then study the bootstrap technique and bagging as methods for reducing both bias and variance simultaneously.\nWe'll do plenty of experiments and use these algorithms on real datasets so you can see first-hand how powerful they are.\nSince deep learning is so popular these days, we will study some interesting commonalities between random forests, AdaBoost, and deep learning neural networks.\nAll the materials for this course are FREE. You can download and install Python, Numpy, and Scipy with simple commands on Windows, Linux, or Mac.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nCalculus (derivatives)\nProbability\nObject-oriented programming\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations\nSimple machine learning models like linear regression and decision trees\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Understand the types of models that win machine learning contests (Netflix prize, Kaggle)",
        "Students studying machine learning",
        "Professionals who want to apply data science and machine learning to their work",
        "Entrepreneurs who want to apply data science and machine learning to optimize their business",
        "Students in computer science who want to learn more about data science and machine learning",
        "Those who know some basic machine learning models but want to know how today's most powerful models (Random Forest, AdaBoost, and other ensemble methods) are built"
      ]
    },
    {
      "title": "OpenAI Assistants with OpenAI Python API",
      "url": "https://www.udemy.com/course/openai-assistants-with-openai-python-api/",
      "bio": "Learn to use the new OpenAI Assistants API, allowing GPT models to run code, read your files, and call functions!",
      "objectives": [
        "Understand the basics of the OpenAI Chat Completion API Call",
        "Learn how to create OpenAI Assistants that can run and execute code on their own.",
        "Discover the power of knowledge retrieval with assistants, allowing them to use your documents!",
        "Build your own assistants that can utilize custom function calling to run your own custom Python functions!"
      ],
      "course_content": {
        "Introduction": [
          "COURSE FAQs and DOWNLOADS",
          "Course Curriculum Overview"
        ],
        "OpenAI Chat Completion API Overview": [
          "OpenAI Account Setup",
          "Messages and Parameters",
          "Chat Completion Exercise",
          "Chat Completion Exercise - Solution Code Along"
        ],
        "OpenAI API Assistants and Code Interpreter": [
          "How do Assistants Work?",
          "Understanding LLM Assistant and Motivations",
          "Assistants, Threads, and Messages",
          "Runs",
          "Assistant Workflow",
          "Assistant Exercise",
          "Assistant Exercise - Solution Code Along"
        ],
        "OpenAI Assistants with Knowledge Retrieval": [
          "How Knowledge Retrieval Works",
          "Single File in Message",
          "File with Code Interpreter",
          "Multiple Files with Assistant",
          "Assistant Knowledge Retrieval - Exercise Overview",
          "Assistant Knowledge Retrieval - Exercise Solution Code Along"
        ],
        "Assistant Function Calling": [
          "Understanding Function Calling with Assistants",
          "Converting Python Function to JSON Request",
          "Function Calling with an Assistant",
          "Assistant with Function Calling - Exercise Overview",
          "Assistant with Function Calling - Exercise Solution"
        ]
      },
      "requirements": [
        "Python Programming Experience Required, recommended you also have familiarity with OpenAI"
      ],
      "description": "Unleash the Power of AI in Your Applications with Our Exclusive OpenAI Assistants API Course!\nWelcome to an extraordinary journey into the world of AI with our Udemy course on the OpenAI Assistants API. This course is designed for enthusiasts, developers, and innovators who are eager to integrate advanced AI capabilities into their applications.\nKey Highlights of the Course:\nAssistant Creation Mastery: Dive into the world of creating versatile AI assistants. Learn to configure assistants using the cutting-edge GPT-3.5 or GPT-4 models. Get hands-on experience in enabling advanced tools like Code Interpreter and Retrieval, and see how you can create specialized assistants, such as a personal math tutor, tailored to your needs.\nConversation Management with Threads: Master the art of managing user interactions through Threads. Understand how each user interaction starts a new Thread with no limit on the number of messages, and how the Assistant efficiently manages input tokens within the maximum context window.\nAdvanced Message Handling: Explore the intricacies of adding text and files to a Thread. Stay ahead with insights into upcoming features like image uploads in messages.\nDynamic Assistant Responses: Learn to run the Assistant effectively to process messages in a Thread, triggering tools automatically. Gain expertise in context window management, crucial for both cost efficiency and performance optimization.\nRun Status and Response Display: Become proficient in monitoring the status of Runs and displaying the Assistant's responses upon completion, a key skill for ensuring seamless user experiences.\nCustomization and Tool Access: Customize Assistants to match specific requirements. Gain knowledge in using OpenAI-hosted tools and creating custom tools through Function Calling.\nFile Handling and Object Architecture: Understand how to handle various file formats and delve into the object architecture of the API, including Assistants, Threads, Messages, Runs, and Run Steps.\nPractical Run and Thread Management: Learn the practical aspects of managing Threads and Messages, and understand the lifecycle and statuses of Runs, including polling and thread locks.\nData Access and API Limitations: Get guidance on appropriate data access controls and authorization, and understand the current limitations of the API.\nTools Deep Dive: Get a comprehensive understanding of tools like Code Interpreter, Knowledge Retrieval, and Function Calling. Learn about their costs, capabilities, and how they enhance the functionality of AI assistants.\nWhy Choose This Course?\nHands-on Learning: Engage in practical, real-world examples and exercises.\nFuture-Ready Skills: Stay ahead in the technology curve by mastering an API that's continually evolving.\nExpert Guidance: Learn from instructors with deep expertise in AI and the OpenAI ecosystem.\nCommunity and Support: Join a community of learners and experts, and receive continuous support throughout your learning journey.\nWhether you're looking to enhance your application with AI, seeking to streamline business processes, or simply curious about the potential of AI assistants, this course is your gateway to unlocking new possibilities.\nEnroll now and be part of the AI revolution!",
      "target_audience": [
        "Python developers looking to learn about the new Assistants API from OpenAI"
      ]
    },
    {
      "title": "Exploring The Technologies Behind ChatGPT, GPT o4 & LLMs",
      "url": "https://www.udemy.com/course/exploring-the-technologies-behind-chatgpt-openai/",
      "bio": "COMPLETELY REDONE - The only course you need to learn large language models (LLMs) - ChatGPT, GPT o4, BERT & More!",
      "objectives": [
        "Identify and select the most suitable transformer-based model for specific NLP tasks.",
        "Comprehend how transformers process text and generate predictions.",
        "Fine-tune transformer-based models with custom datasets.",
        "Develop and implement actionable pipelines using fine-tuned models.",
        "Deploy fine-tuned models for production use.",
        "Perform effective prompt engineering for optimal outputs from GPT-o1 and ChatGPT.",
        "Understand the concepts of attention mechanisms and their application in NLP.",
        "Grasp the principles of transfer learning and its role in NLP.",
        "Utilize BERT for natural language understanding tasks.",
        "Conduct pre-training and fine-tuning of BERT models.",
        "Apply hands-on experience with BERT for various NLP tasks.",
        "Explore natural language generation using GPT models.",
        "Gain practical experience with GPT models for text generation tasks.",
        "Integrate BERT and GPT models for advanced NLP applications.",
        "Understand the fundamentals and applications of the T5 model.",
        "Engage in hands-on projects with T5 for different NLP tasks.",
        "Deploy transformer models in real-world scenarios.",
        "Utilize massively large language models effectively.",
        "Apply best practices and strategies for using ChatGPT and other LLMs in various applications."
      ],
      "course_content": {
        "Welcome": [
          "Introduction to the Course",
          "Welcome Message"
        ],
        "Getting Started with Large Language Models": [
          "Understanding Large Language Models",
          "The Evolution of Natural Language Processing Techniques",
          "Understanding Attention Mechanisms in Deep Learning",
          "Fundamentals of Encoder-Decoder Architectures",
          "Introduction to Transformer Models",
          "Detailed Exploration of Transformer Mechanics",
          "Deep Dive into Scaled Dot Product Attention - Part 1",
          "Understanding Transformers and Attention in NLP (Updated)",
          "Deep Dive into Scaled Dot Product Attention - Part 2",
          "Deep Dive into Scaled Dot Product Attention - Part 3",
          "Comprehensive Study of Multi-Headed Attention - Part 1",
          "Comprehensive Study of Multi-Headed Attention - Part 2",
          "Exploring Transfer Learning Techniques - Part 1",
          "Exploring Transfer Learning Techniques - Part 2",
          "Getting Started with PyTorch for NLP",
          "Streamlining Transformer Models with PyTorch",
          "Introduction to BERT: The New NLP Paradigm - Part 1",
          "Introduction to BERT: The New NLP Paradigm - Part 2",
          "Techniques for Effective Wordpiece Tokenization - Part 1",
          "Techniques for Effective Wordpiece Tokenization - Part 2",
          "Understanding BERT's Embedding Capabilities - Part 1",
          "Understanding BERT's Embedding Capabilities - Part 2",
          "Advanced Language Modeling with BERT - Part 1",
          "Advanced Language Modeling with BERT - Part 2",
          "Next Sentence Prediction Techniques with BERT - Part 1",
          "Next Sentence Prediction Techniques with BERT - Part 2",
          "Advanced Applications of BERT in NLP Tasks - Part 1",
          "Advanced Applications of BERT in NLP Tasks - Part 2",
          "An Overview of BERT Variants",
          "BERT Variants: Advanced Concepts",
          "Basics of Sequence Classification with BERT",
          "Advanced Sequence Classification Techniques with BERT",
          "Practical Applications of BERT in Sequence Classification",
          "Foundations of Token Classification with BERT",
          "Advanced Token Classification Strategies Using BERT",
          "Core Concepts of Question Answering with BERT",
          "In-Depth Analysis of BERT for Question Answering",
          "GPT Model Architecture: An Introduction",
          "Innovative Applications of the GPT Architecture",
          "Techniques in Masked Multi-Head Attention",
          "Advanced Uses of Masked Multi-Head Attention",
          "Practical Implementations of Multi-Head Attention",
          "Strategies for Efficient GPT Pre-Training",
          "Optimizing GPT for Enhanced Performance",
          "Introduction to Few-Shot Learning with GPT",
          "Advanced Few-Shot Learning Techniques",
          "Basics of Stylistic Completion with GPT",
          "Advanced Stylistic Completion Techniques with GPT",
          "Implementing Efficient Code Dictation with GPT",
          "Mastering GPT for Code Dictation: Deep Dive",
          "GPT for Code Dictation: Advanced Implementation and Strategies",
          "Exploring Siamese BERT Networks for Semantic Analysis",
          "Deep Dive into Siamese BERT Networks for Semantic Analysis",
          "Advanced Siamese BERT Networks for Semantic Interpretation",
          "Siamese BERT Networks: Semantic Interpretation Case Studies",
          "Multitasking with GPT: An Introductory Approach",
          "Advancing Multi-Task Performance in GPT",
          "Practical Applications of Multitasking with GPT",
          "Introduction to T5's Encoder-Decoder Architecture",
          "Principles and Techniques of Pre-training the T5 Model",
          "Understanding Cross-Attention Mechanisms in T5",
          "Exploring Pre-Trained T5 Models: Fundamentals",
          "Advanced Applications of Pre-Trained T5 Models",
          "Implementing Abstractive Summarization with T5: Core Techniques",
          "Enhancing Techniques in Abstractive Summarization with T5"
        ],
        "Course materials - Notebooks & Data": [
          "Accessing and Utilizing Course Working Files"
        ],
        "Introduction to Reinforcement Learning": [
          "Exploring RLHF's Role in Advancing ChatGPT",
          "Fundamental Principles of Reinforcement Learning",
          "Mastering OpenAI Software for Reinforcement Learning",
          "A Comprehensive Introduction to Reinforcement Learning",
          "Advanced Concepts and Strategies in Reinforcement Learning",
          "Understanding OpenAI Frameworks for Developers",
          "Practical Exploration of OpenAI Environments",
          "Theoretical and Applied Aspects of Markov Decision Processes",
          "Solving Complex Challenges in Reinforcement Learning"
        ],
        "Solving Reinforcement Learning Problems with OpenAI/ChatGPT": [
          "Strategies for Addressing Reinforcement Learning Problems with OpenAI/GPT - 1",
          "Strategies for Addressing Reinforcement Learning Problems with OpenAI/GPT - 2",
          "Strategies for Addressing Reinforcement Learning Problems with OpenAI/GPT - 3",
          "Strategies for Addressing Reinforcement Learning Problems with OpenAI/GPT - 4"
        ],
        "Deep Reinforcement Learning Applications": [
          "An Introduction to Deep Reinforcement Learning",
          "Case Studies in Solving Deep Reinforcement Learning Problems"
        ],
        "Thank You": [
          "Thank You!"
        ]
      },
      "requirements": [
        "Basic Python programming knowledge is helpful but not required",
        "Internet Access"
      ],
      "description": "Unlock the Future with AI: Master ChatGPT, ChatGPT o4 & the LLM Revolution!\n(Freshly Updated May 2025! This course is continuously revised to keep you at the cutting edge.)\nAre you ready to command the most transformative technology of our era? Welcome to \"Exploring the Technologies Behind ChatGPT, ChatGPT o4 & LLMs\" – your definitive launchpad to mastering the groundbreaking power of Large Language Models. This isn't just another AI course; it's an immersive journey designed to catapult you from curious novice to confident expert in the electrifying world of Natural Language Processing (NLP). Whether you're taking your first steps into AI or seeking to sharpen your advanced skills, prepare to be transformed.\nWhy Is This Your Unmissable Opportunity?\nIn today's hyper-digital landscape, understanding LLMs isn't just an advantage—it's a necessity. These revolutionary technologies are the engines driving innovation across every conceivable industry. They're reshaping how we interact, automating complex tasks, creating compelling content, and unlocking efficiencies previously unimaginable.\nThis course is meticulously crafted for:\nAspiring Developers: Build next-gen AI applications.\nData Scientists: Supercharge your analytical capabilities.\nResearchers: Push the boundaries of NLP.\nAI Enthusiasts: Deepen your passion with practical skills.\nBusiness Professionals: Leverage AI to drive strategic growth.\nWe provide the critical tools, profound insights, and hands-on experience you need to not just understand, but harness these powerful technologies. Join the vanguard of the AI revolution and become an architect of the future!\nWhat Awaits You Inside? Prepare to Achieve Mastery:\nDecode LLM Foundations: Go beyond buzzwords. Truly understand the core principles of ChatGPT, the unique aspects of ChatGPT o4, BERT, and T5. Unravel the genius of transformer architectures and witness how they're rewriting the rules of NLP.\nMaster the Transformer Engine: Dive deep into the mechanics that power modern AI. Grasp attention mechanisms, demystify tokenization, understand embeddings, and see firsthand how transformers achieve state-of-the-art results in text generation, comprehension, and analysis.\nUnlock Advanced AI Wizardry: Elevate your skills from theory to application. Gain hands-on expertise in fine-tuning models like ChatGPT o4 with your custom datasets, learn the art of prompt engineering for precision outputs, and build sophisticated NLP applications from scratch.\nBuild a Portfolio That Speaks Volumes: Transition seamlessly from learning to doing. Engage in practical, real-world projects using PyTorch and interactive Jupyter notebooks. Develop impressive applications like intelligent sentiment analyzers, dynamic chatbots, insightful summarization tools, and automated content creation platforms.\nConquer Specialized Frontiers: Position yourself at the forefront of AI innovation. Explore advanced topics like transfer learning, gain unparalleled insights into the inner workings of BERT, and venture into the exciting realm of Vision Transformers (ViT) for multimodal AI solutions.\nDeploy Like a Pro: Learn the industry secrets to taking your AI innovations from concept to reality. Master strategies for deploying, optimizing, and scaling robust, enterprise-grade AI applications, ensuring peak performance and reliability.\nIgnite Your Career & Become an AI Powerhouse:\nThis course is your express lane to becoming an indispensable AI professional. You'll gain:\nIn-Demand, Future-Proof Skills: Acquire the expertise that top employers are desperately seeking.\nInterview Dominance: Confidently tackle technical interviews, armed with deep knowledge and the ability to articulate complex concepts with clarity.\nPortfolio-Ready Projects: Showcase your capabilities with tangible, impactful projects that make you stand out.\nElevated Professional Stature: This isn't just education; it's a career accelerator, opening doors to lucrative and exciting opportunities in AI and NLP.\nCourse Highlights – Your Path to Excellence:\nLearn from the Best: Be guided by seasoned AI professionals who bring years of real-world industry experience and deep academic understanding directly to you.\nInteractive & Engaging: Immerse yourself in visually rich content, dynamic explanations, and practical coding challenges with PyTorch, designed for deep learning and lasting proficiency.\nHands-On, Impactful Projects: Don't just learn – create! Build a portfolio of projects that demonstrate your skills and impress potential employers.\nAlways Cutting-Edge: Stay ahead of the curve with content that is constantly updated to reflect the latest breakthroughs, methodologies, and trends in the fast-evolving AI landscape.\nThriving Community: Connect with a vibrant network of fellow learners, instructors, and AI experts. Collaborate, share insights, and build relationships that last a lifetime.\nThe Future is Now. Don't Just Witness the AI Revolution – Lead It!\nThis is your moment to seize an unparalleled opportunity. Transform your skillset, skyrocket your career trajectory, and unlock the immense potential of Large Language Models.\nEnroll NOW in \"Exploring the Technologies Behind ChatGPT, ChatGPT o4 & LLMs\" and start shaping the future of intelligence, today!",
      "target_audience": [
        "Anyone interested in ChatGPT",
        "Anyone interested in Reinforcement Learning from Human Feedback (RLHF)",
        "Aspiring data scientists, machine learning engineers, and NLP practitioners",
        "Anyone who want to gain a solid understanding of transformer models and their applications in modern natural language processing tasks.",
        "Software developers seeking to enhance their productivity and leverage the power of NLP models for innovative applications in their work."
      ]
    },
    {
      "title": "Mathematical Foundation For Machine Learning and AI",
      "url": "https://www.udemy.com/course/mathematical-foundation-for-machine-learning-and-ai/",
      "bio": "Learn the core mathematical concepts for machine learning and learn to implement them in R and python",
      "objectives": [
        "Refresh the mathematical concepts for AI and Machine Learning",
        "Learn to implement algorithms in python",
        "Understand the how the concepts extend for real world ML problems"
      ],
      "course_content": {},
      "requirements": [
        "Basic knolwedge of python is assumed as concepts are coded in python and R"
      ],
      "description": "Artificial Intelligence has gained importance in the last decade with a lot depending on the development and integration of AI in our daily lives. The progress that AI has already made is astounding with the self-driving cars, medical diagnosis and even betting humans at strategy games like Go and Chess.\nThe future for AI is extremely promising and it isn’t far from when we have our own robotic companions. This has pushed a lot of developers to start writing codes and start developing for AI and ML programs. However, learning to write algorithms for AI and ML isn’t easy and requires extensive programming and mathematical knowledge.\nMathematics plays an important role as it builds the foundation for programming for these two streams. And in this course, we’ve covered exactly that. We designed a complete course to help you master the mathematical foundation required for writing programs and algorithms for AI and ML.\nThe course has been designed in collaboration with industry experts to help you breakdown the difficult mathematical concepts known to man into easier to understand concepts. The course covers three main mathematical theories: Linear Algebra, Multivariate Calculus and Probability Theory.\nLinear Algebra – Linear algebra notation is used in Machine Learning to describe the parameters and structure of different machine learning algorithms. This makes linear algebra a necessity to understand how neural networks are put together and how they are operating.\nIt covers topics such as:\nScalars, Vectors, Matrices, Tensors\nMatrix Norms\nSpecial Matrices and Vectors\nEigenvalues and Eigenvectors\nMultivariate Calculus – This is used to supplement the learning part of machine learning. It is what is used to learn from examples, update the parameters of different models and improve the performance.\nIt covers topics such as:\nDerivatives\nIntegrals\nGradients\nDifferential Operators\nConvex Optimization\nProbability Theory – The theories are used to make assumptions about the underlying data when we are designing these deep learning or AI algorithms. It is important for us to understand the key probability distributions, and we will cover it in depth in this course.\nIt covers topics such as:\nElements of Probability\nRandom Variables\nDistributions\nVariance and Expectation\nSpecial Random Variables\nThe course also includes projects and quizzes after each section to help solidify your knowledge of the topic as well as learn exactly how to use the concepts in real life.\nAt the end of this course, you will not have not only the knowledge to build your own algorithms, but also the confidence to actually start putting your algorithms to use in your next projects.\nEnroll now and become the next AI master with this fundamentals course!",
      "target_audience": [
        "Any one who wants to refresh or learn the mathematical tools required for AI and machine learning will find this course very useful"
      ]
    },
    {
      "title": "Projects in Machine Learning : Beginner To Professional",
      "url": "https://www.udemy.com/course/machine-learning-for-absolute-beginners/",
      "bio": "A complete guide to master machine learning concepts and create real world ML solutions",
      "objectives": [
        "Learn core concepts of Machine Learning",
        "Learn about differnt types of machine learning algorithms",
        "Build real world projects using Supervised and Unsupervised learning algorithms",
        "Learn to implement neural networks"
      ],
      "course_content": {
        "An Introduction to Machine Learning": [
          "Introduction",
          "What is Machine Learning",
          "Types and Applications of ML",
          "AI vs ML",
          "Essential Math for ML and AI",
          "Quiz- Questions- Section1",
          "Quiz- Answers - Section 1"
        ],
        "Supervised Learning - part 1": [
          "Introduction to Supervised Learning",
          "Linear Methods for Classification",
          "Linear Methods for Regression",
          "Support Vector Machines",
          "Basis Expansions",
          "Model Selection Procedures",
          "Bonus! Supervised Learning Project in Python Part 1",
          "Bonus! Supervised Learning Project in Python Part 2",
          "Quiz- Questions- Section 2",
          "Quiz- Answers - Section 2"
        ],
        "Unsupervised Learning": [
          "Introduction to Unsupervised Learning",
          "Association Rules",
          "Cluster Analysis",
          "Reinforcement Learning",
          "Bonus! KMeans Clustering Project",
          "Quiz- Questions- Section 3",
          "Quiz- Answers - Section 3"
        ],
        "Neural Networks": [
          "Introduction to Neural Networks",
          "The Perceptron",
          "The Backpropagation Algorithm",
          "Training Procedures",
          "Convolutional Neural Networks"
        ],
        "Real World Machine Learning": [
          "Introduction to Real World ML",
          "Choosing an Algorithm",
          "Design and Analysis of ML Experiments",
          "Common Software for ML",
          "Quiz- Questions- Section 5",
          "Quiz- Answers - Section 5"
        ],
        "Warmup Project": [
          "Setting up OpenAI Gym",
          "Building and Training the Network Part 1",
          "Building and Training the Network Part 2"
        ],
        "Project 1Board Game Review Prediction": [
          "Intro",
          "Board Game Review Prediction - Building the Dataset Part 1",
          "Board Game Review Prediction - Building the Dataset Part 2",
          "Board Game Review Prediction - Training the Models"
        ],
        "Project 2 Credit Card Fraud Detection": [
          "Intro",
          "Credit Card Fraud Detection - The Dataset",
          "Credit Card Fraud Detection - The Algorithms"
        ],
        "Project 3 Intro to Natural Language Processing": [
          "Intro",
          "Tokenizing, Stop Words, and Stemming",
          "Tagging, Chunking, and Named Entity Recognition",
          "Text Classification"
        ],
        "Project 4 Object Recognition": [
          "Intro",
          "Loading and Preprocessing the CIFAR10 Dataset",
          "Building and Deploying the All-CNN Network Part 1",
          "Building and Deploying the All-CNN Network Part 2"
        ]
      },
      "requirements": [
        "Basic knolwedge of Python is required to compile and run the examples",
        "Basic knolwedge of mathematics is assumed"
      ],
      "description": "Update: This course has been updated to include 8 projects that will give you a real-world experience with different concepts of Machine Learning. Keep an eye out for more projects that will be added to this course in the future!\nIf you’ve ever wanted Jetsons to be real, well we aren’t that far off from a future like that. If you’ve ever chatted with automated robots, then you’ve definitely interacted with machine learning. From self-driving cars to AI bots, machine learning is slowly spreading it’s reach and making our devices smarter.\nArtificial intelligence is the future of computers, where your devices will be able to decide what is right for you. Machine learning is the core for having a futuristic reality where robot maids and robodogs exist. Machine learning includes the algorithms that allow the computers to think and respond, as well as manipulate the data depending on the scenario that’s placed before them.\nSo, if you’ve ever wanted to play a role in the future of technology development, then here’s your chance to get started with Machine Learning. Because machine learning is complex and tough, we’ve designed a course to help break it down into more simple concepts that are easier to understand.\nThis course covers the basic concepts of machine learning that are crucial to get started on the journey of becoming a developer for machine learning. This course covers all the different algorithms that are required to simulate the right environment for your computer.\nThe course will start at the very beginning and delve right into machine learning, before breaking down the most important concepts principles. However, the course does require you to have a mathematical background as machine learning relies heavily on mathematical concepts. It also requires you to have some experience with Python principles which will be required when we put the algorithms to test in actual real-world Python projects.\nThe course covers a number of different machine learning algorithms such as supervised learning, unsupervised learning, reinforced learning and even neural networks. From there you will learn how to incorporate these algorithms into actual projects so you can see how they work in action! But, that’s not all. In addition to quizzes that you’ll find at the end of each section, the course also includes a 6 brand new projects that can help you experience the power of Machine Learning using real-world examples!\n9 Projects That Are Included in This Course:\nProject 1 -Board Game Review Prediction – In this project, you’ll see how to perform a linear regression analysis by predicting the average reviews on a board game in this project.\nProject 2 – Credit Card Fraud Detection – In this project, you’ll learn to focus on anomaly detection by using probability densities to detect credit card fraud.\nProject 3 – Getting Started with Natural Language Processing In Python – This project will focus on Natural Language Processing (NLP) methodology, such as tokenizing words and sentences, part of speech identification and tagging, and phrase chunking.\nProject 4– Obtaining Near State-of-the-Art Performance on Object Recognition Tasks Using Deep Learning – In this project, will use the CIFAR-10 object recognition dataset as a benchmark to implement a recently published deep neural network.\nProject 5 – Image Super Resolution with the SRCNN – Learn how to implement and use a Tensorflow version of the Super Resolution Convolutional Neural Network (SRCNN) for improving image quality.\nProject 6 – Natural Language Processing: Text Classification – In this project, you’ll learn an advanced approach to Natural Language\nProcessing by solving a text classification task using multiple classification algorithms.\nProject 7 – K-Means Clustering For Image Analysis – In this project, you’ll learn how to use K-Means clustering in an unsupervised\nlearning method to analyze and classify 28 x 28 pixel images from the MNIST dataset.\nProject 8 – Data Compression & Visualization Using Principle Component Analysis – This project will show you how to compress\nour Iris dataset into a 2D feature set and how to visualize it through a normal x-y plot using k-means clustering.\nAll of this and so much more is included in this course. So, what are you waiting for?\nGet started in machine learning with this epic course that makes machine learning simpler and easy to understand! Enroll now to step into the future of programming.",
      "target_audience": [
        "Students who will like to understand and use Machine learning in real world projects will find this course very useful"
      ]
    },
    {
      "title": "Python A-Z™: Python For Data Science With Real Exercises!",
      "url": "https://www.udemy.com/course/python-coding/",
      "bio": "Programming In Python For Data Analytics And Data Science. Learn Statistical Analysis, Data Mining And Visualization",
      "objectives": [
        "Learn to program in Python at a good level",
        "Learn how to code in Jupiter Notebooks",
        "Learn the core principles of programming",
        "Learn how to create variables",
        "Learn about integer, float, logical, string and other types in Python",
        "Learn how to create a while() loop and a for() loop in Python",
        "Learn how to install packages in Python",
        "Understand the Law of Large Numbers"
      ],
      "course_content": {
        "Welcome To The Course": [
          "Installing Python (Windows & MAC)",
          "Get the Datasets here",
          "Prizes $$ for Learning"
        ],
        "Core Programming Principles": [
          "Types of variables",
          "Using Variables",
          "Boolean Variables and Operators",
          "The \"While\" Loop",
          "The \"For\" Loop",
          "The \"If\" statement",
          "Code indentation in Python",
          "Section recap",
          "HOMEWORK: Law of Large Numbers",
          "Core Programming Principles"
        ],
        "Fundamentals Of Python": [
          "What is a List?",
          "Let's create some lists",
          "Using the [] brackets",
          "Slicing",
          "Tuples in Python",
          "Functions in Python",
          "Packages in Python",
          "Numpy and Arrays in Python",
          "Slicing Arrays",
          "Section Recap",
          "HOMEWORK: Financial Statement Analysis",
          "Fundamentals of Python"
        ],
        "Matrices": [
          "Project Brief: Basketball Trends",
          "Matrices",
          "Building Your First Matrix",
          "Matrix Operations",
          "Your first visualization",
          "Dictionaries in Python",
          "Expanded Visualization",
          "Creating Your First Function",
          "Advanced Function Design",
          "Basketball Insights",
          "Section Recap",
          "HOMEWORK: Basketball free throws",
          "Matrices"
        ],
        "Data Frames": [
          "Importing data into Python",
          "Exploring your dataset",
          "Renaming Columns of a Dataframe",
          "Subsetting dataframes in Pandas",
          "Basic operations with a Data Frame",
          "Filtering a Data Frame",
          "Using .at() and .iat() (advanced tutorial)",
          "Introduction to Seaborn",
          "Visualizing With Seaborn: Part 1",
          "Keyword Arguments in Python (advanced tutorial)",
          "Section Recap",
          "HOMEWORK: World Trends",
          "Data Frames"
        ],
        "Advanced Visualization": [
          "What is a Category data type?",
          "Working with JointPlots",
          "Histograms",
          "Stacked histograms in Python",
          "Creating a KDE Plot",
          "Working with Subplots()",
          "Violinplots vs Boxplots",
          "Creating a Facet Grid",
          "Coordinates and Diagonals",
          "EXTRA: Building Dashboards in Python",
          "EXTRA: Styling Tips",
          "EXTRA: Finishing Touches",
          "Section Recap",
          "HOMEWORK: Movie Domestic % Gross",
          "Advanced Visualization"
        ],
        "Homework Solutions": [
          "Homework Solution Section 2: Law Of Large Numbers",
          "Homework Solution Section 3: Financial Statement Analysis (Part 1)",
          "Homework Solution Section 3: Financial Statement Analysis (Part 2)",
          "Homework Solution Section 4: Basketball Free Throws",
          "Homework Solution Section 5: World Trends (Part 1)",
          "Homework Solution Section 5: World Trends (Part 2)",
          "Homework Solution Section 6: Movie Domestic % Gross (Part 1)",
          "Homework Solution Section 6: Movie Domestic % Gross (Part 2)",
          "THANK YOU Video"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Find your Career Path!",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "No prior knowledge or experience needed. Only a passion to be successful!"
      ],
      "description": "Learn Python Programming by doing!\nThere are lots of Python courses and lectures out there. However, Python has a very steep learning curve and students often get overwhelmed. This course is different!\nThis course is truly step-by-step. In every new tutorial we build on what had already learned and move one extra step forward.\nAfter every video you learn a new valuable concept that you can apply right away. And the best part is that you learn through live examples.\nThis training is packed with real-life analytical challenges which you will learn to solve. Some of these we will solve together, some you will have as homework exercises.\nIn summary, this course has been designed for all skill levels and even if you have no programming or statistical background you will be successful in this course!\nI can't wait to see you in class,\nWhat you will learn:\nLearn the core principles of programming\nLearn how to create variables\nHow to visualize data in Seaborn\nHow to create histograms, KDE plots, violin plots and style your charts to perfection\nLearn about integer, float, logical, string and other types in Python\nLearn how to create a while() loop and a for() loop in Python\nAnd much more....\nSincerely,\nKirill Eremenko",
      "target_audience": [
        "This course if for you if you want to learn how to program in Python",
        "This course is for you if you are tired of Python courses that are too complicated",
        "This course is for you if you want to learn Python by doing",
        "This course is for you if you like exciting challenges",
        "You WILL have homework in this course so you have to be prepared to work on it"
      ]
    },
    {
      "title": "Train YOLO for Object Detection with Custom Data",
      "url": "https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/",
      "bio": "Build your own detector by labelling, training and testing on image, video and in real time with camera: YOLO v3 and v4",
      "objectives": [
        "Apply already trained YOLO v3-v4 for Object Detection on image, video and in real time with camera",
        "Label own dataset and structure files in YOLO format",
        "Train YOLO v3-v4 detector in Darknet framework",
        "Assemble custom dataset in YOLO format",
        "Convert existing dataset of Traffic Signs in YOLO format",
        "Build individual PyQt graphical user interface for Object Detection based on YOLO v3-v4 algorithm"
      ],
      "course_content": {},
      "requirements": [
        "Basic knowledge of Object Detection algorithms",
        "Basics on how YOLO works",
        "Intermediate knowledge of Python v3",
        "Basic knowledge of OpenCV",
        "Basics on how to work with Anaconda Environments",
        "Basics on how to work with PyCharm IDE or any other Python IDE",
        "Basics on how to work with Terminal Window or Anaconda Prompt",
        "To have Linux Ubuntu installed is optional, but recommended"
      ],
      "description": "In this hands-on course, you'll train your own Object Detector using YOLO v3-v4 algorithms.\nAs for beginning, you’ll implement already trained YOLO v3-v4 on COCO dataset. You’ll detect objects on image, video and in real time by OpenCV deep learning library. The code templates you can integrate later in your own future projects and use them for your own trained YOLO detectors.\nAfter that, you’ll label individual dataset as well as create custom one by extracting needed images from huge existing dataset.\nNext, you’ll convert Traffic Signs dataset into YOLO format. Code templates for converting you can modify and apply for other datasets in your future work.\nWhen datasets are ready, you’ll train and test YOLO v3-v4 detectors in Darknet framework.\nAs for Bonus part, you’ll build graphical user interface for Object Detection by YOLO and by the help of PyQt. This project you can represent as your results to your supervisor or to make a presentation in front of classmates or even mention it in your resume.\nContent Organization. Each Section of the course contains:\nVideo Lectures\nCoding Activities\nCode Templates\nQuizzes\nDownloadable Instructions\nDiscussion Opportunities\nVideo Lectures of the course have SMART objectives:\nS - specific (the lecture has specific objectives)\nM - measurable (results are reasonable and can be quantified)\nA - attainable (the lecture has clear steps to achieve the objectives)\nR - result-oriented (results can be obtained by the end of the lecture)\nT - time-oriented (results can be obtained within the visible time frame)",
      "target_audience": [
        "Students who study Computer Vision and want to know how to use YOLO for Object Detection",
        "Students who know basics of Object Detection but want to know how to Train YOLO with New Data",
        "Students who study YOLO and want to Label Own Data in YOLO format",
        "Students who use already existing datasets for Object Detection but want to Convert them in YOLO format",
        "Young Researchers who study different Object Detection Algorithms and want to Train YOLO with Custom Data and Compare results with different approaches"
      ]
    },
    {
      "title": "Machine Learning and Data Science Real Life Projects[2025]",
      "url": "https://www.udemy.com/course/real-life-machine-learning-and-data-science-projects/",
      "bio": "Master Real-World Machine Learning and Data Science Projects: Step-by-Step Coding for Career Success",
      "objectives": [
        "Learn to Upload Dataset in the Google Colab and find the path of the dataset",
        "Learn to read the dataset using Pandas",
        "Learn to use Google Colab for Machine Learning and Data Science Projects",
        "Learn to Handling Missing Values in the Real World Dataset for Categorical and Numerical Features",
        "Learn to perform Label Encoding",
        "Learn to Perform Splitting the Dataset in Training and Testing",
        "Learn to Model Formation using KNN, Logistic Regression, SVM, XGBoost Regressor",
        "Learn about Data Visualization using Seaborn and Matplotlib",
        "Learn to Build your own Predictive System",
        "Learn about Developing an intuition to solve Machine Learning and Data Science problem in Real World",
        "Learn to do Model Evalution using R Squared and Accuracy Score"
      ],
      "course_content": {},
      "requirements": [
        "Basic of Python Programming",
        "Internet Connection",
        "Distraction Free Environment for Attention",
        "A willingness to learn and apply the concept"
      ],
      "description": "Real-Life Machine Learning and Data Science Projects [2025]: Unleash the Future of Data Mastery!\n\n\nAre you ready to embark on an extraordinary voyage into the realm of Real Life Machine Learning and Data Science Projects? Brace yourself for an electrifying experience that will elevate your skills, boost your career prospects, and open the doors to limitless possibilities!\n\n\nWhat Awaits You in this Cutting-Edge Course:\n1. Data Empowerment: Navigate the vast data landscape with finesse as you learn to upload datasets in Google Colab and unleash the true potential of your data.\n\n\n2. The Data Sorcerer: Unlock the secrets of data manipulation using the powerful Pandas library, transforming raw data into actionable insights.\n\n\n3. The Data Alchemist: Harness the true power of Google Colab as you embark on thrilling Machine Learning and Data Science Projects that will leave you spellbound.\n\n\n4. Mastering Real-Life Data Challenges: Fearlessly conquer missing values in real-world datasets, both categorical and numerical, becoming a data superhero.\n\n\n5. The Code Whisperer: Unravel the language of data with Label Encoding, empowering you to speak the language of machines fluently.\n6. Data Splitting Zen: Achieve data harmony through expertly splitting datasets into Training and Testing sets, laying the foundation for brilliant model creation.\n\n\n7. The Model Architect: Build robust models using KNN, Logistic Regression, SVM, and XGBoost Regressor, transforming data into valuable predictions.\n\n\n8. The Art of Data Storytelling: Immerse yourself in the mesmerizing world of Data Visualization using Seaborn and Matplotlib, weaving captivating tales from numbers.\n\n\n9. Your Predictive Masterpiece: Create your very own Predictive System, making your mark in the world of Data Science and Machine Learning.\n\n\n10. Unleash Your Inner Data Scientist: Develop a powerful intuition to tackle real-world Data Science and Machine Learning challenges with ease.\n\n\n11.  Excellence in Model Evaluation: Master the art of Model Evaluation using R Squared and Accuracy Score, ensuring your models are nothing short of exceptional.\n\n\nWho Will Conquer This Odyssey of Knowledge?\n- Ambitious beginners and Python Developers eager to unlock their true potential in Data Science and Machine Learning problem-solving.\n- Seasoned ML Experts, Data Scientists, and Data Analysts craving a rapid and turbocharged brush-up of their skills.\n- Visionaries and knowledge seekers yearning for a hands-on, project-based approach to Machine Learning and Data Science.\n- Anyone with a positive mindset, an insatiable thirst for knowledge, and the fiery ambition to seize unparalleled opportunities!\n\n\nJoin us on this extraordinary journey and witness the transformation that will skyrocket your career! Enrol now and experience the magic of Real Life Machine Learning and Data Science Projects [2023]!!!\n\n\nYour future awaits! Let's embark on this thrilling adventure together! Happy Learning!!!",
      "target_audience": [
        "Ambitious beginners and Python Developers eager to unlock their true potential in Data Science and Machine Learning problem-solving.",
        "Seasoned ML Experts, Data Scientists, and Data Analysts craving a rapid and turbocharged brush-up of their skills.",
        "Visionaries and knowledge seekers yearning for a hands-on, project-based approach to Machine Learning and Data Science.",
        "Anyone with a positive mindset, an insatiable thirst for knowledge, and the fiery ambition to seize unparalleled opportunities!"
      ]
    },
    {
      "title": "The Supervised Machine Learning Bootcamp",
      "url": "https://www.udemy.com/course/the-supervised-machine-learning-course/",
      "bio": "Data Science, Python, sk learn, Decision Trees, Random Forests, KNNs, Ridge Lasso Regression, SVMs",
      "objectives": [
        "Regression and Classification Algorithms",
        "Using sk-learn and Python to implement supervised machine learning techniques",
        "K-nearest neighbors for both classification and regression",
        "Naïve Bayes",
        "Ridge and Lasso Regression",
        "Decision Trees",
        "Random Forests",
        "Support Vector Machines",
        "Practical case studies for training, testing and evaluating and improving model performance",
        "Cross-validation for parameter optimization",
        "Learn to use metrics such as Precision, Recall, F1-score, as well as a confusion matrix to evaluate true model performance",
        "You will dive into the theoretical foundation behind each algorithm with the aid of intuitive explanation of formulas and mathematical notions"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Setting up the Environment": [
          "Installing Anaconda",
          "Jupyter Dashboard - Part 1",
          "Jupyter Dashboard - Part 2",
          "Installing the relevant packages"
        ],
        "Naïve Bayes": [
          "Motivation",
          "Bayes' Thought Experiment",
          "Bayes' Thought Experiment",
          "Bayes' Thought Experiment: Assignment",
          "Bayes' Theorem",
          "Bayes' Theorem",
          "The Ham-or-Spam Example",
          "The Ham-or-Spam Example",
          "The Ham-or-Spam Example: Assignment",
          "The YouTube Dataset: Creating the Data Frame",
          "CountVectorizer",
          "The YouTube Dataset: Preprocessing",
          "The YouTube Dataset: Preprocessing: Assignment",
          "The YouTube Dataset: Classification",
          "The YouTube Dataset: Classification: Assignment",
          "The YouTube Dataset: Confusion Matrix",
          "The YouTube Dataset: Accuracy, Precision, Recall, and the F1 score",
          "The YouTube Dataset: Changing the Priors",
          "Naïve Bayes: Assignment"
        ],
        "K-Nearest Neighbors": [
          "Motivation",
          "Motivation",
          "Math Prerequisites: Distance Metrics",
          "Math Prerequisites: Distance Metrics",
          "Random Dataset: Generating the Dataset",
          "Random Dataset: Visualizing the Dataset",
          "Random Dataset: Classification",
          "Random Dataset: How to Break a Tie",
          "Random Dataset: Decision Regions",
          "Random Dataset: Choosing the Best K-value",
          "Random Dataset: Grid Search",
          "Random Dataset: Model Performance",
          "KNeighbors Classifier: Assignment",
          "Theory with a Practical Example",
          "KNN vs Linear Regression: A Linear Problem",
          "KNN vs Linear Regression: A Non-linear Problem",
          "KNeighbors Regressor: Assignment",
          "Pros and Cons"
        ],
        "Decision Trees and Random Forests": [
          "What is a Tree in Computer Science?",
          "The Concept of Decision Trees",
          "Decision Trees in Machine Learning",
          "Decision Trees: Pros and Cons",
          "Practical Example: The Iris Dataset",
          "Practical Example: Creating a Decision Tree",
          "Practical Example: Plotting the Tree",
          "Decision Tree Metrics Intuition: Gini Inpurity",
          "Decision Tree Metrics: Information Gain",
          "Tree Pruning: Dealing with Overfitting",
          "Random Forest as Ensemble Learning",
          "Bootstrapping",
          "From Bootstrapping to Random Forests",
          "Random Forest in Code - Glass Dataset",
          "Census Data and Income - Preprocessing",
          "Training the Decision Tree",
          "Training the Random Forest"
        ],
        "Support Vector Machines": [
          "Introduction to Support Vector Machines",
          "Intro to SVMs",
          "Linearly separable classes - hard margin problem",
          "Non-linearly separable classes - soft margin problem",
          "Soft margin problem",
          "Kernels - Intuition",
          "Kernels",
          "Intro to the practical case",
          "Preprocessing the data",
          "Splitting the data into train and test and rescaling",
          "Implementing a linear SVM",
          "Implementing a linear SVM",
          "Analyzing the results– Confusion Matrix, Precision, and Recall",
          "Cross-validation",
          "Choosing the kernels and C values for cross-validation",
          "Hyperparameter tuning using GridSearchCV",
          "Support Vector Machines - Assignment"
        ],
        "Ridge and Lasso Regression": [
          "Regression Analysis Overview",
          "Overfitting and Multicollinearity",
          "Overfitting and Multicollinearity",
          "Introduction to Regularization",
          "Regularization",
          "Ridge Regression Basics",
          "Ridge Regression Mechanics",
          "Ridge Regression Mechanics",
          "Regularization in More Complicated Scenarios",
          "Lasso Regression Basics",
          "Lasso Regression Basics",
          "Lasso Regression vs Ridge Regression",
          "The Hitters Dataset: Preprocessing and Preparation",
          "Exploratory Data Analysis",
          "Performing Linear Regression",
          "Cross-validation for Choosing a Tuning Parameter",
          "Cross-validation for Choosing a Tuning Parameter",
          "Performing Ridge Regression with Cross-validation",
          "Performing Lasso Regression with Cross-validation",
          "Comparing the Results",
          "Replacing the Missing Values in the DataFrame"
        ]
      },
      "requirements": [
        "The course is open to everyone who wants to learn data science.",
        "You’ll need to install Anaconda and Jupyter Notebook. We will show you how to do that step by step."
      ],
      "description": "Do you want to master supervised machine learning and land a job as a machine learning engineer or data scientist?\nThis Supervised Machine Learning course is designed to equip you with the essential tools to tackle real-world challenges. You'll dive into powerful algorithms like Naïve Bayes, KNNs, Support Vector Machines, Decision Trees, Random Forests, and Ridge and Lasso Regression—skills every top-tier data professional needs.\nBy the end of this course, you'll not only understand the theory behind these six algorithms, but also gain hands-on experience through practical case studies using Python’s sci-kit learn library. Whether you're looking to break into the industry or level up your expertise, this course gives you the knowledge and confidence to stand out in the field.\nFirst, we cover naïve Bayes – a powerful technique based on Bayesian statistics. Its strong point is that it’s great at performing tasks in real-time. Some of the most common use cases are filtering spam e-mails, flagging inappropriate comments on social media, or performing sentiment analysis. In the course, we have a practical example of how exactly that works, so stay tuned!\nNext up is K-nearest-neighbors – one of the most widely used machine learning algorithms. Why is that? Because of its simplicity when using distance-based metrics to make accurate predictions.\nWe’ll follow up with decision tree algorithms, which will serve as the basis for our next topic – namely random forests. They are powerful ensemble learners, capable of harnessing the power of multiple decision trees to make accurate predictions.\nAfter that, we’ll meet Support Vector Machines – classification and regression models, capable of utilizing different kernels to solve a wide variety of problems. In the practical part of this section, we’ll build a model for classifying mushrooms as either poisonous or edible. Exciting!\nFinally, you’ll learn about Ridge and Lasso Regression – they are regularization algorithms that improve the linear regression mechanism by limiting the power of individual features and preventing overfitting. We’ll go over the differences and similarities, as well as the pros and cons of both regression techniques.\nEach section of this course is organized in a uniform way for an optimal learning experience:\n- We start with the fundamental theory for each algorithm. To enhance your understanding of the topic, we’ll walk you through a theoretical case, as well as introduce mathematical formulas behind the algorithm.\n- Then, we move on to building a model in order to solve a practical problem with it. This is done using Python’s famous sklearn library.\n- We analyze the performance of our models with the aid of metrics such as accuracy, precision, recall, and the F1 score.\n- We also study various techniques such as grid search and cross-validation to improve the model’s performance.\nTo top it all off, we have a range of complementary exercises and quizzes, so that you can enhance your skill set. Not only that, but we also offer comprehensive course materials to guide you through the course, which you can consult at any time.\nThe lessons have been created in 365’s unique teaching style many of you are familiar with. We aim to deliver complex topics in an easy-to-understand way, focusing on practical application and visual learning.\nWith the power of animations, quiz questions, exercises, and well-crafted course notes, the Supervised Machine Learning course will fulfill all your learning needs.\nIf you want to take your data science skills to the next level and add in-demand tools to your resume, this course is the perfect choice for you.\nClick ‘Buy this course’ to continue your data science journey today!",
      "target_audience": [
        "Aspiring data scientists and machine learning engineers",
        "Data Scientists and Data Analysts looking to up their skillset",
        "Anyone who wants to gain an understanding of the machine learning field and its vast opportunities"
      ]
    },
    {
      "title": "A/B Testing in Python",
      "url": "https://www.udemy.com/course/ab-testing-in-python/",
      "bio": "Learn How To Define, Start, And Analyze The Results Of An A/B Test. Improve Business Performance Through A/B Testing",
      "objectives": [
        "How to use A/B tests to improve business performance",
        "Define A/B tests",
        "Start A/B tests",
        "Analyze the results of A/B tests",
        "Measure the success of A/B tests",
        "How to define a hypothesis",
        "Design tracking for the metrics",
        "How to prepare for a data science interview (when you get asked about A/B tests)",
        "How to design A/B tests for digital products",
        "Advanced considerations when you run multiple A/B tests at the same time"
      ],
      "course_content": {
        "Introduction: Welcome to the course on A/B testing": [
          "Welcome to the course!",
          "What does the course cover and what is A/B testing",
          "What are the key characteristics of an A/B test?",
          "How A/B tests are created? Who takes part?"
        ],
        "Defining KPIs and metrics. Practical example: Kittengram": [
          "How to measure success",
          "Calculation of metrics for Kittengram - Practical example"
        ],
        "Setting up and executing A/B tests in practice": [
          "Data instrumentation and tracking",
          "Calculating metrics",
          "Designing the experiment",
          "Set up the A/B test",
          "Statistical significance",
          "Calculate the sample size of an A/B test",
          "Example of significance power calculator",
          "A/B test - start & analysis",
          "Presenting results",
          "A/B test analysis process",
          "Compare the activity between the groups"
        ],
        "Advanced considerations": [
          "Advanced A/B testing considerations",
          "Important ethical considerations: Privacy"
        ],
        "Advanced questions for interview preparation": [
          "Introduction",
          "Question 1",
          "Question 2",
          "Question 3",
          "Question 4",
          "Question 5",
          "How to prepare for the interview"
        ],
        "Conclusion": [
          "Final recommendations"
        ]
      },
      "requirements": [
        "No experience in A/B testing is required",
        "Knowledge of basic statistics",
        "You do not need advanced statistical knowledge",
        "Programming abilities are not required (but for some examples we will use Python)"
      ],
      "description": "A/B testing is a tool that helps companies make reliable decisions based on data.\nThis is one of the fundamental skills you need to land a job as a data scientist or data analyst.\nDo you want to become a data scientist or a data analyst?\nIf you do, this is the perfect course for you!\nYour instructor Anastasia is a senior data scientist working at a Stockholm-based music streaming startup. She has earned two Master's degrees in Business Intelligence and Computer Science, and grown from a recent graduate to a Senior role in just 3 years. Anastasia has performed a significant number of A/B tests for large tech companies with hundreds of millions monthly users.\nBy taking this course, you will learn how to:\n· Define an A/B test\n· Start an A/B test\n· Analyse the results of an A/B test on your own\nAlong your learning journey Anastasia will walk you through an A/B testing process for a fictional company with a digital product. This case study unfolds throughout the course and touches on everything from the very beginning of the A/B testing process to the very end including some advanced considerations. Moreover, Anastasia takes some time to share with you her advice on how to prepare for the questions on the A/B test interview for a data scientist or data analyst position.\nOne strong point of differentiation from statistical textbooks and theoretical trainings is that the A/B Testing in Python course will teach you how to design A/B tests for digital products that have millions or hundreds of millions of users. It is a rare overview of the A/B testing process from a business, technical, and data analysis perspective.\nThis is the perfect course for you if you are:\n- a data science student who wants to learn one of the fundamental skills needed on the job\n- junior data scientists with no experience with A/B testing\n- software developers and product managers who want to learn how to run A/B tests in their company to improve the product they are building\nYou will learn an invaluable skill that can transform a company’s business (and your career along the way).\nSo, what are you waiting for?\nClick the ‘Buy now’ button and let’s begin this journey today!",
      "target_audience": [
        "Junior data scientists with no experience in A/B testing",
        "Data science students with no working experience",
        "Software developers",
        "Product managers"
      ]
    },
    {
      "title": "Applied Generative AI and Natural Language Processing",
      "url": "https://www.udemy.com/course/applied-nlp-with-python/",
      "bio": "Understand Generative AI, Prompt Engineering, Huggingface-Models, LLMs, Vector Databases, RAG, OpenAI, Claude, Llama2",
      "objectives": [
        "Introduction to Natural Language Processing (NLP)",
        "model implementation based on huggingface-models",
        "working with OpenAI",
        "Vector Databases",
        "Multimodal Vector Databases",
        "Retrieval-Augmented-Generation (RAG)",
        "Real-World Applications and Case Studies",
        "implement Zero-Shot Classification, Text Classification, Text Generation",
        "fine-tune models",
        "data augmentation",
        "Prompt Engineering",
        "Zero-Shot Promping",
        "Few-Shot Prompting",
        "Chain-of-Thought (Few-Shot CoT, Zero-Shot CoT)",
        "Self-Consistency Chain-of-Thought",
        "Prompt Chaining",
        "Tree-of-Thought",
        "Self-Feedback",
        "Self-Critique",
        "Claude 3",
        "Open Source Models, e.g. LLama 2, Mistral"
      ],
      "course_content": {},
      "requirements": [
        "Python Basic knowledge",
        "Basic knowledge on How Deeplearning works"
      ],
      "description": "Join my comprehensive course on Natural Language Processing (NLP). The course is designed for both beginners and seasoned professionals. This course is your gateway to unlocking the immense potential of NLP and Generative AI in solving real-world challenges. It covers a wide range of different topics and brings you up to speed on implementing NLP solutions.\nCourse Highlights:\nNLP-Introduction\nGain a solid understanding of the fundamental principles that govern Natural Language Processing and its applications.\nBasics of NLP\nWord Embeddings\nTransformers\nApply Huggingface for Pre-Trained Networks\nLearn about Huggingface models and how to apply them to your needs\nModel Fine-Tuning\nSometimes pre-trained networks are not sufficient, so you need to fine-tune an existing model on your specific task and / or dataset. In this section you will learn how.\nVector Databases\nVector Databases make it simple to query information from texts. You will learn how they work and how to implement vector databases.\nTokenization\nImplement Vector DB with ChromaDB\nMultimodal Vector DB\nOpenAI API\nOpenAI with ChatGPT provides a very powerful tool for NLP. You will learn how to make use of it via Python and integrating it in your workflow.\nPrompt Engineering\nLearn strategies to create efficient prompts\nAdvanced Prompt Engineering\nFew-Shot Prompting\nChain-of-Thought\nSelf-Consistency Chain-of-Thought\nPrompt Chaining\nReflection\nTree-of-Thought\nSelf-Feedback\nSelf-Critique\nRetrieval-Augmented Generation\nRAG Theory\nImplement RAG\nCapstone Project \"Chatbot\"\ncreate a chatbot to \"chat\" with a PDF document\ncreate a web application for the chatbot\nOpen Source LLMs\nlearn how to use OpenSource LLMs\nMeta Llama 2\nMistral Mixtral\nData Augmentation\nTheory and Approaches of NLP Data Augmentation\nImplementation of Data Augmentation\nMiscellanious\nClaude 3\nTools and LLM-Function",
      "target_audience": [
        "Developers who want to apply NLP-models"
      ]
    },
    {
      "title": "Complete Tensorflow 2 and Keras Deep Learning Bootcamp",
      "url": "https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/",
      "bio": "Learn to use Python for Deep Learning with Google's latest Tensorflow 2 library and Keras!",
      "objectives": [
        "Learn to use TensorFlow 2.0 for Deep Learning",
        "Leverage the Keras API to quickly build models that run on Tensorflow 2",
        "Perform Image Classification with Convolutional Neural Networks",
        "Use Deep Learning for medical imaging",
        "Forecast Time Series data with Recurrent Neural Networks",
        "Use Generative Adversarial Networks (GANs) to generate images",
        "Use deep learning for style transfer",
        "Generate text with RNNs and Natural Language Processing",
        "Serve Tensorflow Models through an API",
        "Use GPUs for accelerated deep learning"
      ],
      "course_content": {
        "Course Overview, Installs, and Setup": [
          "Auto-Welcome Message",
          "Course Overview",
          "Course Setup and Installation",
          "FAQ - Frequently Asked Questions"
        ],
        "COURSE OVERVIEW CONFIRMATION": [
          "PLEASE WATCH COURSE OVERVIEW LECTURE"
        ],
        "NumPy Crash Course": [
          "Introduction to NumPy",
          "NumPy Arrays",
          "Numpy Index Selection",
          "NumPy Operations",
          "NumPy Exercises",
          "Numpy Exercises - Solutions"
        ],
        "Pandas Crash Course": [
          "Introduction to Pandas",
          "Pandas Series",
          "Pandas DataFrames - Part One",
          "Pandas DataFrames - Part Two",
          "Pandas Missing Data",
          "GroupBy Operations",
          "Pandas Operations",
          "Data Input and Output",
          "Pandas Exercises",
          "Pandas Exercises - Solutions"
        ],
        "Visualization Crash Course": [
          "Introduction to Python Visualization",
          "Matplotlib Basics",
          "Seaborn Basics",
          "Data Visualization Exercises",
          "Data Visualization Exercises - Solutions"
        ],
        "Machine Learning Concepts Overview": [
          "What is Machine Learning?",
          "Supervised Learning Overview",
          "Overfitting",
          "Evaluating Performance - Classification Error Metrics",
          "Evaluating Performance - Regression Error Metrics",
          "Unsupervised Learning"
        ],
        "Basic Artificial Neural Networks - ANNs": [
          "Introduction to ANN Section",
          "Perceptron Model",
          "Neural Networks",
          "Activation Functions",
          "Multi-Class Classification Considerations",
          "Cost Functions and Gradient Descent",
          "Backpropagation",
          "TensorFlow vs. Keras Explained",
          "Keras Syntax Basics - Part One - Preparing the Data",
          "Keras Syntax Basics - Part Two - Creating and Training the Model",
          "Keras Syntax Basics - Part Three - Model Evaluation",
          "Keras Regression Code Along - Exploratory Data Analysis",
          "Keras Regression Code Along - Exploratory Data Analysis - Continued",
          "Keras Regression Code Along - Data Preprocessing and Creating a Model",
          "Keras Regression Code Along - Model Evaluation and Predictions",
          "Keras Classification Code Along - EDA and Preprocessing",
          "Keras Classification - Dealing with Overfitting and Evaluation",
          "TensorFlow 2.0 Keras Project Options Overview",
          "TensorFlow 2.0 Keras Project Notebook Overview",
          "Keras Project Solutions - Exploratory Data Analysis",
          "Keras Project Solutions - Dealing with Missing Data",
          "Keras Project Solutions - Dealing with Missing Data - Part Two",
          "Keras Project Solutions - Categorical Data",
          "Keras Project Solutions - Data PreProcessing",
          "Keras Project Solutions - Creating and Training a Model",
          "Keras Project Solutions - Model Evaluation",
          "Tensorboard"
        ],
        "Convolutional Neural Networks - CNNs": [
          "CNN Section Overview",
          "Image Filters and Kernels",
          "Convolutional Layers",
          "Pooling Layers",
          "MNIST Data Set Overview",
          "CNN on MNIST - Part One - The Data",
          "CNN on MNIST - Part Two - Creating and Training the Model",
          "CNN on MNIST - Part Three - Model Evaluation",
          "CNN on CIFAR-10 - Part One - The Data",
          "CNN on CIFAR-10 - Part Two - Evaluating the Model",
          "Downloading Data Set for Real Image Lectures",
          "CNN on Real Image Files - Part One - Reading in the Data",
          "CNN on Real Image Files - Part Two - Data Processing",
          "CNN on Real Image Files - Part Three - Creating the Model",
          "CNN on Real Image Files - Part Four - Evaluating the Model",
          "CNN Exercise Overview",
          "CNN Exercise Solutions"
        ],
        "Recurrent Neural Networks - RNNs": [
          "RNN Section Overview",
          "RNN Basic Theory",
          "Vanishing Gradients",
          "LSTMS and GRU",
          "RNN Batches",
          "RNN on a Sine Wave - The Data",
          "RNN on a Sine Wave - Batch Generator",
          "RNN on a Sine Wave - Creating the Model",
          "RNN on a Sine Wave - LSTMs and Forecasting",
          "RNN on a Time Series - Part One",
          "RNN on a Time Series - Part Two",
          "RNN Exercise",
          "RNN Exercise - Solutions",
          "Bonus - Multivariate Time Series - RNN and LSTMs"
        ],
        "Natural Language Processing": [
          "Introduction to NLP Section",
          "NLP - Part One - The Data",
          "NLP - Part Two - Text Processing",
          "NLP - Part Three - Creating Batches",
          "NLP - Part Four - Creating the Model",
          "NLP - Part Five - Training the Model",
          "NLP - Part Six - Generating Text"
        ]
      },
      "requirements": [
        "Know how to code in Python",
        "Some math basics such as derivatives"
      ],
      "description": "This course will guide you through how to use Google's latest TensorFlow 2 framework to create artificial neural networks for deep learning! This course aims to give you an easy to understand guide to the complexities of Google's TensorFlow 2 framework in a way that is easy to understand.\nWe'll focus on understanding the latest updates to TensorFlow and leveraging the Keras API (TensorFlow 2.0's official API) to quickly and easily build models. In this course we will build models to forecast future price homes, classify medical images, predict future sales data, generate complete new text artificially and much more!\nThis course is designed to balance theory and practical implementation, with complete jupyter notebook guides of code and easy to reference slides and notes. We also have plenty of exercises to test your new skills along the way!\nThis course covers a variety of topics, including\nNumPy Crash Course\nPandas Data Analysis Crash Course\nData Visualization Crash Course\nNeural Network Basics\nTensorFlow Basics\nKeras Syntax Basics\nArtificial Neural Networks\nDensely Connected Networks\nConvolutional Neural Networks\nRecurrent Neural Networks\nAutoEncoders\nGANs - Generative Adversarial Networks\nDeploying TensorFlow into Production\nand much more!\nKeras, a user-friendly API standard for machine learning, will be the central high-level API used to build and train models. The Keras API makes it easy to get started with TensorFlow 2. Importantly, Keras provides several model-building APIs (Sequential, Functional, and Subclassing), so you can choose the right level of abstraction for your project. TensorFlow’s implementation contains enhancements including eager execution, for immediate iteration and intuitive debugging, and tf.data, for building scalable input pipelines.\nTensorFlow 2 makes it easy to take new ideas from concept to code, and from model to publication. TensorFlow 2.0 incorporates a number of features that enables the definition and training of state of the art models without sacrificing speed or performance\nIt is used by major companies all over the world, including Airbnb, Ebay, Dropbox, Snapchat, Twitter, Uber, SAP, Qualcomm, IBM, Intel, and of course, Google!\nBecome a deep learning guru today! We'll see you inside the course!",
      "target_audience": [
        "Python developers interested in learning about TensorFlow 2 for deep learning and artificial intelligence"
      ]
    },
    {
      "title": "Object Tracking using Python and OpenCV",
      "url": "https://www.udemy.com/course/object-tracking-using-python-and-opencv/",
      "bio": "Implement 12 different algorithms for tracking objects in videos and webcam!",
      "objectives": [
        "Track objects from videos and from the webcam using Python and OpenCV",
        "Understand the basic intuition about tracking algorithms",
        "Implement 12 tracking algorithms",
        "Understand the differences between object detection and object tracking"
      ],
      "course_content": {
        "Introduction": [
          "Course content",
          "Course materials"
        ],
        "Object tracking": [
          "Object tracking vs Object detection",
          "Object tracking algorithms - intuition",
          "Object tracking algorithms - additional materials",
          "Boosting and MIL algorithms",
          "KCF and CSRT algorithms",
          "MedianFlow, TLD, MOSSE and Goturn algorithms",
          "Installing Anaconda and PyCharm",
          "Tracking a single object 1",
          "Tracking a single object 2",
          "Tracking a single object 3",
          "Tracking a single object 4",
          "Tracking multiple objects 1",
          "Tracking multiple objects 2",
          "Tracking multiple objects 3",
          "Tracking objects with Goturn",
          "Object detection",
          "Object detection + object tracking 1",
          "Object detection + object tracking 2",
          "Meanshift algorithm - intuition",
          "Meanshift algorithm - implementation 1",
          "Meanshift algorithm - implementation 2",
          "Meanshift algorithm - implementation 3",
          "CAMShift algorithm - intuition",
          "CAMShift algorithm - implementation",
          "Optical flow algorithm (sparse) – intuition",
          "Optical flow algorithm (sparse) – implementation 1",
          "Optical flow algorithm (sparse) – implementation 2",
          "Optical flow algorithm (sparse) – implementation 3",
          "Optical flow dense algorithm – intuition",
          "Optical flow dense algorithm – implementation"
        ],
        "Final remarks": [
          "Final remarks",
          "BONUS"
        ]
      },
      "requirements": [
        "Programming logic",
        "Basic Python programming"
      ],
      "description": "Object tracking is a subarea of Computer Vision which aims to locate an object in successive frames of a video. An example of application is a video surveillance and security system, in which suspicious actions can be detected. Other examples are the monitoring of traffic on highways and also the analysis of the movement of players in a soccer match! In this last example, it is possible to trace the complete route that the player followed during the match.\nTo take you to this area, in this course you will learn the main object tracking algorithms using the Python language and the OpenCV library! You will learn the basic intuition about 12 (twelve) algorithms and implement them step by step! At the end of the course you will know how to apply tracking algorithms applied to videos, so you will able to develop your own projects. The following algorithms will be covered: Boosting, MIL (Multiple Instance Learning), KCF (Kernel Correlation Filters), CSRT (Discriminative Correlation Filter with Channel and Spatial Reliability), MedianFlow, TLD (Tracking Learning Detection), MOSSE (Minimum Output Sum of Squared) Error), Goturn (Generic Object Tracking Using Regression Networks), Meanshift, CAMShift (Continuously Adaptive Meanshift), Optical Flow Sparse, and Optical Flow Dense.\nYou'll learn the basic intuition about all algorithms and then, we'll implement and test them using PyCharm IDE. It's important to emphasize that the goal of the course is to be as practical as possible, so, don't expect too much from the theory since you are going to learn only the basic aspects of each algorithm. The purpose of showing all these algorithms is for you to have a view that different algorithms can be used according to the types of applications, so you can choose the best ones according to the problem you are trying to solve.",
      "target_audience": [
        "Beginners who are starting to learn Computer Vision and Object Tracking",
        "Undergraduate students who are studying subjects related to Artificial Intelligence",
        "Anyone interested in Artificial Intelligence or Computer Vision",
        "Data scientists who want to grow their portfolio"
      ]
    },
    {
      "title": "Python for Data Science Pro: The Complete Mastery Course",
      "url": "https://www.udemy.com/course/python-for-data-science-pro-the-complete-mastery-course/",
      "bio": "Become a Data Science Pro: Master Data Analysis, Visualization, and Machine Learning with Python",
      "objectives": [
        "What is Python Data Science and Workflow?",
        "Control Flow: Conditionals and Loops",
        "Understanding Arrays and Matrices",
        "Data Cleaning and Preparation",
        "Merging and Joining Data",
        "Subplots and Figures",
        "Measures of Central Tendency",
        "Measures of Variability",
        "Normal, Binomial, and Other Distributions",
        "Types of Machine Learning: Supervised, Unsupervised, Reinforcement Learning",
        "Handling Imbalanced Data",
        "Linear and Logistic Regression",
        "Sentiment Analysis"
      ],
      "course_content": {
        "Module 1: Introduction to Python and Data Science": [
          "Variables, data types, and operators",
          "Control Flow: Conditionals and Loops",
          "Functions and Modules",
          "Functions with Inputs"
        ],
        "Module 2: Data Manipulation with Python": [
          "Understanding Arrays and Matrices",
          "Array Operations",
          "DataFrames and Series",
          "Data Cleaning and Preparation",
          "Handling Missing Data",
          "Merging and Joining Data",
          "Sorting and Filtering Data",
          "Grouping and Aggregation"
        ],
        "Module 3: Data Visualization": [
          "Basic Plots: Line, Bar, Scatter",
          "Customizing Plots",
          "Subplots and Figures",
          "Creating Interactive Charts"
        ],
        "Module 4: Statistical Analysis": [
          "Measures of Central Tendency",
          "Measures of Variability",
          "Normal, Binomial, and Other Distributions",
          "Null and Alternative Hypotheses"
        ],
        "Module 5: Introduction to Machine Learning": [
          "Feature Scaling and Normalization",
          "Encoding Categorical Variables",
          "Handling Imbalanced Data",
          "Linear and Logistic Regression"
        ],
        "Module 6: Advanced Topics in Data Science": [
          "Introduction to Time Series Data",
          "Decomposition, ARIMA Models",
          "Text Preprocessing",
          "Sentiment Analysis"
        ]
      },
      "requirements": [
        "No prior knowledge is required!"
      ],
      "description": "Elevate your data science skills to a professional level with \"Python for Data Science Pro: The Complete Mastery Course.\" This comprehensive course is designed for individuals who want to master Python for data analysis, machine learning, and data visualization, ensuring you are fully equipped to tackle complex data challenges in any industry.\n\n\nStarting with the fundamentals of Python, you’ll quickly progress to advanced topics, including data manipulation with Pandas, statistical analysis, and machine learning with scikit-learn. You’ll also explore powerful data visualization tools like Matplotlib and Seaborn, enabling you to present data insights clearly and effectively. The course is packed with hands-on projects and real-world datasets, providing you with practical experience that mirrors the demands of the data science field.\n\n\nBy the end of this course, you’ll have the expertise to analyze, visualize, and model data using Python, making you a highly sought-after data science professional.\n\n\nWhat You'll Learn:\nPython Basics for Data Science: Get up to speed with Python programming, including syntax, data structures, and essential libraries.\nData Manipulation with Pandas: Learn to clean, manipulate, and analyze large datasets efficiently.\nStatistical Analysis: Master statistical methods and techniques to uncover insights from data.\nMachine Learning with scikit-learn: Build and evaluate machine learning models to predict outcomes and uncover patterns.\nData Visualization: Create impactful visualizations using Matplotlib and Seaborn to communicate data insights effectively.\nBest Practices: Learn industry-standard practices for writing clean, efficient, and reproducible Python code.\n\n\nWho This Course is For:\nAspiring data scientists who want to master Python for data science.\nPython developers looking to specialize in data analysis and machine learning.\nData analysts eager to upgrade their skills with advanced data science techniques.\nProfessionals in any industry who want to leverage data science for decision-making and problem-solving.\n\n\nBy enrolling in this course, you’ll gain a complete mastery of Python for data science, from data manipulation to machine learning. This course is your pathway to becoming a proficient data scientist, capable of extracting valuable insights from data and driving impactful decisions in any organization. Start your journey to data science excellence today!",
      "target_audience": [
        "Anyone who want to learn master Python for data science.",
        "Python developers looking to specialize in data analysis and machine learning."
      ]
    },
    {
      "title": "Machine Learning with Imbalanced Data",
      "url": "https://www.udemy.com/course/machine-learning-with-imbalanced-data/",
      "bio": "Learn to over-sample and under-sample your data, apply SMOTE, ensemble methods, and cost-sensitive learning.",
      "objectives": [
        "Apply random under-sampling to remove observations from majority classes",
        "Perform under-sampling by removing observations that are hard to classify",
        "Carry out under-sampling by retaining observations at the boundary of class separation",
        "Apply random over-sampling to augment the minority class",
        "Create syntethic data to increase the examples of the minority class",
        "Implement SMOTE and its variants to synthetically generate data",
        "Use ensemble methods with sampling techniques to improve model performance",
        "Change the miss-classification cost optimized by the models to accomodate minority classes",
        "Determine model performance with the most suitable metrics for imbalanced datasets"
      ],
      "course_content": {
        "Introduction": [
          "Course Curriculum Overview",
          "Course Material",
          "Code | Jupyter notebooks",
          "Presentations covered in the course",
          "Python package Imbalanced-learn",
          "Download Datasets",
          "Resources to learn machine learning skills"
        ],
        "Machine Learning with Imbalanced Data: Overview": [
          "Imbalanced classes - Introduction",
          "Nature of the imbalanced class",
          "Approaches to work with imbalanced datasets - Overview",
          "Additional Reading Resources (Optional)"
        ],
        "Evaluation Metrics": [
          "Introduction to Performance Metrics",
          "Accuracy",
          "Accuracy - Demo",
          "Precision, Recall and F-measure",
          "Install Yellowbrick",
          "Precision, Recall and F-measure - Demo",
          "Confusion tables, FPR and FNR",
          "Confusion tables, FPR and FNR - Demo",
          "Balanced Accuracy",
          "Balanced accuracy - Demo",
          "Geometric Mean, Dominance, Index of Imbalanced Accuracy",
          "Geometric Mean, Dominance, Index of Imbalanced Accuracy - Demo",
          "ROC-AUC",
          "ROC-AUC - Demo",
          "Precision-Recall Curve",
          "Precision-Recall Curve - Demo",
          "Additional reading resources (Optional)",
          "Probability"
        ],
        "Udersampling": [
          "Under-Sampling Methods - Introduction",
          "Random Under-Sampling - Intro",
          "Random Under-Sampling - Demo",
          "Condensed Nearest Neighbours - Intro",
          "Condensed Nearest Neighbours - Demo",
          "Tomek Links - Intro",
          "Tomek Links - Demo",
          "One Sided Selection - Intro",
          "One Sided Selection - Demo",
          "Edited Nearest Neighbours - Intro",
          "Edited Nearest Neighbours - Demo",
          "Repeated Edited Nearest Neighbours - Intro",
          "Repeated Edited Nearest Neighbours - Demo",
          "All KNN - Intro",
          "All KNN - Demo",
          "Neighbourhood Cleaning Rule - Intro",
          "Neighbourhood Cleaning Rule - Demo",
          "NearMiss - Intro",
          "NearMiss - Demo",
          "Instance Hardness - Intro",
          "Instance Hardness Threshold - Demo",
          "Instance Hardness Threshold Multiclass Demo",
          "Undersampling Method Comparison",
          "Wrapping up the section",
          "Setting up a classifier with under-sampling and cross-validation",
          "Summary Table"
        ],
        "Oversampling": [
          "Over-Sampling Methods - Introduction",
          "Random Over-Sampling",
          "Random Over-Sampling - Demo",
          "ROS with smoothing - Intro",
          "ROS with smoothing - Demo",
          "SMOTE",
          "SMOTE - Demo",
          "SMOTE-NC",
          "SMOTE-NC - Demo",
          "SMOTE-N",
          "SMOTE-N Demo",
          "ADASYN",
          "ADASYN - Demo",
          "Borderline SMOTE",
          "Borderline SMOTE - Demo",
          "SVM SMOTE",
          "Resources on SVMs",
          "SVM SMOTE - Demo",
          "K-Means SMOTE",
          "K-Means SMOTE - Demo",
          "Over-Sampling Method Comparison",
          "Wrapping up the section",
          "How to Correctly Set Up a Classifier with Over-sampling",
          "Setting Up a Classifier - Demo",
          "Summary Table"
        ],
        "Over and Undersampling": [
          "Combining Over and Under-sampling - Intro",
          "Combining Over and Under-sampling - Demo",
          "Comparison of Over and Under-sampling Methods",
          "Combine over and under-sampling manually",
          "Wrapping up"
        ],
        "Ensemble Methods": [
          "Ensemble methods with Imbalanced Data",
          "Foundations of Ensemble Learning",
          "Bagging",
          "Bagging plus Over- or Under-Sampling",
          "Boosting",
          "Boosting plus Re-Sampling",
          "Hybdrid Methods",
          "Ensemble Methods - Demo",
          "Wrapping up",
          "Additional Reading Resources"
        ],
        "Cost Sensitive Learning": [
          "Cost-sensitive Learning - Intro",
          "Types of Cost",
          "Obtaining the Cost",
          "Cost Sensitive Approaches",
          "Misclassification Cost in Logistic Regression",
          "Misclassification Cost in Decision Trees",
          "Cost Sensitive Learning with Scikit-learn",
          "Find Optimal Cost with hyperparameter tuning",
          "Additional Reading Resources"
        ],
        "Putting it all together": [
          "Examples"
        ],
        "Final bonus section": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Knowledge of machine learning basic algorithms, i.e., regression, decision trees and nearest neighbours",
        "Python programming, including familiarity with NumPy, Pandas and Scikit-learn",
        "A Python and Jupyter notebook installation"
      ],
      "description": "Welcome to Machine Learning with Imbalanced Datasets. In this course, you will learn multiple techniques which you can use with imbalanced datasets to improve the performance of your machine learning models.\n\n\nIf you are working with imbalanced datasets right now and want to improve the performance of your models, or you simply want to learn more about how to tackle data imbalance, this course will show you how.\n\n\nWe'll take you step-by-step through engaging video tutorials and teach you everything you need to know about working with imbalanced datasets. Throughout this comprehensive course, we cover almost every available methodology to work with imbalanced datasets, discussing their logic, their implementation in Python, their advantages and shortcomings, and the considerations to have when using the technique. Specifically, you will learn:\n\n\nUnder-sampling methods at random or focused on highlighting certain sample populations\nOver-sampling methods at random and those which create new examples based of existing observations\nEnsemble methods that leverage the power of multiple weak learners in conjunction with sampling techniques to boost model performance\nCost sensitive methods which penalize wrong decisions more severely for minority classes\nThe appropriate metrics to evaluate model performance on imbalanced datasets\n\n\nBy the end of the course, you will be able to decide which technique is suitable for your dataset, and / or apply and compare the improvement in performance returned by the different methods on multiple datasets.\n\n\nThis comprehensive machine learning course includes over 50 lectures spanning more than 10 hours of video, and ALL topics include hands-on Python code examples which you can use for reference and for practice, and re-use in your own projects.\n\n\nIn addition, the code is updated regularly to keep up with new trends and new Python library releases.\n\n\nSo what are you waiting for? Enroll today, learn how to work with imbalanced datasets and build better machine learning models.",
      "target_audience": [
        "Data scientists and machine learning engineers working with imbalanced datasets",
        "Data scientists who want to improve the performance of models trained on imbalanced datasets",
        "Students who want to learn intermediate content on machine learning",
        "Students working with imbalanced multi-class targets"
      ]
    },
    {
      "title": "Feature Engineering for Machine Learning",
      "url": "https://www.udemy.com/course/feature-engineering-for-machine-learning/",
      "bio": "Learn imputation, variable encoding, discretization, feature extraction, how to work with datetime, outliers, and more.",
      "objectives": [
        "Learn multiple techniques for missing data imputation.",
        "Transform categorical variables into numbers while capturing meaningful information.",
        "Learn how to deal with infrequent, rare, and unseen categories.",
        "Learn how to work with skewed variables.",
        "Convert numerical variables into discrete ones.",
        "Remove outliers from your variables.",
        "Extract useful features from dates and time variables.",
        "Learn techniques used in organizations worldwide and in data competitions.",
        "Increase your repertoire of techniques to preprocess data and build more powerful machine learning models."
      ],
      "course_content": {
        "Welcome": [
          "Course curriculum overview",
          "Course requirements",
          "How to approach this course",
          "Setting up your computer",
          "Resources to learn machine learning skills"
        ],
        "Course material": [
          "Course material",
          "Download Jupyter notebooks",
          "Download datasets",
          "Download presentations"
        ],
        "Variable Types": [
          "Variables | Intro",
          "Numerical variables",
          "Categorical variables",
          "Date and time variables",
          "Mixed variables"
        ],
        "Variable Characteristics": [
          "Variable characteristics",
          "Missing data",
          "Cardinality",
          "Rare labels",
          "Variable distribution",
          "Outliers",
          "Linear models assumptions",
          "Linear model assumptions - additional reading resources (optional)",
          "Variable magnitude",
          "Summary table",
          "Additional reading resources"
        ],
        "Missing Data Imputation - Basic": [
          "Basic imputation methods",
          "Mean or median imputation",
          "Arbitrary value imputation",
          "Frequent category imputation",
          "Missing category imputation",
          "Adding a missing indicator",
          "Basic methods - considerations",
          "Basic imputation with pandas",
          "Basic imputation with pandas - demo",
          "Basic methods with Scikit-learn",
          "Mean or median imputation with Scikit-learn",
          "Arbitrary value imputation with Scikit-learn",
          "Frequent category imputation with Scikit-learn",
          "Missing category imputation with Scikit-learn",
          "Adding a missing indicator with Scikit-learn",
          "Imputation with GrdiSearch - Scikit-learn",
          "Basic methods with Feature-engine",
          "Mean or median imputation with Feature-engine",
          "Arbitrary value imputation with Feature-engine",
          "Frequent category imputation with Feature-engine",
          "Arbitrary string imputation with Feature-engine",
          "Adding a missing indicator with Feature-engine",
          "Wrapping up",
          "Treat: our movie pick"
        ],
        "Missing Data Imputation - Alternative Methods": [
          "Alternative imputation methods",
          "Complete Case Analysis",
          "CCA - considerations with code demo",
          "End of distribution imputation",
          "Random sample imputation",
          "Random imputation - considerations with code",
          "Mean or median imputation per group",
          "CCA with pandas",
          "End of distribution imputation with pandas",
          "Random sample imputation with pandas",
          "Mean imputation per group with pandas",
          "CCA with Feature-engine",
          "End of distribution imputation with Feature-engine",
          "Random sample imputation with Feature-engine",
          "Imputation - Summary table",
          "Wrapping up"
        ],
        "Multivariate Missing Data Imputation": [
          "Multivariate imputation",
          "KNN imputation",
          "KNN imputation - Demo",
          "MICE",
          "missForest",
          "MICE and missForest - Demo",
          "Additional reading resources (Optional)",
          "Treat: Our book recommendation"
        ],
        "Categorical Encoding - Basic methods": [
          "Categorical encoding | Introduction",
          "One hot encoding",
          "One hot encoding with pandas",
          "One hot encoding with sklearn",
          "One hot encoding with Feature-engine",
          "One hot encoding with Category encoders",
          "Ordinal encoding",
          "Ordinal encoding with pandas",
          "Ordinal encoding with sklearn",
          "Ordinal encoding with Feature-engine",
          "Ordinal encoding with Category encoders",
          "Count or frequency encoding",
          "Count encoding with pandas",
          "Count encoding with Feature-engine",
          "Count encoding with Category encoders",
          "Unseen categories",
          "Wrapping up"
        ],
        "Categorical encoding - Monotonic": [
          "Categorical encoding | Monotonic",
          "Ordered ordinal encoding",
          "Ordered ordinal encoding with pandas",
          "Ordered ordinal encoding with Feature-engine",
          "Mean encoding",
          "Mean encoding with pandas",
          "Mean encoding with Feature-engine",
          "Mean encoding with Category encoders",
          "Mean encoding plus smoothing",
          "Mean encoding plus smoothing - Category encoders",
          "Mean encoding plus smoothing - Feature-engine",
          "Weight of evidence (WoE)",
          "Weight of Evidence with pandas",
          "Weight of Evidence with Feature-engine",
          "Weight of Evidence with Category encoders",
          "Weight of evidence - gotchas",
          "Unseen categories",
          "Wrapping up",
          "Comparison of categorical variable encoding",
          "Additional reading resources"
        ],
        "Categorical encoding - Rare labels": [
          "Grouping rare labels",
          "One hot encoding of top categories",
          "OHE of top categories with pandas",
          "OHE of top categories with Feature-engine",
          "OHE of top categories with sklearn",
          "Rare label encoding",
          "Rare label encoding with pandas",
          "Rare label encoding with Feature-engine",
          "Wrapping up",
          "Categorical encoding - More..."
        ]
      },
      "requirements": [
        "A Python installation.",
        "Jupyter notebook installation.",
        "Python coding skills.",
        "Some experience with Numpy and Pandas.",
        "Familiarity with machine learning algorithms.",
        "Familiarity with Scikit-Learn."
      ],
      "description": "Welcome to Feature Engineering for Machine Learning, the most comprehensive course on feature engineering available online. In this course, you will learn about variable imputation, variable encoding, feature transformation, discretization, and how to create new features from your data.\n\n\nMaster Feature Engineering and Feature Extraction.\nIn this course, you will learn multiple feature engineering methods that will allow you to transform your data and leave it ready to train machine learning models. Specifically, you will learn:\n\n\nHow to impute missing data\nHow to encode categorical variables\nHow to transform numerical variables and change their distribution\nHow to perform discretization\nHow to remove outliers\nHow to extract features from date and time\nHow to create new features from existing ones\n\n\nCreate useful Features with Math, Statistics and Domain Knowledge\nFeature engineering is the process of transforming existing features or creating new variables for use in machine learning. Raw data is not suitable to train machine learning algorithms. Instead, data scientists devote a lot of time to data preprocessing. This course teaches you everything you need to know to leave your data ready to train your models.\n\n\nWhile most online courses will teach you the very basics of feature engineering, like imputing variables with the mean or transforming categorical variables using one hot encoding, this course will teach you that, and much, much more.\n\n\nIn this course, you will first learn the most popular and widely used techniques for variable engineering, like mean and median imputation, one-hot encoding, transformation with logarithm, and discretization. Then, you will discover more advanced methods that capture information while encoding or transforming your variables to improve the performance of machine learning models.\n\n\nYou will learn methods like the weight of evidence, used in finance, and how to create monotonic relationships between variables and targets to boost the performance of linear models. You will also learn how to create features from date and time variables and how to handle categorical variables with a lot of categories.\n\n\nThe methods that you will learn were described in scientific articles, are used in data science competitions, and are commonly utilized in organizations. And what’s more, they can be easily implemented by utilizing Python's open-source libraries!\nThroughout the lectures, you’ll find detailed explanations of each technique and a discussion about their advantages, limitations, and underlying assumptions, followed by the best programming practices to implement them in Python.\n\n\nBy the end of the course, you will be able to decide which feature engineering technique you need based on the variable characteristics and the models you wish to train. And you will also be well placed to test various transformation methods and let your models decide which ones work best.\n\n\nStep-up your Career in Data Science\nYou’ve taken your first steps into data science. You know about the most commonly used prediction models. You've even trained a few linear regression or classification models. At this stage, you’re probably starting to find some challenges: your data is dirty, lots of values are missing, some variables are not numerical, and others extremely skewed. You may also wonder whether your code is efficient and performant or if there is a better way to program. You search online, but you can’t find consolidated resources on feature engineering. Maybe just blogs? So you may start to wonder: how are things really done in tech companies?\n\n\nIn this course, you will find answers to those questions. Throughout the course, you will learn multiple techniques for the different aspects of variable transformation, and how to implement them in an elegant, efficient, and professional manner using Python. You will leverage the power of Python’s open source ecosystem, including the libraries NumPy, Pandas, Scikit-learn, and special packages for feature engineering: Feature-engine and Category encoders.\n\n\nBy the end of the course, you will be able to implement all your feature engineering steps into a single elegant pipeline, which will allow you to put your predictive models into production with maximum efficiency.\n\n\nLeverage the Power of Open Source\nWe will perform all feature engineering methods utilizing Pandas and Numpy, and we will compare the implementation with Scikit-learn, Feature-engine, and Category encoders, highlighting the advantages and limitations of each library. As you progress in the course, you will be able to choose the library you like the most to carry out your projects.\nThere is a dedicated Python notebook with code to implement each feature engineering method, which you can reuse in your projects to speed up the development of your machine learning models.\n\n\nThe Most Comprehensive Online Course for Feature Engineering\nThere is no one single place to go to learn about feature engineering. It involves hours of searching on the web to find out what people are doing to get the most out of their data.\n\n\nThat is why, this course gathers plenty of techniques used worldwide for feature transformation, learnt from data competitions in Kaggle and the KDD, scientific articles, and from the instructor’s experience as a data scientist. This course therefore provides a source of reference where you can learn new methods and also revisit the techniques and code needed to modify variables whenever you need to.\n\n\nThis course is taught by a lead data scientist with experience in the use of machine learning in finance and insurance, who is also a book author and the lead developer of a Python open source library for feature engineering. And there is more:\n\n\nThe course is constantly updated to include new feature engineering methods.\nNotebooks are regularly refreshed to ensure all methods are carried out with the latest releases of the Python libraries, so your code will never break.\nThe course combines videos, presentations, and Jupyter notebooks to explain the methods and show their implementation in Python.\nThe curriculum was developed over a period of four years with continuous research in the field of feature engineering to bring you the latest technologies, tools, and trends.\n\n\nWant to know more? Read on...\nThis comprehensive feature engineering course contains over 100 lectures spread across approximately 10 hours of video, and ALL topics include hands-on Python code examples that you can use for reference, practice, and reuse in your own projects.\n\n\nREMEMBER, the course comes with a 30-day money-back guarantee, so you can sign up today with no risk.\n\n\nSo what are you waiting for? Enrol today and join the world's most comprehensive course on feature engineering for machine learning.",
      "target_audience": [
        "Data scientists who want to learn how to preprocess datasets in order to build machine learning models.",
        "Data scientists who want to learn more techniques for feature engineering for machine learning.",
        "Data scientists who want to improve their coding skills and programming practices for feature engineering.",
        "Software engineers, mathematicians and academics switching careers into data science.",
        "Data scientists interested in experimenting with various feature engineering techniques on data competitions",
        "Software engineers who want to learn how to use Scikit-learn and other open-source packages for feature engineering."
      ]
    },
    {
      "title": "MLOps Fundamentals - Learn MLOps Concepts with Azure demo",
      "url": "https://www.udemy.com/course/mlops-course/",
      "bio": "Learn MLOps basics of Continuous Integration, Delivery using Azure DevOps and Azure ML. Create MLOps pipeline in Azure.",
      "objectives": [
        "Basics of MLOps, benefits and its implementation.",
        "Challenges in handling ML projects and the importance of MLOps principles in ML projects.",
        "Standards and principles followed in MLOps culture.",
        "What is continuous integration, continuous delivery and continuous training in MLOps space.",
        "Various maturity levels associated with MLOps.",
        "MLOps tools stack and various MLOps platforms comparison.",
        "A quick crash course on Azure Machine Learning studio.",
        "Build and run an end-to-end CI/CD MLOps pipeline using Azure DevOps & Azure Machine learning."
      ],
      "course_content": {
        "Introduction": [
          "What is MLOps",
          "Traditional Machine Learning Lifecycle - Part 1",
          "Traditional Machine Learning Lifecycle - Part 2",
          "Roles & Responsibilities in ML projects"
        ],
        "Challenges in existing ML projects": [
          "Problems in traditional ML lifecycle",
          "Activities needed to productionize models"
        ],
        "MLOps - A solution": [
          "Standards/Principles in MLOps",
          "MLOps implementation",
          "Benefits of MLOps",
          "Difference between DevOps & MLOps"
        ],
        "Maturity levels in MLOps": [
          "MLOps level 0",
          "MLOps level 1",
          "MLOps level 2",
          "Importance of Maturity levels",
          "Quiz 1"
        ],
        "MLOps Tools/Platforms Stack": [
          "MLOps Platform requirements",
          "MLOps Platforms comparison",
          "Which MLOps Platform to choose?"
        ],
        "Demo - Project Requirements": [
          "Note",
          "Project requirements"
        ],
        "Azure Machine Learning Studio - Crash course": [
          "Introduction to Azure Machine Learning",
          "Azure Machine learning studio UI Tour"
        ],
        "Demo - Data scientist's experiment": [
          "EDA notebook",
          "Azure DevOps & Azure ML connections",
          "Training & Evaluation notebook"
        ],
        "Demo - Orchestrated ML codes in Azure": [
          "Model Training code",
          "Model Evaluation code",
          "Model Registry code",
          "Scoring code"
        ],
        "Demo - CI/CD MLOps Pipeline in Azure": [
          "Overview",
          "Continuous Integration (CI) script",
          "Code to publish the pipeline",
          "Code to run the published package",
          "Continuous Deployment (CD) script",
          "Run the Pipeline"
        ]
      },
      "requirements": [
        "Basics of DevOps & Machine learning"
      ],
      "description": "Important Note: The intention of this course is to teach MLOps fundamentals. Azure demo section is included to show the working of an end-to-end MLOps project. All the codes involved in Azure MLOps pipeline are well explained though.\n\"MLOps is a culture with set of principles, guidelines defined in machine learning world for smooth implementation and productionization of Machine learning models.\"\nData scientists have been experimenting with Machine learning models from long time, but to provide the real business value, they must be deployed to production. Unfortunately, due to the current challenges and non-systemization in ML lifecycle, 80% of the models never make it to production and remain stagnated as an academic experiment only.\nMachine Learning Operations (MLOps), emerged as a solution to the problem, is a new culture in the market and a rapidly growing space that encompasses everything required to deploy a machine learning model into production.\nAs per the tech talks in market, 2024 is the year of MLOps and would become the mandate skill set for Enterprise Machine Learning projects.\nWhat's included in the course ?\nMLOps core basics and fundamentals.\nWhat were the challenges in the traditional machine learning lifecycle management.\nHow MLOps is addressing those issues while providing more flexibility and automation in the ML process.\nStandards and principles on which MLOps is based upon.\nContinuous integration (CI), Continuous delivery (CD) and Continuous training (CT) pipelines in MLOps.\nVarious maturity levels associated with MLOps.\nMLOps tools stack and MLOps platforms comparisons.\nQuick crash course on Azure Machine learning components.\nAn end-to-end CI/CD MLOps pipeline for a case study in Azure using Azure DevOps & Azure Machine learning.",
      "target_audience": [
        "Data scientists",
        "Data engineers",
        "ML engineers",
        "Devops engineers",
        "MLOps engineers"
      ]
    },
    {
      "title": "LLM Engineering: Master AI, Large Language Models & Agents",
      "url": "https://www.udemy.com/course/llm-engineering-master-ai-and-large-language-models/",
      "bio": "Become an LLM Engineer in 8 weeks: Build and deploy 8 LLM apps, mastering Generative AI, RAG, LoRA and AI Agents.",
      "objectives": [
        "Project 1: Make AI-powered brochure generator that scrapes and navigates company websites intelligently.",
        "Project 2: Build Multi-modal customer support agent for an airline with UI and function-calling.",
        "Project 3: Develop Tool that creates meeting minutes and action items from audio using both open- and closed-source models.",
        "Project 4: Make AI that converts Python code to optimized C++, boosting performance by 60,000x!",
        "Project 5: Build AI knowledge-worker using RAG to become an expert on all company-related matters.",
        "Project 6: Capstone Part A – Predict product prices from short descriptions using Frontier models.",
        "Project 7: Capstone Part B – Execute Fine-tuned open-source model to compete with Frontier in price prediction.",
        "Project 8: Capstone Part C – Build Autonomous multi agent system collaborating with models to spot deals and notify you of special bargains.",
        "Compare and contrast the latest techniques for improving the performance of your LLM solution, such as RAG, fine-tuning and agentic workflows",
        "Weigh up the leading 10 frontier and 10 open-source LLMs, and be able to select the best choice for a given task"
      ],
      "course_content": {
        "Week 1 - Build Your First LLM Product: Exploring Top Models & Transformers": [
          "Day 1 - Cold Open: Jumping Right into LLM Engineering",
          "Day 1 - Setting Up Ollama for Local LLM Deployment on Windows and Mac",
          "Day 1 - Unleashing the Power of Local LLMs: Build Spanish Tutor Using Ollama",
          "Day 1 - LLM Engineering Roadmap: From Beginner to Master in 8 Weeks",
          "Day 1 - Building LLM Applications: Chatbots, RAG, and Agentic AI Projects",
          "Day 1 - From Wall Street to AI: Ed Donner's Path to Becoming an LLM Engineer",
          "Day 1 - Setting Up Your LLM Development Environment: Tools and Best Practices",
          "Day 1 - Mac Setup Guide: Jupyter Lab and Conda for LLM Projects",
          "Day 1 - Setting Up Anaconda for LLM Engineering: Windows Installation Guide",
          "Day 1 - Alternative Python Setup for LLM Projects: Virtualenv vs. Anaconda Guide",
          "Day 1- Setting Up OpenAI API for LLM Development: Keys, Pricing & Best Practices",
          "Day 1 - Creating a .env File for Storing API Keys Safely",
          "Day 1- Instant Gratification Project: Creating an AI-Powered Web Page Summarizer",
          "Day 1 - Implementing Text Summarization Using OpenAI's GPT-4 and Beautiful Soup",
          "Day 1 - Wrapping Up Day 1: Key Takeaways and Next Steps in LLM Engineering",
          "Day 2 - Mastering LLM Engineering: Key Skills and Tools for AI Development",
          "Day 2 - Understanding Frontier Models: GPT, Claude, and Open Source LLMs",
          "Day 2 - How to Use Ollama for Local LLM Inference: Python Tutorial with Jupyter",
          "Day 2 - Hands-On LLM Task: Comparing OpenAI and Ollama for Text Summarization",
          "Day 3 - Frontier AI Models: Comparing GPT-4, Claude, Gemini, and LLAMA",
          "Day 3 - Comparing Leading LLMs: Strengths and Business Applications",
          "Day 3 - Exploring GPT-4o vs O1 Preview: Key Differences in Performance",
          "Day 3 - Creativity and Coding: Leveraging GPT-4o’s Canvas Feature",
          "Day 3 - Claude 3.5’s Alignment and Artifact Creation: A Deep Dive",
          "Day 3 - AI Model Comparison: Gemini vs Cohere for Whimsical and Analytical Tasks",
          "Day 3 - Evaluating Meta AI and Perplexity: Nuances of Model Outputs",
          "Day 3 - LLM Leadership Challenge: Evaluating AI Models Through Creative Prompts",
          "Day 4 - Revealing the Leadership Winner: A Fun LLM Challenge",
          "Day 4 - Exploring the Journey of AI: From Early Models to Transformers",
          "Day 4 - Understanding LLM Parameters: From GPT-1 to Trillion-Weight Models",
          "Day 4 - GPT Tokenization Explained: How Large Language Models Process Text Input",
          "Day 4 - How Context Windows Impact AI Language Models: Token Limits Explained",
          "Day 4 - Navigating AI Model Costs: API Pricing vs. Chat Interface Subscriptions",
          "Day 4 - Comparing LLM Context Windows: GPT-4 vs Claude vs Gemini 1.5 Flash",
          "Day 4 - Wrapping Up Day 4: Key Takeaways and Practical Insights",
          "Day 5 - Building AI-Powered Marketing Brochures with OpenAI API and Python",
          "Day 5 - JupyterLab Tutorial: Web Scraping for AI-Powered Company Brochures",
          "Day 5 - Structured Outputs in LLMs: Optimizing JSON Responses for AI Projects",
          "Day 5 - Creating and Formatting Responses for Brochure Content",
          "Day 5 - Final Adjustments: Optimizing Markdown and Streaming in JupyterLab",
          "Day 5 - Mastering Multi-Shot Prompting: Enhancing LLM Reliability in AI Projects",
          "Day 5 - Assignment: Developing Your Customized LLM-Based Tutor",
          "Day 5 - Wrapping Up Week 1: Achievements and Next Steps"
        ],
        "Week 2 - Build a Multi-Modal Chatbot: LLMs, Gradio UI, and Agents in Action": [
          "Day 1 - Mastering Multiple AI APIs: OpenAI, Claude, and Gemini for LLM Engineers",
          "Day 1 - Streaming AI Responses: Implementing Real-Time LLM Output in Python",
          "Day 1 - How to Create Adversarial AI Conversations Using OpenAI and Claude APIs",
          "Day 1 - AI Tools: Exploring Transformers & Frontier LLMs for Developers",
          "Day 2 - Building AI UIs with Gradio: Quick Prototyping for LLM Engineers",
          "Day 2 - Gradio Tutorial: Create Interactive AI Interfaces for OpenAI GPT Models",
          "Day 2 - Implementing Streaming Responses with GPT and Claude in Gradio UI",
          "Day 2 - Building a Multi-Model AI Chat Interface with Gradio: GPT vs Claude",
          "Day 2 - Building Advanced AI UIs: From OpenAI API to Chat Interfaces with Gradio",
          "Day 3 - Building AI Chatbots: Mastering Gradio for Customer Support Assistants",
          "Day 3 - Build a Conversational AI Chatbot with OpenAI & Gradio: Step-by-Step",
          "Day 3 - Enhancing Chatbots with Multi-Shot Prompting and Context Enrichment",
          "Day 3 - Mastering AI Tools: Empowering LLMs to Run Code on Your Machine",
          "Day 4 - Using AI Tools with LLMs: Enhancing Large Language Model Capabilities",
          "Day 4 - Building an AI Airline Assistant: Implementing Tools with OpenAI GPT-4",
          "Day 4 - How to Equip LLMs with Custom Tools: OpenAI Function Calling Tutorial",
          "Day 4 - Mastering AI Tools: Building Advanced LLM-Powered Assistants with APIs",
          "Day 5 - Multimodal AI Assistants: Integrating Image and Sound Generation",
          "Day 5 - Multimodal AI: Integrating DALL-E 3 Image Generation in JupyterLab",
          "Day 5 - Build a Multimodal AI Agent: Integrating Audio & Image Tools",
          "Day 5 - How to Build a Multimodal AI Assistant: Integrating Tools and Agents"
        ],
        "Week 3 - Open-Source Gen AI: Building Automated Solutions with HuggingFace": [
          "Day 1 - Hugging Face Tutorial: Exploring Open-Source AI Models and Datasets",
          "Day 1 - Exploring HuggingFace Hub: Models, Datasets & Spaces for AI Developers",
          "Day 1 - Intro to Google Colab: Cloud Jupyter Notebooks for Machine Learning",
          "Day 1 - Hugging Face Integration with Google Colab: Secrets and API Keys Setup",
          "Day 1 - Mastering Google Colab: Run Open-Source AI Models with Hugging Face",
          "Day 2 - Hugging Face Transformers: Using Pipelines for AI Tasks in Python",
          "Day 2 - Hugging Face Pipelines: Simplifying AI Tasks with Transformers Library",
          "Day 2 - Mastering HuggingFace Pipelines: Efficient AI Inference for ML Tasks",
          "Day 3 - Exploring Tokenizers in Open-Source AI: Llama, Phi-2, Qwen, & Starcoder",
          "Day 3 - Tokenization Techniques in AI: Using AutoTokenizer with LLAMA 3.1 Model",
          "Day 3 - Comparing Tokenizers: Llama, PHI-3, and QWEN2 for Open-Source AI Models",
          "Day 3 - Hugging Face Tokenizers: Preparing for Advanced AI Text Generation",
          "Day 4 - Hugging Face Model Class: Running Inference on Open-Source AI Models",
          "Day 4 - Hugging Face Transformers: Loading & Quantizing LLMs with Bits & Bytes",
          "Day 4 - Hugging Face Transformers: Generating Jokes with Open-Source AI Models",
          "Day 4 - Mastering Hugging Face Transformers: Models, Pipelines, and Tokenizers",
          "Day 5 - Combining Frontier & Open-Source Models for Audio-to-Text Summarization",
          "Day 5 - Using Hugging Face & OpenAI for AI-Powered Meeting Minutes Generation",
          "Day 5 - Build a Synthetic Test Data Generator: Open-Source AI Model for Business"
        ],
        "Week 4 - LLM Showdown: Evaluating Models for Code Generation & Business Tasks": [
          "Day 1 - How to Choose the Right LLM: Comparing Open and Closed Source Models",
          "Day 1 - Chinchilla Scaling Law: Optimizing LLM Parameters and Training Data Size",
          "Day 1 - Limitations of LLM Benchmarks: Overfitting and Training Data Leakage",
          "Day 1 - Evaluating Large Language Models: 6 Next-Level Benchmarks Unveiled",
          "Day 1 - HuggingFace OpenLLM Leaderboard: Comparing Open-Source Language Models",
          "Day 1 - Master LLM Leaderboards: Comparing Open Source and Closed Source Models",
          "Day 2 - Comparing LLMs: Top 6 Leaderboards for Evaluating Language Models",
          "Day 2 - Specialized LLM Leaderboards: Finding the Best Model for Your Use Case",
          "Day 2 - LLAMA vs GPT-4: Benchmarking Large Language Models for Code Generation",
          "Day 2 - Human-Rated Language Models: Understanding the LM Sys Chatbot Arena",
          "Day 2 - Commercial Applications of Large Language Models: From Law to Education",
          "Day 2 - Comparing Frontier and Open-Source LLMs for Code Conversion Projects",
          "Day 3 - Leveraging Frontier Models for High-Performance Code Generation in C++",
          "Day 3 - Comparing Top LLMs for Code Generation: GPT-4 vs Claude 3.5 Sonnet",
          "Day 3 - Optimizing Python Code with Large Language Models: GPT-4 vs Claude 3.5",
          "Day 3 - Code Generation Pitfalls: When Large Language Models Produce Errors",
          "Day 3 - Blazing Fast Code Generation: How Claude Outperforms Python by 13,000x",
          "Day 3 - Building a Gradio UI for Code Generation with Large Language Models",
          "Day 3 - Optimizing C++ Code Generation: Comparing GPT and Claude Performance",
          "Day 3 - Comparing GPT-4 and Claude for Code Generation: Performance Benchmarks",
          "Day 4 - Open Source LLMs for Code Generation: Hugging Face Endpoints Explored",
          "Day 4 - How to Use HuggingFace Inference Endpoints for Code Generation Models",
          "Day 4 - Integrating Open-Source Models with Frontier LLMs for Code Generation",
          "Day 4 - Comparing Code Generation: GPT-4, Claude, and CodeQuen LLMs",
          "Day 4 - Mastering Code Generation with LLMs: Techniques and Model Selection",
          "Day 5 - Evaluating LLM Performance: Model-Centric vs Business-Centric Metrics",
          "Day 5 - Mastering LLM Code Generation: Advanced Challenges for Python Developers"
        ],
        "Mastering RAG: Build Advanced Solutions with Vector Embeddings & LangChain": [
          "Day 1 - RAG Fundamentals: Leveraging External Data to Improve LLM Responses",
          "Day 1 - Building a DIY RAG System: Implementing Retrieval-Augmented Generation",
          "Day 1 - Understanding Vector Embeddings: The Key to RAG and LLM Retrieval",
          "Day 2 - Unveiling LangChain: Simplify RAG Implementation for LLM Applications",
          "Day 2 - LangChain Text Splitter Tutorial: Optimizing Chunks for RAG Systems",
          "Day 2 - Preparing for Vector Databases: OpenAI Embeddings and Chroma in RAG",
          "Day 3 - Mastering Vector Embeddings: OpenAI and Chroma for LLM Engineering",
          "Day 3 - Visualizing Embeddings: Exploring Multi-Dimensional Space with t-SNE",
          "Day 3 - Building RAG Pipelines: From Vectors to Embeddings with LangChain",
          "Day 4 - Implementing RAG Pipeline: LLM, Retriever, and Memory in LangChain",
          "Day 4 - Mastering Retrieval-Augmented Generation: Hands-On LLM Integration",
          "Day 4 - Master RAG Pipeline: Building Efficient RAG Systems",
          "Day 5 - Optimizing RAG Systems: Troubleshooting and Fixing Common Problems",
          "Day 5 - Switching Vector Stores: FAISS vs Chroma in LangChain RAG Pipelines",
          "Day 5 - Demystifying LangChain: Behind-the-Scenes of RAG Pipeline Construction",
          "Day 5 - Debugging RAG: Optimizing Context Retrieval in LangChain",
          "Day 5 - Build Your Personal AI Knowledge Worker: RAG for Productivity Boost"
        ],
        "Week 6: Fine-tuning Frontier Large Language Models with LoRA/QLoRA": [
          "Day 1 - Fine-Tuning Large Language Models: From Inference to Training",
          "Day 1 - Finding and Crafting Datasets for LLM Fine-Tuning: Sources & Techniques",
          "Day 1 - Data Curation Techniques for Fine-Tuning LLMs on Product Descriptions",
          "Day 1 - Optimizing Training Data: Scrubbing Techniques for LLM Fine-Tuning",
          "Day 1 - Evaluating LLM Performance: Model-Centric vs Business-Centric Metrics",
          "Day 2 - LLM Deployment Pipeline: From Business Problem to Production Solution",
          "Day 2 - Prompting, RAG, and Fine-Tuning: When to Use Each Approach",
          "Day 2 - Productionizing LLMs: Best Practices for Deploying AI Models at Scale",
          "Day 2 - Optimizing Large Datasets for Model Training: Data Curation Strategies",
          "Day 2 - How to Create a Balanced Dataset for LLM Training: Curation Techniques",
          "Day 2 - Finalizing Dataset Curation: Analyzing Price-Description Correlations",
          "Day 2 - How to Create and Upload a High-Quality Dataset on HuggingFace",
          "Day 3 - Feature Engineering and Bag of Words: Building ML Baselines for NLP",
          "Day 3 - Baseline Models in ML: Implementing Simple Prediction Functions",
          "Day 3: Feature Engineering Techniques for Amazon Product Price Prediction Models",
          "Day 3 - Optimizing LLM Performance: Advanced Feature Engineering Strategies",
          "Day 3 - Linear Regression for LLM Fine-Tuning: Baseline Model Comparison",
          "Day 3 - Bag of Words NLP: Implementing Count Vectorizer for Text Analysis in ML",
          "Day 3 - Support Vector Regression vs Random Forest: Machine Learning Face-Off",
          "Day 3 - Comparing Traditional ML Models: From Random to Random Forest",
          "Day 4 - Evaluating Frontier Models: Comparing Performance to Baseline Frameworks",
          "Day 4 - Human vs AI: Evaluating Price Prediction Performance in Frontier Models",
          "Day 4 - GPT-4o Mini: Frontier AI Model Evaluation for Price Estimation Tasks",
          "Day 4 - Comparing GPT-4 and Claude: Model Performance in Price Prediction Tasks",
          "Day 4 - Frontier AI Capabilities: LLMs Outperforming Traditional ML Models",
          "Day 5 - Fine-Tuning LLMs with OpenAI: Preparing Data, Training, and Evaluation",
          "Day 5 - How to Prepare JSONL Files for Fine-Tuning Large Language Models (LLMs)",
          "Day 5 - Step-by-Step Guide: Launching GPT Fine-Tuning Jobs with OpenAI API",
          "Day 5 - Fine-Tuning LLMs: Track Training Loss & Progress with Weights & Biases",
          "Day 5 - Evaluating Fine-Tuned LLMs Metrics: Analyzing Training & Validation Loss",
          "Day 5 - LLM Fine-Tuning Challenges: When Model Performance Doesn't Improve",
          "Day 5 - Fine-Tuning Frontier LLMs: Challenges & Best Practices for Optimization"
        ],
        "Fine-tuned open-source model to compete with Frontier in price prediction.": [
          "Day 1 - Mastering Parameter-Efficient Fine-Tuning: LoRa, QLoRA & Hyperparameters",
          "Day 1 - Introduction to LoRA Adaptors: Low-Rank Adaptation Explained",
          "Day 1 - QLoRA: Quantization for Efficient Fine-Tuning of Large Language Models",
          "Day 1 - Optimizing LLMs: R, Alpha, and Target Modules in QLoRA Fine-Tuning",
          "Day 1 - Parameter-Efficient Fine-Tuning: PEFT for LLMs with Hugging Face",
          "Day 1 - How to Quantize LLMs: Reducing Model Size with 8-bit Precision",
          "Day 1: Double Quantization & NF4: Advanced Techniques for 4-Bit LLM Optimization",
          "Day 1 - Exploring PEFT Models: The Role of LoRA Adapters in LLM Fine-Tuning",
          "Day 1 - Model Size Summary: Comparing Quantized and Fine-Tuned Models",
          "Day 2 - How to Choose the Best Base Model for Fine-Tuning Large Language Models",
          "Day 2 - Selecting the Best Base Model: Analyzing HuggingFace's LLM Leaderboard",
          "Day 2 - Exploring Tokenizers: Comparing LLAMA, QWEN, and Other LLM Models",
          "Day 2 - Optimizing LLM Performance: Loading and Tokenizing Llama 3.1 Base Model",
          "Day 2 - Quantization Impact on LLMs: Analyzing Performance Metrics and Errors",
          "Day 2 - Comparing LLMs: GPT-4 vs LLAMA 3.1 in Parameter-Efficient Tuning",
          "Day 3 - QLoRA Hyperparameters: Mastering Fine-Tuning for Large Language Models",
          "Day 3 - Understanding Epochs and Batch Sizes in Model Training",
          "Day 3 - Learning Rate, Gradient Accumulation, and Optimizers Explained",
          "Day 3 - Setting Up the Training Process for Fine-Tuning",
          "Day 3 - Configuring SFTTrainer for 4-Bit Quantized LoRA Fine-Tuning of LLMs",
          "Day 3 - Fine-Tuning LLMs: Launching the Training Process with QLoRA",
          "Day 3 - Monitoring and Managing Training with Weights & Biases",
          "Day 4 - Keeping Training Costs Low: Efficient Fine-Tuning Strategies",
          "Day 4 - Efficient Fine-Tuning: Using Smaller Datasets for QLoRA Training",
          "Day 4 - Visualizing LLM Fine-Tuning Progress with Weights and Biases Charts",
          "Day 4 - Advanced Weights & Biases Tools and Model Saving on Hugging Face",
          "Day 4 - End-to-End LLM Fine-Tuning: From Problem Definition to Trained Model",
          "Day 5 - The Four Steps in LLM Training: From Forward Pass to Optimization",
          "Day 5 - QLoRA Training Process: Forward Pass, Backward Pass and Loss Calculation",
          "Day 5 - Understanding Softmax and Cross-Entropy Loss in Model Training",
          "Day 5 - Monitoring Fine-Tuning: Weights & Biases for LLM Training Analysis",
          "Day 5 - Revisiting the Podium: Comparing Model Performance Metrics",
          "Day 5 - Evaluation of our Proprietary, Fine-Tuned LLM against Business Metrics",
          "Day 5 - Visualization of Results: Did We Beat GPT-4?",
          "Day 5 - Hyperparameter Tuning for LLMs: Improving Model Accuracy with PEFT"
        ],
        "Week 8 - Build Autonomous multi agent system collaborating with models": [
          "Day 1 - From Fine-Tuning to Multi-Agent Systems: Next-Level LLM Engineering",
          "Day 1: Building a Multi-Agent AI Architecture for Automated Deal Finding Systems",
          "Day 1 - Unveiling Modal: Deploying Serverless Models to the Cloud",
          "Day 1 - LLAMA on the Cloud: Running Large Models Efficiently",
          "Day 1 - Building a Serverless AI Pricing API: Step-by-Step Guide with Modal",
          "Day 1 - Multiple Production Models Ahead: Preparing for Advanced RAG Solutions",
          "Day 2 - Implementing Agentic Workflows: Frontier Models and Vector Stores in RAG",
          "Day 2 - Building a Massive Chroma Vector Datastore for Advanced RAG Pipelines",
          "Day 2 - Visualizing Vector Spaces: Advanced RAG Techniques for Data Exploration",
          "Day 2 - 3D Visualization Techniques for RAG: Exploring Vector Embeddings",
          "Day 2 - Finding Similar Products: Building a RAG Pipeline without LangChain",
          "Day 2 - RAG Pipeline Implementation: Enhancing LLMs with Retrieval Techniques",
          "Day 2 - Random Forest Regression: Using Transformers & ML for Price Prediction",
          "Day 2 - Building an Ensemble Model: Combining LLM, RAG, and Random Forest",
          "Day 2 - Wrap-Up: Finalizing Multi-Agent Systems and RAG Integration",
          "Day 3 - Enhancing AI Agents with Structured Outputs: Pydantic & BaseModel Guide",
          "Day 3 - Scraping RSS Feeds: Building an AI-Powered Deal Selection System",
          "Day 3 - Structured Outputs in AI: Implementing GPT-4 for Detailed Deal Selection",
          "Day 3 - Optimizing AI Workflows: Refining Prompts for Accurate Price Recognition",
          "Day 3 - Mastering Autonomous Agents: Designing Multi-Agent AI Workflows",
          "Day 4 - The 5 Hallmarks of Agentic AI: Autonomy, Planning, and Memory",
          "Day 4 - Building an Agentic AI System: Integrating Pushover for Notifications",
          "Day 4 Implementing Agentic AI: Creating a Planning Agent for Automated Workflows",
          "Day 4 - Building an Agent Framework: Connecting LLMs and Python Code",
          "Day 4 - Completing Agentic Workflows: Scaling for Business Applications",
          "Day 5 - Autonomous AI Agents: Building Intelligent Systems Without Human Input",
          "Day 5 - AI Agents with Gradio: Advanced UI Techniques for Autonomous Systems",
          "Day 5 - Finalizing the Gradio UI for Our Agentic AI Solution",
          "Day 5 Enhancing AI Agent UI: Gradio Integration for Real-Time Log Visualization",
          "Day 5 - Analyzing Results: Monitoring Agent Framework Performance",
          "Day 5 - AI Project Retrospective: 8-Week Journey to Becoming an LLM Engineer"
        ]
      },
      "requirements": [
        "Familiarity with Python. This course will not cover Python basics and is completed in Python.",
        "A PC with an internet connection is required. Either Mac (Linux) or Windows.",
        "We recommend that you allocate around $5 for API costs to work with frontier models. However, you can complete the course using open-source models if you prefer."
      ],
      "description": "Mastering Generative AI and LLMs: An 8-Week Hands-On Journey\n\n\nAccelerate your career in AI with practical, real-world projects led by industry veteran Ed Donner. Build advanced Generative AI products, experiment with over 20 groundbreaking models, and master state-of-the-art techniques like RAG, QLoRA, and Agents.\n\n\nWhat you’ll learn\n\n\n• Build advanced Generative AI products using cutting-edge models and frameworks.\n• Experiment with over 20 groundbreaking AI models, including Frontier and Open-Source models.\n• Develop proficiency with platforms like HuggingFace, LangChain, and Gradio.\n• Implement state-of-the-art techniques such as RAG (Retrieval-Augmented Generation), QLoRA fine-tuning, and Agents.\n• Create real-world AI applications, including:\n• A multi-modal customer support assistant that interacts with text, sound, and images.\n• An AI knowledge worker that can answer any question about a company based on its shared drive.\n• An AI programmer that optimizes software, achieving performance improvements of over 60,000 times.\n• An ecommerce application that accurately predicts prices of unseen products.\n• Transition from inference to training, fine-tuning both Frontier and Open-Source models.\n• Deploy AI products to production with polished user interfaces and advanced capabilities.\n• Level up your AI and LLM engineering skills to be at the forefront of the industry.\n\nAbout the Instructor\n\n\nI’m Ed Donner, an entrepreneur and leader in AI and technology with over 20 years of experience. I’ve co-founded and sold my own AI startup, started a second one, and led teams in top-tier financial institutions and startups around the world. I’m passionate about bringing others into this exciting field and helping them become experts at the forefront of the industry.\n\n\nProjects:\nProject 1: AI-powered brochure generator that scrapes and navigates company websites intelligently.\nProject 2: Multi-modal customer support agent for an airline with UI and function-calling.\nProject 3: Tool that creates meeting minutes and action items from audio using both open- and closed-source models.\nProject 4: AI that converts Python code to optimized C++, boosting performance by 60,000x!\nProject 5: AI knowledge-worker using RAG to become an expert on all company-related matters.\nProject 6: Capstone Part A – Predict product prices from short descriptions using Frontier models.\nProject 7: Capstone Part B – Fine-tuned open-source model to compete with Frontier in price prediction.\nProject 8: Capstone Part C – Autonomous agent system collaborating with models to spot deals and notify you of special bargains.\n\n\nWhy This Course?\n\n\n• Hands-On Learning: The best way to learn is by doing. You’ll engage in practical exercises, building real-world AI applications that deliver stunning results.\n• Cutting-Edge Techniques: Stay ahead of the curve by learning the latest frameworks and techniques, including RAG, QLoRA, and Agents.\n• Accessible Content: Designed for learners at all levels. Step-by-step instructions, practical exercises, cheat sheets, and plenty of resources are provided.\n• No Advanced Math Required: The course focuses on practical application. No calculus or linear algebra is needed to master LLM engineering.\n\n\nCourse Structure\n\n\nWeek 1: Foundations and First Projects\n\n\n• Dive into the fundamentals of Transformers.\n• Experiment with six leading Frontier Models.\n• Build your first business Gen AI product that scrapes the web, makes decisions, and creates formatted sales brochures.\n\n\nWeek 2: Frontier APIs and Customer Service Chatbots\n\n\n• Explore Frontier APIs and interact with three leading models.\n• Develop a customer service chatbot with a sharp UI that can interact with text, images, audio, and utilize tools or agents.\n\n\nWeek 3: Embracing Open-Source Models\n\n\n• Discover the world of Open-Source models using HuggingFace.\n• Tackle 10 common Gen AI use cases, from translation to image generation.\n• Build a product to generate meeting minutes and action items from recordings.\n\n\nWeek 4: LLM Selection and Code Generation\n\n\n• Understand the differences between LLMs and how to select the best one for your business tasks.\n• Use LLMs to generate code and build a product that translates code from Python to C++, achieving performance improvements of over 60,000 times.\n\n\nWeek 5: Retrieval-Augmented Generation (RAG)\n\n\n• Master RAG to improve the accuracy of your solutions.\n• Become proficient with vector embeddings and explore vectors in popular open-source vector datastores.\n• Build a full business solution similar to real products on the market today.\n\n\nWeek 6: Transitioning to Training\n\n\n• Move from inference to training.\n• Fine-tune a Frontier model to solve a real business problem.\n• Build your own specialized model, marking a significant milestone in your AI journey.\n\n\nWeek 7: Advanced Training Techniques\n\n\n• Dive into advanced training techniques like QLoRA fine-tuning.\n• Train an open-source model to outperform Frontier models for specific tasks.\n• Tackle challenging projects that push your skills to the next level.\n\n\nWeek 8: Deployment and Finalization\n\n\n• Deploy your commercial product to production with a polished UI.\n• Enhance capabilities using Agents.\n• Deliver your first productionized, agentized, fine-tuned LLM model.\n• Celebrate your mastery of AI and LLM engineering, ready for a new phase in your career.",
      "target_audience": [
        "Aspiring AI engineers and data scientists eager to break into the field of Generative AI and LLMs.",
        "Professionals looking to upskill and stay competitive in the rapidly evolving AI landscape.",
        "Developers interested in building advanced AI applications with practical, hands-on experience.",
        "Individuals seeking a career transition or aiming to enhance productivity through LLM-built frameworks."
      ]
    },
    {
      "title": "The Complete Neural Networks Bootcamp: Theory, Applications",
      "url": "https://www.udemy.com/course/the-complete-neural-networks-bootcamp-theory-applications/",
      "bio": "Deep Learning and Neural Networks Theory and Applications with PyTorch! Including Transformers, BERT and GPT!",
      "objectives": [
        "Understand How Neural Networks Work (Theory and Applications)",
        "Understand How Convolutional Networks Work (Theory and Applications)",
        "Understand How Recurrent Networks and LSTMs work (Theory and Applications)",
        "Learn how to use PyTorch in depth",
        "Understand how the Backpropagation algorithm works",
        "Understand Loss Functions in Neural Networks",
        "Understand Weight Initialization and Regularization Techniques",
        "Code-up a Neural Network from Scratch using Numpy",
        "Apply Transfer Learning to CNNs",
        "CNN Visualization",
        "Learn the CNN Architectures that are widely used nowadays",
        "Understand Residual Networks in Depth",
        "Understand YOLO Object Detection in Depth",
        "Visualize the Learning Process of Neural Networks",
        "Learn how to Save and Load trained models",
        "Learn Sequence Modeling with Attention Mechanisms",
        "Build a Chatbot with Attention",
        "Transformers",
        "Build a Chatbot with Transformers",
        "BERT",
        "Build an Image Captioning Model"
      ],
      "course_content": {},
      "requirements": [
        "Some Basic Python Expreience is preferable",
        "Some High School Mathematics"
      ],
      "description": "This course is a comprehensive guide to Deep Learning and Neural Networks. The theories are explained in depth and in a friendly manner. After that, we'll have the hands-on session, where we will be learning how to code Neural Networks in PyTorch, a very advanced and powerful deep learning framework!\nThe course includes the following Sections:\n--------------------------------------------------------------------------------------------------------\nSection 1 - How Neural Networks and Backpropagation Works\nIn this section, you will deeply understand the theories of how neural networks  and the backpropagation algorithm works, in a friendly manner. We will walk through an example and do the calculations step-by-step. We will also discuss the activation functions used in Neural Networks, with their advantages and disadvantages!\nSection 2 - Loss Functions\nIn this section, we will introduce the famous loss functions that are used in Deep Learning and Neural Networks. We will walk through when to use them and how they work.\nSection 3 - Optimization\nIn this section, we will discuss the optimization techniques used in Neural Networks, to reach the optimal Point, including Gradient Descent, Stochastic Gradient Descent, Momentum, RMSProp, Adam, AMSGrad, Weight Decay and Decoupling Weight Decay, LR Scheduler and others.\nSection 4 - Weight Initialization\nIn this section,we will introduce you to the concepts of weight initialization in neural networks, and we will discuss some techniques of weights initialization including Xavier initialization and He norm initialization.\nSection 5 - Regularization Techniques\nIn this section, we will introduce you to the regularization techniques in neural networks. We will first introduce overfitting and then introduce how to prevent overfitting by using regularization techniques, inclusing L1, L2 and Dropout. We'll also talk about normalization as well as batch normalization and Layer Normalization.\nSection 6- Introduction to PyTorch\nIn this section, we will introduce the deep learning framework we'll be using through this course, which is PyTorch. We will show you how to install it, how it works and why it's special, and then we will code some PyTorch tensors and show you some operations on tensors, as well as show you Autograd in code!\nSection 7 - Practical Neural Networks in PyTorch - Application 1\nIn this section, you will apply what you've learned to build a Feed Forward Neural Network to classify handwritten digits. This is the first application of Feed Forward Networks we will be showing.\nSection 8 - Practical Neural Networks in PyTorch - Application 2\nIn this section, we will build a feed forward Neural Network to classify weather a person has diabetes or not. We will train the network on a large dataset of diabetes!\nSection 9 - Visualize the Learning Process\nIn this section, we will visualize how neural networks are learning, and how good they are at separating non-linear data!\nSection 10 - Implementing a Neural Network from Scratch with Python and Numpy\nIn this section, we will understand and code up a neural network without using any deep learning library (from scratch using only python and numpy). This is necessary to understand how the underlying structure works.\nSection 11 - Convolutional Neural Networks\nIn this section, we will introduce you to Convolutional Networks that are used for images. We will show you first the relationship to Feed Forward Networks, and then we will introduce you the concepts of Convolutional Networks one by one!\nSection 12 - Practical Convolutional Networks in PyTorch\nIn this section, we will apply Convolutional Networks to classify handwritten digits. This is the first application of CNNs we will do.\nSection 13- Deeper into CNN: Improving and Plotting\nIn this section, we will improve the CNN that we built in the previous section, as well show you how to plot the results of training and testing! Moreover, we will show you how to classify your own handwritten images through the network!\nSection 14 - CNN Architectures\nIn this section, we will introduce the CNN architectures that are widely used in all deep learning applications. These architectures are: AlexNet, VGG net, Inception Net, Residual Networks and Densely Connected Networks. We will also discuss some object detection architectures.\nSection 15- Residual Networks\nIn this section, we will dive deep into the details and theory of Residual Networks, and then we'll build a Residual Network in PyTorch from scratch!\nSection 16 - Transfer Learning in PyTorch - Image Classification\nIn this section, we will apply transfer learning on a Residual Network, to classify ants and bees. We will also show you how to use your own dataset and apply image augmentation. After completing this section, you will be able to classify any images you want!\nSection 17- Convolutional Networks Visualization\nIn this section, we will visualize what the neural networks output, and what they are really learning. We will observe the feature maps of the network of every layer!\nSection 18 - YOLO Object Detection (Theory)\nIn this section, we will learn one of the most famous Object Detection Frameworks: YOLO!! This section covers the theory of YOLO in depth.\nSection 19 - Autoencoders and Variational Autoencoders\nIn this section, we will cover Autoencoders and Denoising Autoencoders. We will then see the problem they face and learn how to mitigate it with Variational Autoencoders.\nSection 20 - Recurrent Neural Networks\nIn this section, we will introduce you to Recurrent Neural Networks and all their concepts. We will then discuss the Backpropagation through  time, the vanishing gradient problem, and finally about Long Short Term Memory (LSTM) that solved the problems RNN suffered from.\nSection 21 - Word Embeddings\nIn this section, we will discuss how words are represented as features. We will then show you some Word Embedding models.  We will also show you how to implement word embedding in PyTorch!\nSection 22 - Practical Recurrent Networks in PyTorch\nIn this section, we will apply Recurrent Neural Networks using LSTMs in PyTorch to generate text similar to the story of Alice in Wonderland! You can just replace the story with any other text you want, and the RNN will be able to generate text similar to it!\nSection 23 - Sequence Modelling\nIn this section, we will learn about Sequence-to-Sequence Modelling. We will see how Seq2Seq models work and where they are applied. We'll also talk about Attention mechanisms and see how they work.\nSection 24 - Practical Sequence Modelling in PyTorch - Build a Chatbot\nIn this section, we will apply what we learned about sequence modeling and build a Chatbot with Attention Mechanism.\nSection 25 - Saving and Loading Models\nIn this section, we will show you how to save and load models in PyTorch, so you can use these models either for later testing, or for resuming training!\nSection 26 - Transformers\nIn this section, we will cover the Transformer, which is the current state-of-art model for NLP and language modeling tasks. We will go through each component of a transformer.\nSection 27 - Build a Chatbot with Transformers\nIn this section, we will implement all what we learned in the previous section to build a Chatbot using Transformers.",
      "target_audience": [
        "Anyone who in interested in learning about Neural Networks and Deep Learning"
      ]
    },
    {
      "title": "Python for Deep Learning: Build Neural Networks in Python",
      "url": "https://www.udemy.com/course/deep-learning-basics-with-python/",
      "bio": "Complete Deep Learning Course to Master Data science, Tensorflow, Artificial Intelligence, and Neural Networks",
      "objectives": [
        "Learn the fundamentals of the Deep Learning theory",
        "Learn how to use Deep Learning in Python",
        "Learn how to use different frameworks in Python to solve real-world problems using deep learning and artificial intelligence",
        "Make predictions using linear regression, polynomial regression, and multivariate regression",
        "Build artificial neural networks with Tensorflow and Keras"
      ],
      "course_content": {},
      "requirements": [
        "Experience with the basics of coding in Python",
        "Basic mathematical skills",
        "Readiness, flexibility, and passion for learning"
      ],
      "description": "Python is famed as one of the best programming languages for its flexibility. It works in almost all fields, from web development to developing financial applications. However, it's no secret that Python’s best application is in deep learning and artificial intelligence tasks.\nWhile Python makes deep learning easy, it will still be quite frustrating for someone with no knowledge of how machine learning works in the first place.\nIf you know the basics of Python and you have a drive for deep learning, this course is designed for you. This course will help you learn how to create programs that take data input and automate feature extraction, simplifying real-world tasks for humans.\nThere are hundreds of machine learning resources available on the internet. However, you're at risk of learning unnecessary lessons if you don't filter what you learn. While creating this course, we've helped with filtering to isolate the essential basics you'll need in your deep learning journey.\nIt is a fundamentals course that’s great for both beginners and experts alike. If you’re on the lookout for a course that starts from the basics and works up to the advanced topics, this is the best course for you.\nIt only teaches what you need to get started in deep learning with no fluff. While this helps to keep the course pretty concise, it’s about everything you need to get started with the topic.",
      "target_audience": [
        "Programmers who are looking to add deep learning to their skillset",
        "Professional mathematicians willing to learn how to analyze data programmatically",
        "Any Python programming enthusiast willing to add deep learning proficiency to their portfolio"
      ]
    },
    {
      "title": "Learn Streamlit Python",
      "url": "https://www.udemy.com/course/learn-streamlit-python/",
      "bio": "Create Beautiful Data Apps and Machine Learning Web Apps In Python Faster with Streamlit",
      "objectives": [
        "Learn the basics of Streamlit Framework",
        "Use Streamlit to create Machine Learning Web Apps and Data Apps",
        "Deploying Streamlit Python Web Applications"
      ],
      "course_content": {},
      "requirements": [
        "Basic understanding of Python programming language",
        "Understand Machine Learning Concepts in Python",
        "Determination and Desire to Learn New Things"
      ],
      "description": "Are you having difficulties trying to build web applications for your data science projects? Do you spend more time trying to create a simple MVP app with your data to show your clients and others? Then let me introduce you to Streamlit - a python framework for building web apps.\n\n\nWelcome to the coolest  online resource for learning how to create Data Science Apps and Machine Learning Web Apps using the\nawesome Streamlit Framework and Python.\nThis course will teach you Streamlit - the python framework that saves you from spending days and weeks in creating\ndata science and machine learning web applications.\n\n\nIn this course we will cover everything you need to know concerning streamlit such as\nFundamentals and the Basics of Streamlit ;\n- Working with Text\n- Working with Widgets (Buttons,Sliders,\n- Displaying Data\n- Displaying Charts and Plots\n- Working with Media Files (Audio,Images,Video)\n- Streamlit Layouts\n- File Uploads\n- Streamlit Static Components\nCreating cool data visualization apps\nHow to Build A Full Web Application with Streamlit\n\n\nBy the end of this exciting course you will be able to\nBuild data science apps in hours not days\nProductionized your machine learning models into web apps using streamlit\nBuild some cools and fun data apps\nDeploy your streamlit apps using Docker,Heroku,Streamlit Share and more\n\n\nJoin us as we explore the world of building Data and ML Apps.\nSee you in the Course,Stay blessed.\n\n\nTips for getting through the course\nPlease write or code along with us do not just watch,this will enhance your understanding.\nYou can regulate the speed and audio of the video as you wish,preferably at -0.75x if the speed is too fast for you.\nSuggested Prerequisites is understanding of Python\nThis course is about Streamlit an ML Framework to create data apps in hours not weeks. We  will try our best to cover some concepts for the beginner and the pro .",
      "target_audience": [
        "Beginner Python Developers curious about Streamlit",
        "Data Scientist and ML Engineers who want to productionized their Models faster"
      ]
    },
    {
      "title": "Build Your own Self Driving Car | Deep Learning, OpenCV, C++",
      "url": "https://www.udemy.com/course/selfdrivingcar/",
      "bio": "Learn Raspberry Pi, Arduino UNO, Image Processing and Neural Networks (Machine Learning) for any Embedded IOT Project",
      "objectives": [
        "Learn How to Setup Raspberry Pi 3 for any IOT Project",
        "Learn How to Setup Arduino UNO as a Slave micro-controller for any IOT Project",
        "Learn Image Processing using OpenCV4 for any Platform",
        "Learn Machine Learning & Train your own Image Classifier",
        "Learn How to Troubleshoot any Hardware & Software issues",
        "Most Important!! Learn to Design Embedded Product totally from scratch"
      ],
      "course_content": {},
      "requirements": [
        "Basic Understanding of C or C++",
        "Basic Understanding of Digital Logic",
        "Basic Understanding of Soldering and Breadboard Prototyping"
      ],
      "description": "\"Machine Learning will change the lives of all of us. What is Machine Learning? It’s behind what makes self-driving cars a reality\"\nThis unique course is a complete walk-through process to Design, Build and Program a Embedded IOT Project (Self driving Car). Everything is discussed with details and clear explanation. Whole Project is divided into 2 parts.\n\n\n(Course - 1)\n1. Learn to design complete hardware for self driving car\na. Learn to setup Master device ( Raspberry Pi ) for any project\nb. Learn to setup Slave device ( Arduino UNO ) for any project\nc. Learn to Establish Communication link between Master and Slave device\n2. Learn Image Processing using OpenCV4\n3. Learn to driver robot on road lanes\n\n\n(Course - 2)\n1. Learn Essentials of Machine Learning\n2. Learn to train your own cascade classifier to detect Stop Sign, Traffic Lights and any Object\n3. Learn to design LED Dynamic Turn Indicators\n4. Create your GitHub Repository\n\n\nMachine learning is important because it gives enterprises a view of trends in customer behavior and business operational patterns, as well as supports the development of new products. Many of today's leading companies, such as Facebook, Google and Uber, make machine learning a central part of their operations. Machine learning has become a significant competitive differentiator for many companies",
      "target_audience": [
        "College or University student from Electronics/Electrical or Computer Engineering or relevant Diploma",
        "Hobbyist interested in Machine Learning & Image Processing",
        "Anybody Who wants to create Embedded IOT Project"
      ]
    },
    {
      "title": "Machine Learning in JavaScript with TensorFlow.js",
      "url": "https://www.udemy.com/course/machine-learning-in-javascript-with-tensorflow-js/",
      "bio": "Master machine learning with JavaScript and TensorFlowJS. Add artificial intelligence to websites, Node.js and web apps!",
      "objectives": [
        "Machine Learning in Javascript and TensorFlowJS 4",
        "Deep Learning and Neural Network concepts",
        "Why TensorFlow for JavaScript is a game changer",
        "Defining machine learning models",
        "How to install and run TensorFlowJS 4",
        "How TensorFlowJS 4 is optimised",
        "Training machine learning models",
        "Data preparation for machine learning",
        "How to make accurate predictions",
        "Linear regression",
        "Binary classification",
        "Multi-class classification",
        "Heatmap visualisation",
        "Scatter-plot visualisation",
        "Importing and normalising data",
        "How to manage memory in TensorFlowJS 4",
        "Tensor mathematics",
        "Saving machine learning models",
        "Inputting and outputting using a web browser",
        "Javascript and machine learning integration",
        "Shuffling, and splitting data",
        "In-depth labs for practical development"
      ],
      "course_content": {
        "Introduction": [
          "Introduction: What is TensorFlow.js?",
          "Course Overview",
          "Machine Learning Concepts",
          "Overview of Artificial Neural Networks",
          "Lab: TensorFlow Playground",
          "TensorFlow.js and Machine Learning",
          "Summary"
        ],
        "Installing and running TensorFlow.js": [
          "TensorFlow.js environments",
          "Running TensorFlow.js in the browser",
          "WebGL optimisations in TensorFlow.js",
          "Running TensorFlow.js on Node.js",
          "New: TensorFlow.js for React Native",
          "Review",
          "Lab: Install and run TensorFlow.js in the browser",
          "Lab: Install and run TensorFlow.js on Node.js",
          "TensorFlow environments and installation",
          "Summary"
        ],
        "TensorFlow.js Core Concepts": [
          "TensorFlow.js APIs",
          "What is a Tensor?",
          "Tensor Math Operations & Ops API",
          "Memory Management in TensorFlow.js",
          "Review",
          "Lab: Tensor Math and Memory Management",
          "TensorFlow.js math and memory",
          "Summary"
        ],
        "Data Preparation with TensorFlow.js": [
          "Linear Regression",
          "Reading data from CSV",
          "Visualising the data",
          "Preparing Features and Labels",
          "Normalisation with TensorFlow.js",
          "Splitting into Training and Testing data",
          "Review",
          "Lab: Prepare the Data",
          "Data Preparation",
          "Summary"
        ],
        "Defining a model": [
          "Introduction to Layers API",
          "Creating Layers in TensorFlow.js",
          "Inspecting a TensorFlow.js model",
          "Compiling the model",
          "Review",
          "Lab: Creating a Model",
          "TensorFlow.js Models",
          "Summary"
        ],
        "Training and Testing in TensorFlow.js": [
          "Introduction to Training and Testing",
          "Training with model.fit",
          "Visualising loss with tfjs-vis",
          "Testing with model.evaluate",
          "Training and testing: review & lab",
          "Lab: TensorFlow.js Training and Testing",
          "Training and Testing",
          "Summary"
        ],
        "TensorFlow.js Prediction": [
          "Integrating TensorFlow.js with a UI",
          "Saving and loading a model",
          "Making Predictions",
          "Visualising Predictions",
          "Non-linear Regression",
          "Prediction: review & labs",
          "Lab: TensorFlow.js predictions",
          "TensorFlow.js predictions",
          "Lab: Beyond Linear Regression",
          "Lab (optional): Training without Layers API",
          "Summary"
        ],
        "Binary Classification": [
          "Introduction: Binary Classification",
          "Visualising Classification Data",
          "Preparing Multiple Features",
          "Binary Classification Model",
          "Visualising Classification with Heatmaps",
          "Binary Classification Predictions",
          "Binary Classification: Review & Lab",
          "Lab: TensorFlow.js Binary Classification",
          "Quiz: Binary Classification",
          "Summary"
        ],
        "Multi-class Classification": [
          "Introduction: Multi-class Classification",
          "One hot encoding",
          "Multi-class classification model",
          "Visualising Multi-class Predictions",
          "Multi-class prediction",
          "Multi-class Classification: Review & Lab",
          "Lab: TensorFlow.js Multi-class Classification",
          "Multi-class Classification",
          "Summary"
        ],
        "Conclusion & Next Steps": [
          "Course Review",
          "Next steps with TensorFlow.js",
          "Resources for going deeper with TensorFlow.js"
        ]
      },
      "requirements": [
        "Javascript basics",
        "Some high school maths (but we give links if you need a refresher!)"
      ],
      "description": "Interested in using Machine Learning in JavaScript applications and websites? Then this course is for you!\nThis is the tutorial you've been looking for to become a modern JavaScript machine learning master in 2024. It doesn’t just cover the basics, by the end of the course you will have advanced machine learning knowledge you can use on you resume. From absolute zero knowledge to master - join the TensorFlow.js revolution.\nThis course has been designed by a specialist team of software developers who are passionate about using JavaScript with Machine Learning. We will guide you through complex topics in a practical way, and reinforce learning with in-depth labs and quizzes.\nThroughout the course we use house price data to ask ever more complicated questions; “can you predict the value of this house?”, “can you tell me if this house has a waterfront?”, “can you classify it as having 1, 2 or 3+ bedrooms?”. Each example builds on the one before it, to reinforce learning in easy and steady steps.\nMachine Learning in TensorFlow.js provides you with all the benefits of TensorFlow, but without the need for Python. This is demonstrated using web based examples, stunning visualisations and custom website components.\nThis course is fun and engaging, with Machine Learning learning outcomes provided in bitesize topics:\nPart 1 - Introduction to TensorFlow.js\nPart 2 - Installing and running TensorFlow.js\nPart 3 - TensorFlow.js Core Concepts\nPart 4 - Data Preparation with TensorFlow.js\nPart 5 - Defining a model\nPart 6 - Training and Testing in TensorFlow.js\nPart 7 - TensorFlow.js Prediction\nPart 8 - Binary Classification\nPart 9 - Multi-class Classification\nPart 10 - Conclusion & Next Steps\nAs a bonus, for every student, we provide you with JavaScript and HTML code templates that you can download and use on your own projects.",
      "target_audience": [
        "Anyone who wants to start using machine learning in their apps and websites using Javascript"
      ]
    },
    {
      "title": "AWS SageMaker Practical for Beginners | Build 6 Projects",
      "url": "https://www.udemy.com/course/practical-aws-sagemaker-6-real-world-case-studies/",
      "bio": "Master AWS SageMaker Algorithms (Linear Learner, XGBoost, PCA, Image Classification) & Learn SageMaker Studio & AutoML",
      "objectives": [
        "Train and deploy AI/ML models using AWS SageMaker",
        "Optimize model parameters using hyperparameters optimization search.",
        "Develop, train, test and deploy linear regression model to make predictions.",
        "Deploy production level multi-polynomial regression model to predict store sales based on the given features.",
        "Develop a deploy deep learning-based model to perform image classification.",
        "Develop time series forecasting models to predict future product prices using DeepAR.",
        "Develop and deploy sentiment analysis model using SageMaker.",
        "Deploy trained NLP model and interact/make predictions using secure API.",
        "Train and evaluate Object Detection model using SageMaker built-in algorithms."
      ],
      "course_content": {},
      "requirements": [
        "Basic knowledge of programming",
        "Basic knowledge in AWS",
        "Basic knowledge in machine learning"
      ],
      "description": "# Update 22/04/2021 - Added a new case study on AWS SageMaker Autopilot.\n# Update 23/04/2021 - Updated code scripts and addressed Q&A bugs.\nMachine and deep learning are the hottest topics in tech! Diverse fields have adopted ML and DL techniques, from banking to healthcare, transportation to technology.\nAWS is one of the most widely used ML cloud computing platforms worldwide – several Fortune 500 companies depend on AWS for their business operations.\nSageMaker is a fully managed service within AWS that allows data scientists and AI practitioners to train, test, and deploy AI/ML models quickly and efficiently.\nIn this course, students will learn how to create AI/ML models using AWS SageMaker.\nProjects will cover various topics from business, healthcare, and Tech. In this course, students will be able to master many topics in a practical way such as: (1) Data Engineering and Feature Engineering, (2) AI/ML Models selection, (3) Appropriate AWS SageMaker Algorithm selection to solve business problem, (4) AI/ML models building, training, and deployment, (5) Model optimization and Hyper-parameters tuning.\n\n\nThe course covers many topics such as data engineering, AWS services and algorithms, and machine/deep learning basics in a practical way:\nData engineering: Data types, key python libraries (pandas, Numpy, scikit Learn, MatplotLib, and Seaborn), data distributions and feature engineering (imputation, binning, encoding, and normalization).\nAWS services and algorithms: Amazon SageMaker, Linear Learner (Regression/Classification), Amazon S3 Storage services, gradient boosted trees (XGBoost), image classification, principal component analysis (PCA), SageMaker Studio and AutoML.\nMachine and deep learning basics: Types of artificial neural networks (ANNs) such as feedforward ANNs, convolutional neural networks (CNNs), activation functions (sigmoid, RELU and hyperbolic tangent), machine learning training strategies (supervised/ unsupervised), gradient descent algorithm, learning rate, backpropagation, bias, variance, bias-variance trade-off, regularization (L1 and L2), overfitting, dropout, feature detectors, pooling, batch normalization, vanishing gradient problem, confusion matrix, precision, recall, F1-score, root mean squared error (RMSE), ensemble learning, decision trees, and random forest.\n\n\nWe teach SageMaker’s vast range of ML and DL tools with practice-led projects. Delve into:\nProject #1: Train, test and deploy simple regression model to predict employees’ salary using AWS SageMaker Linear Learner\nProject #2: Train, test and deploy a multiple linear regression machine learning model to predict medical insurance premium.\nProject #3: Train, test and deploy a model to predict retail store sales using XGboost regression and optimize model hyperparameters using SageMaker Hyperparameters tuning tool.\nProject #4: Perform Dimensionality reduction Using SageMaker built-in PCA algorithm and build a classifier model to predict cardiovascular disease using XGBoost Classification model.\nProject #5: Develop a traffic sign classifier model using Sagemaker and Tensorflow.\nProject #6: Deep Dive in AWS SageMaker Studio, AutoML, and model debugging.\nThe course is targeted towards beginner developers and data scientists wanting to get fundamental understanding of AWS SageMaker and solve real world challenging problems. Basic knowledge of Machine Learning, python programming and AWS cloud is recommended. Here’s a list of who is this course for:\nBeginners Data Science wanting to advance their careers and build their portfolio.\nSeasoned consultants wanting to transform businesses by leveraging AI/ML using SageMaker.\nTech enthusiasts who are passionate and new to Data science & AI and want to gain practical experience using AWS SageMaker.\nEnroll today and I look forward to seeing you inside.",
      "target_audience": [
        "AI practitioners",
        "Aspiring data scientists",
        "Tech enthusiasts",
        "Data science consultants"
      ]
    },
    {
      "title": "The Data Visualization Course: Excel, Tableau, Python, R",
      "url": "https://www.udemy.com/course/the-complete-data-visualization-course/",
      "bio": "Data visualization in Excel, Tableau, Python, and R. Create stunning charts and learn the most in-demand skills in 2020",
      "objectives": [
        "Master data visualization",
        "Learn how to label and style a graph",
        "Interpret data",
        "Select the right type of chart",
        "Discover findings through data visualization",
        "Create stunning visualizations",
        "How to create a Bar chart",
        "How to create a Pie chart",
        "How to create a Stacked area chart",
        "How to create a Line chart",
        "How to create a Histogram",
        "How to create a Scatter plot",
        "How to create a Scatter plot with a trendline (regression plot)"
      ],
      "course_content": {},
      "requirements": [
        "No prior experience is required. We will start from the very basics",
        "You’ll need to install at least one of the 4 software (Excel, Tableau, Python, or R). We will show you how to do that step by step"
      ],
      "description": "Do you want to learn how to create a wide variety of graphs and charts?\nDo you wish to improve your data interpretation skills?\nDoes your workplace require data visualization proficiency?\nYes, yes, and most likely yes.\nThe Complete Data Visualization Course is here for you with TEMPLATES for all the common types of charts and graphs in Excel, Tableau, Python, and R!\nThis is four different data visualization courses in one!\nNo matter your preferred environment—Excel, Tableau, Python, or R—this course will enable you to start creating beautiful data visualizations in no time!\nYou will learn not only how to create charts but also how to label, style, and interpret them. Moreover, you will receive immediate access to all templates used in the lessons. Simply download the course files, replace the data set, and prepare to amaze your audience!\nGraphs and charts included in The Complete Data Visualization Course:\nBar chart\nPie chart\nStacked area chart\nLine chart\nHistogram\nScatter plot\nScatter plot with a trendline (regression plot)\nWe live in the age of data. Being able to gather good data, preprocess it, and model it is crucial.\nThere is nothing more important than being able to interpret that data.\nData visualization allows us to achieve just that. It is the face of data. Many people look at the data and see nothing. The reason for that is that they are not creating good visualizations. Or even worse – they are creating nice graphs but cannot interpret them accurately.\nThis course will tackle both of these problems. We will make sure you can confidently create any chart that you need to provide a meaningful visualization of the data you are working with. Not only that – you will be able to label and style data visualizations to achieve a ready-for-presentation graph. Furthermore, through this course, you will learn how to interpret different types of charts and when to use them. We will provide examples of great charts as well as terrible charts. We will spare no effort in transforming you into the key person for data visualizations in any team.\nWe are confident that by the time you complete this course, creating and understanding data visualizations will be a piece of cake for you!\nWhat makes this course different from the rest of the Data Visualization courses out there?\n4 different data visualization courses in 1 course – we cover Excel, Tableau, Python and R\nReady-to-use templates for all charts included in the course\nHigh-quality production – Full HD and HD video and animations crafted professionally by our experienced team of visual artists\nKnowledgeable instructor team with experience in teaching on Udemy\nComplete training – we will cover all common graphs and charts you need to become an invaluable member of your data science team\nExcellent support - if you don’t understand a concept or you simply want to drop us a line, you’ll receive an answer within 1 business day\nDynamic - we don’t want to waste your time! The instructor sets a very good pace throughout the whole course\nWhy do you need these skills?\nSalary/Income – careers in the field of data science are some of the most popular in the corporate world today. Literally every company nowadays needs to visualize their data, therefore the data viz position is very well paid\nPromotions – being the person who creates the data visualizations makes you the bridge between the data and the decision-makers; all stakeholders in the company will value your input, ensuring your spot on the strategy team\nSecure future – being able to understand data in today’s world is the most important skill to possess and it is only developed by seeing, visualizing and interpreting many datasets\nPlease bear in mind that the course comes with Udemy’s 30-day money-back guarantee. And why not give such a guarantee? We are certain this course will provide a ton of value for you.\nLet's start learning together now!",
      "target_audience": [
        "Ideal for beginners",
        "Anyone who wants to start a career in data science or business intelligence",
        "People who want to level-up their career with data visualization skills",
        "Anyone who wants to add value to their company"
      ]
    },
    {
      "title": "Data Science A-Z: Hands-On Exercises & ChatGPT Prize [2025]",
      "url": "https://www.udemy.com/course/datascience/",
      "bio": "Learn Data Science step by step through real Analytics examples. Data Mining, Modeling, Tableau Visualization and more!",
      "objectives": [
        "Successfully perform all steps in a complex Data Science project",
        "Create Basic Tableau Visualisations",
        "Perform Data Mining in Tableau",
        "Understand how to apply the Chi-Squared statistical test",
        "Apply Ordinary Least Squares method to Create Linear Regressions",
        "Assess R-Squared for all types of models",
        "Assess the Adjusted R-Squared for all types of models",
        "Create a Simple Linear Regression (SLR)",
        "Create a Multiple Linear Regression (MLR)",
        "Create Dummy Variables",
        "Interpret coefficients of an MLR",
        "Read statistical software output for created models",
        "Use Backward Elimination, Forward Selection, and Bidirectional Elimination methods to create statistical models",
        "Create a Logistic Regression",
        "Intuitively understand a Logistic Regression",
        "Operate with False Positives and False Negatives and know the difference",
        "Read a Confusion Matrix",
        "Create a Robust Geodemographic Segmentation Model",
        "Transform independent variables for modelling purposes",
        "Derive new independent variables for modelling purposes",
        "Check for multicollinearity using VIF and the correlation matrix",
        "Understand the intuition of multicollinearity",
        "Apply the Cumulative Accuracy Profile (CAP) to assess models",
        "Build the CAP curve in Excel",
        "Use Training and Test data to build robust models",
        "Derive insights from the CAP curve",
        "Understand the Odds Ratio",
        "Derive business insights from the coefficients of a logistic regression",
        "Understand what model deterioration actually looks like",
        "Apply three levels of model maintenance to prevent model deterioration",
        "Install and navigate SQL Server",
        "Install and navigate Microsoft Visual Studio Shell",
        "Clean data and look for anomalies",
        "Use SQL Server Integration Services (SSIS) to upload data into a database",
        "Create Conditional Splits in SSIS",
        "Deal with Text Qualifier errors in RAW data",
        "Create Scripts in SQL",
        "Apply SQL to Data Science projects",
        "Create stored procedures in SQL",
        "Present Data Science projects to stakeholders"
      ],
      "course_content": {},
      "requirements": [
        "Only a passion for success",
        "All software used in this course is either available for Free or as a Demo version"
      ],
      "description": "Extremely Hands-On... Incredibly Practical... Unbelievably Real!\nThis is not one of those fluffy classes where everything works out just the way it should and your training is smooth sailing. This course throws you into the deep end.\nIn this course you WILL experience firsthand all of the PAIN a Data Scientist goes through on a daily basis. Corrupt data, anomalies, irregularities - you name it!\nThis course will give you a full overview of the Data Science journey. Upon completing this course you will know:\nHow to clean and prepare your data for analysis\nHow to perform basic visualisation of your data\nHow to model your data\nHow to curve-fit your data\nAnd finally, how to present your findings and wow the audience\nThis course will give you so much practical exercises that real world will seem like a piece of cake when you graduate this class. This course has homework exercises that are so thought provoking and challenging that you will want to cry... But you won't give up! You will crush it. In this course you will develop a good understanding of the following tools:\nSQL\nSSIS\nTableau\nGretl\nThis course has pre-planned pathways. Using these pathways you can navigate the course and combine sections into YOUR OWN journey that will get you the skills that YOU need.\nOr you can do the whole course and set yourself up for an incredible career in Data Science.\nThe choice is yours. Join the class and start learning today!\nSee you inside,\nSincerely,\nKirill Eremenko",
      "target_audience": [
        "Anybody with an interest in Data Science",
        "Anybody who wants to improve their data mining skills",
        "Anybody who wants to improve their statistical modelling skills",
        "Anybody who wants to improve their data preparation skills",
        "Anybody who wants to improve their Data Science presentation skills"
      ]
    },
    {
      "title": "4x1 Data Management/Governance/Security/Ethics Masterclass",
      "url": "https://www.udemy.com/course/data-management-and-governance/",
      "bio": "You'll learn about data literacy, Data Quality operations, Data Governance policies and Data Security/Privacy controls.",
      "objectives": [
        "You'll learn about the frequent data disciplines (Data Management, Data Governance, Data Stewardship, Data Science), their differences and nuances.",
        "You'll learn about the most frequent types of Data Quality (DQ) operations, including profiling, parsing, cleansing, standardisation, record merging, and others",
        "You'll learn about the levels of data sophistication in an organisation, and the usual DM/DG progression from projects to programs to centralised processes.",
        "You'll learn about the 4 major types of data (master data, reference data, transactional data and metadata), as well as what each means, and how they intersect."
      ],
      "course_content": {
        "Course Introduction": [
          "Masterclass Intro",
          "Useful Information"
        ],
        "Data Literacy and Considerations": [
          "Module Intro",
          "4 Key Principles",
          "4 Key Principles Quiz",
          "Data Disciplines",
          "Data Disciplines Quiz",
          "DG/DM Key Activities",
          "DG/DM Key Activities Quiz",
          "Projects to Processes",
          "Projects to Processes Quiz",
          "Sophistication Levels",
          "Sophistication Levels Quiz",
          "The Information Lifecycle",
          "The Information Lifecycle Quiz",
          "Module Outro"
        ],
        "Data and Data Quality (DQ)": [
          "Module Intro",
          "The 4 Types of Data",
          "The 4 Types of Data Quiz",
          "DQ Problems and Impact",
          "DQ Problems and Impact Quiz",
          "DQ Management: Introduction",
          "DQ Management: DQ Improvement",
          "DQ Management: DQ Improvement Quiz",
          "DQ Management: DQ Actions",
          "DQ Management: DQ Actions Quiz",
          "DQ Management: Data Dimensions",
          "DQ Management: Data Dimensions Quiz",
          "DQ Management: Big Data and AI",
          "DQ Management: Big Data and AI Quiz",
          "DQ Tools/Techniques: Introduction",
          "DQ Tools/Techniques: DQ Tool Overview",
          "DQ Tools/Techniques: DQ Tool Overview Quiz",
          "DQ Tools/Techniques: Data Profiling",
          "DQ Tools/Techniques: Data Profiling Quiz",
          "DQ Tools/Techniques: Cleansing and Standardisation",
          "DQ Tools/Techniques: Cleansing and Standardisation Quiz",
          "DQ Tools/Techniques: Merging and Linking",
          "DQ Tools/Techniques: Merging and Linking Quiz",
          "DQ Tools/Techniques: Data Enhancement",
          "DQ Tools/Techniques: Data Enhancement Quiz",
          "Business Case Building",
          "Business Case Building Quiz",
          "Module Outro"
        ],
        "Data Governance": [
          "Module Intro",
          "The DG Function: Intro",
          "The DG Function: Common Functions/Capabilities",
          "The DG Function: Common Functions/Capabilities Quiz",
          "The DG Function: Roles and Responsibilities",
          "The DG Function: Roles and Responsibilities Quiz",
          "The DG Function: Data Classification",
          "The DG Function: Data Classification Quiz",
          "The DG Function: Data Stewardship",
          "The DG Function: Data Stewardship Quiz",
          "The DG Function: A Day in the Life",
          "The DG Function: A Day in the Life Quiz",
          "DG Implementation: Intro",
          "DG Implementation: Planning: Intro",
          "DG Implementation: Planning: Assessment and Scope",
          "DG Implementation: Planning: Assessment and Scope",
          "DG Implementation: Planning: Engagement and Buy-In",
          "DG Implementation: Planning: Engagement and Buy-In Quiz",
          "DG Implementation: Planning: Architecture and Framework",
          "DG Implementation: Planning: Architecture and Framework Quiz",
          "DG Implementation: Roll-Out: Intro",
          "DG Implementation: Roll-Out: Initial Deployment",
          "DG Implementation: Roll-Out: Initial Deployment Quiz",
          "DG Implementation: Roll-Out: Complementary Initiatives",
          "DG Implementation: Roll-Out: Complementary Initiatives Quiz",
          "DG Implementation: Iteration: Intro",
          "DG Implementation: Iteration: Sustaining",
          "DG Implementation: Iteration: Sustaining Quiz",
          "DG Implementation: Iteration: Scaling",
          "DG Implementation: Iteration: Scaling Quiz",
          "DG Implementation: Monitoring",
          "DG Implementation: Monitoring Quiz",
          "DG Implementation: Cultural Aspects",
          "DG Implementation: Cultural Aspects Quiz",
          "Module Outro"
        ],
        "Data Ethics, Security and Privacy": [
          "Module Intro",
          "Data Security: Intro",
          "Data Security: Crytopgraphic Protection",
          "Data Security: Crytopgraphic Protection Quiz",
          "Data Security: Data Retention and Disposal",
          "Data Security: Data Retention and Disposal Quiz",
          "Data Security: Locked Rooms / Devices / Ports",
          "Data Security: Locked Rooms / Devices / Ports Quiz",
          "Data Security: Physical Media Protection",
          "Data Security: Physical Media Protection Quiz",
          "Data Security: Provider Assessment and Monitoring",
          "Data Security: Provider Assessment and Monitoring Quiz",
          "Data Privacy: Intro",
          "Data Privacy: Geographical Regulation",
          "Data Privacy: Geographical Regulation Quiz",
          "Data Privacy: Data Governance Structures",
          "Data Privacy: Data Governance Structures Quiz",
          "Data Privacy: Controls by Data Classification",
          "Data Privacy: Controls by Data Classification Quiz",
          "Data Privacy: Media Downgrading and Redacting",
          "Data Privacy: Media Downgrading and Redacting Quiz",
          "Data Privacy: Data De-Identification and Anonymisation",
          "Data Privacy: Data De-Identification and Anonymisation Quiz",
          "Data Ethics: Intro",
          "Data Ethics: The \"Compliance Approach\"",
          "Data Ethics: The \"Compliance Approach\" Quiz",
          "Data Ethics: Implementing Ethics",
          "Data Ethics: Implementing Ethics Quiz",
          "Data Ethics: Algorithms and Processes",
          "Data Ethics: Algorithms and Processes Quiz",
          "Data Ethics: Ethical Data Dimensions",
          "Data Ethics: Ethical Data Dimensions Quiz",
          "Data Ethics: Data Usage Purpose/Authority",
          "Data Ethics: Data Usage Purpose/Authority Quiz",
          "Module Outro"
        ],
        "Additional Module: Extra Security Controls": [
          "Intro",
          "Acquisition Strategy",
          "Acquisition Strategy Quiz",
          "Code Analysis",
          "Code Analysis Quiz",
          "Code Signing",
          "Code Signing Quiz",
          "Criticality Analysis",
          "Criticality Analysis Quiz",
          "Cyber Threat Hunting",
          "Cyber Threat Hunting Quiz",
          "Defense-In-Depth",
          "Defense-In-Depth Quiz",
          "Information Tainting",
          "Information Tainting Quiz",
          "Security/Privacy Architectures",
          "Security/Privacy Architectures Quiz",
          "System Safe Modes",
          "System Safe Modes Quiz",
          "Thin/Diskless Devices",
          "Thin/Diskless Devices Quiz",
          "Usage Agreements",
          "Usage Agreements Quiz",
          "Visitor Controls",
          "Visitor Controls Quiz",
          "Outro"
        ],
        "Additional Module: Pitching Technical Projects": [
          "Introduction",
          "Assembling: Introduction",
          "Assembling: Actions and Implementation",
          "Assembling: Roles and Responsibilities",
          "Assembling: Scope, Framework, Roadmap",
          "Assembling: Governance Structures",
          "Assembling: Trackable Metrics",
          "Presenting: Introduction",
          "Presenting: Recency and Primacy",
          "Presenting: Leveraging Specifics",
          "Presenting: Displayed Authority",
          "Presenting: The Hero's Journey",
          "Presenting: Tiredness and Distraction",
          "Dealing with Objections: Introduction",
          "Dealing with Objections: Flipping and Diagnosing",
          "Dealing with Objections: UP Answers",
          "Dealing with Objections: Progress and Loss",
          "Dealing with Objections: Political Capital",
          "Securing Buy-In: Introduction",
          "Securing Buy-In: Implementation and Opinions",
          "Securing Buy-In: Tailored Benefits",
          "Securing Buy-In: Effort Shaping",
          "Securing Buy-In: Future Lock-In",
          "Full Runthroughs: Introduction",
          "Full Runthroughs: Pitching PCI-DSS",
          "Full Runthroughs: Pitching Vendor Assessments",
          "Full Runthroughs: Pitching Data Management",
          "Full Runthroughs: Pitching Data Governance",
          "Outro"
        ],
        "Bonus Lecture": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "You don't need prior knowledge (knowledge of data management/data governance naturally helps, but is NOT necessary)"
      ],
      "description": "MANAGE YOUR KNOWLEDGE, MANAGE YOUR DATA\nThere are many activities related to data in organisations.\nData Management (DM), Data Governance (DG), Data Stewardship, Data Science, and many others.\nAll of these are crucial activities for organisations, especially those trying to protect their data from cyberattacks, complying with regulation, or just trying to improve the quality of their analytics and reports.\nFrequently, you can find information on one of these activities, but not all.\nAnd on top of that, many courses use different definitions, so you may become confused.\nIn short, most courses on data don't fit the minimum requirements.\nAnd this has consequences not just for your career, but yourself personally as well.\nWhat happens when you don't have enough information (or in the adequate format)?\nYou'll become confused by the myriad data activities, the tools used, which roles and responsibilities each person has, and how they intersect;\nYou won't be able to properly identify what belongs to Data Management or what belongs to Data Governance - and what should not be done at all;\nYou'll become frustrated and irritated that you don't know why an operation works, or why it doesn't;\nYou won't be able to identify what a specific data tool should be used for, and when your current tools don't fit the job;\nYou won't know how to optimize your DM and DG operations in an organisation, resulting people not taking data seriously, or making obvious mistakes;\nYou won't be able to know what security controls to apply to what specific data classes, or how to protect data subjects;\nSo if you want to know everything about Data Management, Data Governance, Data Security and a lot more, what is my proposed solution?\nThis new course masterclass, of course!\n\n\n\n\nA HIGH-QUALITY COURSE FOR HIGH-QUALITY DATA\nUnlike other data management or data governance courses you'll find out there, this course is comprehensive and updated.\nIn other words, not only did I make sure that you'll find more topics (and more in-depth) than other courses you may find, but I also made sure to keep the information relevant to the types of data quality issues you'll find nowadays.\nData operations may seem complex by nature, but they rely on simple principles.\nIn this course, you'll learn about the essentials of how data are managed with activities such as profiling and remediation, as well as how data are governed with processes and policies.\nNot only that, we'll dive deep into the activities, stakeholders, projects and resources that each discipline entails.\nIn this 15-hour+ masterclass, you'll find the following modules:\nYou'll learn about the essential Data Literacy and Considerations (what are the key principles, what are the different data disciplines, usual processes of each, the information lifecycle, and sophistication levels in an organisation);\nYou'll get to know about Data and Data Quality in specific (including the types of data that exist, the types of data quality issues and their financial impact, the Data Management activity process flow, as well as the data dimensions and tools used);\nYou'll learn about Data Governance (including how to define principles and policies, what are the specific activities such as data classification, data lineage tracking and more, as well as how to implement DG from scratch in an organisation);\nYou'll learn about Data Security, Privacy and Ethics (including what security and privacy controls to use to protect data both in physical and logical formats, how to ensure that data subjects are treated fairly by algorithms and across geographies, how to implement and measure data ethics, and more);\nBy the end of this course, you will know exactly how data are managed and how they are governed in an organisation, to a deep level, including the necessary tools, people, and activities.\nThe best of this masterclass? Inside you'll find all of these 4 modules - plus countless bonuses.\nIn short, even if you only fit one of the three profiles (only Data Management, only Data Governance, only Data Security/Ethics, or only \"general\" Data Quality knowledge), you will still have a course dedicated to it!\nAnd naturally, if you are interested in multiple of these topics... this is the ultimate package for you.\n\n\n\n\nTHE PERFECT COURSE... FOR WHOM?\nThis course is targeted at different types of people.\nNaturally, if you're a current or future data professional, you will find this course useful, as well as if you are any other professional or executive involved in a data project in your organisation.\nBut even if you're any other type of professional that aims to know more about how data work, you'll find the course useful.\nMore specifically, you're the ideal student for this course if:\nYou're someone who wants to know more about data management itself (how to profile datasets, how to parse/cleanse/standardise them, how to link and merge records, or how to enhance data);\nYou're someone who is interested in data governance (how to define rules and controls for data, how to institute policies, how to define required metadata, how to fill said metadata for different data sources, and many other activities);\nYou're some who is interested in data security and privacy, or in data ethics (that is, how to ensure that data subjects are protected in terms of data safety, but also in terms of actually being treated fairly by processes and algorithms);\nYou're someone who wants to know more about data quality in general (what are the usual types of problems, how do they create financial impact in organisations, what are the usual activities to improve DQ, and so on);\n\n\n\n\nLET ME TELL YOU... EVERYTHING\nSome people - including me - love to know what they're getting in a package.\nAnd by this, I mean, EVERYTHING that is in the package.\nSo, here is a list of everything that this masterclass covers:\nData Literacy and Fundamentals\nYou'll learn about the 4 key principles for any successful data initiative - considering data at assets, monetising them, seeing DG as business and not IT, and gauging your organisation's sophistication level;\nYou'll learn about the key data disciplines - that is, what is Data Management (DM), what is Data Governance (DG), what is Data Stewardship, and other activities such as Data Science, and terms such as Data Quality (DQ), as well as the specific roles and operations related to each of these in specific;\nYou'll learn about the main activities in DM and DG. In the case of DM, activities such as profiling data, remediating them, and setting future data validity requirements, and in the case of DG, uncovering business rules, setting policies and expectations for data, and controls to measure DQ, among others;\nYou'll get to know the different stages of the information lifecycle. Data being created, accessed, changed, deleted, and possibly other intermediate steps, as well as the usual preoccupations and controls at each stage;\nYou'll learn about the usual progression from projects to processes - how both DM and DG usually start as specific projects with local scope, and usually grow within an organisation, culminating in replicable and centralised processes to manage and govern data;\nYou'll get to know the possible sophistication levels of an organisation in terms of managing data. Being reactive, with no allocated tools or people, versus having centralised and standardised roles, tools and processes for data operations, and gauging your organisation;\nData and Data Quality Management:\nYou'll get to know the 4 main types of data. Master data, reference data, transactional data and metadata, as well as the nuances of each and how they intersect;\nYou'll learn about the types of DQ issues and their financial impact, usually in one of 3 main ways: direct costs, operational inefficiencies, and/or compliance or regulatory sanctions;\nYou'll learn about the usual DQ improvement process, starting with profiling, usually followed by triage, remediation of the data, and possible setup of automated controls to prevent future errors;\nYou'll learn about the three main types of DQ actions. Remediating data on the spot, analyzing the root cause of data problems, and/or instituting rules with automated controls to measure/prevent future data problems;\nYou'll get to know the different data dimensions used when analyzing DQ problems. Completeness, accuracy, timeliness, lineage, and other relevant ones;\nYou'll know more about the effect of Big Data and/or AI in data management, specifically the consequences both have in terms of the remediation possibilities and the data pipelines;\nYou'll know more about the tools used for DQ management, including profiling, parsing and standardisation, linking and merging, and data enhancement tools;\nYou'll get to know data profiling tools and their specific uses, including validating values in datasets, detecting outliers, validating data formats and rules, and/or uncovering implicit business rules;\nYou'll learn more about parsing and standardisation tools, which usually take data in different formats, parse them into a unified format, and then standardise data in that format, including the possible removal/editing of wrong values (\"cleansing\");\nYou'll get to know linking and merging tools, used to prevent duplicates, which usually use a comparison algorithm to establish a match between records, as being the same, which can then be merged;\nYou'll learn about data enhancement and annotation tools, which allow you to add more data to the current data, when these can't be edited - or don't need to be edited;\nYou'll learn more about building a business case for DM/DG, including the usual operations and steps, the usual costs and savings mentioned, and how to present it;\nData Governance:\nYou'll learn about the common functions and capabilities enabled by DG, from privacy and security controls to lineage tracking, metadata management, data classification, monitoring and more;\nYou'll learn about the usual roles and responsibilities in DG, from data owners to data stewards, Accountable Executives, who is the Data Council, the differences between the CDO (Chief Data Officer) and CIO (Chief Information Officer), and others;\nYou'll learn about data classification, including the 3 major systems used (by priority and criticality, by sensitivity and privacy requirements, and by processing stage), as well as the consequences of each;\nYou'll learn about what is data stewardship, and how it bridges Data Governance and reality, including multiple tasks related to metadata, master data, reference data, tracking DQ issues, and other activities, as well as the differences between business, technical and operational data stewards;\nYou'll learn about a day in the life in Data Governance, including the tasks that each role performs, how they coordinate, and how decisions are made;\nYou'll learn about assessing your organisation in preparation to implement DG, including data management maturity assessments and change capacity assessments, and how to define the scope of a DG program selecting processes, people and tools;\nYou'll learn about how to engage and obtain buy-in from different stakeholders, how to deal with resistance, how to prioritise the available projects, and how to ensure commitment to DG;\nYou'll learn about how to architect and define the initial DG program, including the tools used, the data lifecycle stages, the roles and responsibilities involved, and how to bring it all together in a single operating model;\nYou'll learn about how to deploy DG initially, with a roadmap, milestones, an operating model, and metrics to track;\nYou'll learn about the possible initiatives complementary to DG, including data-centric projects such as MDM (Master Data Management) or EIM (Enterprise Information Management), analytics and AI projects, and big ERP implementations, as well as how to let DG \"piggyback\" on these;\nYou'll learn about how to maintain a DG program, including maintaining key processes such as training and communication, while enforcing behavioral change through change management and controls;\nYou'll learn about how to scale a DG program, including dealing with resistance by new stakeholders from new projects and defining the federation of DG as it grows within the organisation;\nYou'll learn about how culture affects DG, including how aspects such as data literacy, commitment and others define the resources and the people accountable for DG;\nData Security, Privacy and Ethics\nYou'll learn about cryptographic protection (how to protect data with encryption), and measures to take into account;\nYou'll learn about data retention and disposal - that is, how to minimise retention time for data which may be leaked, as well as dispose of them securely;\nYou'll learn about locked rooms, locked devices and/or locked ports, placing physical barriers in front of attackers trying to steal data;\nYou'll learn about physical media protection, that is, how to protect media such as HDDs, USB drives, or paper while at rest, in transit and during their destruction;\nYou'll learn how service provider assessment and monitoring is done, to prevent third parties from introducing vulnerabilities in your organisation;\nYou'll learn about how geographical regulation affects data privacy - that is, how different countries may have different demands in terms of data subject privacy;\nYou'll learn about data governance structures, and how they can ensure that data ethics are taken care of in a centralised, standardised manner;\nYou'll learn about defining security controls by data classification, which allows us to protect more sensitive data with stricter controls and vice-versa;\nYou'll learn about media downgrading and/or redacting, which removes sensitive information from a medium to allow it to be shared more freely;\nYou'll learn about data de-identification and anonymisation, consisting of removing the personal elements of data to be able to use them without exposing PII (personally identifiable information);\nYou'll learn about the \"compliance approach\" to data ethics, consisting of treating ethics like a type of compliance the organisation must ensure, and how it enforces ethics;\nYou'll learn about actually implementing ethics in an organisation, with principles, governance structures and feedback loops, allowing transparency and iteration;\nYou'll learn about ethical data dimensions, which are just like the \"vanilla\" dimensions in DM, but specifically to measure ethics, such as Transparency, Fairness, Utility and others;\nYou'll learn about data processing purposes and authority, which consists of defining specific purposes for which data may be used (or not);\n\n\n\n\nMY INVITATION TO YOU\nRemember that you always have a 30-day money-back guarantee, so there is no risk for you.\nAlso, I suggest you make use of the free preview videos to make sure the course really is a fit. I don't want you to waste your money.\nIf you think this course is a fit and can take your data quality knowledge to the next level... it would be a pleasure to have you as a student.\nSee you on the other side!",
      "target_audience": [
        "You're a current (or future) data management or data governance professional",
        "You're someone involved (willingly or not!) in a data-related project, and need to understand it",
        "You're involved in decision-making related to managing or governing data in an organisation",
        "You're anyone else curious about data management, data governance, or any other activity related to data"
      ]
    },
    {
      "title": "Complete Machine Learning & Data Science with Python | A-Z",
      "url": "https://www.udemy.com/course/complete-machine-learning-data-science-with-python-a-z/",
      "bio": "Use Scikit, learn NumPy, Pandas, Matplotlib, Seaborn and dive into machine learning A-Z with Python and Data Science.",
      "objectives": [
        "Machine learning isn’t just useful for predictive texting or smartphone voice recognition. Machine learning is constantly being applied to new industries.",
        "Learn Machine Learning with Hands-On Examples",
        "What is Machine Learning?",
        "Machine Learning Terminology",
        "Evaluation Metrics",
        "What are Classification vs Regression?",
        "Evaluating Performance-Classification Error Metrics",
        "Evaluating Performance-Regression Error Metrics",
        "Supervised Learning",
        "Cross Validation and Bias Variance Trade-Off",
        "Use matplotlib and seaborn for data visualizations",
        "Machine Learning with SciKit Learn",
        "Linear Regression Algorithm",
        "Logistic Regresion Algorithm",
        "K Nearest Neighbors Algorithm",
        "Decision Trees And Random Forest Algorithm",
        "Support Vector Machine Algorithm",
        "Unsupervised Learning",
        "K Means Clustering Algorithm",
        "Hierarchical Clustering Algorithm",
        "Principal Component Analysis (PCA)",
        "Recommender System Algorithm",
        "Python instructors on OAK Academy specialize in everything from software development to data analysis, and are known for their effective.",
        "Python is a general-purpose, object-oriented, high-level programming language.",
        "Python is a multi-paradigm language, which means that it supports many programming approaches. Along with procedural and functional programming styles",
        "Python is a widely used, general-purpose programming language, but it has some limitations. Because Python is an interpreted, dynamically typed language",
        "Python is a general programming language used widely across many industries and platforms. One common use of Python is scripting, which means automating tasks.",
        "Python is a popular language that is used across many industries and in many programming disciplines. DevOps engineers use Python to script website.",
        "Python has a simple syntax that makes it an excellent programming language for a beginner to learn. To learn Python on your own, you first must become familiar",
        "Machine learning describes systems that make predictions using a model trained on real-world data.",
        "Machine learning is being applied to virtually every field today. That includes medical diagnoses, facial recognition, weather forecasts, image processing.",
        "It's possible to use machine learning without coding, but building new systems generally requires code.",
        "Python is the most used language in machine learning. Engineers writing machine learning systems often use Jupyter Notebooks and Python together.",
        "Machine learning is generally divided between supervised machine learning and unsupervised machine learning. In supervised machine learning.",
        "Machine learning is one of the fastest-growing and popular computer science careers today. Constantly growing and evolving.",
        "Machine learning is a smaller subset of the broader spectrum of artificial intelligence. While artificial intelligence describes any \"intelligent machine\"",
        "A machine learning engineer will need to be an extremely competent programmer with in-depth knowledge of computer science, mathematics, data science.",
        "Python machine learning, complete machine learning, machine learning a-z"
      ],
      "course_content": {
        "First Contact with Machine Learning": [
          "What is Machine Learning?",
          "Machine Learning Terminology",
          "Machine Learning: Project Files",
          "FAQ regarding Python",
          "FAQ regarding Machine Learning",
          "Machine Learning Python Quiz",
          "Python Machine Learning Quiz"
        ],
        "Installations for Python": [
          "Installing Anaconda Distribution for Windows",
          "Installing Anaconda Distribution for MacOs",
          "Installing Anaconda Distribution for Linux",
          "Overview of Jupyter Notebook and Google Colab"
        ],
        "Evaluation Metrics in Machine Learning": [
          "Classification vs Regression in Machine Learning",
          "Machine Learning Model Performance Evaluation: Classification Error Metrics",
          "Evaluating Performance: Regression Error Metrics in Python",
          "Machine Learning With Python",
          "Machine Learning A-Z Quiz"
        ],
        "Supervised Learning with Machine Learning": [
          "What is Supervised Learning in Machine Learning?"
        ],
        "Linear Regression Algorithm in Machine Learning A-Z": [
          "Linear Regression Algorithm Theory in Machine Learning A-Z",
          "Linear Regression Algorithm With Python Part 1",
          "Linear Regression Algorithm With Python Part 2",
          "Linear Regression Algorithm with Python Part 3",
          "Linear Regression Algorithm with Python Part 4",
          "Quiz",
          "Quiz"
        ],
        "Bias Variance Trade-Off in Machine Learning": [
          "What is Bias Variance Trade-Off?",
          "Quiz"
        ],
        "Logistic Regression Algorithm in Machine Learning A-Z": [
          "What is Logistic Regression Algorithm in Machine Learning?",
          "Logistic Regression Algorithm with Python Part 1",
          "Logistic Regression Algorithm with Python Part 2",
          "Logistic Regression Algorithm with Python Part 3",
          "Logistic Regression Algorithm with Python Part 4",
          "Logistic Regression Algorithm with Python Part 5",
          "Quiz"
        ],
        "K-fold Cross-Validation in Machine Learning A-Z": [
          "K-Fold Cross-Validation Theory",
          "K-Fold Cross-Validation with Python"
        ],
        "K Nearest Neighbors Algorithm in Machine Learning A-Z": [
          "K Nearest Neighbors Algorithm Theory",
          "K Nearest Neighbors Algorithm with Python Part 1",
          "K Nearest Neighbors Algorithm with Python Part 2",
          "K Nearest Neighbors Algorithm with Python Part 3",
          "Quiz"
        ],
        "Hyperparameter Optimization": [
          "Hyperparameter Optimization Theory",
          "Hyperparameter Optimization with Python"
        ]
      },
      "requirements": [
        "Basic knowledge of Python Programming Language",
        "Be Able To Operate & Install Software On A Computer",
        "Free software and tools used during the machine learning a-z course",
        "Determination to learn machine learning and patience.",
        "Motivation to learn the the second largest number of job postings relative program language among all others",
        "Data visualization libraries in python such as seaborn, matplotlib",
        "Curiosity for machine learning python",
        "Desire to learn Python",
        "Desire to work on python machine learning",
        "Desire to learn matplotlib",
        "Desire to learn pandas",
        "Desire to learn numpy",
        "Desire to work on seaborn",
        "Desire to learn machine learning a-z, complete machine learning"
      ],
      "description": "Hello there,\nWelcome to the “Complete Machine Learning & Data Science with Python | A-Z” course\nUse Scikit, learn NumPy, Pandas, Matplotlib, Seaborn, and dive into machine learning A-Z with Python and Data Science\n\nMachine learning isn’t just useful for predictive texting or smartphone voice recognition Machine learning is constantly being applied to new industries and new problems Whether you’re a marketer, video game designer, or programmer, my course on OAK Academy here to help you apply machine learning to your work Complete machine learning & data science with python | a-z, machine learning a-z, Complete machine learning & data science with python, complete machine learning and data science with python a-z, machine learning using python, complete machine learning and data science, machine learning, complete machine learning, data science\nIt’s hard to imagine our lives without machine learning Predictive texting, email filtering, and virtual personal assistants like Amazon’s Alexa and the iPhone’s Siri, are all technologies that function based on machine learning algorithms and mathematical models Python, machine learning, django, python programming, machine learning python, python for beginners, data science\nPython instructors on OAK Academy specialize in everything from software development to data analysis, and are known for their effective, friendly instruction for students of all levels\nWhether you work in machine learning or finance, or are pursuing a career in web development or data science, Python is one of the most important skills you can learn Python's simple syntax is especially suited for desktop, web, and business applications Python's design philosophy emphasizes readability and usability Python was developed upon the premise that there should be only one way (and preferably one obvious way) to do things, a philosophy that has resulted in a strict level of code standardization The core programming language is quite small and the standard library is also large In fact, Python's large library is one of its greatest benefits, providing a variety of different tools for programmers suited for many different tasks\nDo you know data science needs will create 11 5 million job openings by 2026?\nDo you know the average salary is $100 000 for data science careers!\n\nData Science Careers Are Shaping The Future\nData science experts are needed in almost every field, from government security to dating apps Millions of businesses and government departments rely on big data to succeed and better serve their customers So data science careers are in high demand\nIf you want to learn one of the employer’s most request skills?\nIf you are curious about Data Science and looking to start your self-learning journey into the world of data with Python?\nIf you are an experienced developer and looking for a landing in Data Science!\nIn all cases, you are at the right place!\nWe've designed for you “Complete Machine Learning & Data Science with Python | A-Z” a straightforward course for Python Programming Language and Machine Learning\nIn the course, you will have down-to-earth way explanations with projects With this course, you will learn machine learning step-by-step I made it simple and easy with exercises, challenges, and lots of real-life examples\nWe will open the door of the Data Science and Machine Learning a-z world and will move deeper You will learn the fundamentals of Machine Learning A-Z and its beautiful libraries such as Scikit Learn\nThroughout the course, we will teach you how to use Python to analyze data, create beautiful visualizations, and use powerful machine learning python algorithms\nThis Machine Learning course is for everyone!\nMy \"Machine Learning with Hands-On Examples in Data Science\" is for everyone! If you don’t have any previous experience, not a problem! This course is expertly designed to teach everyone from complete beginners, right through to professionals ( as a refresher)\nWhy we use a Python programming language in Machine learning?\nPython is a general-purpose, high-level, and multi-purpose programming language The best thing about Python is, it supports a lot of today’s technology including vast libraries for Twitter, data mining, scientific calculations, designing, back-end server for websites, engineering simulations, artificial learning, augmented reality and what not! Also, it supports all kinds of App development\nWhat you will learn?\nIn this course, we will start from the very beginning and go all the way to the end of \"Machine Learning\" with examples\nBefore each lesson, there will be a theory part After learning the theory parts, we will reinforce the subject with practical examples\nDuring the course you will learn the following topics:\nWhat is Machine Learning?\nMore About Machine Learning\nMachine Learning Terminology\nEvaluation Metrics\nWhat is Classification vs Regression?\nEvaluating Performance-Classification Error Metrics\nEvaluating Performance-Regression Error Metrics\nMachine Learning with Python\nSupervised Learning\nCross-Validation and Bias Variance Trade-Off\nUse Matplotlib and seaborn for data visualizations\nMachine Learning with SciKit Learn\nLinear Regression Theory\nLogistic Regression Theory\nLogistic Regression with Python\nK Nearest Neighbors Algorithm Theory\nK Nearest Neighbors Algorithm With Python\nK Nearest Neighbors Algorithm Project Overview\nK Nearest Neighbors Algorithm Project Solutions\nDecision Trees And Random Forest Algorithm Theory\nDecision Trees And Random Forest Algorithm With Python\nDecision Trees And Random Forest Algorithm Project Overview\nDecision Trees And Random Forest Algorithm Project Solutions\nSupport Vector Machines Algorithm Theory\nSupport Vector Machines Algorithm With Python\nSupport Vector Machines Algorithm Project Overview\nSupport Vector Machines Algorithm Project Solutions\nUnsupervised Learning Overview\nK Means Clustering Algorithm Theory\nK Means Clustering Algorithm With Python\nK Means Clustering Algorithm Project Overview\nK Means Clustering Algorithm Project Solutions\nHierarchical Clustering Algorithm Theory\nHierarchical Clustering Algorithm With Python\nPrincipal Component Analysis (PCA) Theory\nPrincipal Component Analysis (PCA) With Python\nRecommender System Algorithm Theory\nRecommender System Algorithm With Python\nComplete machine learning\nPython machine learning\nMachine learning a-z\nWith my up-to-date course, you will have a chance to keep yourself up-to-date and equip yourself with a range of Python programming skills I am also happy to tell you that I will be constantly available to support your learning and answer questions\n\nWhat is machine learning?\nMachine learning describes systems that make predictions using a model trained on real-world data For example, let's say we want to build a system that can identify if a cat is in a picture We first assemble many pictures to train our machine learning model During this training phase, we feed pictures into the model, along with information around whether they contain a cat While training, the model learns patterns in the images that are the most closely associated with cats This model can then use the patterns learned during training to predict whether the new images that it's fed contain a cat In this particular example, we might use a neural network to learn these patterns, but machine learning can be much simpler than that Even fitting a line to a set of observed data points, and using that line to make new predictions, counts as a machine learning model\nWhat is machine learning used for?\nMachine learning a-z is being applied to virtually every field today That includes medical diagnoses, facial recognition, weather forecasts, image processing, and more In any situation in which pattern recognition, prediction, and analysis are critical, machine learning can be of use Machine learning is often a disruptive technology when applied to new industries and niches Machine learning engineers can find new ways to apply machine learning technology to optimize and automate existing processes With the right data, you can use machine learning technology to identify extremely complex patterns and yield highly accurate predictions\nDoes Machine learning require coding?\nIt's possible to use machine learning data science without coding, but building new systems generally requires code For example, Amazon’s Rekognition service allows you to upload an image via a web browser, which then identifies objects in the image This uses a pre-trained model, with no coding required However, developing machine learning systems involves writing some Python code to train, tune, and deploy your models It's hard to avoid writing code to pre-process the data feeding into your model Most of the work done by a machine learning practitioner involves cleaning the data used to train the machine They also perform “feature engineering” to find what data to use and how to prepare it for use in a machine learning model Tools like AutoML and SageMaker automate the tuning of models Often only a few lines of code can train a model and make predictions from it\nWhat is the best language for machine learning?\nPython is the most used language in machine learning using python Engineers writing machine learning systems often use Jupyter Notebooks and Python together Jupyter Notebooks is a web application that allows experimentation by creating and sharing documents that contain live code, equations, and more Machine learning involves trial and error to see which hyperparameters and feature engineering choices work best It's useful to have a development environment such as Python so that you don't need to compile and package code before running it each time Python is not the only language choice for machine learning Tensorflow is a popular framework for developing neural networks and offers a C++ API There is a complete machine learning framework for C# called ML NET Scala or Java are sometimes used with Apache Spark to build machine learning systems that ingest massive data sets\nWhat are the different types of machine learning?\nMachine learning is generally divided between supervised machine learning and unsupervised machine learning In supervised machine learning, we train machine learning models on labeled data For example, an algorithm meant to detect spam might ingest thousands of email addresses labeled 'spam' or 'not spam ' That trained model could then identify new spam emails even from data it's never seen In unsupervised learning, a machine learning model looks for patterns in unstructured data One type of unsupervised learning is clustering In this example, a model could identify similar movies by studying their scripts or cast, then group the movies together into genres This unsupervised model was not trained to know which genre a movie belongs to Rather, it learned the genres by studying the attributes of the movies themselves There are many techniques available within\nIs Machine learning a good career?\nMachine learning python is one of the fastest-growing and popular computer science careers today Constantly growing and evolving, you can apply machine learning to a variety of industries, from shipping and fulfillment to medical sciences Machine learning engineers work to create artificial intelligence that can better identify patterns and solve problems The machine learning discipline frequently deals with cutting-edge, disruptive technologies However, because it has become a popular career choice, it can also be competitive Aspiring machine learning engineers can differentiate themselves from the competition through certifications, boot camps, code repository submissions, and hands-on experience\nWhat is the difference between machine learning and artifical intelligence?\nMachine learning is a smaller subset of the broader spectrum of artificial intelligence While artificial intelligence describes any \"intelligent machine\" that can derive information and make decisions, machine learning describes a method by which it can do so Through machine learning, applications can derive knowledge without the user explicitly giving out the information This is one of the first and early steps toward \"true artificial intelligence\" and is extremely useful for numerous practical applications In machine learning applications, an AI is fed sets of information It learns from these sets of information about what to expect and what to predict But it still has limitations A machine learning engineer must ensure that the AI is fed the right information and can use its logic to analyze that information correctly\nWhat skills should a machine learning engineer know?\nA python machine learning engineer will need to be an extremely competent programmer with in-depth knowledge of computer science, mathematics, data science, and artificial intelligence theory Machine learning engineers must be able to dig deep into complex applications and their programming As with other disciplines, there are entry-level machine learning engineers and machine learning engineers with high-level expertise Python and R are two of the most popular languages within the machine learning field\nWhat is python?\nMachine learning python is a general-purpose, object-oriented, high-level programming language Whether you work in artificial intelligence or finance or are pursuing a career in web development or data science, Python bootcamp is one of the most important skills you can learn Python's simple syntax is especially suited for desktop, web, and business applications Python's design philosophy emphasizes readability and usability Python was developed on the premise that there should be only one way (and preferably, one obvious way) to do things, a philosophy that resulted in a strict level of code standardization The core programming language is quite small and the standard library is also large In fact, Python's large library is one of its greatest benefits, providing different tools for programmers suited for a variety of tasks\nPython vs R: What is the Difference?\nPython and R are two of today's most popular programming tools When deciding between Python and R in data science , you need to think about your specific needs On one hand, Python is relatively easy for beginners to learn, is applicable across many disciplines, has a strict syntax that will help you become a better coder, and is fast to process large datasets On the other hand, R has over 10,000 packages for data manipulation, is capable of easily making publication-quality graphics, boasts superior capability for statistical modeling, and is more widely used in academia, healthcare, and finance\nWhat does it mean that Python is object-oriented?\nPython is a multi-paradigm language, which means that it supports many data analysis programming approaches Along with procedural and functional programming styles, Python also supports the object-oriented style of programming In object-oriented programming, a developer completes a programming project by creating Python objects in code that represent objects in the actual world These objects can contain both the data and functionality of the real-world object To generate an object in Python you need a class You can think of a class as a template You create the template once, and then use the template to create as many objects as you need Python classes have attributes to represent data and methods that add functionality A class representing a car may have attributes like color, speed, and seats and methods like driving, steering, and stopping\nWhat are the limitations of Python?\nPython is a widely used, general-purpose programming language, but it has some limitations Because Python in machine learning is an interpreted, dynamically typed language, it is slow compared to a compiled, statically typed language like C Therefore, Python is useful when speed is not that important Python's dynamic type system also makes it use more memory than some other programming languages, so it is not suited to memory-intensive applications The Python virtual engine that runs Python code runs single-threaded, making concurrency another limitation of the programming language Though Python is popular for some types of game development, its higher memory and CPU usage limits its usage for high-quality 3D game development That being said, computer hardware is getting better and better, and the speed and memory limitations of Python are getting less and less relevant\nHow is Python used?\nPython is a general programming language used widely across many industries and platforms One common use of Python is scripting, which means automating tasks in the background Many of the scripts that ship with Linux operating systems are Python scripts Python is also a popular language for machine learning, data analytics, data visualization, and data science because its simple syntax makes it easy to quickly build real applications You can use Python to create desktop applications Many developers use it to write Linux desktop applications, and it is also an excellent choice for web and game development Python web frameworks like Flask and Django are a popular choice for developing web applications Recently, Python is also being used as a language for mobile development via the Kivy third-party library\nWhat jobs use Python?\nPython is a popular language that is used across many industries and in many programming disciplines DevOps engineers use Python to script website and server deployments Web developers use Python to build web applications, usually with one of Python's popular web frameworks like Flask or Django Data scientists and data analysts use Python to build machine learning models, generate data visualizations, and analyze big data Financial advisors and quants (quantitative analysts) use Python to predict the market and manage money Data journalists use Python to sort through information and create stories Machine learning engineers use Python to develop neural networks and artificial intelligent systems\nHow do I learn Python on my own?\nPython has a simple syntax that makes it an excellent programming language for a beginner to learn To learn Python on your own, you first must become familiar with the syntax But you only need to know a little bit about Python syntax to get started writing real code; you will pick up the rest as you go Depending on the purpose of using it, you can then find a good Python tutorial, book, or course that will teach you the programming language by building a complete application that fits your goals If you want to develop games, then learn Python game development If you're going to build web applications, you can find many courses that can teach you that, too Udemy’s online courses are a great place to start if you want to learn Python on your own\nWhat is data science?\nWe have more data than ever before But data alone cannot tell us much about the world around us We need to interpret the information and discover hidden patterns This is where data science comes in Data science uses algorithms to understand raw data The main difference between data science and traditional data analysis is its focus on prediction Data science seeks to find patterns in data and use those patterns to predict future data It draws on machine learning to process large amounts of data, discover patterns, and predict trends Data science includes preparing, analyzing, and processing data It draws from many scientific fields, and as a science, it progresses by creating new algorithms to analyze data and validate current methods\nWhat does a data scientist do?\nData Scientists use machine learning to discover hidden patterns in large amounts of raw data to shed light on real problems This requires several steps First, they must identify a suitable problem Next, they determine what data are needed to solve such a situation and figure out how to get the data Once they obtain the data, they need to clean the data The data may not be formatted correctly, it might have additional unnecessary data, it might be missing entries, or some data might be incorrect Data Scientists must, therefore, make sure the data is clean before they analyze the data To analyze the data, they use machine learning techniques to build models Once they create a model, they test, refine, and finally put it into production\nWhat are the most popular coding languages for data science?\nPython is the most popular programming language for data science It is a universal language that has a lot of libraries available It is also a good beginner language R is also popular; however, it is more complex and designed for statistical analysis It might be a good choice if you want to specialize in statistical analysis You will want to know either Python or R and SQL SQL is a query language designed for relational databases Data scientists deal with large amounts of data, and they store a lot of that data in relational databases Those are the three most-used programming languages Other languages such as Java, C++, JavaScript, and Scala are also used, albeit less so If you already have a background in those languages, you can explore the tools available in those languages However, if you already know another programming language, you will likely be able to pick up Python very quickly\nHow long does it take to become a data scientist?\nThis answer, of course, varies The more time you devote to learning new skills, the faster you will learn It will also depend on your starting place If you already have a strong base in mathematics and statistics, you will have less to learn If you have no background in statistics or advanced mathematics, you can still become a data scientist; it will just take a bit longer Data science requires lifelong learning, so you will never really finish learning A better question might be, \"How can I gauge whether I know enough to become a data scientist?\" Challenge yourself to complete data science projects using open data The more you practice, the more you will learn, and the more confident you will become Once you have several projects that you can point to as good examples of your skillset as a data scientist, you are ready to enter the field\nHow can I learn data science on my own?\nIt is possible to learn data science on your own, as long as you stay focused and motivated Luckily, there are a lot of online courses and boot camps available Start by determining what interests you about data science If you gravitate to visualizations, begin learning about them Starting with something that excites you will motivate you to take that first step If you are not sure where you want to start, try starting with learning Python It is an excellent introduction to programming languages and will be useful as a data scientist Begin by working through tutorials or Udemy courses on the topic of your choice Once you have developed a base in the skills that interest you, it can help to talk with someone in the field Find out what skills employers are looking for and continue to learn those skills When learning on your own, setting practical learning goals can keep you motivated\nDoes data science require coding?\nThe jury is still out on this one Some people believe that it is possible to become a data scientist without knowing how to code, but others disagree A lot of algorithms have been developed and optimized in the field You could argue that it is more important to understand how to use the algorithms than how to code them yourself As the field grows, more platforms are available that automate much of the process However, as it stands now, employers are primarily looking for people who can code, and you need basic programming skills The data scientist role is continuing to evolve, so that might not be true in the future The best advice would be to find the path that fits your skillset\nWhat skills should a data scientist know?\nA data scientist requires many skills They need a strong understanding of statistical analysis and mathematics, which are essential pillars of data science A good understanding of these concepts will help you understand the basic premises of data science Familiarity with machine learning is also important Machine learning is a valuable tool to find patterns in large data sets To manage large data sets, data scientists must be familiar with databases Structured query language (SQL) is a must-have skill for data scientists However, nonrelational databases (NoSQL) are growing in popularity, so a greater understanding of database structures is beneficial The dominant programming language in Data Science is Python — although R is also popular A basis in at least one of these languages is a good starting point Finally, to communicate findings, data scientists require knowledge of visualizations Data visualizations allow them to share complex data in an accessible manner\nIs data science a good career?\nThe demand for data scientists is growing We do not just have data scientists; we have data engineers, data administrators, and analytics managers The jobs also generally pay well This might make you wonder if it would be a promising career for you A better understanding of the type of work a data scientist does can help you understand if it might be the path for you First and foremost, you must think analytically Data science is about gaining a more in-depth understanding of info through data Do you fact-check information and enjoy diving into the statistics? Although the actual work may be quite technical, the findings still need to be communicated Can you explain complex findings to someone who does not have a technical background? Many data scientists work in cross-functional teams and must share their results with people with very different backgrounds If this sounds like a great work environment, then it might be a promising career for you\nWhat does it mean that Python is object-oriented?\nPython is a multi-paradigm language, which means that it supports many programming approaches Along with procedural and functional programming styles, Python also supports the object-oriented style of programming In object-oriented programming, a developer completes a programming project by creating Python objects in code that represent objects in the actual world These objects can contain both the data and functionality of the real-world object To generate an object in Python you need a class You can think of a class as a template You create the template once, and then use the template to create as many objects as you need Python classes have attributes to represent data and methods that add functionality A class representing a car may have attributes like color, speed, and seats and methods like driving, steering, and stopping The concept of combining data with functionality in an object is called encapsulation, a core concept in the object-oriented programming paradigm\nWhy would you want to take this course?\nOur answer is simple: The quality of teaching\nOAK Academy based in London is an online education company OAK Academy gives education in the field of IT, Software, Design, development in English, Portuguese, Spanish, Turkish, and a lot of different languages on the Udemy platform where it has over 1000 hours of video education lessons OAK Academy both increases its education series number by publishing new courses, and it makes students aware of all the innovations of already published courses by upgrading\nWhen you enroll, you will feel the OAK Academy`s seasoned developers' expertise Questions sent by students to our instructors are answered by our instructors within 48 hours at the latest\nVideo and Audio Production Quality\nAll our videos are created/produced as high-quality video and audio to provide you the best learning experience\nYou will be,\nSeeing clearly\nHearing clearly\nMoving through the course without distractions\n\n\nYou'll also get:\nLifetime Access to The Course\nFast & Friendly Support in the Q&A section\nUdemy Certificate of Completion Ready for Download\nWe offer full support, answering any questions\nIf you are ready to learn the “Complete Machine Learning & Data Science with Python | A-Z” course\nDive in now! See you in the course!",
      "target_audience": [
        "Machine learning isn’t just useful for predictive texting or smartphone voice recognition. Machine learning is constantly being applied to new industries and new problems. It is for everyone",
        "Anyone who wants to start learning \"Machine Learning\"",
        "Anyone who needs a complete guide on how to start and continue their career with machine learning",
        "Software developer who wants to learn \"Machine Learning\"",
        "Students Interested in Beginning Data Science Applications in Python Environment",
        "People Wanting to Specialize in Anaconda Python Environment for Data Science and Scientific Computing",
        "Students Wanting to Learn the Application of Supervised Learning (Classification) on Real Data Using Python",
        "Anyone eager to learn python for data science and machine learning bootcamp with no coding background",
        "Anyone interested in data sciences",
        "Anyone who plans a career in data scientist,",
        "Software developer whom want to learn python,",
        "Anyone interested in machine learning a-z",
        "People who want to become data scientist",
        "People who want to learn machine learning a-z",
        "Poeple who want tp learn complete machine learning"
      ]
    },
    {
      "title": "Optical Character Recognition (OCR) in Python",
      "url": "https://www.udemy.com/course/ocr-optical-character-recognition-in-python/",
      "bio": "OpenCV, Tesseract, EasyOCR and EAST applied to images and videos! Create your own OCR from scratch using Deep Learning!",
      "objectives": [
        "Use Tesseract, EAST and EasyOCR tools for text recognition in images and videos",
        "Understand the differences between OCR in controlled and natural environments",
        "Apply image pre-processing techniques to improve image quality, such as: thresholding, inversion, resizing, morphological operations and noise reduction",
        "Use EAST architecture and EasyOCR library for better performance in natural scenes",
        "Train an OCR from scratch using Deep Learning and Convolutional Neural Networks",
        "Application of natural language processing techniques in the texts extracted by OCR (word cloud and named entity recognition)",
        "License plate reading"
      ],
      "course_content": {},
      "requirements": [
        "Programming logic",
        "Python programming basic"
      ],
      "description": "Within the area of Computer Vision is the sub-area of Optical Character Recognition (OCR), which aims to transform images into texts. OCR can be described as converting images containing typed, handwritten or printed text into characters that a machine can understand. It is possible to convert scanned or photographed documents into texts that can be edited in any tool, such as the Microsoft Word. A common application is automatic form reading, in which you can send a photo of your credit card or your driver's license, and the system can read all your data without the need to type them manually. A self-driving car can use OCR to read traffic signs and a parking lot can guarantee access by reading the license plate of the cars!\nTo take you to this area, in this course you will learn in practice how to use OCR libraries to recognize text in images and videos, all the code implemented step by step using the Python programming language! We are going to use Google Colab, so you do not have to worry about installing libraries on your machine, as everything will be developed online using Google's GPUs! You will also learn how to build your own OCR from scratch using Deep Learning and Convolutional Neural Networks! Below you can check the main topics of the course:\n\nRecognition of texts in images and videos using Tesseract, EasyOCR and EAST\nSearch for specific terms in images using regular expressions\nTechniques for improving image quality, such as: thresholding, color inversion, grayscale, resizing, noise removal, morphological operations and perspective transformation\nEAST architecture and EasyOCR library for better performance in natural scenes\nTraining an OCR from scratch using TensorFlow and modern Deep Learning techniques, such as Convolutional Neural Networks\nApplication of natural language processing techniques in the texts extracted by OCR (word cloud and named entity recognition)\nLicense plate reading\nThese are just some of the main topics! By the end of the course, you will know everything you need to create your own text recognition projects using OCR!",
      "target_audience": [
        "Anyone interested in OCR (Optical Character Recognition)",
        "Undergraduate students who are studying subjects related to Artificial Intelligence, Digital Image Processing or Computer Vision",
        "Data Scientists who want to increase their knowledge in Computer Vision",
        "Professionals interested in developing professional optical character recognition solutions",
        "People interested in creating their own custom OCR"
      ]
    },
    {
      "title": "Taming Big Data with Apache Spark 4 and Python - Hands On!",
      "url": "https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/",
      "bio": "PySpark tutorial with 40+ hands-on examples of analyzing large data sets on your desktop or on Hadoop with Python!",
      "objectives": [
        "Use DataFrames and Structured Streaming in Spark 3",
        "Use the MLLib machine learning library to answer common data mining questions",
        "Understand how Spark Streaming lets your process continuous streams of data in real time",
        "Frame big data analysis problems as Spark problems",
        "Use Amazon's Elastic MapReduce service to run your job on a cluster with Hadoop YARN",
        "Install and run Apache Spark on a desktop computer or on a cluster",
        "Use Spark's Resilient Distributed Datasets to process and analyze large data sets across many CPU's",
        "Implement iterative algorithms such as breadth-first-search using Spark",
        "Understand how Spark SQL lets you work with structured data",
        "Tune and troubleshoot large jobs running on a cluster",
        "Share information between nodes on a Spark cluster using broadcast variables and accumulators",
        "Understand how the GraphX library helps with network analysis problems"
      ],
      "course_content": {
        "Getting Started with Spark": [
          "Introduction",
          "How to Use This Course",
          "Udemy 101: Getting the Most From This Course",
          "Important Spark setup notes",
          "[Activity]Getting Set Up: Installing Python, a JDK, Spark, and its Dependencies.",
          "Alternate MovieLens download location",
          "[Activity] Installing the MovieLens Movie Rating Dataset",
          "[Activity] Run your first Spark program! Ratings histogram example."
        ],
        "Spark Basics and the Legacy RDD Interface": [
          "What's new in Spark 4?",
          "Introduction to Spark",
          "The Resilient Distributed Dataset (RDD)",
          "Ratings Histogram Walkthrough",
          "Key/Value RDD's, and the Average Friends by Age Example",
          "[Activity] Running the Average Friends by Age Example",
          "Filtering RDD's, and the Minimum Temperature by Location Example",
          "[Activity]Running the Minimum Temperature Example, and Modifying it for Maximums",
          "[Activity] Running the Maximum Temperature by Location Example",
          "[Activity] Counting Word Occurrences using flatmap()",
          "[Activity] Improving the Word Count Script with Regular Expressions",
          "[Activity] Sorting the Word Count Results",
          "[Exercise] Find the Total Amount Spent by Customer",
          "[Excercise] Check your Results, and Now Sort them by Total Amount Spent.",
          "Check Your Sorted Implementation and Results Against Mine."
        ],
        "SparkSQL, DataFrames, and DataSets": [
          "Introducing SparkSQL",
          "[Activity] Executing SQL commands and SQL-style functions on a DataFrame",
          "Using DataFrames instead of RDD's",
          "[Exercise] Friends by Age, with DataFrames",
          "Exercise Solution: Friends by Age, with DataFrames",
          "[Activity] Word Count, with DataFrames",
          "[Activity] Minimum Temperature, with DataFrames (using a custom schema)",
          "[Exercise] Implement Total Spent by Customer with DataFrames",
          "Exercise Solution: Total Spent by Customer, with DataFrames",
          "Pandas and Spark DataFrame Integration",
          "[Activity] Pandas API on Spark",
          "[Activity] User-Defined Functions (UDF) and User-Defined Table Functions (UDTF)",
          "[Activity] Using Apache Spark in a Jupyter Notebook (and a log analysis example)"
        ],
        "Advanced Examples of Spark Programs": [
          "[Activity] Find the Most Popular Movie",
          "[Activity] Use Broadcast Variables to Display Movie Names Instead of ID Numbers",
          "Find the Most Popular Superhero in a Social Graph",
          "[Activity] Run the Script - Discover Who the Most Popular Superhero is!",
          "[Exercise] Find the Most Obscure Superheroes",
          "Exercise Solution: Most Obscure Superheroes",
          "Superhero Degrees of Separation: Introducing Breadth-First Search",
          "Superhero Degrees of Separation: Accumulators, and Implementing BFS in Spark",
          "[Activity] Superhero Degrees of Separation: Review the Code and Run it",
          "Item-Based Collaborative Filtering in Spark, cache(), and persist()",
          "[Activity] Running the Similar Movies Script using Spark's Cluster Manager",
          "[Exercise] Improve the Quality of Similar Movies"
        ],
        "Running Spark on a Cluster": [
          "Spark Connect: Client / Server Apache Spark",
          "Introducing Elastic MapReduce",
          "[Activity] Setting up your AWS / Elastic MapReduce Account and Setting Up PuTTY",
          "Partitioning",
          "Create Similar Movies from One Million Ratings - Part 1",
          "[Activity] Create Similar Movies from One Million Ratings - Part 2",
          "Create Similar Movies from One Million Ratings - Part 3",
          "Troubleshooting Spark on a Cluster",
          "More Troubleshooting, and Managing Dependencies"
        ],
        "Machine Learning with Spark ML": [
          "Introducing MLLib",
          "A note on Machine Learning with Spark.",
          "[Activity] Using Spark ML to Produce Movie Recommendations",
          "Analyzing the ALS Recommendations Results",
          "[Activity] Linear Regression with Spark ML",
          "[Exercise] Using Decision Trees in Spark ML to Predict Real Estate Prices",
          "Exercise Solution: Decision Trees with Spark"
        ],
        "Spark Streaming, Structured Streaming, and GraphX": [
          "Spark Streaming",
          "[Activity] Structured Streaming in Python",
          "[Exercise] Use Windows with Structured Streaming to Track Most-Viewed URL's",
          "Exercise Solution: Using Structured Streaming with Windows",
          "Streaming Formats (File, Kafka, Rate, Socket)",
          "[Activity] Structured Streaming with Rate format, UDF, and SQL",
          "[Exercise] Simulate a Stream of Social Media Posts, and Track the Top Hashtags",
          "Exercise Solution: Top Hashtags from BlueSky with Structured Streaming & UDTFs",
          "[Activity] Joining Spark Structured Streams, Watermarks, and Triggers",
          "GraphX"
        ],
        "You Made It! Where to Go from Here.": [
          "Learning More about Spark and Data Science",
          "Bonus Lecture: More courses to explore!"
        ]
      },
      "requirements": [
        "Access to a personal computer. This course uses Windows, but the sample code will work fine on Linux as well.",
        "Some prior programming or scripting experience. Python experience will help a lot, but you can pick it up as we go."
      ],
      "description": "New! Updated for Spark 4's newest features\n“Big data\" analysis is a hot and highly valuable skill – and this course will teach you the hottest technology in big data: Apache Spark and specifically PySpark. Employers including Amazon, EBay, NASA JPL, and Yahoo all use Spark to quickly extract meaning from massive data sets across a fault-tolerant Hadoop cluster. You'll learn those same techniques, using your own Windows system right at home. It's easier than you might think.\nLearn and master the art of framing data analysis problems as Spark problems through over 20 hands-on examples, and then scale them up to run on cloud computing services in this course. You'll be learning from an ex-engineer and senior manager from Amazon and IMDb.\n\n\nLearn the concepts of Spark's DataFrames and Resilient Distributed Datastores\nDevelop and run Spark jobs quickly using Python and pyspark\nTranslate complex analysis problems into iterative or multi-stage Spark scripts\nScale up to larger data sets using Amazon's Elastic MapReduce service\nUnderstand how Hadoop YARN distributes Spark across computing clusters\nLearn about other Spark technologies, like Spark SQL, Spark Streaming, and GraphX\nPractice using Spark's latest features, including Pandas-On-Spark, Spark Connect, and User-Defined Table Functions (UDTFs).\nBy the end of this course, you'll be running code that analyzes gigabytes worth of information – in the cloud – in a matter of minutes.\nThis course uses the familiar Python programming language; if you'd rather use Scala to get the best performance out of Spark, see my \"Apache Spark with Scala - Hands On with Big Data\" course instead.\n\nWe'll have some fun along the way. You'll get warmed up with some simple examples of using Spark to analyze movie ratings data and text in a book. Once you've got the basics under your belt, we'll move to some more complex and interesting tasks. We'll use a million movie ratings to find movies that are similar to each other, and you might even discover some new movies you might like in the process! We'll analyze a social graph of superheroes, and learn who the most “popular\" superhero is – and develop a system to find “degrees of separation\" between superheroes. Are all Marvel superheroes within a few degrees of being connected to The Incredible Hulk? You'll find the answer.\nThis course is very hands-on; you'll spend most of your time following along with the instructor as we write, analyze, and run real code together – both on your own system, and in the cloud using Amazon's Elastic MapReduce service. 8 hours of video content is included, with over 40 real examples of increasing complexity you can build, run and study yourself. Move through them at your own pace, on your own schedule. The course wraps up with an overview of other Spark-based technologies, including Spark SQL, Spark Structured Streaming, and GraphX.\nWrangling big data with Apache Spark is an important skill in today's technical world. Enroll now!\n\n\n\" I studied \"Taming Big Data with Apache Spark and Python\" with Frank Kane, and helped me build a great platform for Big Data as a Service for my company. I recommend the course!  \" - Cleuton Sampaio De Melo Jr.\n\n\n\"Awesome course on running big data jobs on Apache Spark using Python. As usual, Frank explains things very clearly and points out various items to watch out for and make sure you have set up correctly. There are many ways that a Spark job can fail or have issues, such as running out of memory, and Frank does a great job of pointing many of those out.\" -James Gershfiel\n\n\n\"Easy steps so even a beginner should be able to install Spark and run the examples right away. Good examples and fun to do. Giving a nice set of useful examples as a toolbox.\" - HansEV\n\n\n\"Great course to get you going with Apache Spark and Python! Frank’s delivery is very thorough yet unpretentious; his explanations for each new concept that he introduces is down to earth and easy to follow.\" - Amiri McCain",
      "target_audience": [
        "People with some software development background who want to learn the hottest technology in big data analysis will want to check this out. This course focuses on Spark from a software development standpoint; we introduce some machine learning and data mining concepts along the way, but that's not the focus. If you want to learn how to use Spark to carve up huge datasets and extract meaning from them, then this course is for you.",
        "If you've never written a computer program or a script before, this course isn't for you - yet. I suggest starting with a Python course first, if programming is new to you.",
        "If your software development job involves, or will involve, processing large amounts of data, you need to know about Spark.",
        "If you're training for a new career in data science or big data, Spark is an important part of it."
      ]
    },
    {
      "title": "Databricks Fundamentals & Apache Spark Core",
      "url": "https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/",
      "bio": "Learn how to process big-data using Databricks & Apache Spark 2.4 and 3.0.0 - DataFrame API and Spark SQL",
      "objectives": [
        "Databricks",
        "Apache Spark Architecture",
        "Apache Spark DataFrame API",
        "Apache Spark SQL",
        "Selecting, and manipulating columns of a DataFrame",
        "Filtering, dropping, sorting rows of a DataFrame",
        "Joining, reading, writing and partitioning DataFrames",
        "Aggregating DataFrames rows",
        "Working with User Defined Functions",
        "Use the DataFrameWriter API"
      ],
      "course_content": {},
      "requirements": [
        "Basic Scala knowledge",
        "Basic SQL knowledge"
      ],
      "description": "Welcome to this course on Databricks and Apache Spark 2.4 and 3.0.0\nApache Spark is a Big Data Processing Framework that runs at scale.\nIn this course, we will learn how to write Spark Applications using Scala and SQL.\nDatabricks is a company founded by the creator of Apache Spark.\nDatabricks offers a managed and optimized version of Apache Spark that runs in the cloud.\nThe main focus of this course is to teach you how to use the DataFrame API & SQL to accomplish tasks such as:\nWrite and run Apache Spark code using Databricks\nRead and Write Data from the Databricks File System - DBFS\nExplain how Apache Spark runs on a cluster with multiple Nodes\nUse the DataFrame API and SQL to perform data manipulation tasks such as\nSelecting, renaming and manipulating columns\nFiltering, dropping and aggregating rows\nJoining DataFrames\nCreate UDFs and use them with DataFrame API or Spark SQL\nWriting DataFrames to external storage systems\nList and explain the element of Apache Spark execution hierarchy such as\nJobs\nStages\nTasks",
      "target_audience": [
        "Software developers curious about big-data, data engeneering and data science",
        "Beginner data engineer who want to learn how to do work with databricks",
        "Beginner data scientist who want to learn how to do work with databricks"
      ]
    },
    {
      "title": "Statistics and Hypothesis Testing for Data science",
      "url": "https://www.udemy.com/course/statistics-and-hypothesis-testing-for-data-science/",
      "bio": "\"Mastering Data Analysis and Making Informed Decisions with Statistical Hypothesis Testing in Data Science\".",
      "objectives": [
        "Fundamental concepts and importance of statistics in various fields.",
        "How to use statistics for effective data analysis and decision-making.",
        "Introduction to Python for statistical analysis, including data manipulation and visualization.",
        "Different types of data and their significance in statistical analysis.",
        "Measures of central tendency, spread, dependence, shape, and position.",
        "How to calculate and interpret standard scores and probabilities.",
        "Key concepts in probability theory, set theory, and conditional probability.",
        "Understanding Bayes' Theorem and its applications.",
        "Permutations, combinations, and their role in solving real-world problems.",
        "Practical knowledge of various statistical tests, including t-tests, chi-squared tests, and ANOVA, for hypothesis testing and inference."
      ],
      "course_content": {
        "Introduction to Statistics": [
          "Introduction to Statistics and its importance",
          "Explain the role of statistics in data analysis",
          "Introduction to Python for Statistical Analysis",
          "Quiz on Introduction to Statistics"
        ],
        "Introduction to Descriptive Statistics": [
          "Types of Data",
          "Measures of Central Tendency",
          "Measures of Spread",
          "Measures of Dependence",
          "Measures of Shape and Position",
          "Measures of Standard Scores",
          "Quiz on Descriptive Statistics"
        ],
        "Introduction to Basic and Conditional Probability": [
          "Introduction to Basic Probability",
          "Introduction to Set Theory",
          "Introduction to Conditional Probability",
          "Introduction to Bayes Theorem",
          "Introduction to Permutations and Combinations",
          "Introduction to Random Variables",
          "Introduction to Probability Distribution Functions",
          "Quiz on Basic and Conditional Probability"
        ],
        "Introduction to Inferential Statistics": [
          "Introduction to Normal Distribution",
          "Introduction to Skewness and Kurtosis",
          "Introduction to Statistical Transformations",
          "Introduction to Sample and Population Mean",
          "Introduction to Central Limit Theorem",
          "Introduction to Bias and Variance",
          "Introduction to Maximum Likelihood Estimation",
          "Introduction to Confidence Intervals",
          "Introduction to Correlations",
          "Introduction to Sampling Methods",
          "Quiz on Inferential Statistics"
        ],
        "Introduction to Hypothesis Testing": [
          "Fundamentals of Hypothesis Testing",
          "Introduction to T Tests",
          "Introduction to Z Tests",
          "Introduction to Chi Squared Tests",
          "Introduction to Anova Tests",
          "Quiz on Hypothesis Testing"
        ]
      },
      "requirements": [
        "Access to a computer with internet connectivity.",
        "A basic understanding of mathematics, including algebra and arithmetic.",
        "Familiarity with fundamental concepts in data analysis and problem-solving.",
        "A willingness to learn and engage with statistical concepts and Python programming.",
        "Basic knowledge of Python is a plus but not mandatory."
      ],
      "description": "Welcome to \"Statistics and Hypothesis Testing for Data Science\" – a comprehensive Udemy course that will empower you with the essential statistical knowledge and data analysis skills needed for success in the world of data science.\n\n\nHere's what you'll learn:\nDelve into the world of data-driven insights and discover how statistics plays a pivotal role in shaping our understanding of information.\nEquip yourself with the essential Python skills required for effective data manipulation and visualization.\nLearn to categorize data, setting the stage for meaningful analysis.\nDiscover how to summarize data with measures like mean, median, and mode.\nExplore the variability in data using concepts like range, variance, and standard deviation.\nUnderstand relationships between variables with correlation and covariance.\nGrasp the shape and distribution of data using techniques like quartiles and percentiles.\nLearn to standardize data and calculate z-scores.\nDive into probability theory and its practical applications.\nLay the foundation for probability calculations with set theory.\nExplore the probability of events under certain conditions.\nUncover the power of Bayesian probability in real-world scenarios.\nSolve complex counting problems with ease.\nUnderstand the concept of random variables and their role in probability.\nExplore various probability distributions and their applications.\n\n\nThis course will empower you with the knowledge and skills needed to analyze data effectively, make informed decisions, and apply statistical methods in a data science context. Whether you're a beginner or looking to deepen your statistical expertise, this course is your gateway to mastering statistics for data science. Enroll now and start your Journey!",
      "target_audience": [
        "Students or professionals in various fields, including business, science, social sciences, and healthcare, who want to enhance their data analysis skills.",
        "Data analysts, researchers, and scientists seeking to strengthen their statistical foundations and Python programming skills.",
        "Anyone interested in gaining a deeper understanding of statistical concepts and their practical applications.",
        "Beginners with no prior statistical knowledge but with a curiosity to learn and apply statistical methods.",
        "Professionals looking to advance their career by acquiring valuable statistical and data analysis skills.",
        "Individuals preparing for standardized tests or exams that include statistical and data analysis components."
      ]
    },
    {
      "title": "Image Recognition for Beginners using CNN in R Studio",
      "url": "https://www.udemy.com/course/cnn-for-computer-vision-with-keras-and-tensorflow-in-r/",
      "bio": "Deep Learning based Convolutional Neural Networks (CNN) for Image recognition using Keras and Tensorflow in R Studio",
      "objectives": [
        "Get a solid understanding of Convolutional Neural Networks (CNN) and Deep Learning",
        "Build an end-to-end Image recognition project in R",
        "Learn usage of Keras and Tensorflow libraries",
        "Use Artificial Neural Networks (ANN) to make predictions"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course Resources"
        ],
        "Setting Up R Studio and R crash course": [
          "Installing R and R studio",
          "This is a Milestone!",
          "Basics of R and R studio",
          "Packages in R",
          "Inputting data part 1: Inbuilt datasets of R",
          "Inputting data part 2: Manual data entry",
          "Inputting data part 3: Importing from CSV or Text files",
          "Creating Barplots in R",
          "Creating Histograms in R"
        ],
        "Single Cells - Perceptron and Sigmoid Neuron": [
          "Perceptron",
          "Activation Functions",
          "Quiz"
        ],
        "Neural Networks - Stacking cells to create network": [
          "Basic Terminologies",
          "Gradient Descent",
          "Back Propagation",
          "Quiz"
        ],
        "Important concepts: Common Interview questions": [
          "Some Important Concepts",
          "Quiz"
        ],
        "Standard Model Parameters": [
          "Hyperparameters",
          "Quiz"
        ],
        "Tensorflow and Keras": [
          "Keras and Tensorflow",
          "Installing Keras and Tensorflow",
          "Quiz"
        ],
        "R - Dataset for classification problem": [
          "Data Normalization and Test-Train Split",
          "More about test-train split"
        ],
        "R - Building and training the Model": [
          "Building, Compiling and Training",
          "Evaluating and Predicting"
        ],
        "The NeuralNets Package": [
          "ANN with NeuralNets Package"
        ]
      },
      "requirements": [
        "Students will need to install R and RStudio software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Convolutional Neural Network (CNN) course that teaches you everything you need to create an Image Recognition model in R, right?\nYou've found the right Convolutional Neural Networks course!\nAfter completing this course you will be able to:\nIdentify the Image Recognition problems which can be solved using CNN Models.\nCreate CNN models in R using Keras and Tensorflow libraries and analyze their results.\nConfidently practice, discuss and understand Deep Learning concepts\nHave a clear understanding of Advanced Image Recognition models such as LeNet, GoogleNet, VGG16 etc.\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Convolutional Neural networks course.\nIf you are an Analyst or an ML scientist, or a student who wants to learn and apply Deep learning in Real world image recognition problems, this course will give you a solid base for that by teaching you some of the most advanced concepts of Deep Learning and their implementation in R without getting too Mathematical.\nWhy should you choose this course?\nThis course covers all the steps that one should take to create an image recognition model using Convolutional Neural Networks.\nMost courses only focus on teaching how to run the analysis but we believe that having a strong theoretical understanding of the concepts enables us to create a good model . And after running the analysis, one should be able to judge how good the model is and interpret the results to actually be able to help the business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using Deep learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 300,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Practice test, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take practice test to check your understanding of concepts. There is a final practical assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Neural network based model i.e. a Deep Learning model, to solve business problems.\nBelow are the course contents of this course on ANN:\nPart 1 (Section 2)- Setting up R and R Studio with R crash course\nThis part gets you started with R.\nThis section will help you set up the R and R studio on your system and it'll teach you how to perform some basic operations in R.\nPart 2 (Section 3-6) - ANN Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nPart 3 (Section 7-11) - Creating ANN model in R\nIn this part you will learn how to create ANN models in R.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. Lastly we learn how to save and restore models.\nWe also understand the importance of libraries such as Keras and TensorFlow in this part.\nPart 4 (Section 12) - CNN Theoretical Concepts\nIn this part you will learn about convolutional and pooling layers which are the building blocks of CNN models.\nIn this section, we will start with the basic theory of convolutional layer, stride, filters and feature maps. We also explain how gray-scale images are different from colored images. Lastly we discuss pooling layer which bring computational efficiency in our model.\nPart 5 (Section 13-14) - Creating CNN model in R\nIn this part you will learn how to create CNN models in R.\nWe will take the same problem of recognizing fashion objects and apply CNN model to it. We will compare the performance of our CNN model with our ANN model and notice that the accuracy increases by 9-10% when we use CNN. However, this is not the end of it. We can further improve accuracy by using certain techniques which we explore in the next part.\nPart 6 (Section 15-18) - End-to-End Image Recognition project in R\nIn this section we build a complete image recognition project on colored images.\nWe take a Kaggle image recognition competition and build CNN model to solve it. With a simple model we achieve nearly 70% accuracy on test set. Then we learn concepts like Data Augmentation and Transfer Learning which help us improve accuracy level from 70% to nearly 97% (as good as the winners of that competition).\nBy the end of this course, your confidence in creating a Convolutional Neural Network model in R will soar. You'll have a thorough understanding of how to use CNN to create predictive models and solve image recognition problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n------------\nBelow are some popular FAQs of students who want to start their Deep learning journey-\n\n\nWhy use R for Deep Learning?\nUnderstanding R is one of the valuable skills needed for a career in Machine Learning. Below are some reasons why you should learn Deep learning in R\n1. It’s a popular language for Machine Learning at top tech firms. Almost all of them hire data scientists who use R. Facebook, for example, uses R to do behavioral analysis with user post data. Google uses R to assess ad effectiveness and make economic forecasts. And by the way, it’s not just tech firms: R is in use at analysis and consulting firms, banks and other financial institutions, academic institutions and research labs, and pretty much everywhere else data needs analyzing and visualizing.\n2. Learning the data science basics is arguably easier in R. R has a big advantage: it was designed specifically with data manipulation and analysis in mind.\n3. Amazing packages that make your life easier. Because R was designed with statistical analysis in mind, it has a fantastic ecosystem of packages and other resources that are great for data science.\n4. Robust, growing community of data scientists and statisticians. As the field of data science has exploded, R has exploded with it, becoming one of the fastest-growing languages in the world (as measured by StackOverflow). That means it’s easy to find answers to questions and community guidance as you work your way through projects in R.\n5. Put another tool in your toolkit. No one language is going to be the right tool for every job. Adding R to your repertoire will make some projects easier – and of course, it’ll also make you a more flexible and marketable employee when you’re looking for jobs in data science.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Deep Learning journey",
        "Anyone curious to master image recognition from Beginner level in short span of time"
      ]
    },
    {
      "title": "Data Analysis with Pandas and Python",
      "url": "https://www.udemy.com/course/data-analysis-with-pandas/",
      "bio": "Analyze data quickly and easily with Python's powerful pandas library! All datasets included --- beginners welcome!",
      "objectives": [
        "Perform a multitude of data operations in Python's popular pandas library including grouping, pivoting, joining and more!",
        "Learn hundreds of methods and attributes across numerous pandas objects",
        "Possess a strong understanding of manipulating 1D, 2D, and 3D data sets",
        "Resolve common issues in broken or incomplete data sets"
      ],
      "course_content": {
        "Installation and Setup": [
          "Introduction to the Course",
          "macOS - Download and Install the Anaconda Distribution",
          "Windows - Download and Install the Anaconda Distribution",
          "How to Uninstall the Anaconda Distribution",
          "Use Anaconda Navigator to Create a New Environment",
          "Download Course Materials",
          "Unpack Course Materials + The Startdown and Shutdown Process",
          "Intro to the Jupyter Lab Interface",
          "Code Cell Execution",
          "Import Libraries into Jupyter Lab",
          "Installation and Setup"
        ],
        "Python Crash Course": [
          "Comments",
          "Basic Data Types",
          "Operators",
          "Variables",
          "Declare Variables",
          "Built-in Functions",
          "Built-in Functions",
          "Custom Functions",
          "Custom Functions",
          "String Methods",
          "String Methods",
          "Lists",
          "Creating Lists",
          "Index Positions and Slicing",
          "Index Positions and Slicing",
          "Dictionaries",
          "Creating Dictionaries",
          "Classes",
          "Navigating Libraries using Jupyter Lab",
          "Python Crash Course"
        ],
        "Series": [
          "Create a Series Object from a List",
          "Create a Series Object from a Dictionary",
          "Create a Series Object",
          "Intro to Series Methods",
          "Intro to Attributes",
          "Attributes and Methods on a Series",
          "Parameters and Arguments",
          "Parameters and Arguments",
          "Import Series with the pd.read_csv Function",
          "Import Series with the read_csv Function",
          "The head and tail Methods",
          "The head and tail Methods",
          "Passing Series to Python Built-In Functions",
          "Check for Inclusion with Python's in Keyword",
          "The sort_values Method",
          "The sort_values Method",
          "The sort_index Method",
          "The sort_index Method",
          "Check for Inclusion with Python's in Keyword",
          "Extract Series Values by Index Position",
          "Extract Series Values by Index Label",
          "Extract Series Values by Index Position or Index Label",
          "The get Method",
          "Overwrite a Series Value",
          "The copy Method",
          "Math Methods on Series Objects",
          "Broadcasting",
          "The value_counts Method",
          "The value_counts Method",
          "The apply Method",
          "The map Method",
          "Series"
        ],
        "DataFrames I: Introduction": [
          "Methods and Attributes between Series and DataFrames",
          "Differences between Shared Methods",
          "Select One Column from a DataFrame",
          "Select One Column from a DataFrame",
          "Select Multiple Columns from a DataFrame",
          "Select Multiple Columns from a DataFrame",
          "Add New Column to DataFrame",
          "A Review of the value_counts Method",
          "Drop DataFrame Rows with Missing Values",
          "Drop DataFrame Rows with Missing Values",
          "Fill in Missing Values with the fillna Method",
          "The astype Method I",
          "The astype Method II",
          "The astype Method",
          "Sort a DataFrame with the sort_values Method I",
          "Sort a DataFrame with the sort_values Method II",
          "Sort a DataFrame with the sort_values Method",
          "Sort DataFrame with the sort_index Method",
          "Rank Series Values with the rank Method",
          "DataFrames I"
        ],
        "DataFrames II: Filtering Data": [
          "This Module's Dataset + Memory Optimization",
          "Filter a DataFrame Based on a Condition",
          "Filter a DataFrame Based on a Condition",
          "Filter with More than One Condition (AND - &)",
          "Filter DataFrame with More than One Condition (AND - &)",
          "Filter with More than One Condition (OR - |)",
          "Filter DataFrame with More than One Condition (OR - |)",
          "The isin Method",
          "The isin Method",
          "The isnull and notnull Methods",
          "The between Method",
          "The between Method",
          "The duplicated Method",
          "The drop_duplicates Method",
          "The unique and nunique Methods"
        ],
        "DataFrames III: Data Extraction": [
          "This Module's Dataset",
          "The set_index and reset_index Methods",
          "Retrieve Rows by Index Position with iloc Accessor",
          "Retrieve Rows by Index Label with loc Accessor",
          "Second Arguments to loc and iloc Accessors",
          "Overwrite Value in a DataFrame",
          "Overwrite Multiple Values in a DataFrame",
          "Rename Index Labels or Columns in a DataFrame",
          "Delete Rows or Columns from a DataFrame",
          "Create Random Sample with the sample Method",
          "The nsmallest and nlargest Methods",
          "Filtering with the where Method",
          "The apply Method with DataFrames"
        ],
        "Working with Text Data": [
          "This Module's Dataset",
          "Common String Methods",
          "Common String Methods",
          "Filtering with String Methods",
          "String Methods on Index and Columns",
          "The split Method",
          "More Practice with Splits",
          "The expand and n Parameters of the split Method"
        ],
        "MultiIndex": [
          "Intro to the MultiIndex Module",
          "Create a MultiIndex",
          "Create a MultiIndex",
          "Extract Index Level Values",
          "Extract Index Level Values",
          "Rename Index Lebels",
          "The sort_index Method on a MultiIndex DataFrame",
          "Extract Rows from a MultiIndex DataFrame",
          "Extract Rows from a MultiIndex DataFrame",
          "The transpose Method",
          "The stack Method",
          "The unstack Method",
          "The pivot Method",
          "The melt Method",
          "The melt Method",
          "The pivot_table Method"
        ],
        "GroupBy": [
          "Intro to the GroupBy Module",
          "The groupby Method",
          "Retrieve A Group with the get_group Method",
          "Methods on the GroupBy Object",
          "Grouping by Multiple Columns",
          "The agg Method",
          "Iterating through Groups"
        ],
        "Merging DataFrames": [
          "Intro to the Merging DataFrames Module",
          "The pd.concat Function I",
          "The pd.concat Function II",
          "Left Joins",
          "The left_on and right_on Parameters",
          "Inner Joins I",
          "Inner Joins II",
          "Full-Outer Joins",
          "Merging by Indexes with the left_index and right_index Parameters",
          "The join Method"
        ]
      },
      "requirements": [
        "Basic / intermediate experience with Microsoft Excel or another spreadsheet software (common functions, vlookups, Pivot Tables etc)",
        "Basic experience with the Python programming language",
        "Strong knowledge of data types (strings, integers, floating points, booleans) etc"
      ],
      "description": "Student Testimonials:\nThe instructor knows the material, and has detailed explanation on every topic he discusses. Has clarity too, and warns students of potential pitfalls. He has a very logical explanation, and it is easy to follow him. I highly recommend this class, and would look into taking a new class from him. - Diana\n\nThis is excellent, and I cannot complement the instructor enough. Extremely clear, relevant, and high quality - with helpful practical tips and advice. Would recommend this to anyone wanting to learn pandas. Lessons are well constructed. I'm actually surprised at how well done this is. I don't give many 5 stars, but this has earned it so far. - Michael\n\nThis course is very thorough, clear, and well thought out. This is the best Udemy course I have taken thus far. (This is my third course.) The instruction is excellent! - James\n\nWelcome to the most comprehensive Pandas course available on Udemy! An excellent choice for both beginners and experts looking to expand their knowledge on one of the most popular Python libraries in the world!\nData Analysis with Pandas and Python offers 19+ hours of in-depth video tutorials on the most powerful data analysis toolkit available today. Lessons include:\ninstalling\nsorting\nfiltering\ngrouping\naggregating\nde-duplicating\npivoting\nmunging\ndeleting\nmerging\nvisualizing\nand more!\nWhy learn pandas?\n\nIf you've spent time in a spreadsheet software like Microsoft Excel, Apple Numbers, or Google Sheets and are eager to take your data analysis skills to the next level, this course is for you!\n\n\n\nData Analysis with Pandas and Python introduces you to the popular Pandas library built on top of the Python programming language.\nPandas is a powerhouse tool that allows you to do anything and everything with colossal data sets -- analyzing, organizing, sorting, filtering, pivoting, aggregating, munging, cleaning, calculating, and more!\nI call it \"Excel on steroids\"!\n\nOver the course of more than 19 hours, I'll take you step-by-step through Pandas, from installation to visualization! We'll cover hundreds of different methods, attributes, features, and functionalities packed away inside this awesome library. We'll dive into tons of different datasets, short and long, broken and pristine, to demonstrate the incredible versatility and efficiency of this package.\n\nData Analysis with Pandas and Python is bundled with dozens of datasets for you to use. Dive right in and follow along with my lessons to see how easy it is to get started with pandas!\nWhether you're a new data analyst or have spent years (*cough* too long *cough*) in Excel, Data Analysis with pandas and Python offers you an incredible introduction to one of the most powerful data toolkits available today!",
      "target_audience": [
        "Data analysts and business analysts",
        "Excel users looking to learn a more powerful software for data analysis"
      ]
    },
    {
      "title": "Talend + SQL + Datawarehousing - Beginner to Professional",
      "url": "https://www.udemy.com/course/talendsql/",
      "bio": "Talend + SQL + Datawarehousing",
      "objectives": [
        "Learn Basic concepts of ETL",
        "Learn Talend Open Studio for Data Integration with scenarios to implement.",
        "Learn Basic concepts of Datawareshousing",
        "Learn Basic concepts of SQL"
      ],
      "course_content": {
        "Introduction to our Talend Course": [
          "Introduction about Me and the Course",
          "What is ETL? What are the advantages of Talend over other tools?"
        ],
        "Installation of Java, Talend": [
          "Resource - Installation of Free Talend, PostgreSQL, Notepad++, Java",
          "Installation of Talend 7.3 on Windows",
          "Installation of Talend 8 on Windows",
          "Generic Folder Structure on the machine as best practice"
        ],
        "Talend Studio Tour": [
          "Few Tips for best learning and use of course",
          "Repository, Designer, Palette, Outline, Configuration tab",
          "Creating your first job, Java behind Talend"
        ],
        "Import/Export jobs in Talend": [
          "Exporting/Importing job"
        ],
        "Reading the data from files (Extract)": [
          "Extracting from File Excel - tFileInputExcel",
          "Extracting from File delimited - tFileInputDelimited",
          "Extracting from File delimited/Excel using Metadata",
          "Assignment"
        ],
        "Reading the data from Databse (Extract)": [
          "Installation of PostgreSQL and Dbeaver",
          "Basic of SQL - Create, drop, insert, truncate delete records into the table",
          "Basic of SQL - Delete, Alter, Update, Commit, Rollback",
          "Extracting from DB table/using Metadata - tDBInput",
          "Assignment"
        ],
        "Process the Data (Transformation) - Processing components": [
          "Processing component - tFilterRow",
          "Using SQL - Where clause",
          "tFilterColumn",
          "tSortRow and Using SQL - Order by clause",
          "Assignment"
        ],
        "Load the data (Load)": [
          "Load the data into File, Excel - tFileOutputDelimited, tFileOutputExcel",
          "Load the data into Table - tDBOutput",
          "Assignment"
        ],
        "File Management Components": [
          "Connection Links",
          "tFileList",
          "tFileCopy, tFileDelete",
          "tFileArchive, tFileUnarchive",
          "tFileCompare, tFileRowCount, tFileProperties",
          "tChangeFileEncoding, tFileTouch",
          "Assignment"
        ],
        "Processing Components": [
          "tMap basics",
          "tMap advance",
          "tMap Join",
          "SQL - Joins",
          "tJoin",
          "tReplace,tConvertType",
          "SQL- Replace, Typecasting",
          "tAggregateRow, tAggregateRowSorted",
          "SQL - Aggregate Functions",
          "tNormalize, tDenormalize",
          "tSampleRow",
          "Assignment"
        ]
      },
      "requirements": [
        "No Programming experience needed, you will learn everything during the course. Follow through the class and practice!!"
      ],
      "description": "Talend is an Open Source/Enterprise ETL Tool, which can be used by Small to Large scale companies to perform Extract Transform and Load their data into Databases or any File Format (Talend supports almost all file formats and Database vendors available in the market including Cloud and other niche services).\nThis Course is for anyone who wants to learn Talend from Beginner to Professional, it will also help in Enhancing your skills if you have prior experience with the tool.\nIn the course we teach Talend - ETL tool, PostgreSQL - SQL and all the basic Datawarehousing concepts that you would need to work and excel in the organization or freelance.\nWe give real world scenarios and try to explain the use of component so that it becomes more relevant and useful for your real world projects.\nPrepares you for the Certification Exam.\nBy the end of the Course you will become the Master in Talend Data Integration and will help you land the job as ETL or Talend Developer, which is high in demand.\nCall/Message us for Virtual training and many more courses: 9059731631\n\n\nPrerequisites ?\nBasic Knowledge of working on PC\nTarget Audience ?\nAnyone who wants to enter the IT industry from non technical background.\nAnyone who wants to enhance their concepts of Talend Studio to perform data integration.\nAnyone who wants to get job as a Talend Developer.\nAnyone who wants to learn basics of SQL.\nAnyone who wants to learn basics of datawarehousing.\nSystem Requirements ?\nPC or lappy with preferably more than 4GB RAM and i3 above processor.\nTalend Open Studio Software - FREE for everyone\nPostgreSQL Database - FREE for local implementation",
      "target_audience": [
        "Anyone who wants to get through the Talend Certification or want to get job as ETL developer"
      ]
    },
    {
      "title": "Want to be a Big Data Scientist?",
      "url": "https://www.udemy.com/course/what-to-be-a-big-data-scientist/",
      "bio": "Should you pursue a career in Data Science? Data Science basics, process, team, roles, skills, transition, opportunities",
      "objectives": [
        "Understanding of Data Science concepts and Process",
        "Knowledge or roles and suitability for them",
        "Challenges in Data Science",
        "Building a transition Plan"
      ],
      "course_content": {
        "What is Data Science ?": [
          "Introduction",
          "Data Science Definition",
          "Elements of Data Science",
          "Life Cycle of a Data Science project",
          "Data Science Use Cases & R Example",
          "Big Data and Data Science"
        ],
        "How to become a Big Data Scientist?": [
          "Data Science Teams and Responsibilities",
          "Roles and Skills needed",
          "Challenges faced by Data Scientists",
          "Transitioning into Data Science",
          "Finding your first opportunity",
          "Closing Remarks and Next Steps"
        ]
      },
      "requirements": [
        "A Background in S/W development, Statistics or Business",
        "Understanding of Data Analytics"
      ],
      "description": "\"Data Science is the sexiest job of the 21st century - It has exciting work and incredible pay\". You have been hearing about this a lot. You try to get more information on this and start querying and browsing. You get so many definitions, requirements, predictions, opinions and recommendations. At the end, you are perplexed. And you ask - \"What exactly is this field all about? Is it a good career option for me?\"\n**** Please note: This is a career advice course, not a technology course.\nData Science has been growing exponentially in the last 5 years. It is also a hybrid field that requires multiple skills and training. We have been training students in Data Science. A number of them committed to training without realizing what it really is. Some were happy, some adjusted their expectations and some regretted committing too soon. We felt that professionals thinking of getting into Data Science needed a primer in what this field is all about. Hence, we came up with this course.\nThrough this course, you will learn about\nData Science goals and concepts\nProcess Flow\nRoles and Responsibilities\nWhere you will fit in to a Data Science team.\nBuilding a transition plan\nGetting into the Data Science field involves significant investment of time. Learn about the field in order to make an informed decision.",
      "target_audience": [
        "Professionals looking to switch to a career in Data Science",
        "Professional needing career advice on how to get work in Data Science"
      ]
    },
    {
      "title": "2025 Mastering dbt (Data Build Tool) - From Beginner to Pro",
      "url": "https://www.udemy.com/course/mastering-dbt-data-build-tool-bootcamp/",
      "bio": "Hands-on Analytics Engineering Bootcamp With: Theory, Building a dbt Project from Scratch, and Deploying to dbt Cloud",
      "objectives": [
        "How to build a complete dbt project from scratch",
        "The main benefits of dbt, and a bit of background as to how it came about",
        "All of the dbt fundamentals: sources, models, tests, documentation, snapshots, seeds, macros, hooks, and operations",
        "How to structure a dbt project: staging, intermediate, and mart models - and naming conventions",
        "How to version control changes to your code with GitHub and VSCode",
        "Advanced dbt testing - creating your own custom singular & generic tests, setting severity, and setting warn/error thresholds",
        "Advanced dbt data modelling - model materialisation and governance (access, contracts, and versions)",
        "Advanced dbt commands - how to use different selectors, different profiles, tags, indirect test selection and building a local dbt documents site",
        "Advanced dbt jinja & macros - creating your own macros to use in hooks / functions / operations, using jinja for loops and variables, and the target function",
        "How to deploy your project on dbt Cloud, how to use the dbt Cloud UI, and using environment variables",
        "How to use tests & macros from external packages to supercharge your dbt project",
        "Best practises to use when running a dbt project (based on lots of experience!)",
        "How to create a complete setup for Mac or Windows: installing all of the tools and getting a dbt specific VSCode setup!"
      ],
      "course_content": {
        "Introduction": [
          "Instructor Introduction",
          "Course outline",
          "Course Introduction",
          "A Brief History of the Data Stack",
          "Benefits of dbt - Inferring Dependencies",
          "Benefits of dbt - Documentation & Testing",
          "Benefits of dbt - Python-Like Functionality",
          "How dbt Has Solved a Lot of Problems in the Data Stack",
          "How dbt Fits in the Data Stack",
          "dbt Core vs. dbt Cloud",
          "Section Recap"
        ],
        "Getting Set Up with Your Tools": [
          "Section Overview",
          "Note on Continual Course Updates",
          "Help If You Get Stuck During This Course",
          "Creating a Gmail Account",
          "Setting up a BigQuery Project With Billing",
          "(Optional) If You Have Issues With BigQuery Billing",
          "The BigQuery UI",
          "The Dataset You'll Be Using",
          "(Mac) Installing Python 3.10",
          "(Windows) Installing Python 3.10",
          "Downloading VSCode and Setting Up Shortcuts",
          "Creating a GitHub account",
          "Forking Vs. Cloning",
          "Forking the Repository",
          "(Optional) If You Have Issues Syncing Your Forked Repository",
          "Installing the recommended VSCode Extensions",
          "What's a Virtual Environment (venv)?",
          "Setting Up Our Virtual Environment and Installing Packages",
          "Setting Up dbt for BigQuery",
          "Trialling Our Model dbt Project",
          "(Optional) Setting Up dbt Autocomplete",
          "Run Through of How Our Final Project Will Look",
          "Section Recap"
        ],
        "Building the Basic dbt Project": [
          "Section Overview",
          "The dbt init Command",
          "Version Control with GitHub",
          "Setting up dbt Power User",
          "How We'll Structure Our Project",
          "Creating Our First Source (src) yml File",
          "(Windows) Issues with the dbt Power User extension",
          "Creating Our First Staging (stg) SQL Model",
          "Running Our First Staging (stg) SQL Model",
          "Creating Our First Model yml File",
          "Adding Tests to Our First Model yml File",
          "Setting Up Our Models to Materialise as Tables Instead of Views",
          "Getting the Rest of Our Staging (stg) SQL Models Set Up",
          "Using dbt clean to Get Table Materialisation Working",
          "Getting the Rest of the Staging (stg) yml Files Set Up",
          "Taking Stock of Our Staging (stg) Data Models",
          "The Target Folder",
          "Getting Our First Intermediate (int) SQL Model Set Up",
          "Getting Our First Intermediate (int) yml File Set Up",
          "Getting Our Mart SQL Model Set Up",
          "Getting Our Mart yml File Set Up",
          "Our Basic dbt Project Is Now Complete!",
          "Section Recap"
        ],
        "Advanced dbt: Testing": [
          "Section Overview",
          "Setting Default Test Severity",
          "Setting Test Severity and Thresholds",
          "The External dbt Packages We'll Be Using",
          "dbt_utils and dbt_expectations",
          "Custom Singular Tests",
          "Custom Generic Tests",
          "Applying Advanced Tests to Our Whole Project",
          "Source Freshness Tests",
          "Unit Tests",
          "Section Recap"
        ],
        "Advanced dbt: Data Modelling": [
          "Section Overview",
          "The doc Function",
          "Seed Files",
          "dbt Snapshots",
          "Materialisation Types",
          "Materialisation: Ephemeral Models",
          "Materialisation: Incremental Models",
          "Materialisation: Microbatch Incremental Models",
          "(Optional) Partitioning a Table in BigQuery",
          "Model Governance Overview",
          "Model Governance - Access & Groups",
          "Model Governance - Contracts",
          "Model Governance - Versions",
          "Section Recap"
        ],
        "Advanced dbt: Commands and Selectors": [
          "Section Overview",
          "Commands For a Clean dbt run",
          "Using Different dbt Profiles",
          "Selectors",
          "Tags",
          "Indirect Test Selection",
          "dbt test With --warn-error",
          "dbt build",
          "dbt docs generate / serve",
          "The \"Empty\" Flag",
          "Section Recap"
        ],
        "Advanced dbt: Jinja and Macros": [
          "Section Overview",
          "Jinja Comments, Statements, and Expressions",
          "The 3 Types of Macro: Functions, Hooks, Operations",
          "(Optional) dbt Jinja Function Reference",
          "Macros: Operations",
          "Macros: Functions (Building a Basic Macro)",
          "Macros: Hooks",
          "Jinja Statements: for Loops and Setting Variables",
          "(Optional) Jinja: Using the Target Function",
          "Section Recap"
        ],
        "dbt Cloud": [
          "Section Overview",
          "Creating a dbt Cloud Account",
          "Setting Up a Service Account",
          "Connecting GitHub to dbt Cloud",
          "The dbt Cloud IDE",
          "Deploying Jobs on dbt Cloud",
          "Section Recap"
        ]
      },
      "requirements": [
        "Basic SQL",
        "No Python experience needed",
        "Mac / Windows machine which is capable of installing Python, Git, and VSCode (we'll run through all of this in the course!)"
      ],
      "description": "A complete course to help anyone with basic SQL skills learn advanced dbt, a key tool for Analytics Engineering!\nWelcome to the 2025 Mastering dbt (data build tool) course! This course runs through everything from the theory behind dbt to building an advanced dbt project (from scratch) and deploying it on dbt Cloud.\nI have over 9 years of experience across Analytics / Analytics Engineering / Data Science, including 5 years using dbt on a daily basis. I was also involved in the rollout of dbt in my time at Monzo Bank!\nIn this course I've taken everything I've learnt over the past 5 years, and what I use on a daily basis, and condensed it to take anyone who knows SQL to an advanced level of dbt as quickly as possible.\n\n\nMY APPROACH TO THIS COURSE:\nWe'll cover everything you need to know about dbt: from the basic data modelling right through to all of the advanced features such as creating custom tests and macros. We'll be doing this step by step, and build from the basics upwards.\nIt's focused on practical outcomes - we won't be spending ages on database theory, or going into lots of detail on the eCommerce dataset we'll be using, instead we'll be aiming to get you up to advanced dbt levels as quickly as possible.\nFor every video where we're writing code, I've created lesson attachments with the final outputs. This means you can either code as you go along, or watch the videos and look at the handouts afterwards! I've also included some theory with these handouts to help hammer home the points made in the videos.\nThere's also a public GitHub repository (which you'll be using for this course) that contains a model final project you can reference throughout.\n\n\nThis course isn't static! I'd love to hear your feedback and will be updating this course on an ongoing basis.\n\n\nCOURSE STRUCTURE:\nThis course focuses on first getting a good understanding of what problems dbt solves, then building a basic dbt project, before layering on more advanced concepts and finally deploying our project with dbt Cloud.\n\n\nIntroduction\nSome theory (<1 hour) around dbt, what problems existed in the data stack before it came along, and how it solves them.\n\n\nTool setup\nGetting set up with Python, GitHub, Google BigQuery, VSCode, and of course dbt! If you're familiar with any of these tools already then you are more than welcome to skip the appropriate lessons.\n\nWe'll also be exploring the fictional eCommerce dataset that we'll be using throughout the course.\n\n\nBuilding our basic dbt project\nThis section focuses on creating our project from scratch, including how we will structure our project.\n\nWe'll be building out staging (stg), intermediate (int), and mart data models, including documentation & testing with the out-of-the-box dbt tests.\n\n\nAdvanced dbt testing\nWe'll start to build on our basic dbt project by setting test severity & thresholds, using the dbt-utils and dbt-expectations external packages for their excellent selection of tests, creating our own custom singular & generic tests, unit testing, and testing the freshness of our source data.\n\n\nAdvanced data modelling with dbt\nNext, we'll be looking at how we can create reusable documentation, seed files (version controlled .csv files), snapshots (capturing changes to data tables), and materialisation methods.\n\nMost of this section will be focused on the last part - the materialisation methods: ephemeral, view, table, and incremental (including microbatch). By this point we'll have encountered view & table models and we will be building both an incremental and an ephemeral model - and you will gain an understanding of what to use and when.\n\nThis section includes all model governance features from dbt version 1.5! This includes model access, groups, contracts, and versions.\n\n\n\nAdvanced dbt commands\nThis section will focus less on changing our dbt project, but instead all of the major dbt commands and how (and when) to use them.\n\n\n\nAdvanced Jinja & macros\nThe final changes to our project will involve using Jinja - a core feature of dbt and arguably it's most complex but powerful feature - and using it to create our own macros.\n\nThis section will run through how you can use Jinja macros for hooks, operations, and as reusable functions in your SQL models. It'll also run through some theory around Jinja, common mistakes, and what I (personally) find to be what it's most useful for!\n\n\n\ndbt Cloud\nFinally, we'll be exploring how to take our project and deploy it on dbt Cloud - including how to schedule it to run on a regular basis. We'll also be looking at dbt Cloud itself and its main benefits.",
      "target_audience": [
        "Data Analysts",
        "Data Scientists",
        "Analytics Engineers",
        "Data Engineers",
        "BI Professionals",
        "Anyone interested in getting into data!"
      ]
    },
    {
      "title": "Deep Learning: GANs and Variational Autoencoders",
      "url": "https://www.udemy.com/course/deep-learning-gans-and-variational-autoencoders/",
      "bio": "Generative Adversarial Networks and Variational Autoencoders in Python, Theano, and Tensorflow",
      "objectives": [
        "Learn the basic principles of generative models",
        "Build a variational autoencoder in Theano and Tensorflow",
        "Build a GAN (Generative Adversarial Network) in Theano and Tensorflow",
        "Understand important foundations for OpenAI ChatGPT, GPT-4, DALL-E, Midjourney, and Stable Diffusion"
      ],
      "course_content": {
        "Introduction and Outline": [
          "Welcome",
          "Where does this course fit into your deep learning studies?",
          "Where to get the code and data",
          "How to Succeed in this Course"
        ],
        "Generative Modeling Review": [
          "What does it mean to Sample?",
          "Sampling Demo: Bayes Classifier",
          "Gaussian Mixture Model Review",
          "Sampling Demo: Bayes Classifier with GMM",
          "Why do we care about generating samples?",
          "Neural Network and Autoencoder Review",
          "Tensorflow Warmup",
          "Theano Warmup",
          "Suggestion Box"
        ],
        "Variational Autoencoders": [
          "Variational Autoencoders Section Introduction",
          "Variational Autoencoder Architecture",
          "Parameterizing a Gaussian with a Neural Network",
          "The Latent Space, Predictive Distributions and Samples",
          "Cost Function",
          "Tensorflow Implementation (pt 1)",
          "Tensorflow Implementation (pt 2)",
          "Tensorflow Implementation (pt 3)",
          "The Reparameterization Trick",
          "Theano Implementation",
          "Visualizing the Latent Space",
          "Bayesian Perspective",
          "Variational Autoencoder Section Summary"
        ],
        "Generative Adversarial Networks (GANs)": [
          "GAN - Basic Principles",
          "GAN Cost Function (pt 1)",
          "GAN Cost Function (pt 2)",
          "DCGAN",
          "Batch Normalization Review",
          "Fractionally-Strided Convolution",
          "Tensorflow Implementation Notes",
          "Tensorflow Implementation",
          "Theano Implementation Notes",
          "Theano Implementation",
          "GAN Summary"
        ],
        "Theano and Tensorflow Basics Review": [
          "(Review) Theano Basics",
          "(Review) Theano Neural Network in Code",
          "(Review) Tensorflow Basics",
          "(Review) Tensorflow Neural Network in Code"
        ],
        "Appendix / FAQ Intro": [
          "What is the Appendix?"
        ],
        "Setting Up Your Environment (FAQ by Student Request)": [
          "Pre-Installation Check",
          "Anaconda Environment Setup",
          "How to How to install Numpy, Theano, Tensorflow, etc..."
        ],
        "Extra Help With Python Coding for Beginners (FAQ by Student Request)": [
          "How to Code by Yourself (part 1)",
          "How to Code by Yourself (part 2)",
          "Proof that using Jupyter Notebook is the same as not using it",
          "Python 2 vs Python 3",
          "Is Theano Dead?"
        ],
        "Effective Learning Strategies for Machine Learning (FAQ by Student Request)": [
          "How to Succeed in this Course (Long Version)",
          "Is this for Beginners or Experts? Academic or Practical? Fast or slow-paced?",
          "Machine Learning and AI Prerequisite Roadmap (pt 1)",
          "Machine Learning and AI Prerequisite Roadmap (pt 2)"
        ],
        "Appendix / FAQ Finale": [
          "BONUS"
        ]
      },
      "requirements": [
        "Know how to build a neural network in Theano and/or Tensorflow",
        "Probability",
        "Multivariate Calculus",
        "Numpy, etc."
      ],
      "description": "Ever wondered how AI technologies like OpenAI DALL-E, Midjourney, and Stable Diffusion really work? In this course, you will learn the foundations of these groundbreaking applications.\nVariational autoencoders and GANs have been 2 of the most interesting developments in deep learning and machine learning recently.\nYann LeCun, a deep learning pioneer, has said that the most important development in recent years has been adversarial training, referring to GANs.\nGAN stands for generative adversarial network, where 2 neural networks compete with each other.\nWhat is unsupervised learning?\nUnsupervised learning means we’re not trying to map input data to targets, we’re just trying to learn the structure of that input data.\nOnce we’ve learned that structure, we can do some pretty cool things.\nOne example is generating poetry - we’ve done examples of this in the past.\n\nBut poetry is a very specific thing, how about writing in general?\nIf we can learn the structure of language, we can generate any kind of text. In fact, big companies are putting in lots of money to research how the news can be written by machines.\nBut what if we go back to poetry and take away the words?\nWell then we get art, in general.\nBy learning the structure of art, we can create more art.\nHow about art as sound?\nIf we learn the structure of music, we can create new music.\nImagine the top 40 hits you hear on the radio are songs written by robots rather than humans.\nThe possibilities are endless!\nYou might be wondering, \"how is this course different from the first unsupervised deep learning course?\"\nIn this first course, we still tried to learn the structure of data, but the reasons were different.\n\nWe wanted to learn the structure of data in order to improve supervised training, which we demonstrated was possible.\nIn this new course, we want to learn the structure of data in order to produce more stuff that resembles the original data.\nThis by itself is really cool, but we'll also be incorporating ideas from Bayesian Machine Learning, Reinforcement Learning, and Game Theory. That makes it even cooler!\nThanks for reading and I’ll see you in class. =)\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\nCalculus\nProbability\nObject-oriented programming\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations\nLinear regression\nGradient descent\nKnow how to build a feedforward and convolutional neural network in Theano or TensorFlow\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)\n\n\nUNIQUE FEATURES\nEvery line of code explained in detail - email me any time if you disagree\nNo wasted time \"typing\" on the keyboard like other courses - let's be honest, nobody can really write code worth learning about in just 20 minutes from scratch\nNot afraid of university-level math - get important details about algorithms that other courses leave out",
      "target_audience": [
        "Anyone who wants to improve their deep learning knowledge"
      ]
    },
    {
      "title": "Natural Language Processing for Text Summarization",
      "url": "https://www.udemy.com/course/text-summarization-natural-language-processing-python/",
      "bio": "Understand the basic theory and implement three algorithms step by step in Python! Implementations from scratch!",
      "objectives": [
        "Understand the theory and mathematical calculations of text summarization algorithms",
        "Implement the following summarization algorithms step by step in Python: frequency-based, distance-based and the classic Luhn algorithm",
        "Use the following libraries for text summarization: sumy, pysummarization and BERT summarizer",
        "Summarize articles extracted from web pages and feeds",
        "Use the NLTK and spaCy libraries and Google Colab for your natural language processing implementations",
        "Create HTML visualizations for the presentation of the summaries"
      ],
      "course_content": {
        "Introduction": [
          "Course content",
          "Introduction to natural language processing",
          "Source code and slides"
        ],
        "Frequency-based algorithm": [
          "Plan of attack",
          "Algorithm - intuition",
          "Preprocessing the texts 1",
          "Preprocessing the texts 2",
          "Word frequency",
          "Weighted word frequency",
          "Sentence tokenization",
          "Generating the summary",
          "Visualizing the summary in HTML",
          "Extracting texts from the Internet",
          "Function to summarize the texts",
          "Function to visualize the results",
          "Summarizing multiple texts"
        ],
        "Luhn algorithm": [
          "Plan of attack",
          "Preparing the environment",
          "Implementation 1",
          "Implementation 2",
          "Implementation 3",
          "Implementation 4",
          "Implementation 5",
          "Extracting texts from the Internet",
          "Reading articles from RSS feeds",
          "Word cloud",
          "Extracting named entities",
          "Summarizing articles from feed",
          "Summary in HTML files"
        ],
        "Cosine similarity": [
          "Plan of attack",
          "Preparing the environment",
          "Similarity between sentences 1",
          "Similarity between sentences 2",
          "Similarity matrix",
          "Summarizing texts",
          "Extracting texts from the Internet"
        ],
        "Libraries for text summarization": [
          "Plan of attack",
          "Preparing the environment",
          "Sumy library",
          "Pysummarization library",
          "BERT summarizer library",
          "Additional content: abstractive summarization"
        ],
        "Final remarks": [
          "Final remarks",
          "BONUS"
        ]
      },
      "requirements": [
        "Programming logic",
        "Basic Python programming"
      ],
      "description": "The area of Natural Language Processing (NLP) is a subarea of Artificial Intelligence that aims to make computers capable of understanding human language, both written and spoken. Some examples of practical applications are: translators between languages, translation from text to speech or speech to text, chatbots, automatic question and answer systems (Q&A), automatic generation of descriptions for images, generation of subtitles in videos, classification of sentiments in sentences, among many others! Another important application is the automatic document summarization, which consists of generating text summaries. Suppose you need to read an article with 50 pages, however, you do not have enough time to read the full text. In that case, you can use a summary algorithm to generate a summary of this article. The size of this summary can be adjusted: you can transform 50 pages into only 20 pages that contain only the most important parts of the text!\nBased on this, this course presents the theory and mainly the practical implementation of three text summarization algorithms: (i) frequency-based, (ii) distance-based (cosine similarity with Pagerank) and (iii) the famous and classic Luhn algorithm, which was one of the first efforts in this area. During the lectures, we will implement each of these algorithms step by step using modern technologies, such as the Python programming language, the NLTK (Natural Language Toolkit) and spaCy libraries and Google Colab, which will ensure that you will have no problems with installations or configurations of software on your local machine.\nIn addition to implementing the algorithms, you will also learn how to extract news from blogs and the feeds, as well as generate interesting views of the summaries using HTML! After implementing the algorithms from scratch, you have an additional module in which you can use specific libraries to summarize documents, such as: sumy, pysummarization and BERT summarizer. At the end of the course, you will know everything you need to create your own summary algorithms! If you have never heard about text summarization, this course is for you! On the other hand, if you are already experienced, you can use this course to review the concepts.",
      "target_audience": [
        "People interested in natural language processing and text summarization",
        "People interested in the spaCy and NLTK libraries",
        "Students who are studying subjects related to Artificial Intelligence",
        "Data Scientists who want to increase their knowledge in natural language processing",
        "Professionals interested in developing text summarization solutions",
        "Beginners who are starting to learn natural language processing"
      ]
    },
    {
      "title": "RAG, AI Agents and Generative AI with Python and OpenAI 2025",
      "url": "https://www.udemy.com/course/generative-ai-rag/",
      "bio": "Mastering Retrieval-Augmented Generation (RAG), Generative AI (Gen AI), AI Agents, Agentic RAG, OpenAI API with Python",
      "objectives": [
        "Build Retrieval-Augmented Generation (RAG) systems using Python and OpenAI.",
        "Develop AI Agents with state management and memory using OpenAI Swarm.",
        "Master Generative AI models like OpenAI GPTs for text generation.",
        "Leverage FAISS and LangChain for efficient retrieval systems.",
        "Integrate Multimodal RAG using text, audio, and images with Whisper and CLIP models.",
        "Build real-world projects, including a capstone project analyzing financial data.",
        "Stay ahead with the latest advancements in AI, Generative AI, and AI Agents in 2025.",
        "Develop AI Agents using CrewAI for advanced task automation and orchestration.",
        "Deploy an Agentic RAG System with LangGraph for a Digital Waiter.",
        "Fine-tune GPT-4o models using Python for customized AI solutions."
      ],
      "course_content": {
        "RAG and Generative AI with Python": [
          "RAG, AI Agents and Generative AI with Python and OpenAI Promotional Video",
          "Course Overview: RAG, AI Agents and Generative AI with Python and OpenAI",
          "RAG and Generative AI Course Assistant Link",
          "Your RAG, AI Agents and Generative AI Course Assistant is Here",
          "Diogo's Introduction and Background",
          "[ACTION] Get Your Course Materials: RAG and Generative AI",
          "Unlimited Updates and Enhancements 2025",
          "Submit Your Update and Enhancement Requests Here"
        ],
        "Python for RAG and AI": [
          "Game for Python for RAG and AI",
          "Functions",
          "Exercise 1",
          "Exercise 2",
          "Exercise 3",
          "Introduction to Classes",
          "Exercise 4",
          "Exercise 5",
          "Exercise 6"
        ],
        "Python Self Assessment - One Piece Edition": [
          "Python Self-Assessment Notes",
          "Why take this self-assessment?",
          "Gear Transformation Power Calculation",
          "Solutions - Gear Transformation Power Calculation",
          "Poneglyph Collection",
          "Solutions - Poneglyph Collection",
          "Devil Fruit User Class",
          "Solutions - Devil Fruit User Class",
          "Section Feedback"
        ],
        "Basics of Retrieval Systems for RAG and Generative AI": [
          "Game Plan for Fundamentals of Retrieval Systems",
          "Overview of Information Retrieval",
          "Understanding Tokenization in NLP",
          "OpenAI Tokenizer",
          "Haven't Downloaded the Course Materials?",
          "Python - Libraries and Data Handling for RAG",
          "Python - Tokenization Techniques",
          "Python - Preprocessing Steps",
          "Types of Retrieval Systems",
          "Vector Space Model (TF-IDF)",
          "Python - Implementing TF-IDF Part 1",
          "Python - Implementing TF-IDF Part 2",
          "Python - TF-IDF Function and Output Analysis",
          "Boolean Retrieval Model",
          "Python - Preprocessing Steps Part 2",
          "Python - Setting a Directory",
          "Python - Boolean Retrieval Implementation",
          "Probabilistic Retrieval Model",
          "Python - Probabilistic Retrieval Model Setup",
          "Python - Probabilistic Retrieval Model",
          "How Google Search Works?",
          "Key Concepts: Indexing, Querying, and Ranking",
          "Section Recap: Key Learnings",
          "Section Feedback"
        ],
        "Scientific Literature Review - Prompt Engineering": [
          "Chain-of-Thought Prompt Engineering",
          "ReAct Prompt Engineering",
          "Section Feedback"
        ],
        "Basics of Generation Models for RAG": [
          "Game Plan for Basics of Generation Models",
          "Introduction to Text Generation",
          "Understanding Transformers",
          "Rock-Paper-Scissors, Dices, and Strawberries",
          "Haven't Downloaded the Course Materials?",
          "Getting a Hugging Face Key",
          "Quick note about the next video",
          "June 2025: Issues with the Microsoft Phi 3.5 mini",
          "Python - Langchain and Hugging Face Setup",
          "Python - Basic Text Generation",
          "Attention Mechanisms in NLP",
          "What is the System Message and Parameters?",
          "Understanding Generation Model Parameters",
          "System Message and Parameters",
          "Python - Text Generation with System Message",
          "Python - Text Generation with Parameters",
          "OpenAI Playground - Top P",
          "Basics of Generation Models Recap: Key Learnings",
          "Section Feedback"
        ],
        "Scientific Literature Review - LLMs": [
          "LLMs, Few-shot, Scaling, and Factuality",
          "Section Feedback"
        ],
        "Introduction to RAG": [
          "Game Plan for Integrating Retrieval and Generation",
          "Introduction to RAG Architecture",
          "Haven't Downloaded the Course Materials?",
          "Python - Hugging Face Setup",
          "Python - Tokenization and Embeddings for RAG",
          "FAISS Index: Efficient Similarity Search",
          "Python - Building a Retrieval System",
          "Quick note about the next video",
          "June 2025: Issues with Microsoft Phi 3.5 mini",
          "Python - Developing a Generative Model",
          "Python - Implementing the RAG System",
          "What Have We Learned and Where Do We Go from Here?",
          "Meeting with General Manager",
          "Section Feedback"
        ],
        "Scientific Literature Review - RAG": [
          "LongRAG and LightRAG",
          "Section Feedback"
        ],
        "OpenAI API": [
          "Game Plan for OpenAI API",
          "OpenAI API for Text",
          "Haven't Downloaded the Course Materials?",
          "Python - Setting Up OpenAI API Key",
          "Python - OpenAI API Setup",
          "Python - Generating Text with OpenAI API",
          "Python - OpenAI API Parameters",
          "OpenAI API for Images",
          "Python - With Image URL",
          "Python - Converting Images to Base64",
          "Python - Assess My Python Course Thumbnail",
          "Key Learnings and Outcomes: OpenAI API",
          "Section Feedback"
        ]
      },
      "requirements": [
        "Python Proficiency (For Loops, Functions)"
      ],
      "description": "UPDATES JUNE 2025\nLaunched 2 sections: Image Generation with OpenAI and Reasoning Models\nMCP is now live!\nUPDATES MAY 2025\nLaunch of 2 new sections: RAG with OpenAI File Search and RAGAS\nMinor video remakes due to mistakes.\nUPDATES APRIL 2025:\nRemake of 3 sections: Retrieval Fundamentals, Generative Fundaments and Introduction to RAG\nAdded Knowledge Graphs with Light RAG\nUPDATES DECEMBER 2024:\nFine Tuning OpenAI GPT-4o\nPython Crash Course + Self-assessment\nUPDATES NOVEMBER 2024:\nCrewAI and CrewAI Capstone Project launched\nThe section on OpenAI API for Text and Images is live + OpenAI API Capstone Project\nUPDATES OCTOBER 2024:\nOpenAI Swarm is live\nAgentic RAG is live\nMultimodal RAG Project is live\n\n\nUnlock the Power of RAG, AI Agents, and Generative AI with Python and OpenAI in 2025!\nWelcome to \"RAG, AI Agents, and Generative AI with Python and OpenAI 2025\"—the ultimate course to master Retrieval-Augmented Generation (RAG), AI Agents, and Generative AI using Python and OpenAI's cutting-edge technologies.\nIf you aspire to become a leader in artificial intelligence, machine learning, and natural language processing, this is the course you've been waiting for!\n\n\nWhy Choose This Course?\nComprehensive Curriculum: Dive deep into RAG systems, AI agents, and generative AI models with over 300 lectures and 30 extensive sections.\nHands-On Python Projects: Implement real-world applications using Python, OpenAI GPT models, FAISS, LangChain, and more.\nLatest Technologies: Stay ahead with the most recent advancements in OpenAI, Generative AI, Multimodal RAG, and AI Agents.\nExpert Instruction: Learn from Diogo, an industry expert with years of experience in AI and machine learning.\nUnlimited Updates: Get access to course enhancements and updates through 2025!\n\n\nAbout Your Instructor\nHi, I'm Diogo, a data expert with a Master's degree in Management specializing in Analytics from ESMT Berlin.\nWith extensive experience tackling complex business challenges—from managing billion-euro sales planning to conducting A/B tests that led to significant investments—I bring real-world expertise to this course.\nAs a startup founder helping restaurants worldwide optimize menus and pricing through data insights, I'm passionate about leveraging AI for practical solutions.\n\n\nPersonalized Support\nOne of the key benefits of this course is the direct access to me as your instructor.\nI personally respond to all your questions within 24 hours.\nNo outsourced support—just personalized guidance to help you overcome challenges and advance your skills.\n\n\nContinuous Improvements\nI'm dedicated to keeping this course up-to-date with the latest advancements in AI.\nYour feedback shapes the course—I'm always listening and ready to add new content that benefits your learning journey.\n\n\nWhat You'll Learn\nFundamentals of Retrieval Systems: Understand tokenization, indexing, querying, and ranking in information retrieval.\nBasics of Generation Models: Master text generation using GPT, transformers, and attention mechanisms.\nIntegration of Retrieval and Generation: Build RAG systems combining retrieval models with generative models.\nAdvanced OpenAI GPT Models: Leverage OpenAI's GPT models for powerful text generation and embeddings.\nHandling Unstructured Data: Work with data in various formats like Excel, Word, PowerPoint, EPUB, and PDF using LangChain.\nMultimodal RAG: Explore multimodal retrieval systems incorporating text, audio, and visual data using Whisper and CLIP models.\nAI Agents and Agentic RAG: Develop AI agents with state management and memory for complex tasks using OpenAI Swarm.\nCapstone Projects: Apply your knowledge in real-world scenarios, including analyzing Starbucks financial data and building robust retrieval systems.\n\n\nWhy Master RAG and AI Agents Now?\nThe future of AI lies in systems that can retrieve relevant information and generate intelligent responses—Retrieval-Augmented Generation is at the forefront of this revolution.\nBy mastering RAG, AI agents, and generative models, you position yourself at the cutting edge of technology, making you invaluable in today's tech landscape.\n\n\nGet Started Today!\nLifetime Access: Enroll now and get lifetime access to all course materials and updates.\nInteractive Learning: Engage with coding exercises, challenges, and real-world projects.\nSupport: Get your questions answered from me, Diogo, in less than 24 hours.\nCertification: Receive a certificate upon completion to showcase your new skills.\n\n\nDon't Miss Out!\nThe world of AI is advancing rapidly.\nStay ahead of the curve by enrolling in \"RAG, AI Agents, and Generative AI with Python and OpenAI 2025\" today. Unlock endless possibilities in AI and machine learning!\nEnroll Now and transform your career with the most comprehensive RAG and Generative AI course available!",
      "target_audience": [
        "Data Scientists and Machine Learning Engineers looking to deepen their knowledge of generative AI systems.",
        "AI Researchers and Enthusiasts interested in exploring the latest advancements in (RAG) and generative AI technologies.",
        "oftware Developers and Programmers who want to expand their skill set to include AI and machine learning techniques.",
        "Technical Product Managers and AI Strategists who manage AI projects and need a deeper technical understanding of how RAG systems work and their potential applications.",
        "AI Consultants and Data Analysts aiming to add AI capabilities to their skillset",
        "Entrepreneurs and business leaders in the tech space who want to understand the potential of RAG systems and generative AI to innovate."
      ]
    },
    {
      "title": "Probability and Statistics: Complete Course 2025",
      "url": "https://www.udemy.com/course/probability-and-statistics-complete-course/",
      "bio": "Learn the Probability and Statistics You Need to Succeed in Data Science and Business Analytics",
      "objectives": [
        "Descriptive Statistics",
        "Visualizing Data",
        "Probability Theory",
        "Bayesian Statistics",
        "Discrete Distributions (Binomial, Poisson and More)",
        "Continuous Distributions (Normal and Others)",
        "Hypothesis Tests",
        "Regression",
        "Type I and Type II Errors",
        "Chi-Squared Test"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course Overview"
        ],
        "Descriptive Statistics": [
          "Data for this chapter",
          "The Mean Average",
          "The Mean Average - Quiz",
          "The Median Average",
          "The Median Average - Quiz",
          "The Modal Average",
          "The Modal Average - Quiz",
          "Comparing Averages",
          "Comparing Averages - Quiz",
          "Quantiles, Range and Inter-Quartile Range",
          "Quantiles, Range and Inter-Quartile Range - Data",
          "Quantiles, Range and Inter-Quartile Range - Quiz",
          "Standard Deviation and Variance",
          "Standard Deviation and Variance - Data",
          "Standard Deviation and Variance - Quiz",
          "The Coefficient of Variation",
          "The Coefficient of Variation - Data",
          "The Coefficient of Variation - Quiz",
          "Skew",
          "Skew - data",
          "Skew - Quiz",
          "Kurtosis",
          "Kurtosis - Quiz",
          "Correlation Coefficients",
          "Correlation Coefficients - Data",
          "Correlation Coefficients - Quiz",
          "Slides"
        ],
        "Cleaning Data": [
          "Anomalies and Outliers",
          "Anomalies and Outliers - Data",
          "Anomalies and Outliers - Quiz",
          "Coding Your Data",
          "Coding Your Data - Quiz",
          "Slides"
        ],
        "Data Visualization": [
          "Line Graphs",
          "Line Graph - Assignment",
          "Bar Charts",
          "Bar Charts - Quiz",
          "Dual Axis Charts",
          "Dual Axis Charts - Assignment",
          "Pie Charts",
          "Pie Charts - Quiz",
          "Histograms",
          "Histograms - Data",
          "Histograms - Quiz",
          "Box Plots",
          "Box Plots - Quiz",
          "Cumulative Frequency",
          "Cumulative Frequency - Quiz",
          "Comparing Visualizations",
          "Slides"
        ],
        "Sampling": [
          "Populations and Samples",
          "Populations and Samples - Quiz",
          "Random Sampling",
          "Random Sampling - Quiz",
          "Non-Random Sampling",
          "Non-Random Sampling - Quiz",
          "Slides"
        ],
        "Probability": [
          "What is Probability?",
          "What is Probability - Quiz",
          "Set Notation",
          "Set Notation - Quiz",
          "Independent Events",
          "Independent Events - Quiz",
          "Mutually Exclusive Events",
          "Mutually Exclusive Events - Quiz",
          "Tree Diagrams",
          "Tree Diagrams - Quiz",
          "Venn Diagrams",
          "Venn Diagram - Quiz",
          "Conditional Probability",
          "Conditional Probability - Quiz",
          "Bayes' Theorem",
          "Slides"
        ],
        "Discrete Distributions": [
          "What is a Discrete Random Variable?",
          "What is a Discrete Random Variable - Quiz",
          "Probability Mass Functions",
          "Probability Mass Functions - Quiz",
          "The Expectation of a Discrete Random Variable",
          "The Expectation of a Discrete Random Variable",
          "The Variance of a Discrete Random Variable",
          "The Variance of a Discrete Random Variable - Data",
          "The Variance of a Discrete Random Variable - Quiz",
          "The Binomial Distribution - Intro",
          "The Binomial Distribution - Intro - Quiz",
          "The Binomial Distribution Formula - Part 1",
          "The Binomial Distribution Formula - Part 1",
          "The Binomial Distribution Formula - Part 2",
          "The Binomial Distribution Formula - Part 2 - Quiz",
          "Using Excel to Solve Binomial Problems",
          "Using Excel to Solve Binomial Problems",
          "Applying the Binomial Distribution to Real-World Problems",
          "Applying the Binomial Distribution to Real-World Problems - Quiz",
          "Conditional Probability with the Binomial Distribution",
          "Conditional Probability with the Binomial Distribution - Quiz",
          "The Poisson Distribution - Intro",
          "The Poisson Distribution - Intro - Quiz",
          "Using Excel to Solve Poisson Problems",
          "Using Excel to Solve Poisson Problems - Quiz",
          "Applying the Poisson Distribution Real-World Problems",
          "Applying the Poisson Distribution Real-World Problems - Quiz",
          "Conditional Probability with the Poisson Distribution",
          "Conditional Probability with the Poisson Distribution - Quiz",
          "The Geometric Distribution",
          "The Geometric Distribution - Quiz",
          "Expectation and Variance of Distributions",
          "Expectation and Variance of Distributions - Quiz",
          "Approximating the Binomial Distribution with the Poisson Distribution",
          "Approximating the Binomial Distribution with the Poisson Distribution - Quiz",
          "Derivation of the Poisson Formula",
          "Slides"
        ],
        "Continuous Distributions": [
          "What is a Continuous Distribution?",
          "What is a Continuous Distribution - Quiz",
          "The Normal Distribution - Intro",
          "The Normal Distribution - Intro - Quiz",
          "Calculating Probabilities with the Normal Distribution",
          "Calculating Probabilities with the Normal Distribution - Quiz",
          "The Inverse Normal Distribution",
          "The Inverse Normal Distribution - Quiz",
          "Z-Scores",
          "Finding Unknown Means and Standard Deviations",
          "Finding Unknown Means and Standard Deviations - Quiz",
          "Conditional Probability with the Normal Distribution",
          "Conditional Probability with the Normal Distribution - Quiz",
          "Normal Approximations to Binomial Distributions - Part 1",
          "Normal Approximations to Binomial Distributions - Part 1 - Quiz",
          "Normal Approximations to Binomial Distributions - Part 2",
          "Normal Approximations to Binomial Distributions - Part 2 - Quiz",
          "Normal Approximations to Poisson Distributions",
          "Normal Approximations to Poisson Distributions - Quiz",
          "The Central Limit Theorem",
          "The Central Limit Theorem - Quiz",
          "The Limitations of the Central Limit Theorem",
          "The Limitations of the Central Limit Theorem - Quiz",
          "Continuous Random Variables - Probability Density Functions",
          "Continuous Random Variables - Cumulative Distribution Functions",
          "Continuous Random Variables - Expectation and Variance",
          "Continuous Random Variables - Medians and Quartiles",
          "Slides"
        ],
        "Hypothesis Tests": [
          "Introduction to Hypothesis Tests - P-Values",
          "Introduction to Hypothesis Tests - P-Values",
          "Binomial Hypothesis Tests - Part 1",
          "Binomial Hypothesis Tests - Part 2",
          "Binomial Hypothesis Tests - Quiz",
          "Binomial Hypothesis Tests - Critical Regions",
          "Binomial Hypothesis Tests - Critical Regions - Quiz",
          "Two-Tailed Tests",
          "Two-Tailed Tests",
          "Poisson Hypothesis Tests",
          "Poisson Hypothesis Tests - Quiz",
          "Poisson Critical Regions",
          "Poisson Critical Regions - Quiz",
          "Normal Hypothesis Tests",
          "Normal Hypothesis Tests - Quiz",
          "Normal Hypothesis Tests - Critical Regions",
          "Normal Hypothesis Tests - Critical Regions - Quiz",
          "T-Tests",
          "Confidence Intervals",
          "Slides"
        ],
        "Regression": [
          "Correlation",
          "Data for Linear Regression Video",
          "Linear Regression",
          "Evaluating a Regression Line",
          "Correlation Hypothesis Tests - Intro",
          "Carrying Out a Test for Correlation",
          "Correlation Confidence Intervals",
          "Working with Non-Linear Data - Exponential Models",
          "Working with Non-Linear Data - Polynomial Models",
          "Slides"
        ]
      },
      "requirements": [
        "No pre-requisites for most of the course. One small optional section requires knowledge and calculus, but other than that this is suitable for beginners."
      ],
      "description": "This is course designed to take you from beginner to expert in probability and statistics. It is designed to be practical, hands on and suitable for anyone who wants to use statistics in data science, business analytics or any other field to make better informed decisions.\nVideos packed with worked examples and explanations so you never get lost, and every technique covered is implemented in Microsoft Excel so that you can put it to use immediately.\nKey concepts taught in the course are:\nDescriptive Statistics: Averages, measures of spread, correlation and much more.\nCleaning Data: Identifying and removing outliers\nVisualization of Data: All standard techniques for visualizing data, embedded in Excel.\nProbability: Independent Events, conditional probability and Bayesian statistics.\nDiscrete Distributions: Binomial, Poisson, expectation and variance and approximations.\nContinuous Distributions: The Normal distribution, the central limit theorem and continuous random variables.\nHypothesis Tests: Using binomial, Poisson and normal distributions, T-tests and confidence intervals.\nRegression: Linear regression analysis, correlation, testing for correlation, non-linear regression models.\nQuality of Tests: Type I and Type II errors, power and size, p-hacking.\nChi-Squared Tests: The chi-squared distribution and how to use it to test for association and goodness of fit.\nMuch, much more!\nIt requires no prior knowledge, with the exception of 2 optional videos at the end of the continuous distribution chapter, in which knowledge of calculus is required).",
      "target_audience": [
        "Data Scientists",
        "Business Analysts",
        "Business Students",
        "People studying Statistics",
        "Anyone looking to power their decision making with a thorough understanding of statistics."
      ]
    },
    {
      "title": "LLM Mastery: ChatGPT, Gemini, Claude, Llama3, OpenAI & APIs",
      "url": "https://www.udemy.com/course/llm-mastery-chatgpt-gemini-claude-llama3-openai-apis/",
      "bio": "Basics to AI-Agents: OpenAI API, Gemini API, Open-source LLMs, GPT-4o, RAG, LangChain Apps, Colab, Prompt Engineering",
      "objectives": [
        "Functionality of LLMs: Parameters, Weights, Inference, and Neural Networks",
        "Understanding Neural Networks",
        "Operation of Neural Networks with Tokens in LLMs",
        "Transformer Architecture and Mixture of Experts",
        "Fine-Tuning and the Creation of the Assistant Model",
        "Reinforcement Learning (RLHF) in LLMs",
        "LLM Scaling Laws: GPU & Data for Improvements",
        "Capabilities and Future Developments of LLMs",
        "Use of Tools by LLMs: Calculator, Python Libraries, and More",
        "Multimodality and Visual Processing with LLMs",
        "Multimodality in Language as in the Movie 'Her'",
        "Systems Thinking and Future Prospects for LLMs",
        "Self-Improvement after AlphaGo (Self-Improvement)",
        "Improvement Possibilities: Prompts, RAG, and Customization",
        "Prompt Engineering: Effective Use of LLMs with Chain of Thought and Tree of Thoughts Prompting & More",
        "Adaptation of LLMs through System Prompts and Personalization with ChatGPT Memory",
        "Long-Term Memory with RAG and GPTs",
        "The GPT Store: Everything You Need to Know",
        "Using GPTs for Data Analysis, PDFs, or Tetris Programming",
        "Embeddings and Vector Databases for RAG",
        "Integrating Zapier Actions in GPTs",
        "Open-Source vs. Closed-Source LLMs",
        "API Basics",
        "Usage of the Google Gemini API and Claude API",
        "Microsoft Copilot and Its Use in Microsoft 365",
        "GitHub Copilot: The Solution for Programmers",
        "The OpenAI API: Features, Pricing Models, and Everything You Need to Know About the OpenAI API Including App Creation",
        "Introduction to Google Colab for API Calls to OpenAI",
        "Creation of AI Apps and Chatbots with Langchain, Flowise, Vectorshift, LangGraph, CrewAI, Autogen, Langflow & more",
        "Creation of AI Agents for Various Tasks like Social Media Contetn with Agency Swarm and Langchain Agents",
        "Security in LLMs: Jailbreaks and Prompt Injections & more",
        "Comparison of the Best LLMs",
        "Google Gemini in Standard Interface and Google Labs with NotebookLM",
        "Claude by Anthropic: Overview",
        "Everything About Perplexity and POE",
        "OpenAI Playground: Features, Billing Account & Temperature of LLMs",
        "Google Gemini API: Video Analysis and More",
        "Open-Source LLMs: Models and Use of Llama 3, Mixtral, Command R+, and Many More",
        "HuggingChat: Interface for Open-Source LLMs",
        "Running Local LLMs with Ollama and Building Local Rag Chatbots",
        "Groq: Fastest Interface with LPU",
        "Installation of LM Studio for Using Local Open-Source like Llama3 LLMs for Maximum Security",
        "Using Open-Source Models in LM Studio and Censored vs. Uncensored LLMs",
        "Fine-Tuning an Open-Source Model with Huggingface",
        "Creating Your Own Apps via APIs in Google Colab with Dall-E, Whisper, GPT-4o, Vision, and More",
        "Microsoft Autogen for AI Agents",
        "CrewAI for AI Agents",
        "Flowise with LangChain Function Calling",
        "OpenAI Assistant API with function Calling for AI-Agents in different Frameworks",
        "Flowise with Open-Source LLM as ChatBot",
        "Security in LLMs and Methods to Hack LLMs",
        "Future of LLMs as Operating Systems in Robots and PCs"
      ],
      "course_content": {
        "Introduction and Overview": [
          "Welcome",
          "Course Overview",
          "My Goal and Some Tips",
          "Explanation of Links and Downloads",
          "Important Links",
          "Instructor Introduction: Arnold Oberleiter (Arnie)"
        ],
        "How LLMs Work: Parameters, Weights, Inference, Neural Networks & More": [
          "What This Section Is About?",
          "An LLM Consists of Only Two Files Parameter File and a Few Lines of Code",
          "How Are the Parameters Created Pretraining (Initial Training of the LLM)",
          "What Is a Neural Network and how it works?",
          "How a Neural Network Works in an LLM with Tokens",
          "The Transformer Architecture Is Not Fully Understood (Yet?)",
          "Other Possibilities of the Transformer Architecture: Mixture of Experts Explaied",
          "After Pretraining Comes Finetuning: The Assistant Model Is Created",
          "The Final Step: Reinforcement Learning (RLHF)",
          "LLM Scaling Laws: To Improve LLM, We Only Need Two Things, GPU & Data",
          "Review: What Have You Learned So Far"
        ],
        "Additional Capabilities of LLMs & Future Developments": [
          "What This Section Is About",
          "LLMs Can Use Various Tools, Like Calculators, Python Libraries, etc.",
          "Multimodality, Visual Processing (Vision), and Image Recognition",
          "Multimodality with Language Like in the Movie \"Her\"",
          "What Could Happen in the Future? Systems Thinking! [Thinking Fast and Slow]",
          "Updates: ChatGPT Search, Canvas, & o1 (System thinking / Test-Time-Compute)",
          "OpenAI o3 Infos",
          "Self-Improvement Inspired by AlphaGo",
          "Further Ways to Improve LLMs: Prompts, RAG, Customization/System Prompts",
          "LLMs as the New Operating System: What the Future Could Look Like",
          "Review: What Have You Learned in This Section"
        ],
        "Prompt Engineering: Effective Use of LLMs in the Standard Interface": [
          "What This Section Is About and the Interface of LLMs",
          "What is the Token Limit and why is it important",
          "Why Is Prompt Engineering Important? An Example!",
          "Prompt Engineering Basics: Semantic Association",
          "Prompt Engineering for LLMs: The Simplest Strategies (Structured Prompts)",
          "3 Important \"hacks\" for Prompt Engineering and the Instruction Pormpting",
          "Role Prompting in ChatGPT and other LLMs",
          "Shot Prompting: Zero-Shot, One-Shot und Few-Shot",
          "Reverse Prompt Engineering and the \"OK\" Trick",
          "Chain of Thought Prompting: Step by Step to the Goal",
          "Tree of Thoughts (ToT) Prompting",
          "The Combination of Prompting Concepts",
          "Real-World Use Cases for Large Language Models",
          "Review and a bit of Homework"
        ],
        "LLM Customization: System Prompts, Memory, RAG & Creating Expert Models or GPTs": [
          "What This Section Is About",
          "The Simplest Form of Personalization: ChatGPT Memory",
          "Customization Through System Prompts and Custom Instructions",
          "In-Context Learning: Short-Term Memory as Simple as Possible",
          "In-Context Learning: \"The Short-Term Memory\" but Efficient with SPR",
          "Embeddings and Vector Databases for RAG: A Detailed Explanation",
          "Long-Term Memory with RAG: As Simple as Possible with GPTs & RAG",
          "The GPT Store: Everything You Need to Know & Testing of GPTs for Code, PDFs & YT",
          "Three ways to make Money with GPTs",
          "First: You need a Builder Profile to generate Leads from GPTs",
          "Create a GPT with Knowledge that can generate Leads and makes Upsells",
          "What is a API?",
          "Zapier Actions in GPTs: Automate Gmail, Google Docs, & more with the Zapir API",
          "How to Integrate Every API in your GPT",
          "Summary: What You Have Learned in This Section"
        ],
        "Closed-Source LLMs: An Overview of Available Models and how to use them": [
          "Open-Source vs. Closed-Source LLMs",
          "What is the difference: Parameters, Architecture, Pretraining size & more",
          "Google Gemini in the Standard Interface: Everything you need to know",
          "Google Labs with NotebookLM: The Best Method to Learn Books",
          "Claude by Anthropic: An Overview",
          "The Leading Companies Are OpenAI, Google & Anthropic: Many Are Building on Them",
          "Perplexity: Advantages and Disadvantages, and Applications",
          "Poe, The Versatile All-in-One Platform",
          "What is the Microsoft Copilot: How it works and is my Data Save?",
          "Using Microsoft Copilot in the Web Interface",
          "Microsoft Copilot PCs",
          "Microsoft 365: Differences Between Free and Paid Subscription",
          "The Right Copilot Subscription and a Free Alternative.",
          "Copilot in Microsoft Word: Write Faster Than Ever",
          "Copilot in Microsoft PowerPoint: The Quick Presentation",
          "Copilot in Microsoft Outlook: Write and Reply to Your Emails Faster",
          "Copilot in Microsoft Excel: Big Possibilities but Still a Bit Early",
          "Microsoft Copilot GPT: Create your own personalized ChatBots",
          "GitHub Copilot: The AI Solution for Programmers",
          "Conclusion on Microsoft Copilot",
          "Review of the Closed-Source LLMs"
        ],
        "APIs of Closed-Source LLMs": [
          "What Is This About? APIs of Closed-Source LLMs",
          "Overview of the OpenAI API",
          "Pricing Models of the OpenAI API",
          "Important: OpenAI Playground overview and Billing Account",
          "The OpenAI Playgroundin action",
          "The Google Gemini API: Video Analysis and Other Features",
          "The Anthropic API for the Claude Models",
          "Summary of the Closed-Source APIs"
        ],
        "Open-Source LLMs: Available Models and Their Use in the Claude & Locally": [
          "What Are Open-Source LLMs and Which Ones Are Available",
          "Huggingface: An Introduction",
          "HuggingChat: An Interface for Using Open-Source LLMs with Function Calling",
          "Groq: The Fastest Interface with an LPU Instead of a GPU",
          "Installation of LM Studio for Opensource LLMs: You need GPU, CPU, Cuda, Ram",
          "Using Open-Source Models in LM Studio: Llama3, Mistral; Phi-3 & more",
          "Censored vs. Uncensored LLMs (Llama3 Dolphin)",
          "Setting Up Your Own Local Server with LM Studio",
          "Finetuning an Open-Source Model with Huggingface or Google Colab",
          "Grok from xAI",
          "UPDATE (AUGUST 2024): Llama 3.1 Infos and What Models should you use",
          "Update Feb. 2025: DeepSeek R1 with Test-Time-Compute",
          "Try DeepSeek R1: API, Browses and local Insallation",
          "What You Should Remember"
        ],
        "First Steps to Creating Your Own Apps via APIs in Google Colab": [
          "What Is This Section About",
          "GitHub Overview: Why You Should Have an Account",
          "Introduction to Google Colab",
          "What You Will Create in Google Colab",
          "Our First API Call to OpenAI in Google Colab (Text Generation)",
          "DALL-E via the OpenAI API in Google Colab (Image Generation)",
          "Text-to-Speech (TTS) with the OpenAI API in Google Colab",
          "Transcribing with Whisper via the OpenAI API in Google Colab",
          "Image Recognition with Vision via the OpenAI API in Google Colab",
          "Overview of Your Entire Notebook and a few tricks",
          "What You Have Learned Here"
        ],
        "Apps, Chatbots, and AI Agents: Build cool Stuff for Automation and FUN": [
          "AI Agents: Definition and Available Tools for Creating Apps and Helpers",
          "The Vectorshift Interface for AI Agents and AI Apps",
          "The easy way to make a ChatBot in Vectorshift",
          "Knowledge Through RAG: Training the AI Agent on Data with Automatic Updates",
          "Bot Deployment: As a Standalone App, in WhatsApp, Slack, or on Websites",
          "Overview of a AI Agent with Multiple Experts in Vectorshift",
          "Langchain, Langflow, CrewAI, Autogen.... What Do I Need and is Python important?",
          "Three Ways to Run Flowise: Locally with Node.js or Externally in the Claude",
          "Installing Flowise with Node.js (JavaScript Runtime Environment)",
          "The Flowise Interface: Simpler Than langflow, builds on Langchain & LangGraph",
          "A Example for a Chatbot: Q&A Chain, Memory & RAG with Vectordatabase",
          "Function Calling, Memory & RAG: Simplified with the OpenAI Assistant API",
          "Installation of Ollama, downloading Llama3, and hosting it on a local server",
          "Local RAG Chatbot with Llama3 & Ollama: A Local Langchain App",
          "AI Agents Like with Langchain + LangGraph or Autogen & CrewAI (with Flowise)",
          "Langchain-Style AI Agent with different Experts for Social Media, Math and Code",
          "Use your Agent as a standalone Application",
          "Hosting Chatbots for Customers Externally on Render (Cloud Hosting)",
          "Embed a Chatbot into Websites: HTML, WordPress, Shopify Page & More",
          "Flowise Tips: Getting Leads, API Endpoints, Speech Recognition & More",
          "Creating a Free Chatbot with Open-Source Models and Tools: Mistral Mixture",
          "Insanely fast inference with the Groq API",
          "Check Out the Marketplace: If Else Chain and More",
          "Overview of Microsoft Autogen, CrewAI and Agency Swarm on Github",
          "Review and What You Should Do!"
        ]
      },
      "requirements": [
        "No prior knowledge required, everything will be shown step by step."
      ],
      "description": "Have you ever thought about how Large Language Models (LLMs) are transforming the world and creating unprecedented opportunities?\n\"AI won't take your job, but someone who knows how to use AI might,\" says Richard Baldwin.\nAre you ready to master the intricacies of LLMs and leverage their full potential for various applications, from data analysis to the creation of chatbots and AI agents?\nThen this course is for you!\nDive into 'LLM Mastery: ChatGPT, Gemini, Claude, Llama3, OpenAI & APIs'—where you will explore the fundamental and advanced concepts of LLMs, their architectures, and practical applications. Transform your understanding and skills to lead in the AI revolution.\nThis course is perfect for developers, data scientists, AI enthusiasts, and anyone who wants to be at the forefront of LLM technology. Whether you want to understand neural networks, fine-tune AI models, or develop AI-driven applications, this course offers everything you need.\nWhat to expect in this course:\nComprehensive Knowledge of LLMs:\nUnderstanding LLMs: Learn about parameters, weights, inference, and neural networks.\nNeural Networks: Understand how neural networks function with tokens in LLMs.\nTransformer Architecture: Explore the Transformer architecture and Mixture of Experts.\nFine-Tuning: Understand the fine-tuning process and the development of the Assistant model.\nReinforcement Learning (RLHF): Dive into reinforcement learning with human feedback.\nAdvanced Techniques and Future Trends:\nScaling Laws: Learn about the scaling laws of LLMs, including GPU and data improvements.\nFuture of LLMs: Discover the capabilities and future developments in LLM technology.\nMultimodal Processing: Understand multimodality and visual processing with LLMs, inspired by movies like \"Her.\"\nPractical Skills and Applications:\nTool Utilization: Use tools with LLMs like calculators and Python libraries.\nSystems Thinking: Dive into systems thinking and future perspectives for LLMs.\nSelf-Improvement: Learn self-improvement methods inspired by AlphaGo.\nOptimization Techniques: Enhance LLM performance with prompts, RAG, function calling, and customization.\nPrompt Engineering:\nAdvanced Prompts: Master techniques like Chain of Thought and Tree of Thoughts prompting.\nCustomization: Customize LLMs with system prompts and personalize with ChatGPT memory.\nLong-Term Memory: Implement RAG and GPTs for long-term memory capabilities.\nAPI and Integration Skills:\nAPI Basics: Understand the basics of API usage, including OpenAI API, Google Gemini, and Claude APIs.\nMicrosoft and GitHub Copilot: Utilize Microsoft Copilot in 365 and GitHub Copilot for programming.\nOpenAI API Mastery: Explore functionalities, pricing models, and app creation with the OpenAI API.\nAI App Development:\nGoogle Colab: Learn API calls to OpenAI with Google Colab.\nAI Agents: Create AI agents for various tasks in LangChain frameworks like Langgraph, Langflow, Vectorshift, Autogen, CrewAI, Flowise, and more.\nSecurity: Ensure security with methods to prevent jailbreaks and prompt injections.\nComparative Insights:\nComparing Top LLMs: Compare the best LLMs, including Google Gemini, Claude, and more.\nOpen-Source Models: Explore and utilize open-source models like Llama 3, Mixtral, and Command R+ with the possibility of running everything locally on your PC for maximum security.\nPractical Applications:\nEmbedding and Vector Databases: Implement embeddings for RAG.\nZapier Integration: Integrate Zapier actions into GPTs.\nOpen-Source LLMs: Install and use LM Studio for local open-source LLMs for maximum security.\nModel Fine-Tuning: Fine-tune open-source models with Huggingface.\nAPI-Based App Development: Create apps with DALL-E, Whisper, GPT-4o, Vision, and more in Google Colab.\nInnovative Tools and Agents:\nMicrosoft Autogen: Use Microsoft Autogen for developing AI agents.\nCrewAI: Develop AI agents with CrewAI.\nLangChain: Understand the framework with divisions like LangGraph, LangFlow, and more.\nFlowise: Implement Flowise with function calls and open-source LLM as a chatbot.\nEthical and Security Considerations:\nLLM Security: Understand and apply security measures to prevent hacking.\nFuture of LLMs: Explore the potential of LLMs as operating systems in robots and PCs.\nThis course is ideal for anyone looking to delve deeper into the world of LLMs—from developers and creatives to entrepreneurs and AI enthusiasts.\nHarness the transformative power of LLM technology to develop innovative solutions and expand your understanding of their diverse applications.\nBy the end of 'LLM Mastery: ChatGPT, Gemini, Claude, Llama3, OpenAI & APIs' you will have a comprehensive understanding of LLMs, their applications, and the skills to harness their power for various purposes. If you are ready to embark on a transformative journey into AI and position yourself at the forefront of this technological revolution, this course is for you.\nEnroll today and start your journey to becoming an expert in the world of Large Language Models!",
      "target_audience": [
        "To everyone who wants to learn something new and gain deep insights into LLMs",
        "To entrepreneurs who want to become more efficient and save money",
        "To individuals who are interested in AI and want to build their own models"
      ]
    },
    {
      "title": "Complete 2022 Data Science & Machine Learning Bootcamp",
      "url": "https://www.udemy.com/course/python-data-science-machine-learning-bootcamp/",
      "bio": "Learn Python, Tensorflow, Deep Learning, Regression, Classification, Neural Networks, Artificial Intelligence & more!",
      "objectives": [
        "You will learn how to program using Python through practical projects",
        "Use data science algorithms to analyse data in real life projects such as spam classification and image recognition",
        "Build a portfolio of data science projects to apply for jobs in the industry",
        "Understand how to use the latest tools in data science, including Tensorflow, Matplotlib, Numpy and many more",
        "Create your own neural networks and understand how to use them to perform deep learning",
        "Understand and apply data visualisation techniques to explore large datasets"
      ],
      "course_content": {
        "Introduction to the Course": [
          "What is Machine Learning?",
          "What is Data Science?",
          "Download the Syllabus",
          "Top Tips for Succeeding on this Course",
          "Course Resources List"
        ],
        "Predict Movie Box Office Revenue with Linear Regression": [
          "Introduction to Linear Regression & Specifying the Problem",
          "Gather & Clean the Data",
          "Explore & Visualise the Data with Python",
          "The Intuition behind the Linear Regression Model",
          "Analyse and Evaluate the Results",
          "Download the Complete Notebook Here",
          "Join the Student Community",
          "Any Feedback on this Section?"
        ],
        "Python Programming for Data Science and Machine Learning": [
          "Windows Users - Install Anaconda",
          "Mac Users - Install Anaconda",
          "Does LSD Make You Better at Maths?",
          "Download the 12 Rules to Learn to Code",
          "[Python] - Variables and Types",
          "Python Variable Coding Exercise",
          "[Python] - Lists and Arrays",
          "Python Lists Coding Exercise",
          "[Python & Pandas] - Dataframes and Series",
          "[Python] - Module Imports",
          "[Python] - Functions - Part 1: Defining and Calling Functions",
          "Python Functions Coding Exercise - Part 1",
          "[Python] - Functions - Part 2: Arguments & Parameters",
          "Python Functions Coding Exercise - Part 2",
          "[Python] - Functions - Part 3: Results & Return Values",
          "Python Functions Coding Exercise - Part 3",
          "[Python] - Objects - Understanding Attributes and Methods",
          "How to Make Sense of Python Documentation for Data Visualisation",
          "Working with Python Objects to Analyse Data",
          "[Python] - Tips, Code Style and Naming Conventions",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Introduction to Optimisation and the Gradient Descent Algorithm": [
          "What's Coming Up?",
          "How a Machine Learns",
          "Introduction to Cost Functions",
          "LaTeX Markdown and Generating Data with Numpy",
          "Understanding the Power Rule & Creating Charts with Subplots",
          "[Python] - Loops and the Gradient Descent Algorithm",
          "Python Loops Coding Exercise",
          "[Python] - Advanced Functions and the Pitfalls of Optimisation (Part 1)",
          "[Python] - Tuples and the Pitfalls of Optimisation (Part 2)",
          "Understanding the Learning Rate",
          "How to Create 3-Dimensional Charts",
          "Understanding Partial Derivatives and How to use SymPy",
          "Implementing Batch Gradient Descent with SymPy",
          "[Python] - Loops and Performance Considerations",
          "Reshaping and Slicing N-Dimensional Arrays",
          "Concatenating Numpy Arrays",
          "Introduction to the Mean Squared Error (MSE)",
          "Transposing and Reshaping Arrays",
          "Implementing a MSE Cost Function",
          "Understanding Nested Loops and Plotting the MSE Function (Part 1)",
          "Plotting the Mean Squared Error (MSE) on a Surface (Part 2)",
          "Running Gradient Descent with a MSE Cost Function",
          "Visualising the Optimisation on a 3D Surface",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Predict House Prices with Multivariable Linear Regression": [
          "Defining the Problem",
          "Gathering the Boston House Price Data",
          "Clean and Explore the Data (Part 1): Understand the Nature of the Dataset",
          "Clean and Explore the Data (Part 2): Find Missing Values",
          "Visualising Data (Part 1): Historams, Distributions & Outliers",
          "Visualising Data (Part 2): Seaborn and Probability Density Functions",
          "Working with Index Data, Pandas Series, and Dummy Variables",
          "Understanding Descriptive Statistics: the Mean vs the Median",
          "Introduction to Correlation: Understanding Strength & Direction",
          "Calculating Correlations and the Problem posed by Multicollinearity",
          "Visualising Correlations with a Heatmap",
          "Techniques to Style Scatter Plots",
          "A Note for the Next Lesson",
          "Working with Seaborn Pairplots & Jupyter Microbenchmarking Techniques",
          "Understanding Multivariable Regression",
          "How to Shuffle and Split Training & Testing Data",
          "Running a Multivariable Regression",
          "How to Calculate the Model Fit with R-Squared",
          "Introduction to Model Evaluation",
          "Improving the Model by Transforming the Data",
          "How to Interpret Coefficients using p-Values and Statistical Significance",
          "Understanding VIF & Testing for Multicollinearity",
          "Model Simplification & Baysian Information Criterion",
          "How to Analyse and Plot Regression Residuals",
          "Residual Analysis (Part 1): Predicted vs Actual Values",
          "Residual Analysis (Part 2): Graphing and Comparing Regression Residuals",
          "Making Predictions (Part 1): MSE & R-Squared",
          "Making Predictions (Part 2): Standard Deviation, RMSE, and Prediction Intervals",
          "Build a Valuation Tool (Part 1): Working with Pandas Series & Numpy ndarrays",
          "[Python] - Conditional Statements - Build a Valuation Tool (Part 2)",
          "Python Conditional Statement Coding Exercise",
          "Build a Valuation Tool (Part 3): Docstrings & Creating your own Python Module",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Pre-Process Text Data for a Naive Bayes Classifier to Filter Spam Emails: Part 1": [
          "How to Translate a Business Problem into a Machine Learning Problem",
          "Gathering Email Data and Working with Archives & Text Editors",
          "How to Add the Lesson Resources to the Project",
          "The Naive Bayes Algorithm and the Decision Boundary for a Classifier",
          "Basic Probability",
          "Joint & Conditional Probability",
          "Bayes Theorem",
          "Reading Files (Part 1): Absolute Paths and Relative Paths",
          "Reading Files (Part 2): Stream Objects and Email Structure",
          "Extracting the Text in the Email Body",
          "[Python] - Generator Functions & the yield Keyword",
          "Create a Pandas DataFrame of Email Bodies",
          "Cleaning Data (Part 1): Check for Empty Emails & Null Entries",
          "Cleaning Data (Part 2): Working with a DataFrame Index",
          "Saving a JSON File with Pandas",
          "Data Visualisation (Part 1): Pie Charts",
          "Data Visualisation (Part 2): Donut Charts",
          "Introduction to Natural Language Processing (NLP)",
          "Tokenizing, Removing Stop Words and the Python Set Data Structure",
          "Word Stemming & Removing Punctuation",
          "Removing HTML tags with BeautifulSoup",
          "Creating a Function for Text Processing",
          "A Note for the Next Lesson",
          "Advanced Subsetting on DataFrames: the apply() Function",
          "[Python] - Logical Operators to Create Subsets and Indices",
          "Word Clouds & How to install Additional Python Packages",
          "Creating your First Word Cloud",
          "Styling the Word Cloud with a Mask",
          "Solving the Hamlet Challenge",
          "Styling Word Clouds with Custom Fonts",
          "Create the Vocabulary for the Spam Classifier",
          "Coding Challenge: Check for Membership in a Collection",
          "Coding Challenge: Find the Longest Email",
          "Sparse Matrix (Part 1): Split the Training and Testing Data",
          "Sparse Matrix (Part 2): Data Munging with Nested Loops",
          "Sparse Matrix (Part 3): Using groupby() and Saving .txt Files",
          "Coding Challenge Solution: Preparing the Test Data",
          "Checkpoint: Understanding the Data",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Train a Naive Bayes Classifier to Create a Spam Filter: Part 2": [
          "Setting up the Notebook and Understanding Delimiters in a Dataset",
          "Create a Full Matrix",
          "Count the Tokens to Train the Naive Bayes Model",
          "Sum the Tokens across the Spam and Ham Subsets",
          "Calculate the Token Probabilities and Save the Trained Model",
          "Coding Challenge: Prepare the Test Data",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Test and Evaluate a Naive Bayes Classifier: Part 3": [
          "Set up the Testing Notebook",
          "Joint Conditional Probability (Part 1): Dot Product",
          "Joint Conditional Probablity (Part 2): Priors",
          "Making Predictions: Comparing Joint Probabilities",
          "The Accuracy Metric",
          "Visualising the Decision Boundary",
          "False Positive vs False Negatives",
          "The Recall Metric",
          "The Precision Metric",
          "The F-score or F1 Metric",
          "A Naive Bayes Implementation using SciKit Learn",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Introduction to Neural Networks and How to Use Pre-Trained Models": [
          "The Human Brain and the Inspiration for Artificial Neural Networks",
          "Layers, Feature Generation and Learning",
          "Costs and Disadvantages of Neural Networks",
          "Preprocessing Image Data and How RGB Works",
          "Importing Keras Models and the Tensorflow Graph",
          "Making Predictions using InceptionResNet",
          "Coding Challenge Solution: Using other Keras Models",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ],
        "Build an Artificial Neural Network to Recognise Images using Keras & Tensorflow": [
          "Solving a Business Problem with Image Classification",
          "Installing Tensorflow and Keras for Jupyter",
          "Gathering the CIFAR 10 Dataset",
          "Exploring the CIFAR Data",
          "Pre-processing: Scaling Inputs and Creating a Validation Dataset",
          "Compiling a Keras Model and Understanding the Cross Entropy Loss Function",
          "Interacting with the Operating System and the Python Try-Catch Block",
          "Fit a Keras Model and Use Tensorboard to Visualise Learning and Spot Problems",
          "Use Regularisation to Prevent Overfitting: Early Stopping & Dropout Techniques",
          "Use the Model to Make Predictions",
          "Model Evaluation and the Confusion Matrix",
          "Model Evaluation and the Confusion Matrix",
          "Download the Complete Notebook Here",
          "Any Feedback on this Section?"
        ]
      },
      "requirements": [
        "No programming experience needed! I'll teach you everything you need to know.",
        "No statistics knowledge required! I’ll teach you everything you need to know.",
        "No calculus knowledge required! as long as you've done some high school maths, I'll take you step by step through the difficult parts.",
        "Also, no paid software required - all projects use free and open source software",
        "All you need is Mac or PC computer with access to the internet"
      ],
      "description": "Welcome to the Complete Data Science and Machine Learning Bootcamp, the only course you need to learn Python and get into data science.\n\n\nAt over 40+ hours, this Python course is without a doubt the most comprehensive data science and machine learning course available online. Even if you have zero programming experience, this course will take you from beginner to mastery. Here's why:\nThe course is taught by the lead instructor at the App Brewery, London's leading in-person programming bootcamp.\nIn the course, you'll be learning the latest tools and technologies that are used by data scientists at Google, Amazon, or Netflix.\nThis course doesn't cut any corners, there are beautiful animated explanation videos and real-world projects to build.\nThe curriculum was developed over a period of three years together with industry professionals, researchers and student testing and feedback.\nTo date, we’ve taught over 200,000 students how to code and many have gone on to change their lives by getting jobs in the industry or starting their own tech startup.\nYou'll save yourself over $12,000 by enrolling, but get access to the same teaching materials and learn from the same instructor and curriculum as our in-person programming bootcamp.\n\n\nWe'll take you step-by-step through video tutorials and teach you everything you need to know to succeed as a data scientist and machine learning professional.\n\n\nThe course includes over 40+ hours of HD video tutorials and builds your programming knowledge while solving real-world problems.\n\n\nIn the curriculum, we cover a large number of important data science and machine learning topics, such as:\nData Cleaning and Pre-Processing\nData Exploration and Visualisation\nLinear Regression\nMultivariable Regression\nOptimisation Algorithms and Gradient Descent\nNaive Bayes Classification\nDescriptive Statistics and Probability Theory\nNeural Networks and Deep Learning\nModel Evaluation and Analysis\nServing a Tensorflow Model\n\n\nThroughout the course, we cover all the tools used by data scientists and machine learning experts, including:\nPython 3\nTensorflow\nPandas\nNumpy\nScikit Learn\nKeras\nMatplotlib\nSeaborn\nSciPy\nSymPy\nBy the end of this course, you will be fluently programming in Python and be ready to tackle any data science project. We’ll be covering all of these Python programming concepts:\n\n\nData Types and Variables\nString Manipulation\nFunctions\nObjects\nLists, Tuples and Dictionaries\nLoops and Iterators\nConditionals and Control Flow\nGenerator Functions\nContext Managers and Name Scoping\nError Handling\n\n\nBy working through real-world projects you get to understand the entire workflow of a data scientist which is incredibly valuable to a potential employer.\n\n\nSign up today, and look forward to:\n178+ HD Video Lectures\n30+ Code Challenges and Exercises\nFully Fledged Data Science and Machine Learning Projects\nProgramming Resources and Cheatsheets\nOur best selling 12 Rules to Learn to Code eBook\n$12,000+ data science & machine learning bootcamp course materials and curriculum\n\n\nDon't just take my word for it, check out what existing students have to say about my courses:\n\n\n“One of the best courses I have taken. Everything is explained well, concepts are not glossed over. There is reinforcement in the challenges that helps solidify understanding. I'm only half way through but I feel like it is some of the best money I've ever spent.” -Robert Vance\n\n\n“I've spent £27,000 on University..... Save some money and buy any course available by Philipp! Great stuff guys.” -Terry Woodward\n\n\n\"This course is amazingly immersive and quite all-inclusive from end-to-end to develop an app! Also gives practicality to apply the lesson straight away and full of fun with bunch of sense of humor, so it's not boring to follow throughout the whole course. Keep up the good work guys!\" - Marvin Septianus\n\n\n“Great going so far. Like the idea of the quizzes to challenge us as we go along. Explanations are clear and easy to follow” -Lenox James\n\n\n“Very good explained course. The tasks and challenges are fun to do learn an do! Would recommend it a thousand times.” -Andres Ariza\n\n\n“I enjoy the step by step method they introduce the topics. Anyone with an interest in programming would be able to follow and program” -Isaac Barnor\n\n\n“I am learning so much with this course; certainly beats reading older Android Ebooks that are so far out of date; Phillippe is so easy any understandable to learn from. Great Course have recommended to a few people.” -Dale Barnes\n\n\n“This course has been amazing. Thanks for all the info. I'll definitely try to put this in use. :)” -Devanshika Ghosh\n\n\n“Great Narration and explanations. Very interactive lectures which make me keep looking forward to the next tutorial” -Bimal Becks\n\n\n“English is not my native language but in this video, Phillip has great pronunciation so I don't have problem even without subtitles :)” -Dreamerx85\n\n\n“Clear, precise and easy to follow instructions & explanations!” -Andreea Andrei\n\n\n“An incredible course in a succinct, well-thought-out, easy to understand package. I wish I had purchased this course first.” -Ian\n\n\n\n\nREMEMBER… I'm so confident that you'll love this course that we're offering a FULL money back guarantee for 30 days! So it's a complete no-brainer, sign up today with ZERO risks and EVERYTHING to gain.\n\n\nSo what are you waiting for? Click the buy now button and join the world's best data science and machine learning course.",
      "target_audience": [
        "If you want to learn to code through building fun and useful projects, then take this course.",
        "If you want to solve real-life problems using data.",
        "If you want to learn how to build machine learning algorithms such as deep learning and neural networks.",
        "If you are a seasoned programmer, take this course to get up to speed quickly with the workflow of a data scientist.",
        "If you want to take ONE COURSE and learn everything you need to know about data science and machine learning then take this course."
      ]
    },
    {
      "title": "AWS SageMaker Machine Learning Engineer in 30 Days + ChatGPT",
      "url": "https://www.udemy.com/course/become-an-aws-machine-learning-engineer-in-30-days-new-2022/",
      "bio": "Build 30+ ML Projects in 30 Days in AWS, Master SageMaker JumpStart, Canvas, AutoPilot, DataWrangler, Lambda & S3",
      "objectives": [
        "Build, Train, Test and Deploy Machine Learning Models in AWS",
        "Leverage ChatGPT and GPT-4 to Automate Coding Tasks, Perform Code Debugging, Write Documentation and Add New Features to your Code",
        "Define and Perform Image and Text Labeling Jobs Using AWS SageMaker GroundTruth",
        "Prepare, Clean and Visualize data Using AWS SageMaker Data Wrangler without Writing any Code",
        "Optimize ML model hyperparameters using GridSearch, Bayesian & Random Search Optimization Techniques",
        "Master Key AWS services such as Simple Storage Service (S3), Elastic Compute Cloud (EC2), Identity and Access Management (IAM) and CloudWatch",
        "Understand Machine Learning workflow automation using AWS Lambda, Step functions and SageMaker Pipelines.",
        "Learn how to define a lambda function in AWS management console, understand the anatomy of Lambda functions, and how to configure a test event in Lambda",
        "Train a Machine Learning Regression and Classifier Models Using No-code AWS Canvas",
        "Learn how to leverage Amazon SageMaker Autopilot and SageMaker Canvas to train multiple models without writing any code.",
        "Perform Exploratory Data Analysis and Visualization Using Pandas, Searborn and Matplotlib Libraries",
        "Understand Regression Models KPIs Such as RMSE, MSE, MAE, R2 and Adjusted R2",
        "Understand Classification Models KPIs such as Accuracy, Precision, Recall, F1-Score, ROC, and AUC",
        "Define a Machine Learning Training Job Using AWS SageMaker JumpStart",
        "Deploy an Endpoint Using Amazon SageMaker, Perform Inference and Generate Predictions",
        "Define a Lambda function using Boto3 SDK and Test the lambda function using Eventbridge (cloudwatch events)",
        "Understand the difference between synchronous and asynchronous Lambda Functions invocations",
        "Perform AI/ML Models Prototyping Using AutoGluon Library",
        "How to monitor billing dashboard, set alarms, S3/EC2 instances pricing and request service limits increase",
        "Understand the difference between Artificial Intelligence (AI), Machine Learning (ML), Data Science (DS) and Deep Learning (DL)",
        "Learn the fundamentals of Amazon SageMaker, SageMaker Components, training options including built-in algorithms, AWS Marketplace, & customized ML Algorithms",
        "Leverage a Yolo V3 Object Detection Algorithm available on the AWS Marketplace",
        "Understand the format and Use Case of Json Lines and Manifest Files",
        "Learn auto-labeling workflow and understand the difference between SageMaker GroundTruth and GroundTruth Plus",
        "Learn how to define a labeling job with bounding boxes (object detection), pixel-level Semantic Segmentation, and text data",
        "Understand the difference between data labeling workforces in AWS such as public mechanical Turks, private labelers and AWS curated third-party vendors",
        "Learn the difference between Supervised, Unsupervised and Reinforcement Machine Learning Strategies",
        "Perform data visualization using Seaborn & Matplotlib libraries, plots include line plot, pie charts, subplots, pairplots, countplots, and correlations heatmaps",
        "Export a data wrangler workflow into Python script, create a custom formula and apply it to a given column in the data, and generate summary tables/bias report",
        "Learn how to train an XG-boost algorithm in SageMaker using AWS JumpStart, assess trained model performance, plot residuals, & deploy an endpoint",
        "Understand Bias-Variance Trade-off, L1 and L2 Regularization Techniques",
        "Train/Test several ML Classifiers such as Logistic Regression, Support Vector Machine, K-Nearest Neighbors, Decision Trees, and Random Forest Classifiers",
        "Learn SageMaker Built-in Algorithms such as Linear Learner, XG-Boost, Principal Component Analysis (PCA), and K-Nearest Neighbors"
      ],
      "course_content": {},
      "requirements": [
        "Basic Python Programming Knowledge",
        "Basic knowledge in AWS",
        "Basic knowledge in machine learning"
      ],
      "description": "Do you want to become an AWS Machine Learning Engineer Using SageMaker in 30 days?\nDo you want to build super-powerful production-level Machine Learning (ML) applications in AWS but don’t know where to start?\nAre you an absolute beginner and want to break into AI, ML, and Cloud Computing and looking for a course that includes everything you need?\nAre you an aspiring entrepreneur who wants to maximize business revenues and reduce costs with ML but don’t know how to get there quickly and efficiently?\nDo you want to leverage ChatGPT as a programmer to automate your coding tasks?\n\n\nIf the answer is yes to any of these questions, then this course is for you!\nMachine Learning is the future one of the top tech fields to be in right now! ML and AI will change our lives in the same way electricity did 100 years ago. ML is widely adopted in Finance, banking, healthcare, transportation, and technology. The field is exploding with opportunities and career prospects\nAWS is the one of the most widely used cloud computing platforms in the world and several companies depend on AWS for their cloud computing purposes. AWS SageMaker is a fully managed service offered by AWS that allows data scientist and AI practitioners to train, test, and deploy AI/ML models quickly and efficiently.\nThis course is unique and exceptional in many ways, it includes several practice opportunities, quizzes, and final capstone projects. In this course, students will learn how to create production-level ML models using AWS. The course is divided into 8 main sections as follows:\nSection 1 (Days 1 – 3): we will learn the following: (1) Start with an AWS and Machine Learning essentials “starter pack” that includes key AWS services such as Simple Storage Service (S3), Elastic Compute Cloud (EC2), Identity and Access Management (IAM) and CloudWatch, (2) The benefits of cloud computing, the difference between regions and availability zones and what’s included in the AWS Free Tier Package, (3) How to setup a brand-new account in AWS, setup a Multi-Factor Authentication (MFA) and navigate through the AWS Management Console, (4) How to monitor billing dashboard, set alarms, S3/EC2 instances pricing and request service limits increase, (5) The fundamentals of Machine Learning and understand the difference between Artificial Intelligence (AI), Machine Learning (ML), Data Science (DS) and Deep Learning (DL), (6) Learn the difference between supervised, unsupervised and reinforcement learning, (7) List the key components to build any machine learning models including data, model, and compute, (8) Learn the fundamentals of Amazon SageMaker, SageMaker Components, training options offered by SageMaker including built-in algorithms, AWS Marketplace, and customized ML algorithms, (9) Cover AWS SageMaker Studio and learn the difference between AWS SageMaker JumpStart, SageMaker Autopilot and SageMaker Data Wrangler, (10) Learn how to write our first code in the cloud using Jupyter Notebooks. We will then have a tutorial covering AWS Marketplace object detection algorithms such as Yolo V3, (11) Learn how to train our first machine learning model using the brand-new AWS SageMaker Canvas without writing any code!\nSection 2 (Days 4 – 5): we will learn the following: (1) Label images and text using Amazon SageMaker GroundTruth, (2) learn the difference between data labeling workforces such as public mechanical Turks, private labelers and AWS curated third-party vendors, (3) cover several companies’ success stories that have leveraged data to maximize revenues, reduce costs and optimize processes, (4) cover data sources, types, and the difference between good and bad data, (5) learn about Json Lines formats and Manifest Files, (6) cover a detailed tutorial to define an image classification labeling job in SageMaker, (7) auto-labeling workflow and learn the difference between SageMaker GroundTruth and GroundTruth Plus, (8) learn how to define a labeling job with bounding boxes (object detection and pixel-level Semantic Segmentation), (9) Label Text data using Amazon SageMaker GroundTruth.\nSection 3 (Days 6 – 10): we will learn: (1) how to perform exploratory data analysis (EDA), (2) master Pandas, a super powerful open-source library to perform data analysis in Python, (3) analyze corporate employee information using Pandas in Jupyter Notebooks in AWS SageMaker Studio, (4) define a Pandas Dataframe, read CSV data using Pandas, perform basic statistical analysis on the data, set/reset Pandas DataFrame index, select specific columns from the DataFrame, add/delete columns from the DataFrame, Perform Label/integer-based elements selection, perform broadcasting operations, and perform Pandas DataFrame sorting/ordering, (5) perform statistical data analysis on real world datasets, deal with missing data using pandas, change pandas DataFrame datatypes, define a function, and apply it to a Pandas DataFrame column, perform Pandas operations, and filtering, calculate and display correlation matrix, use seaborn library to show heatmap, (6) analyze cryptocurrency prices and daily returns of Bitcoin (BTC), Ethereum (ETH), Litecoin (LTC), Cardano (ADA) and Ripple (XRP) using Matplotlib and Seaborn libraries in AWS SageMaker Studio, (7) perform data visualization using Seaborn and Matplotlib libraries, plots include line plot, pie charts, multiple subplots, pairplot, count plot, correlations heatmaps, distribution plot (distplot), Histograms, and Scatterplots, (8) Use Amazon SageMaker Data wrangler in AWS to prepare, clean and visualize the data, (9) understand feature engineering strategies and tools, understand the fundamentals of Data Wrangler in AWS, perform one hot encoding and normalization, perform data visualization Using Data Wrangler, export a data wrangler workflow into Python script, create a custom formula and apply it to a given column in the data, generate summary table tables in Data Wrangler, and generate bias reports.\nSection 4 (Days 11 – 18): we will learn: (1) machine learning regression fundamentals including simple/multiple linear regression and least sum of squares, (2) build our first simple linear regression model in Scikit-Learn, (3) list all available built-in algorithms in SageMaker, (4) build, train, test and deploy a machine learning regression model using SageMaker Linear Learner algorithm, (5) list machine learning regression algorithms KPIs such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Percentage Error (MPE), Coefficient of Determination (R2), and adjusted R2, (6) Launch a training job using the AWS Management Console and deploy an endpoint without writing any code, (7) cover the theory and intuition behind XG-Boost algorithm and how to use it to solve regression type problems in Scikit-Learn and using SageMaker Built-in algorithms, (8) learn how to train an XG-boost algorithm in SageMaker using AWS JumpStart, assess trained regression models performance, plot the residuals, and deploy an endpoint and perform inference.\nSection 5 (Days 19 – 20): we will learn: (1) hyperparameters optimization strategies such as grid search, randomized search, and Bayesian optimization, (2) Understand bias variance trade-off and L1 and L2 regularization, (3) perform hyperparameters optimization using Scikit-Learn library and using SageMaker SDK.\nSection 6 (Days 21 – 24): we will learn: (1) how to train several classification algorithms such as Logistic Regression, Support Vector Machine, K-Nearest Neighbors, and Random Forest Classifier, (2) list the difference between various classifier models KPIs such as accuracy, precision, recall, F1-score, Receiver Operating Characteristic Curve (ROC) and Area Under the Curve (AUC), (3) train an XG-boost and Linear Learner algorithms in SageMaker to solve classification type problems, (4) learn the theory and intuition behind K Nearest Neighbors (KNN) in SageMaker and learn how to build, train and test a KNN classifier model in SageMaker. This section also includes bonus materials on how to leverage ChatGPT and generative AI models as a programmer.\nSection 7 (Days 25 – 28): we will learn: (1) how to use AutoGluon library to perform prototyping of AI/ML models using few lines of code, (2) leverage AutoGluon to train multiple regression and classification models and deploy the best one, (3) leverage Amazon SageMaker Autopilot and SageMaker Canvas to train multiple models without writing any code.\nSection 8 (Days 29 – 30): we will learn: (1) how to define and invoke lambda functions in AWS, (2) understand Machine Learning workflow automation using AWS Lambda, Step functions and SageMaker Pipelines, (3) learn how to define a lambda function in AWS management console, (4) understand the anatomy of Lambda functions, (5) learn how to configure a test event in Lambda, and monitor Lambda invocations in CloudWatch, (6) define a Lambda function using Boto3 SDK, (7) test the lambda function using Eventbridge (cloudwatch events), (8) understand the difference between synchronous and asynchronous invocations, and Invoke a Lambda function using Boto3 SDK.",
      "target_audience": [
        "Beginner developers who want to break into machine learning in AWS",
        "Beginners Data Scientists wanting to advance their careers and build their portfolio",
        "Seasoned consultants wanting to transform businesses by leveraging AI/ML using AWS",
        "Tech enthusiasts who are passionate and new to Data science & AI and want to gain practical experience using AWS"
      ]
    },
    {
      "title": "PyTorch for Deep Learning Bootcamp",
      "url": "https://www.udemy.com/course/pytorch-for-deep-learning/",
      "bio": "Learn PyTorch. Become a Deep Learning Engineer. Get Hired.",
      "objectives": [
        "Everything from getting started with using PyTorch to building your own real-world models",
        "Understand how to integrate Deep Learning into tools and applications",
        "Build and deploy your own custom trained PyTorch neural network accessible to the public",
        "Master deep learning and become a top candidate for recruiters seeking Deep Learning Engineers",
        "The skills you need to become a Deep Learning Engineer and get hired with a chance of making US$100,000+ / year",
        "Why PyTorch is a fantastic way to start working in machine learning",
        "Create and utilize machine learning algorithms just like you would write a Python program",
        "How to take data, build a ML algorithm to find patterns, and then use that algorithm as an AI to enhance your applications",
        "To expand your Machine Learning and Deep Learning skills and toolkit"
      ],
      "course_content": {},
      "requirements": [
        "A computer (Linux/Windows/Mac) with an internet connection is required",
        "Basic Python knowledge is required",
        "Previous Machine Learning knowledge is recommended, but not required (we provide sufficient supplementary resources to get you up to speed!)"
      ],
      "description": "What is PyTorch and why should I learn it?\nPyTorch is a machine learning and deep learning framework written in Python.\nPyTorch enables you to craft new and use existing state-of-the-art deep learning algorithms like neural networks powering much of today’s Artificial Intelligence (AI) applications.\nPlus it's so hot right now, so there's lots of jobs available!\nPyTorch is used by companies like:\nTesla to build the computer vision systems for their self-driving cars\nMeta to power the curation and understanding systems for their content timelines\nApple to create computationally enhanced photography.\nWant to know what's even cooler?\nMuch of the latest machine learning research is done and published using PyTorch code so knowing how it works means you’ll be at the cutting edge of this highly in-demand field.\nAnd you'll be learning PyTorch in good company.\nGraduates of Zero To Mastery are now working at Google, Tesla, Amazon, Apple, IBM, Uber, Meta, Shopify + other top tech companies at the forefront of machine learning and deep learning.\nThis can be you.\nBy enrolling today, you’ll also get to join our exclusive live online community classroom to learn alongside thousands of students, alumni, mentors, TAs and Instructors.\nMost importantly, you will be learning PyTorch from a professional machine learning engineer, with real-world experience, and who is one of the best teachers around!\nWhat will this PyTorch course be like?\nThis PyTorch course is very hands-on and project based. You won't just be staring at your screen. We'll leave that for other PyTorch tutorials and courses.\nIn this course you'll actually be:\nRunning experiments\nCompleting exercises to test your skills\nBuilding real-world deep learning models and projects to mimic real life scenarios\nBy the end of it all, you'll have the skillset needed to identify and develop modern deep learning solutions that Big Tech companies encounter.\nFair warning: this course is very comprehensive. But don't be intimidated, Daniel will teach you everything from scratch and step-by-step!\nHere's what you'll learn in this PyTorch course:\n1. PyTorch Fundamentals — We start with the barebone fundamentals, so even if you're a beginner you'll get up to speed.\nIn machine learning, data gets represented as a tensor (a collection of numbers). Learning how to craft tensors with PyTorch is paramount to building machine learning algorithms. In PyTorch Fundamentals we cover the PyTorch tensor datatype in-depth.\n2. PyTorch Workflow — Okay, you’ve got the fundamentals down, and you've made some tensors to represent data, but what now?\nWith PyTorch Workflow you’ll learn the steps to go from data -> tensors -> trained neural network model. You’ll see and use these steps wherever you encounter PyTorch code as well as for the rest of the course.\n3. PyTorch Neural Network Classification — Classification is one of the most common machine learning problems.\nIs something one thing or another?\nIs an email spam or not spam?\nIs credit card transaction fraud or not fraud?\nWith PyTorch Neural Network Classification you’ll learn how to code a neural network classification model using PyTorch so that you can classify things and answer these questions.\n4. PyTorch Computer Vision — Neural networks have changed the game of computer vision forever. And now PyTorch drives many of the latest advancements in computer vision algorithms.\nFor example, Tesla use PyTorch to build the computer vision algorithms for their self-driving software.\nWith PyTorch Computer Vision you’ll build a PyTorch neural network capable of seeing patterns in images of and classifying them into different categories.\n5. PyTorch Custom Datasets — The magic of machine learning is building algorithms to find patterns in your own custom data. There are plenty of existing datasets out there, but how do you load your own custom dataset into PyTorch?\nThis is exactly what you'll learn with the PyTorch Custom Datasets section of this course.\nYou’ll learn how to load an image dataset for FoodVision Mini: a PyTorch computer vision model capable of classifying images of pizza, steak and sushi (am I making you hungry to learn yet?!).\nWe’ll be building upon FoodVision Mini for the rest of the course.\n6. PyTorch Going Modular — The whole point of PyTorch is to be able to write Pythonic machine learning code.\nThere are two main tools for writing machine learning code with Python:\nA Jupyter/Google Colab notebook (great for experimenting)\nPython scripts (great for reproducibility and modularity)\nIn the PyTorch Going Modular section of this course, you’ll learn how to take your most useful Jupyter/Google Colab Notebook code and turn it reusable Python scripts. This is often how you’ll find PyTorch code shared in the wild.\n7. PyTorch Transfer Learning — What if you could take what one model has learned and leverage it for your own problems? That’s what PyTorch Transfer Learning covers.\nYou’ll learn about the power of transfer learning and how it enables you to take a machine learning model trained on millions of images, modify it slightly, and enhance the performance of FoodVision Mini, saving you time and resources.\n8. PyTorch Experiment Tracking — Now we're going to start cooking with heat by starting Part 1 of our Milestone Project of the course!\nAt this point you’ll have built plenty of PyTorch models. But how do you keep track of which model performs the best?\nThat’s where PyTorch Experiment Tracking comes in.\nFollowing the machine learning practitioner’s motto of experiment, experiment, experiment! you’ll setup a system to keep track of various FoodVision Mini experiment results and then compare them to find the best.\n9. PyTorch Paper Replicating — The field of machine learning advances quickly. New research papers get published every day. Being able to read and understand these papers takes time and practice.\nSo that’s what PyTorch Paper Replicating covers. You’ll learn how to go through a machine learning research paper and replicate it with PyTorch code.\nAt this point you'll also undertake Part 2 of our Milestone Project, where you’ll replicate the groundbreaking Vision Transformer architecture!\n10. PyTorch Model Deployment — By this stage your FoodVision model will be performing quite well. But up until now, you’ve been the only one with access to it.\nHow do you get your PyTorch models in the hands of others?\nThat’s what PyTorch Model Deployment covers. In Part 3 of your Milestone Project, you’ll learn how to take the best performing FoodVision Mini model and deploy it to the web so other people can access it and try it out with their own food images.\nWhat's the bottom line?\nMachine learning's growth and adoption is exploding, and deep learning is how you take your machine learning knowledge to the next level. More and more job openings are looking for this specialized knowledge.\nCompanies like Tesla, Microsoft, OpenAI, Meta (Facebook + Instagram), Airbnb and many others are currently powered by PyTorch.\nAnd this is the most comprehensive online bootcamp to learn PyTorch and kickstart your career as a Deep Learning Engineer.\nSo why wait? Advance your career and earn a higher salary by mastering PyTorch and adding deep learning to your toolkit?",
      "target_audience": [
        "Anyone who wants a step-by-step guide to learning PyTorch and be able to get hired as a Deep Learning Engineer making over $100,000 / year",
        "Students, developers, and data scientists who want to demonstrate practical machine learning skills by actually building and training real models using PyTorch",
        "Anyone looking to expand their knowledge and toolkit when it comes to AI, Machine Learning and Deep Learning",
        "Bootcamp or online PyTorch tutorial graduates that want to go beyond the basics",
        "Students who are frustrated with their current progress with all of the beginner PyTorch tutorials out there that don't go beyond the basics and don't give you real-world practice or skills you need to actually get hired"
      ]
    },
    {
      "title": "Convolutional Neural Networks in Python: CNN Computer Vision",
      "url": "https://www.udemy.com/course/cnn-for-computer-vision-with-keras-and-tensorflow-in-python/",
      "bio": "Python for Computer Vision & Image Recognition - Deep Learning Convolutional Neural Network (CNN) - Keras & TensorFlow 2",
      "objectives": [
        "Get a solid understanding of Convolutional Neural Networks (CNN) and Deep Learning",
        "Build an end-to-end Image recognition project in Python",
        "Learn usage of Keras and Tensorflow libraries",
        "Use Artificial Neural Networks (ANN) to make predictions",
        "Use Pandas DataFrames to manipulate data and make statistical computations."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course Resources"
        ],
        "Setting up Python and Jupyter Notebook": [
          "Installing Python and Anaconda",
          "This is a Milestone!",
          "Opening Jupyter Notebook",
          "Introduction to Jupyter",
          "Arithmetic operators in Python: Python Basics",
          "Quick coding exercise on arithmetic operators",
          "Strings in Python: Python Basics",
          "Quick coding exercise on String operations",
          "Lists, Tuples and Directories: Python Basics",
          "Quiz",
          "Quick coding exercise on Tuples",
          "Working with Numpy Library of Python",
          "Quick coding exercise on NumPy Library",
          "Working with Pandas Library of Python",
          "Quick coding exercise on Pandas Library",
          "Working with Seaborn Library of Python",
          "Quizzes"
        ],
        "Integrating ChatGPT with Python": [
          "Integrating ChatGPT with Jupyter notebook"
        ],
        "Single Cells - Perceptron and Sigmoid Neuron": [
          "Perceptron",
          "Activation Functions",
          "Python - Creating Perceptron model",
          "Quiz"
        ],
        "Neural Networks - Stacking cells to create network": [
          "Basic Terminologies",
          "Gradient Descent",
          "Back Propagation",
          "Quiz"
        ],
        "Important concepts: Common Interview questions": [
          "Some Important Concepts",
          "Quiz"
        ],
        "Standard Model Parameters": [
          "Hyperparameters",
          "Quiz"
        ],
        "Tensorflow and Keras": [
          "Keras and Tensorflow",
          "Installing Tensorflow and Keras",
          "Quiz"
        ],
        "Python - Dataset for classification problem": [
          "Dataset for classification",
          "Normalization and Test-Train split",
          "More about test-train split"
        ],
        "Python - Building and training the Model": [
          "Different ways to create ANN using Keras",
          "Building the Neural Network using Keras",
          "Compiling and Training the Neural Network model",
          "Evaluating performance and Predicting using Keras"
        ]
      },
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Convolutional Neural Network (CNN) course that teaches you everything you need to create a Image Recognition model in Python, right?\nYou've found the right Convolutional Neural Networks course!\nAfter completing this course you will be able to:\nIdentify the Image Recognition problems which can be solved using CNN Models.\nCreate CNN models in Python using Keras and Tensorflow libraries and analyze their results.\nConfidently practice, discuss and understand Deep Learning concepts\nHave a clear understanding of Advanced Image Recognition models such as LeNet, GoogleNet, VGG16 etc.\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Convolutional Neural networks course.\nIf you are an Analyst or an ML scientist, or a student who wants to learn and apply Deep learning in Real world image recognition problems, this course will give you a solid base for that by teaching you some of the most advanced concepts of Deep Learning and their implementation in Python without getting too Mathematical.\nWhy should you choose this course?\nThis course covers all the steps that one should take to create an image recognition model using Convolutional Neural Networks.\nMost courses only focus on teaching how to run the analysis but we believe that having a strong theoretical understanding of the concepts enables us to create a good model . And after running the analysis, one should be able to judge how good the model is and interpret the results to actually be able to help the business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using Deep learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 1,300,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Practice test, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take practice test to check your understanding of concepts. There is a final practical assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a Neural network based model i.e. a Deep Learning model, to solve business problems.\nBelow are the course contents of this course on ANN:\nPart 1 (Section 2)- Python basics\nThis part gets you started with Python.\nThis part will help you set up the python and Jupyter environment on your system and it'll teach you how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nPart 2 (Section 3-6) - ANN Theoretical Concepts\nThis part will give you a solid understanding of concepts involved in Neural Networks.\nIn this section you will learn about the single cells or Perceptrons and how Perceptrons are stacked to create a network architecture. Once architecture is set, we understand the Gradient descent algorithm to find the minima of a function and learn how this is used to optimize our network model.\nPart 3 (Section 7-11) - Creating ANN model in Python\nIn this part you will learn how to create ANN models in Python.\nWe will start this section by creating an ANN model using Sequential API to solve a classification problem. We learn how to define network architecture, configure the model and train the model. Then we evaluate the performance of our trained model and use it to predict on new data. Lastly we learn how to save and restore models.\nWe also understand the importance of libraries such as Keras and TensorFlow in this part.\nPart 4 (Section 12) - CNN Theoretical Concepts\nIn this part you will learn about convolutional and pooling layers which are the building blocks of CNN models.\nIn this section, we will start with the basic theory of convolutional layer, stride, filters and feature maps. We also explain how gray-scale images are different from colored images. Lastly we discuss pooling layer which bring computational efficiency in our model.\nPart 5 (Section 13-14) - Creating CNN model in Python\nIn this part you will learn how to create CNN models in Python.\nWe will take the same problem of recognizing fashion objects and apply CNN model to it. We will compare the performance of our CNN model with our ANN model and notice that the accuracy increases by 9-10% when we use CNN. However, this is not the end of it. We can further improve accuracy by using certain techniques which we explore in the next part.\nPart 6 (Section 15-18) - End-to-End Image Recognition project in Python\nIn this section we build a complete image recognition project on colored images.\nWe take a Kaggle image recognition competition and build CNN model to solve it. With a simple model we achieve nearly 70% accuracy on test set. Then we learn concepts like Data Augmentation and Transfer Learning which help us improve accuracy level from 70% to nearly 97% (as good as the winners of that competition).\nBy the end of this course, your confidence in creating a Convolutional Neural Network model in Python will soar. You'll have a thorough understanding of how to use CNN to create predictive models and solve image recognition problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow are some popular FAQs of students who want to start their Deep learning journey-\n\n\nWhy use Python for Deep Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Deep Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nDeep Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Deep Learning journey",
        "Anyone curious to master image recognition from Beginner level in short span of time"
      ]
    },
    {
      "title": "Interactive Python Dashboards with Plotly and Dash",
      "url": "https://www.udemy.com/course/interactive-python-dashboards-with-plotly-and-dash/",
      "bio": "Learn how to create interactive plots and intelligent dashboards with Plotly, Python, and the Dash library!",
      "objectives": [
        "Learn about Plotly to create plots like Bar Charts, Line Charts, Scatter Plots, Heat Maps, and more!",
        "Create Layouts with Plotly's Dash library.",
        "Use Dash to create interactive components with Plotly.",
        "Learn how to connect multiple inputs and outputs with a dashboard.",
        "Update live interactive graphs with clicks, hover overs, and more.",
        "Connect the interactive dashboard to live updating data for streaming information.",
        "Learn how to secure your interactive dashboards with App Authorization.",
        "Deploy your interactive dashboards to the internet with services like Heroku."
      ],
      "course_content": {
        "Course Introduction": [
          "Course Overview",
          "Course FAQ",
          "Overview of Notes and Guidebook",
          "Installation Overview"
        ],
        "Introduction to Data Basics": [
          "Plotly and Dash Overview",
          "NumPy Crash Course",
          "Pandas Crash Course",
          "Exercise: NumPy/Pandas Practice",
          "NumPy/Pandas Practice Exercise Solution"
        ],
        "Plotly Basics": [
          "Plotly Basics Overview",
          "Scatter Plots",
          "Line Charts",
          "Line Charts Part Two",
          "Line Charts Exercise",
          "Line Charts Exercise - Solution Code Along",
          "Bar Charts",
          "Bar Charts Exercise",
          "Bar Charts Exercise - Solution",
          "Bubble Plots",
          "Bubble Charts Exercise",
          "Bubble Charts Exercise Solution",
          "Box Plots",
          "Box Plots Exercise",
          "Box Plots Exercise Solution",
          "Histograms",
          "Histograms Exercise",
          "Histograms Exercise Solution",
          "Distplots",
          "DistPlots Exercise",
          "DistPlots Exercise Solution",
          "Heatmaps",
          "Heatmaps Exercise",
          "Heatmaps Exercise Solution"
        ],
        "Dash Basics - Layout": [
          "Introduction to Dash Basics",
          "Dash Layouts - Part One",
          "Dash Layouts - Part Two - Styling",
          "Converting Simple Plotly Plot to Dashboard with Dash"
        ],
        "DashBoard Exercise": [
          "Exercise: Create a Simple Dashboard",
          "Simple Dashboard Exercise Solution"
        ],
        "DashBoard Components": [
          "Dash Components",
          "HTML Components",
          "Core Components",
          "Markdown with Dash",
          "Using Help() with Dash"
        ],
        "Interactive Components": [
          "Single Callbacks for Interactivity",
          "Dash Callbacks for Graphs",
          "Multiple Inputs",
          "Multiple Outputs",
          "Exercise: Interactive Components",
          "Interactive Components Exercise Solution"
        ],
        "Callbacks with State": [
          "Controlling Callbacks with Dash State"
        ],
        "Interacting with Visualizations": [
          "Hover Over Data",
          "Click Data",
          "Selection Data",
          "Updating Graphs on Interactions",
          "Updating Graphs on Interactions Part 2",
          "Updating Graphs on Interactions - Part Three"
        ],
        "Code Along Milestone Project": [
          "Code Along Milestone Project Overview",
          "Milestone Project Part One - Imports and Graph Setup",
          "Milestone Project Part Two - Input Box and Basic Callback",
          "Milestone Project Part Three - Reading Data with Pandas Datareader",
          "Milestone Project Part Four - Adding DatePickers for Choosing Dates",
          "Milestone Project Part Five - Adding in Dash State",
          "Milestone Project Part Six - Multiple Stock Option Dropdown"
        ]
      },
      "requirements": [
        "Knowledge of Basic Python",
        "Computer with Internet Access"
      ],
      "description": "Welcome to Python Visualization Dashboards with Plotly's Dash Library!\nThis course will teach your everything you need to know to use Python to create interactive dashboard's with Plotly's new Dash library! Have you ever wanted to take your Python skills to the next level in data visualization? With this course you will be able to create fully customization, interactive dashboards with the open source libraries of Plotly and Dash.\nDash instructional courses from Plotly usually cost more than $1000, but now you can get the bootcamp experience for a fraction of that price in this self-paced course that includes example code, explanatory videos, student support in our chat channels, Question and Answer Forums, and interactive exercises.\nWe'll start off by teaching you enough Numpy and Pandas that you feel comfortable working and generating data in our quick crash course. Then we'll continue by teaching you about basic data visualization with Plotly, including scatter plots, line charts, bar charts, bubble charts, box plots, histograms, distribution plots, heat maps, and more! We'll also give you an intuition of when to use each plot type.\nAfter this and at the end of each section you'll be given exercise tasks to test and evaluate your new skills, a feature no other Plotly Dash training offers!\nOnce you have a grasp on Plotly basics we'll move on to the bulk of the course which is utilizing the Dash library to leverage the power of plotly plots to create interactive dashboards. We'll discuss how to create layouts for dashboards, how to have interactive callbacks, dealing with multiple inputs and outputs, creating interactive components, and more!\nWe'll finish off the course by going over live updating dashboards that automatically update in real time and even show you how you can deploy your dashboards live to the web with the Heroku service.\nBy taking this course you will be learning the bleeding edge of data visualization technology with Python and gain a valuable new skill to show your colleagues or potential employers. After completing the course you will have a certification you can post to your LinkedIn profile and a portfolio of dashboard projects you can share as well.\nAll of this comes with a 30 day money back guarantee, so what are you waiting for? Enroll today and we'll see you inside the course!",
      "target_audience": [
        "Python developers who are interested in learning how to create interactive dashboards and visualizations"
      ]
    },
    {
      "title": "MLOps with AWS - Bootcamp - Zero to Hero Series",
      "url": "https://www.udemy.com/course/practical-mlops-for-data-scientists-devops-engineers-aws/",
      "bio": "Empower Your MLOps Journey: AWS AI/ML Mastery - From Notebook to Production Operation with Expert Guidance - MLOps 2024",
      "objectives": [
        "Configuring the CI/CD Pipeline for Machine Learning Projects",
        "Ability to track the source code & training images, configuration files with Git Based Repository – AWS CodeCommit",
        "Ability to Perform the Build using AWS CodeBuild",
        "Ability to Deploy the Application on Server using AWS CodeDeploy",
        "Orchestrate the MLOps steps using AWS CodePipeline",
        "Identify appropriate AWS services to implement ML solutions",
        "Perform the Load testing",
        "Monitoring the End Point Performance",
        "Monitoring the Model Drift",
        "The ability to follow model-training best practices",
        "The ability to follow deployment best practices",
        "The ability to follow operational best practices"
      ],
      "course_content": {
        "About AWS MLOps Course and Instructor": [
          "Welcome to MLOps Journey",
          "Slide Resources & Source Code",
          "About the MLOps with AWS Course",
          "How to make the most of this course?"
        ],
        "Introduction to MLOps": [
          "What & Why MLOps",
          "Why MLOps",
          "Quick Hands On Demo on MLOps",
          "MLOps Fundamentals",
          "MLOps Fundamentals",
          "MLOps Fundamentals - Deep Dive",
          "MLOps Fundamentals - Deep Dive",
          "Why DevOps alone is not Suitable for Machine Learning ?",
          "Why DevOps not Suitable for ML ?",
          "What is AWS & its Benefits",
          "Technical Stack of AWS for MLOps & Machine Learning",
          "Assignment - 1"
        ],
        "DevOps for Data Scientists": [
          "What is SDLC & Why its Important",
          "Types of SDLC",
          "Waterfall Vs Agile Vs DevOps",
          "DevOps Lifecycle & Tools in AWS"
        ],
        "Getting Started with AWS": [
          "What do we cover in this section ?",
          "Create AWS Account",
          "Setting up MFA on Root Account",
          "Create IAM Account and Account Alias",
          "Setup CLI with Credentials",
          "IAM Policy",
          "IAM Policy generator & attachment",
          "Delete the IAM User",
          "S3 Bucket and Storage Classes",
          "Creation of S3 Bucket from Console",
          "Creation of S3 Bucket from CLI",
          "Version Enablement in S3",
          "Introduction EC2 instances",
          "Launch EC2 instance & SSH into EC2 Instances",
          "Clean Up Activity"
        ],
        "Linux Operating System for DevOps and Data Scientists": [
          "What do we learn in this section ?",
          "Linux Features & Bash",
          "How to Launch EC2 Instances (Quick Refresh)",
          "Linux Basic Commands"
        ],
        "Source code Management using GIT - CodeCommit": [
          "Introduction to CI CD Pipeline",
          "Update on CodeCommit - Not Available for New Users",
          "Introduction to AWS Code Commit & DVCS",
          "Git Initial config & Git Commands",
          "Setting up the workspace for Git",
          "Git Workflow",
          "Adding files to Staging Area",
          "Staged Differences",
          "Git Unstage",
          "Git Reset & Revert",
          "Update on CodeCommit",
          "AWS Code Commit Remote Git Commands",
          "Cloning and Branching",
          "Git Branching Hands On Part 1",
          "Git Branching Hands On Part 2",
          "Git Conflicts & Resolving them",
          "Git Rebase Vs Git Merge",
          "Git Stash Introduction",
          "Git Stash Hands On",
          "AWS Code Commit Security",
          "AWS Code Commit Security - Hands On",
          "AWS Code Commit Integration - Triggers - Notifications - CloudWatch - EventBridg",
          "Summary"
        ],
        "Source code Management using GIT - Github": [
          "Introduction to Version Control Systems",
          "Local Repo vs Remote Repo",
          "Git Configurations",
          "Getting Started with Local Repo",
          "Concept of Working Directory - Staging Area - Commit",
          "Getting Started with git",
          "Git Workflow - Local Repo",
          "Git Branch",
          "Switching the Branches",
          "Merging",
          "Checking Out Commits",
          "Git Hosting Services",
          "Working with Remote Repositories",
          "Cloning and Delete Branches",
          "3 way merge",
          "Summary"
        ],
        "YAML Crash Course": [
          "YAML Crash Course"
        ],
        "AWS CodeBuild": [
          "Introduction to AWS CodeBuild",
          "Create First CodeBuild Project",
          "Create First CodeBuild Project with GitHub",
          "Important Update from Instructor",
          "buildspec.yml deep dive",
          "Code Build Hands On",
          "Environment Variables in CodeBuild & buildspec.yml deep dive Hands On",
          "Working CodeBuild Artifacts Hands On",
          "AWS CodeBuild Triggers",
          "CleanUp Activity"
        ],
        "AWS Code Deploy": [
          "AWS CodeDeploy Introduction",
          "First AWS CodeDeploy - Intro to Hands On",
          "First AWS CodeDeploy",
          "appspec.yml - Deep Dive",
          "CodeDeploy Summary"
        ]
      },
      "requirements": [
        "Basic knowledge of AWS",
        "Account with AWS for practical Hand-On",
        "Basic knowledge of Machine Learning & Deep Learning"
      ],
      "description": "Welcome to \"Practical MLOps for Data Scientists & DevOps Engineers with AWS\"\nAre you ready to propel your career in artificial intelligence and machine learning (AI/ML) development or data science to new heights? This comprehensive course is meticulously crafted for individuals with aspirations to excel in these domains, providing a Production Level mindset that goes beyond the basics.\nCourse Overview: Mastering MLOps with AWS\n**1. Elevate Your Skills:\nDesign, build, deploy, optimize, train, tune, and maintain ML solutions using AWS Cloud.\nAdopt a Production Level mindset tailored for Machine Learning in conjunction with DevOps best practices.\n**2. Beyond Basics:\nEmploy model-training best practices on extensive cloud-based datasets.\nDemonstrate expertise in deployment best practices for consistent functionality.\nImplement operational best practices to guarantee zero downtime.\n**3. Structured Learning Path:\nFollow a logical, structured path with in-depth explanations, practical exercises, and relevant demonstrations.\nGain proficiency in tackling real-world business challenges by implementing scalable solutions on AWS.\nCourse Structure: Journey Through Mastery\nSection 1: Introduction to the AWSMLOPS Course and Instructor\nGet acquainted with the course objectives and the experienced instructor leading the way.\nSection 2: Understanding MLOps\nDelve into the core concepts of MLOps, understanding its significance and application.\nSection 3: DevOps Principles for Data Scientists\nExplore the principles of DevOps tailored for data scientists, bridging the gap between development and operations.\nSection 4: Getting Started with AWS\nAcquaint yourself with the AWS platform, laying the foundation for subsequent sections.\nSections 5-16: In-Depth Exploration\nA comprehensive exploration of key topics, including AWS CodeBuild, AWS CodeDeploy, AWS CodePipeline, Docker Containers, Amazon SageMaker, Feature Engineering, SageMaker Pipelines, and much more.\nHands-On Learning: Real-World Applications\nTools and Technologies Covered:\nData Ingestion and Collection\nData Processing and ETL (Extract, Transform, Load)\nData Analysis and Visualization\nModel Training and Deployment/Inference\nOperational Aspects of Machine Learning\nAWS Machine Learning Application Services\nNotebooks and Integrated Development Environments (IDEs)\nVersion Control with AWS CodeCommit\nAmazon Athena, AWS Batch, Amazon EC2\nAmazon Elastic Container Registry (Amazon ECR), AWS Glue\nAmazon CloudWatch, AWS Lambda\nAmazon S3 for Storage and Scalability\nAccess to Course Materials:\nAll course materials, including source code, are available on GitHub for convenient access from anywhere.\nStay updated with the latest advancements through easy access to the latest updates.\nEmbark on the MLOps Journey: Elevate Your Skills Today\nWhy Choose This Course?\nGain a Production Level mindset tailored for AI/ML in conjunction with DevOps practices.\nAcquire proficiency in deploying solutions on scalable datasets beyond personal laptops.\nComprehensive exploration of AWS services crucial for MLOps.\nReal-world applications and hands-on projects for practical learning.\nYour Success in MLOps Begins Here:\nEquip yourself with the latest tools and best practices on the AWS platform.\nTackle complex business challenges with confidence.\nPropel your career to new heights in the world of MLOps.\nEnroll Now: Take the leap into mastering MLOps with AWS. Click the \"Enroll Now\" button to embark on a transformative learning journey. Elevate your AI/ML and DevOps skills to the next level and solve complex business challenges effectively. Your success in the world of MLOps begins here and now!",
      "target_audience": [
        "Anyone preparing for Data Science , Machine Learning & Deep Learning Interviews",
        "Anyone interested in learning how Machine Learning is implemented on Large scale data",
        "Anyone interested in AWS cloud-based machine learning and data science",
        "Anyone looking to learn the best practices to deploy the Machine Learning Models on Cloud",
        "Anyone looking to learn the best practices to Operationalize the Machine Learning Models"
      ]
    },
    {
      "title": "LLM Fine Tuning on OpenAI",
      "url": "https://www.udemy.com/course/llm-fine-tuning-on-openai/",
      "bio": "Learn to use OpenAI to fine tune LLMs on your own datasets!",
      "objectives": [
        "Gain a comprehensive understanding of the fundamental principles and advanced concepts in artificial intelligence and language modeling.",
        "Explore various datasets specific to different fields, learning how to identify and understand their unique structures and challenges.",
        "Recognize and understand the various strategies and techniques used in fine-tuning language models for specialized applications.",
        "Master the skills necessary to preprocess datasets effectively, ensuring they are in the ideal format for AI training.",
        "Delve into various methods to enhance the accuracy and efficiency of AI models for specific domains.",
        "Learn to assess the proficiency of fine-tuned models using different evaluation metrics and methods.",
        "Investigate the vast potential of fine-tuned AI models in practical, real-world scenarios across multiple industries.",
        "Acquire knowledge on how to estimate and manage the costs associated with AI model training, making the process efficient and economical."
      ],
      "course_content": {
        "Course Overview": [
          "COURSE FAQs and DOWNLOADS HERE",
          "Course Welcome",
          "How LLM Fine-Tuning Works",
          "OpenAI Account and API Key"
        ],
        "LLM Fine Tuning via OpenAI API": [
          "Dataset Processing",
          "Dataset Statistics",
          "Data Formatting",
          "Training",
          "Visualizing Losses",
          "Application of Fine Tuned Model"
        ],
        "Fine Tuning via OpenAI Platform GUI": [
          "OpenAI Platform Fine-Tuning GUI"
        ]
      },
      "requirements": [
        "Python experience required, including pandas and basic API knowledge",
        "Experience with the OpenAI API basics"
      ],
      "description": "Unleash the Potential of Tailored AI Understanding: Master the Art of Fine-Tuning AI Models Across Diverse Fields Welcome to the Advanced Realm of AI Training!\nAbout the Course: Dive into the sophisticated world of AI and language models with our comprehensive course. Here, you'll learn how to fine-tune OpenAI's state-of-the-art language models for a variety of specialized fields. Whether you're a professional in healthcare, finance, education, or another domain, or a researcher or student keen on exploring the depths of AI language comprehension, this course is your key to mastering domain-specific AI language understanding.\nCourse Content: You'll begin by exploring the intricacies of domain-specific datasets, learning how to dissect and understand the unique structures and challenges they present. The course then guides you through refining these datasets to prime them for AI training. You'll gain hands-on experience in fine-tuning techniques, learning how to tweak and enhance AI models for domain-specific accuracy. We'll also cover performance evaluation, offering strategies to assess and boost your model's effectiveness in your chosen field. Moreover, the course delves into the real-world applications of your fine-tuned model, showcasing its potential across various industries.\nCourse Highlights: Experience practical, hands-on training with real-world data in your field of interest. Our expert-led guidance walks you through every step of dataset preparation and model tuning. You'll engage with dynamic learning tools like Jupyter Notebooks for an interactive educational experience, gaining rich insights into the challenges and solutions in training AI for specialized domains. This cost-effective training also teaches you how to estimate and manage AI training expenses efficiently.\nWho Should Enroll: Professionals in various fields seeking to integrate AI tools for enhanced data analysis, researchers and students in specialized areas looking to deepen their AI knowledge, and AI enthusiasts eager to explore domain-specific model training.\nCourse Outcome: By the end of this course, you'll have fine-tuned a sophisticated language model, boosting its proficiency in your specific area of interest. You'll possess the skills to navigate and utilize AI across various sectors, paving the way for innovative applications and research opportunities.\nEnroll now and begin your journey towards mastering domain-specific AI and transforming industries with your expertise!",
      "target_audience": [
        "Experienced Python developers looking to learn how to fine-tune OpenAI LLMs"
      ]
    },
    {
      "title": "Statistics for Business Analytics and Data Science A-Z™",
      "url": "https://www.udemy.com/course/data-statistics/",
      "bio": "Learn The Core Stats For A Data Science Career. Master Statistical Significance, Confidence Intervals And Much More!",
      "objectives": [
        "Understand what a Normal Distribution is",
        "Understand standard deviations",
        "Explain the difference between continuous and discrete variables",
        "Understand what a sampling distribution is",
        "Understand the Central Limit Theorem",
        "Apply the Central Limit Theorem in practice",
        "Apply Hypothesis Testing for Means",
        "Apply Hypothesis Testing for Proportions",
        "Use the Z-Score and Z-Tables",
        "Use the t-Score and t-Tables",
        "Understand the difference between a normal distribution and a t-distribution",
        "Understand and apply statistical significance",
        "Create confidence intervals",
        "Understand the potential pitfalls of overusing p-Values"
      ],
      "course_content": {
        "Welcome to the course": [
          "Welcome Challenge!",
          "Welcome!",
          "Learning Paths",
          "Some Additional Resources!!"
        ],
        "Distributions": [
          "Plan of Attack",
          "Continuous vs Discrete",
          "What is a Distribution?",
          "What is Standard Deviation?",
          "Normal Distribution",
          "Skewness",
          "Mean, Median, Mode",
          "Homework",
          "Homework Solution - Part 1",
          "Homework Solution - Part 2",
          "EXTRA: Cantor's Diagonal Argument",
          "Distributions"
        ],
        "Central Limit Theorem": [
          "Plan of Attack",
          "Populations and Samples",
          "Sampling Distribution",
          "Central Limit Theorem",
          "Central Limit Theorem - Intuition",
          "Central Limit Theorem - Visualization",
          "Z-Score",
          "Hands-On CLT - Analytics Challenge",
          "Homework",
          "Homework Solution",
          "Central Limit Theorem"
        ],
        "Hypothesis Testing / Statistical Significance": [
          "Plan of Attack",
          "Statistical Significance: What is P-Value?",
          "Hypothesis Testing - Steps",
          "Hypothesis Testing - Rejection Region",
          "Hypothesis Testing Assumptions",
          "Proportion Testing",
          "Homework",
          "Homework Solution",
          "Hypothesis Testing/ Statistical Significance"
        ],
        "Advanced Hypothesis Testing": [
          "What it means when you cannot reject the Null Hypothesis",
          "Student's t-distribution",
          "T-Tests",
          "1-Tailed And 2-Tailed Tests",
          "2-Tailed Test Exercise",
          "Warning about misuse and overuse of p-values",
          "Real Life Example",
          "Advanced Hypothesis Testing",
          "THANK YOU Video"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Huge Congrats for completing the challenge!",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Just a basic knowledge of high school maths"
      ],
      "description": "If you are aiming for a career as a Data Scientist or Business Analyst then brushing up on your statistics skills is something you need to do.\n\nBut it's just hard to get started... Learning / re-learning ALL of stats just seems like a daunting task.\n\nThat's exactly why I have created this course!\nHere you will quickly get the absolutely essential stats knowledge for a Data Scientist or Analyst.\n\nThis is not just another boring course on stats.\nThis course is very practical.\nI have specifically included real-world examples of business challenges to show you how you could apply this knowledge to boost YOUR career.\n\nAt the same time you will master topics such as distributions, the z-test, the Central Limit Theorem, hypothesis testing, confidence intervals, statistical significance and many more!\nSo what are you waiting for?\nEnroll now and empower your career!",
      "target_audience": [
        "Anybody who wants to master stats for business analytics",
        "Anybody who wants to learn stats from the ground up",
        "Anybody who wants to get hands-on experience with stats"
      ]
    },
    {
      "title": "Artificial Intelligence (AI) in Software Testing",
      "url": "https://www.udemy.com/course/artificial-intelligence-ai-in-software-testing/",
      "bio": "The Future of Automated Testing with Machine Learning - Implementing Artificial Intelligence (AI) in Test Automation",
      "objectives": [
        "You will learn what is Artificial Intelligence (AI) and what is the relationship of AI with Machine Learning, Deep Learning and Data Science. You will also learn how Machines are learning faster than ever.",
        "You will learn how you can use Artificial Intelligence (AI) to drive your UI test automation projects. You will also learn how AI test automation tool uses machine learning to speed-up the authoring, execution and maintenance of automated tests."
      ],
      "course_content": {
        "Introduction": [
          "Course Introduction"
        ],
        "Introduction to Artificial Intelligence (AI)": [
          "What is Artificial Intelligence (AI)?",
          "Types of Artificial Intelligence (AI)",
          "Why Artificial Intelligence (AI)?",
          "Using Artificial Intelligence (AI)",
          "Examples - Applications of Artificial Intelligence (AI)"
        ],
        "Introduction to Machine Learning": [
          "What is Machine Learning?",
          "Why Machine Learning?",
          "Examples - Algorithms behind Machine Learning"
        ],
        "Introduction to Deep Learning": [
          "What is Deep Learning?",
          "Why Deep Learning?",
          "Example - Deep Learning Vs Machine Learning"
        ],
        "Introduction to Data Science": [
          "What is Data Science?",
          "Why Data Science?",
          "Examples - Use Cases of Data Science"
        ],
        "Artificial Intelligence (AI) in Software Testing": [
          "What is AI in Software Testing?",
          "The Role of AI Testing",
          "Why do we Need AI in Software Testing?",
          "Pros and Cons of AI in Software Testing",
          "Applications of AI in Software Testing",
          "Is it time for Testers or QA Teams to worry about AI?"
        ],
        "Automated Testing with Artificial Intelligence (AI)": [
          "Implementing AI in Test Automation",
          "Training the AI Bots",
          "Challenges with AI-powered Applications",
          "Examples - Real World use cases using Artificial Intelligence",
          "Demo - Facial Emotion Detection Using Artificial Intelligence",
          "Demo - Text Analysis API Using Artificial Intelligence",
          "Demo - EYE SPY Mobile App Using Artificial Intelligence"
        ],
        "Innovative AI Test Automation Tools for the Future": [
          "Tools used for Implementing AI in Automation Testing",
          "What is NEXT?",
          "AI Test Automation Demo using Testim"
        ],
        "Interview Questions": [
          "Interview Questions - Specific to AI Machine Learning in Software Testing"
        ],
        "Bonus Lecture: Robotic Process Automation (RPA)": [
          "What is Robotic Process Automation (RPA)?",
          "Difference between RPA and AI?",
          "Benefits of RPA",
          "Example - How does RPA work?"
        ]
      },
      "requirements": [
        "All should need to know is What is importance of Software Testing and What is the benefits of implementing Test Automation over manual testing."
      ],
      "description": "**************************THIS COURSE IS RECENTLY UPDATED with quick course summary contents in the form of pdf files (In Section 11: Summary) to host Lunch and Learns sessions for your friends or coworkers****************************************\nThe reason behind is, I have received lot of good feedback about this course from different group of peoples. They are really excited to know about how Artificial Intelligence can help in Software Testing. They want to teach their friends or coworkers the importance of Artificial Intelligence in Software Testing. They requested that I can come up with 30-40 min quick presentation from my detail course so they can host lunch and learn session for their friends or coworkers. I liked their idea and that’s why I have created quick pdf document called: Learn the Basic Fundamentals of AI in Software Testing in less than 30 minutes.\n\n\n**************************THIS COURSE IS RECENTLY UPDATED with season 2 course contents. In this season 2, I have added ONE NEW LECTURE CALLED: AI Test Automation Demo using Testim in “Innovative AI Test Automation Tools for the Future” section of the course*********************************\nHIGHLIGHTS:\nThe NEW LECTURE shows how to create AI Test Automation project for Web Application using TESTIM tool. If your company's application is web application then you can create automation script using TESTIM which uses AI Machine Learning technique. In that video, you will see how you can create automation scripts. You will also see the difference between Coded UI/Selenium scripts and AI scripts. You will be amazed to see that how AI automation scripts PASSED the test execution even if you change the web element all attributes value.\nPlease periodically check out this course since I am also planning to add new topics (Smart API Test Generator - which uses Artificial Intelligence to convert your Web UI tests into Automated API Tests) including replacing some static slides to animated slides.\n***********************************************************************************************************************\nArtificial Intelligence (AI) in Software Testing course is the first ever course on UDemy which talks about future of Automated Testing with AI Machine Learning.\n\nI have decided to release this course into two seasons. it requires students to understand basic fundamental of Artificial Intelligence (AI) and the need for AI in Software Testing on first season before we jump into next season where we can deep dive into  AI test automation and discussed some innovative tools that we can use for implementing AI in test automation.\nThis course is designed for both testers and developers. Tester who want to develop their testing skills in the test automation with Artificial Intelligence (AI) and Developer who want to execute their unit test in automated way using Artificial Intelligence (AI).\n\nThis course will teach you how AI-assisted test automation can transform the UI. This course will also teach you Artificial Intelligence (AI) and it's relationship with Machine Learning, Deep Learning and Data Science. After you have completed this course you should be able to build test automation projects for your company's applications using Artificial Intelligence (AI). This course should also help you for your AI test automation job interview.",
      "target_audience": [
        "This course is designed for both testers and developers.",
        "Tester who want to develop their testing skills in the test automation with Artificial Intelligence (AI) and Developer who want to execute their unit test in automated way using Artificial Intelligence (AI)."
      ]
    },
    {
      "title": "Master Apache Spark - Hands On!",
      "url": "https://www.udemy.com/course/the-ultimate-apache-spark-with-java-course-hands-on/",
      "bio": "Learn how to slice and dice data using the next generation big data platform - Apache Spark!",
      "objectives": [
        "Utilize the most powerful big data batch and stream processing engine to solve big data problems",
        "Master the new Spark Java Datasets API to slice and dice big data in an efficient manner",
        "Build, deploy and run Spark jobs on the cloud and bench mark performance on various hardware configurations",
        "Optimize spark clusters to work on big data efficiently and understand performance tuning",
        "Transform structured and semi-structured data using Spark SQL, Dataframes and Datasets",
        "Implement popular Machine Learning algorithms in Spark such as Linear Regression, Logistic Regression, and K-Means Clustering"
      ],
      "course_content": {
        "Introduction": [
          "Why Spark",
          "Spark High Level Components",
          "Creating a Spark Maven Project",
          "Dedicated TA Support",
          "Join our Online Community (Discord)",
          "Import Source Code into Eclipse",
          "First Spark Application",
          "Spark Standalone Cluster Architecture",
          "Apache Spark Introduction"
        ],
        "Spark Java Dataset API Basics": [
          "Ingesting CSV and JSON Files",
          "How to reduce logging in the console",
          "Real World Dataframes Example",
          "Union Dataframes and Other Set Transformations",
          "Converting Between Datasets and Dataframes"
        ],
        "Diving Deeper with Datasets, Dataframes, Transformations and the DAG": [
          "Map and Reduce Transformation Functions",
          "Using Datasets with User Defined POJOs",
          "Using Datasets with Unstructured Textual Data",
          "Joining Dataframes and Using Various Filter Transformations",
          "Aggregation Transformations + Join Assignment",
          "More on Transformations, Actions and the DAG"
        ],
        "Running Spark Jobs on the Cloud": [
          "Using Spark to Analyze Reddit Comments",
          "Running the Reddit Spark Application on an EMR Cluster",
          "Instructions for Configuring a Spark Stand-alone Cluster"
        ],
        "Spark Streaming Applications": [
          "Streaming Network Socket Example",
          "Stock Market Files Streaming Example",
          "Using Kafka with Spark Streaming"
        ],
        "Machine Learning with Spark MLlib": [
          "Machine Learning Resources",
          "Overview of Linear Regression",
          "Spark Java Linear Regression Example",
          "Overview of Logistic Regression",
          "Spark Java Logistic Regression (Classification Algorithm)",
          "Overview of K-Means Clustering",
          "Spark Java K-Means Clustering Example"
        ],
        "Course Extras!": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Some basic Java programming experience is required. A crash course on Java 8 lambdas is included",
        "You will need a personal computer with an internet connection.",
        "The software needed for this course is completely freely and I'll walk you through the steps on how to get it installed on your computer"
      ],
      "description": "Welcome to Apache Spark Mastery – Hands-On Big Data Processing!\nAre you a Java developer or data engineer eager to harness the power of big data?\nDo you want to design scalable data processing pipelines using one of today’s most powerful platforms?\nHave you been challenged by real-time data streams or the complexities of performance tuning in distributed systems?\nIf you answered yes, then you’re in the right place.\nWhat Makes This Course Stand Out?\nHands-On Experience: Build over 15 real-world Spark applications that tackle actual data challenges.\nComprehensive Curriculum: Dive deep into Spark’s Java Datasets API, Spark SQL, Dataframes, and Streaming to transform and analyze data efficiently.\nCloud Deployment & Performance Tuning: Learn how to deploy Spark jobs on the cloud, benchmark performance, and optimize clusters for maximum efficiency.\nIndustry-Relevant Projects: Work with diverse data sources—from text and CSV to JSON—and analyze large-scale datasets like millions of Reddit comments.\nWhy This Course Is Essential:\nApache Spark is the next generation batch and stream processing engine. It's been proven to be almost 100 times faster than Hadoop and much much easier to develop distributed big data applications with. It's demand has sky rocketed in recent years and having this technology on your resume is truly a game changer. Over 3000 companies are using Spark in production right now and the list is growing very quickly!  Some of the big names include: Oracle, Hortonworks, Cisco, Verizon, Visa, Microsoft, Amazon as well as most of the big world banks and financial institutions!\nYou'll be developing over 15 practical Spark Java applications crunching through real world data and slicing and dicing it in various ways using several data transformation techniques. This course is especially important for people who would like to be hired as a java developer or data engineer because Spark is a hugely sought after skill. We'll even go over how to setup a live cluster and configure Spark Jobs to run on the cloud. You'll also learn about the practical implications of performance tuning and scaling out a cluster to work with big data so you'll definitely be learning a ton in this course.\nTopics Covered in the Apache Spark Course\nIn this course, you'll learn everything you need to know about using Apache Spark in your organization while using their latest and greatest Java Datasets API.  Below are some of the things you'll learn:\nHow to develop Spark Java Applications using Spark SQL Dataframes\nUnderstand how the Spark Standalone cluster works behind the scenes\nHow to use various transformations to slice and dice your data in Spark Java\nHow to marshall/unmarshall Java domain objects (pojos) while working with Spark Datasets\nMaster joins, filters, aggregations and ingest data of various sizes and file formats (txt, csv, Json etc.)\nAnalyze over 18 million real-world comments on Reddit to find the most trending words used\nDevelop programs using Spark Streaming for streaming stock market index files\nStream network sockets and messages queued on a Kafka cluster\nLearn how to develop the most popular machine learning algorithms using Spark MLlib\nCovers the most popular algorithms: Linear Regression, Logistic Regression and K-Means Clustering\nKEY BENEFITS OF APACHE SPARK MASTERY\nMastering Apache Spark positions you at the forefront of big data technology. With this expertise, you’ll be able to design efficient, scalable data processing pipelines that are in high demand across industries. Spark’s widespread adoption by over 3000 companies—including Oracle, Cisco, and Amazon—underscores its value in today's competitive tech landscape. This course will not only boost your technical skills but also enhance your resume, opening doors to exciting career opportunities in data engineering and data science.\nKEY TAKEAWAY\nBy the end of this course, you’ll have the practical skills and in-depth knowledge to harness Apache Spark for building high-performance, scalable data solutions. Whether you’re looking to boost your career or transform how your organization handles big data, Apache Spark Mastery is your gateway to success.\nThis course has a 30 day money back guarantee. You will have access to all of the code used in this course.\nReady to transform your big data capabilities? Enroll now and start mastering Apache Spark today!",
      "target_audience": [
        "Anyone who is a Java developer and want's to add this seriously marketable technology on their resume",
        "Anyone who wants to get into the data science field",
        "Anyone who is interested in into the world of big data",
        "Anyone who wants to implement machine learning algorithms in spark"
      ]
    },
    {
      "title": "TensorFlow 2.0 Practical Advanced",
      "url": "https://www.udemy.com/course/tensorflow-2-practical-advanced/",
      "bio": "Master Tensorflow 2.0, Google’s most powerful Machine Learning Library, with 5 advanced practical projects",
      "objectives": [
        "Build, train, test and deploy Advanced Artificial Neural Networks (ANNs) models using Google’s newly released TensorFlow 2.0.",
        "Understand the underlying theory and mathematics behind Generative Adversarial Neural Networks (GANs).",
        "Apply revolutionary GANs to generate brand new images using Keras API in TF 2.0.",
        "Understand the underlying theory and mathematics behind Auto encoders and Variational Auto Encoders (VAEs).",
        "Train and test Auto-Encoders to perform image compression and de-noising using Keras API in TF 2.0.",
        "Understand the underlying theory and mathematics behind DeepDream algorithm. Develop, train, and test State-of-the art DeepDream algorithm to create AI-based art masterpieces using Keras API in TF 2.0!",
        "Understand the intuition behind Long Short Term Memory (LSTM) Recurrent Neural Networks (RNNs).",
        "Train Long Short Term Memory (LSTM) networks to generate new Shakespeare-style text using Keras API in TF 2.0!",
        "Apply transfer learning to transfer knowledge from pre-trained MobileNet and ResNet networks to classify new images using TensorFlow 2.0 Hub.",
        "Develop ANNs models and train them in Google’s Colab while leveraging the power of GPUs and TPUs.",
        "Deploy AI models in practice using TensorFlow 2.0 Serving."
      ],
      "course_content": {
        "INTRODUCTION AND COURSE OUTLINE": [
          "Course Introduction and Welcome Message",
          "Course Overview",
          "EXTRA: Learning Path",
          "ML, AI and DL",
          "Machine Learning Big Picture",
          "TF 2.0 and Google Colab Overview",
          "Whats New in TensorFlow 2.0",
          "What is Google Colab",
          "Google Colab Demo",
          "Eager Execution",
          "Keras API",
          "Get the materials"
        ],
        "REVIEW OF ARTIFICIAL NEURAL NETWORKS AND CONVOLUTIONAL NEURAL NETWORKS": [
          "ANN and CNN - Part 1",
          "ANN and CNN - Part 2",
          "ANN and CNN - Part 3",
          "ANN and CNN - Part 4",
          "ANN and CNN - Part 5",
          "ANN and CNN - Part 6",
          "ANN and CNN - Part 7",
          "ANN and CNN - Part 8",
          "Project 1 - Solution Part 1",
          "Project 1 - Solution Part 2"
        ],
        "TRANSFER LEARNING (TF HUB)": [
          "What is Transfer learning?",
          "Transfer Learning Process",
          "Transfer Learning Strategies",
          "ImageNet",
          "Transfer Learning Project 1 - Coding P1",
          "Transfer Learning Project 1 - Coding P2",
          "Transfer Learning Project 1 - Coding P3",
          "Transfer Learning Project 1 - Coding P4",
          "Transfer Learning Project 1 - Coding P5",
          "Transfer Learning Project 2 - Coding P1",
          "Transfer Learning Project 2 - Coding P2",
          "Transfer Learning Project 2 - Coding P3"
        ],
        "AUTOENCODERS": [
          "Autoencoders intuition",
          "Autencoders Math",
          "Linear Autoencoders vs. PCA",
          "Autoencoders Applications",
          "Variational Autoencoders (VARS)",
          "Autoencoders CNN Dimensionality Review",
          "Autoencoders Project 1 - Coding P1",
          "Autoencoders Project 1 - Coding P2",
          "Autoencoders Project 1 - Coding P3",
          "Autoencoders Project 1 - Coding P4",
          "Autoencoders Project 1 - Coding P5",
          "Autoencoders Project 2 - Coding P1",
          "Autoencoders Project 2 - Coding P2"
        ],
        "DEEP DREAM": [
          "What is Deep Dream",
          "How does DeepDream Algo work",
          "Deep Dream Simpified",
          "Deep Dream Coding P1",
          "Deep Dream Coding P2",
          "Deep Dream Coding P3",
          "Deep Dream Coding P4",
          "Deep Dream Coding P5"
        ],
        "GANs": [
          "GANS intuition",
          "Discriminator and Generator Networks",
          "Let's put the Discriminator and Generator together",
          "GAN Lab",
          "GANs applications",
          "GANS Project 1 P1",
          "GANS Project 1 P2",
          "GANS Project 1 P3",
          "GANS Project 1 P4",
          "GANS Project 1 P5"
        ],
        "RECURRENT NEURAL NETWORKS (RNNs) AND LSTMs": [
          "Recurrent Neural Networks Intuition",
          "RNN Architecture",
          "What makes RNN so special",
          "RNN Math",
          "Fun with RNN",
          "Vanishing Gradient Problem",
          "Long Short Term Memory LSTM",
          "RNN Project #1 - Part #1",
          "RNN Project #1 - Part #2",
          "RNN Project #1 - Part #3",
          "RNN Project #1 - Part #4"
        ],
        "TENSORFLOW SERVING AND TENSORBOARD": [
          "TF Serving Coding Part 1",
          "TF Serving Coding Part 2",
          "TF Serving Coding Part 3",
          "Tensorboard Example 1",
          "Tensorboard Example 2",
          "Distributed Strategy"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "PC with internet connection",
        "Recommended - The Ultimate Tensorflow 2.0 Practical Course"
      ],
      "description": "Google has recently released TensorFlow 2.0 which is Google’s most powerful open source platform to build and deploy AI models in practice. Tensorflow 2.0 release is a huge win for AI developers and enthusiast since it enabled the development of super advanced AI techniques in a much easier and faster way.\nThe purpose of this course is to provide students with practical knowledge of building, training, testing and deploying Advanced Artificial Neural Networks and Deep Learning models using TensorFlow 2.0 and Google Colab. This course will cover advanced, state-of-the–art AI models implementation in TensorFlow 2.0 such as DeepDream, AutoEncoders, Generative Adversarial Networks (GANs), Transfer Learning using TensorFlow Hub, Long Short Term Memory (LSTM) Recurrent Neural Networks and many more. The applications of these advanced AI models are endless including new realistic human photographs generation, text translation, image de-noising, image compression, text-to-image translation, image segmentation, and image captioning.\nThe global AI and machine learning technology sectors are expected to grow from $1.4B to $8.8B by 2022 and it is predicted that AI tech sector will create around 2.3 million jobs by 2020. The technology is progressing at a massive scale and being adopted in almost every sector. The course provides students with practical hands-on experience in training Advanced Artificial Neural Networks using real-world dataset using TensorFlow 2.0 and Google Colab. This course covers several technique in a practical manner, the projects include but not limited to:\n\n\nDevelop, train, and test State-of-the art DeepDream algorithm to create AI-based art masterpieces!\n\n\nImplement revolutionary Generative Adversarial Networks known as GANs to generate brand new images.\n\n\nDevelop Long Short Term Memory (LSTM) networks to generate new Shakespeare-style text!\n\n\nDeploy AI models in practice using TensorFlow 2.0 Serving.\n\n\nApply Auto-Encoders to perform image compression and de-noising.\n\n\nApply transfer learning to transfer knowledge from pre-trained networks to classify new images using TensorFlow 2.0 Hub.\n\n\nThe course is targeted towards students wanting to gain a fundamental understanding of how to build, train, test and deploy advanced models in Tensorflow 2.0. Basic knowledge of programming and Artificial Neural Networks is recommended. Students who enroll in this course will master Advanced AI and Deep Learning techniques and can directly apply these skills to solve real world challenging problems.",
      "target_audience": [
        "Data Scientists who want to apply their knowledge on Real World Case Studies",
        "AI Developers",
        "AI Researchers"
      ]
    },
    {
      "title": "Artificial Intelligence A-Z 2025: Agentic AI, Gen AI, and RL",
      "url": "https://www.udemy.com/course/artificial-intelligence-az/",
      "bio": "Combine the power of Agentic AI, Generative AI, Reinforcement Learning to create powerful AI for Real-World applications",
      "objectives": [
        "Build 7 different AIs for 7 different applications",
        "Understand the theory behind Artificial Intelligence",
        "Master the State of the Art AI models",
        "Solve Real World Problems with AI",
        "Q-Learning",
        "Deep Q-Learning",
        "Deep Convolutional Q-Learning",
        "A3C (Asynchronous Advantage Actor-Critic)",
        "PPO (Proximal Policy Optimization)",
        "SAC (Soft Actor-Critic)",
        "LLMs",
        "Transformers",
        "Low-Rank Adaptation (LoRA) and Quantization (QLoRA)",
        "NLP techniques for Chatbots: Tokenization and Padding",
        "Fine-Tuning an LLM with Knowledge Augmentation",
        "As Extras: DDPG, Full World Model, Evolution Strategies & Genetic Algorithms"
      ],
      "course_content": {
        "Welcome to the course!": [
          "Get the Course Materials!",
          "How to Build Your First AI Chatbot Using AWS PartyRock | No Coding Required",
          "Prizes $$ for Learning"
        ],
        "Agentic AI": [
          "Build a Cloud-powered AI Agent for Business Assistance, completely from scratch!"
        ],
        "---------- Part 0 - Fundamentals Of Reinforcement Learning ----------": [
          "Welcome to Part 0 - Fundamentals of Reinforcement Learning"
        ],
        "Q-Learning Intuition": [
          "Deep Learning Fundamentals: Neural Networks & Activation Functions Explained",
          "How Reinforcement Learning Works: A Beginner's Guide to AI Training Methods",
          "Bellman Equation in Reinforcement Learning: A Step-by-Step Introduction",
          "From State Values to Optimal Plans: Bellman Equation in AI Decision Making",
          "Markov Decision Processes in Reinforcement Learning: A Complete Guide",
          "RL Tutorial: Optimal Policy vs Fixed Plans in AI Decision Making",
          "Living Penalty in Reinforcement Learning: Optimize AI Agent Decision Making",
          "Q-Learning in Reinforcement Learning: From V-Values to Q-Values Explained",
          "Temporal Difference in Q-Learning: A Complete Guide for Reinforcement Learning",
          "Quiz 1"
        ],
        "Q-Learning Implementation": [
          "A Q-Learning Implementation for Process Optimization"
        ],
        "---------- Part 1 - Deep Q-Learning ----------": [
          "Welcome to Part 1 - Deep Q-Learning"
        ],
        "Deep Q-Learning Intuition": [
          "Deep Learning Fundamentals: Neural Networks & Activation Functions Explained",
          "Deep Q-Learning vs Traditional Q-Learning: Key Differences Explained",
          "How Deep Q-Learning Works: Neural Networks & Reinforcement Learning Explained",
          "Experience Replay in Deep Q-Learning: How it Works & Why it Matters",
          "Q-Learning: Guide to Epsilon-Greedy & Softmax Action Selection Algorithms"
        ],
        "Deep Q-Learning Implementation": [
          "How to Train an AI Lunar Lander using Deep Q-Learning and Python",
          "Get the Codes here",
          "Step 1 - Deep Q-Learning Environment Setup: From Gmail to Lunar Lander Training",
          "Google Colab Setup: Deep Q-Learning for Lunar Lander Tutorial",
          "Step 3 - PyTorch DQN Architecture: Building the AI Brain for OpenAI Lunar Lander",
          "PyTorch Deep Q-Learning: Implementing Forward Method for Neural Nets",
          "Step 5 - Configure LunarLander-v2 Environment Parameters for DQN Training",
          "DQN Hyperparameters: Learning Rate & Replay Buffer Setup Guide (Step 6)",
          "Step 7: Implementing Experience Replay Memory in DQN with Python",
          "Step 8: DQN Push Method - Adding Experiences to Replay Memory Buffer",
          "Step 9: Coding DQN Memory Sampling - PyTorch Experience Replay Tutorial",
          "DQN Tutorial: Initialize Q-Networks, Optimizer & Replay Memory Buffer",
          "Step 11: DQN Step Method - Store & Learn from Experiences in Python",
          "Step 12: DQN Action Selection - State Processing to Policy Implementation",
          "Step 13: Deep Q-Network Training - Implementing Learn Method for RL",
          "Step 14 - Deep Q-Network Implementation: Soft Update Method for Stable Training",
          "Step 15: Creating Your First AI Agent - Deep Q-Network (DQN) Tutorial",
          "Step 16 - Epsilon-Greedy Strategy: Initializing AI Training Hyperparameters",
          "Step 17: Deep Q-Learning Training Loop - Complete Lunar Lander Guide",
          "Step 18: DQN Training Visualization - Dynamic Score Tracking Implementation",
          "Step 19: Visualizing Deep Q-Learning - AI Perfects Lunar Lander Landing",
          "ChatGPT vs Custom DQN: Comparing Deep RL Implementations"
        ],
        "---------- Part 2 - Deep Convolutional Q-Learning ----------": [
          "Welcome to Part 2 - Deep Convolutional Q-Learning"
        ],
        "Deep Convolutional Q-Learning Intuition": [
          "Deep Learning Fundamentals: Neural Networks & Activation Functions Explained",
          "Deep Convolutional Q-Learning: Build AI Agents for Game Environments",
          "Deep Q-Learning vs Eligibility Trace: AI Algorithm Comparison & Guide"
        ]
      },
      "requirements": [
        "High School Maths",
        "Basic Python knowledge"
      ],
      "description": "Welcome to Artificial Intelligence A-Z!\n\n\nLearn key AI concepts with intuition lectures to get you quickly up to speed with all things AI and practice them by building 8 different AIs:\n\n\nBuild an AI Agent with a Foundation Model (LLM) for business assistance, all powered by the Cloud.\nBuild an AI with a Q-Learning model and train it to optimize warehouse flows in a Process Optimization case study.\nBuild an AI with a Deep Q-Learning model and train it to land on the moon.\nBuild an AI with a Deep Convolutional Q-Learning model and train it to play the game of Pac-Man.\nBuild an AI with an A3C (Asynchronous Advantage Actor-Critic) model and train it to fight Kung Fu.\nBuild an AI with a PPO (Proximal Policy Optimization) model and train it for a Self-Driving Car.\nBuild an AI with a SAC (Soft Actor-Critic) model and train it for a Self-Driving Car.\nBuild an AI by fine-tuning a powerful pre-trained LLM (Llama 2 by Meta) with Hugging Face and re-train it to chat with you about medical terms. Simply put, we build here an AI Doctor Chatbot.\n\n\nBut that's not all... Once you complete the course, you will get 3 extra AIs: DDPG, Full World Model, and Evolution Strategies & Genetic Algorithms. We build these AIs with ChatGPT for a Self-Driving Car and a Humanoid application. For each of these extra AIs you will get a long video lecture explaining the implementation, a mini PDF, and the Python code.\n\n\nBesides, you will get a free 3-hour extra course on Generative AI and LLMs with Cloud Computing as a Prize for completing the course.\n\n\nAnd last but not least, here is what you will get with this course:\n\n\n1. Complete beginner to expert AI skills – Learn to code self-improving AI for a range of purposes. In fact, we code together with you. Every tutorial starts with a blank page and we write up the code from scratch. This way you can follow along and understand exactly how the code comes together and what each line means.\n2. Hassle-Free Coding and Code templates – We will build all our AIs in Google Colab, which means that we will have absolutely NO hassle installing libraries or packages because everything is already pre-installed in Google Colab notebooks. Plus, you’ll get downloadable Python code templates (in .py and .ipynb) for every AI you build in the course. This makes building truly unique AI as simple as changing a few lines of code. If you unleash your imagination, the potential is unlimited.\n3. Intuition Tutorials – Where most courses simply bombard you with dense theory and set you on your way, we believe in developing a deep understanding for not only what you’re doing, but why you’re doing it. That’s why we don’t throw complex mathematics at you, but focus on building up your intuition in AI for much better results down the line.\n4. Real-world solutions – You’ll achieve your goal in not only one AI model but in 5. Each module is comprised of varying structures and difficulties, meaning you’ll be skilled enough to build AI adaptable to any environment in real life, rather than just passing a glorified memory “test and forget” like most other courses. Practice truly does make perfect.\n5. In-course support – We’re fully committed to making this the most accessible and results-driven AI course on the planet. This requires us to be there when you need our help. That’s why we’ve put together a team of professional Data Scientists to support you in your journey, meaning you’ll get a response from us within 48 hours maximum.\n\n\nSo, are you ready to embrace the fascinating world of AI?\nCome join us, never stop learning, and enjoy AI!",
      "target_audience": [
        "Anyone interested in Artificial Intelligence, Machine Learning or Deep Learning"
      ]
    },
    {
      "title": "Machine Learning using Python Programming",
      "url": "https://www.udemy.com/course/machine-learning-using-python-programming/",
      "bio": "Learn the core concepts of Machine Learning and its algorithms and how to implement them in Python 3",
      "objectives": [
        "Machine Learning Algorithms & Terminologies",
        "Artificial Intelligence",
        "Python Libraries - Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn"
      ],
      "course_content": {
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning",
          "2. Features of Machine Learning",
          "3. Traditional Programming vs Machine Learning"
        ],
        "Types of Machine Learning": [
          "4. Difference between Supervised and Unsupervised Learning",
          "5. Algorithms in Supervised and Unsupervised Learning"
        ],
        "The Machine Learning Pipeline": [
          "6. The Machine Learning Pipeline - Data Collection",
          "7. Importance of Data Prepocessing",
          "8. Importance of Feature Selection and Feature Engineering",
          "9. The Machine Learning Terminologies",
          "10. Introduction to iPython Environment",
          "Important Libraries in Python"
        ],
        "Numpy Library": [
          "11.Creating a numpy array",
          "12. Processing the numpy arrays",
          "13. Accessing Columns from Numpy Matrices",
          "14. Statistical methods in Numpy",
          "15. Matrix Operations in Numpy",
          "16. Iterating through the numpy array"
        ],
        "Pandas Library": [
          "17. An Intuition on Pandas Dataframe and Series",
          "18. Using numpy arrays to create Pandas Series",
          "19. Using dictionary to create Pandas Series",
          "20. Using a scalar to create Pandas Series",
          "21. Series Processing",
          "22. Creating Pandas Dataframe from series",
          "23. Using lists of data to create a Pandas Dataframe",
          "24. Another approach to create Dataframes",
          "25. Directly creating a pandas dataframe from numpy arrays"
        ],
        "Analysis of Datasets using Pandas and Matplotlib Library": [
          "26. Loading the dataset (Important)",
          "27. Analysis of Datasets - I",
          "28. Analysis of Datasets by Plotting - II"
        ],
        "The Scikit-learn Library and Preprocessing Techniques": [
          "29. Working with Iris Dataset from sklearn",
          "30. Binarization",
          "31. Feature Scaling"
        ],
        "Supervised Learning - Linear Regression": [
          "32. Analysis of Linear Regression",
          "Use of Gradient Descent Optimizer",
          "The Gradient Descent Optimizer Algorithm",
          "33. Demand vs Price Problem to understand Linear Regression",
          "34. Implementation of Linear Regression - I",
          "35. Implementation of Linear Regression - II",
          "36. Visualizing the LBF using matplotlib"
        ],
        "Logistic Regression for Classification Problems": [
          "37. Why does Linear Regression fail for a classification problem?",
          "38. The Sigmoid function in Logistic Regression",
          "39. The Confusion Matrix",
          "40. Implementation of Logistic Regression - I",
          "41. Creating an heatmap of the confusion matrix"
        ],
        "Support Vector Machines": [
          "42. Understanding Support Vector Machines and Hyperplanes",
          "43. Understanding the Kernels of SVM",
          "44. Implementing Support Vector Classifiers in Python"
        ]
      },
      "requirements": [
        "Yes, A Basic Knowledge in Python is preferred"
      ],
      "description": "'Machine Learning is all about how a machine with an artificial intelligence learns like a human being'\nWelcome to the course on Machine Learning and Implementing it using Python 3. As the title says, this course recommends to have a basic knowledge in Python 3 to grasp the implementation part easily but it is not compulsory.\nThis course has strong content on the core concepts of ML such as it's features, the steps involved in building a ML Model - Data Preprocessing, Finetuning the Model, Overfitting, Underfitting, Bias, Variance, Confusion Matrix and performance measures of a ML Model. We'll understand the importance of many preprocessing techniques such as Binarization, MinMaxScaler, Standard Scaler\nWe can implement many ML Algorithms in Python using scikit-learn library in a few lines. Can't we? Yet, that won't help us to understand the algorithms. Hence, in this course, we'll first look into understanding the mathematics and concepts behind the algorithms and then, we'll implement the same in Python. We'll also visualize the algorithms in order to make it more interesting. The algorithms that we'll be discussing in this course are:\n1. Linear Regression\n2. Logistic Regression\n3. Support Vector Machines\n4. KNN Classifier\n5. KNN Regressor\n6. Decision Tree\n7. Random Forest Classifier\n8. Naive Bayes' Classifier\n9. Clustering\nAnd so on. We'll be comparing the results of all the algorithms and making a good analytical approach. What are you waiting for?",
      "target_audience": [
        "Beginner Python developers"
      ]
    },
    {
      "title": "Web Scraping in Python With BeautifulSoup and Selenium",
      "url": "https://www.udemy.com/course/web-scraping-in-python-with-beautifulsoup-and-selenium/",
      "bio": "The most up to date and project based Web Scraping course in Python using BeautifulSoup and Selenium!",
      "objectives": [
        "Understanding the fundamentals of Web Scraping",
        "Build your own web scraping projects",
        "Learn core components of two of the most powerful scraping libraries: BeautifulSoup and Selenium",
        "How to click on a button, send text to an input box, and self-scroll using Selenium",
        "Scraping data off of single page, multiple page, and infinite scrolling websites",
        "5 projects each with it's own unique challenge",
        "Automate python scripts",
        "Understand HTML and Xpath selectors"
      ],
      "course_content": {
        "Introduction": [
          "Applications Of Web Scraping",
          "Overview of Web Scraping",
          "Web Scraping Example - Twitter",
          "How to Make Money, Using Web Scraping",
          "Installing Python",
          "Installing Packages",
          "Fixing Video Quality - MUST WATCH!",
          "Asking Questions"
        ],
        "How Websites are Displayed": [
          "How Websites are Displayed",
          "Installing Google Chrome"
        ],
        "Basics of BeautifulSoup": [
          "Section Overview",
          "How To Get The HTML",
          "Tags",
          "Navigable Strings",
          "Attributes",
          "Comments",
          "Basics of BeautifulSoup Quiz"
        ],
        "Searching and Extracting From the HTML": [
          "Section Overview",
          "find()",
          "find_all() - Part 1",
          "find_all() - Part 2",
          "find_all() - Part 3",
          "Extracting Data From Nested HTML Tags",
          "Coding Exercise - Stocks",
          "Answer - Coding Exercise"
        ],
        "Project #1 - Scraping a Table": [
          "Section Overview",
          "Scraping a Table - Part 1",
          "Scraping a Table - Part 2",
          "Scraping a Table - Part 3",
          "Coding Exercise - NFL Stats",
          "Answer - Coding Exercise"
        ],
        "Project #2 - Dealing with Multiple Pages": [
          "Section Overview",
          "**WATCH THIS VIDEO**",
          "Dealing with Multiple Pages - Part 1",
          "Dealing with Multiple Pages - Part 2",
          "Dealing with Multiple Pages - Part 3",
          "Dealing with Multiple Pages - Part 4",
          "Coding Exercise - Carpages",
          "Answer - Coding Exercise",
          "Note: Answer - Coding Exercise"
        ],
        "JavaScript Driven Webpages": [
          "Section Overview",
          "JavaScript Driven Webpages",
          "Selenium",
          "Installing Selenium"
        ],
        "Selenium": [
          "Section Overview",
          "Using the Web Driver",
          "Xpath",
          "find_element()",
          "Sending Text Into an Input Box",
          "Clicking On a Button",
          "Taking a Screenshot",
          "Self-Scrolling",
          "Wait Times",
          "Selenium Quiz",
          "Coding Exercise - Imdb",
          "Answer - Coding Exercise",
          "Issue With My Downloadable Code"
        ],
        "Project #3 - Infinite Scrolling": [
          "Section Overview",
          "Infinite Scrolling - Part1",
          "Infinite Scrolling - Part 2",
          "Infinite Scrolling - Part 3",
          "Infinite Scrolling - Part 4",
          "Coding Exercise - Union Los Angeles",
          "Answer - Coding Exercise"
        ],
        "Project #4 - Twitter": [
          "Section Overview",
          "Twitter - Part 1",
          "Twitter - Part 2",
          "Twitter - Part 3",
          "Twitter - Part 4",
          "Twitter - Part 5",
          "Coding Exercise - Indeed",
          "Answer - Coding Exercise"
        ]
      },
      "requirements": [
        "Beginners knowledge of Python",
        "Internet Access"
      ],
      "description": "Web Scraping has become one of the hottest topics in the data science world, for getting access to data can make or break you.\nThis is why Fortune 500 companies like Walmart, CNN, Target, and Amazon use web scraping to get ahead and stay ahead with data.\nIt’s the original growth tool and one of their best-kept secrets\n\n\n…And it can easily be yours too.\n\n\nWelcome to Web Scraping in Python with BeautiuflSoup and Selenium!\nThe most up to date and project-oriented course out there currently.\n\n\nIn this course, you're going to learn how to scrape data off some of the most well-known websites which include:\nTwitter\nAirbnb\nNike\nGoogle\nIndeed\nNFL\nMarketWatch\nWorldometers\nIMDb\nCarpages\n\n\nAt the end of this course, you will understand the most important components of web scraping and be able to build your own web scrapers to obtain new data from any website, automate any task using web scraping, and more.\nPlus, familiarize yourself with some of the most common scraping techniques and sharpen your Python programming skills while you’re at it!\n\n\nFirst, learn the essentials of web scraping, explore the framework of a website, and get your local environment ready to take on scraping challenges with BeautifulSoup, and Selenium.\nNext, cover the basics of BeautifulSoup, utilize the requests library and LXML parser, and scale up to deploy a new scraping algorithm to scrape data from any table online, and from multiple pages.\nThird, set up Selenium to deal with JavaScript-driven webpages, and use the unique functions of Selenium to interact with pages.\nCombine the concepts of BeautifulSoup and Selenium to create the most effective scrapers to deal with some of the most challenging websites.\nFinally, learn how to make web scraping fully automatic by running your scraper at a specific time each day.\n\n\nWhat makes this course different from the others, and why you should enroll?\nFirst, this is the most updated course currently out\nSecond, this is the most project-based course you will find, where we will scrape many of the internets most well-known websites\nYou will have an in-depth step by step guide on how to become a professional web scraper.\nYou will learn how to use Selenium to scrape JavaScript websites and I can assure you, you won't find any tutorials out there that teach you how to really use Selenium like I'll be doing in this course.\nYou will learn how to create a fully automated web scraping script that runs periodically without any intervention from you.\n30 days money-back guarantee by Udemy\n\n\nSo whether you’re a data scientist, machine learning, or AI engineer who wants to access more data sources; a web developer looking to automate tasks, or a data buff with a general interest in data science and web scraping…\n\n\nThis course delivers an in-depth presentation of web scraping basics, methodologies, and approaches that you can easily apply to your own personal projects, or out there in the real world of business.\n\n\nJoin me now and let’s start scraping the web together. Enroll today.",
      "target_audience": [
        "Anyone interested in harnessing the power of data, web scraping, and data mining",
        "Data Scientists who want to take their skills to the next level",
        "Web developers looking to obtain new information or automate tasks",
        "Anyone who wants to stop spending hours manually copying data off a website, and let a computer do it for them"
      ]
    },
    {
      "title": "2025 Data Visualization in Tableau & Python (2 Courses in 1)",
      "url": "https://www.udemy.com/course/2023-data-visualization-in-tableau-python-2-courses-in-1/",
      "bio": "Learn Data Visualization with Tableau & Python from the basic to advanced level using Real-Life Projects",
      "objectives": [
        "Understanding the principles of data visualization",
        "Proficiency in Tableau",
        "Data preparation and manipulation",
        "Creating static visualizations with Matplotlib",
        "Exploratory data analysis with Seaborn",
        "Designing interactive visualizations",
        "Visual storytelling and communication",
        "Critically evaluating visualizations",
        "Project-based learning"
      ],
      "course_content": {
        "Introduction": [
          "Introduction to Data Visualization"
        ],
        "Tableau Fundamentals": [
          "Getting Started with Tableau Software",
          "Connecting to Data Sources (Downloadable Text File)",
          "Connecting to Data Sources (Downloadable EXCEL File)",
          "Creating Basic Visualizations in Tableau",
          "Formatting Sheets in Tableau"
        ],
        "Designing Interactive Visualizations with Tableau": [
          "Joining two tables and Creating Bar Chart (Downloadable Excel Data Set)",
          "Creating different types of Bar Charts",
          "Creating Line Chart",
          "Creating Area Chart",
          "Creating Circle Views and Side-by-Side Circles",
          "Creating Scatter Plots",
          "Creating Pie Chart",
          "Creating histogram",
          "Creating text tables and highlight tables",
          "Creating Maps",
          "Creating Heat Maps"
        ],
        "Project based Learning to Create Interactive Dashboards in Tableau": [
          "Creating Dashboards with Tableau using Olympic Athletes Data Set",
          "Create a Dashboard and story for World bank indicators Data Set",
          "Create a Dashboard and Story for Sales Data Set"
        ],
        "Data Visualization Using Matplotlib in Python": [
          "Introduction to Matplotlib",
          "Creating Line Graph",
          "Creating Bar Graph",
          "Creating Scatter Graph",
          "Creating Histogram Graph",
          "Creating Pie Chart",
          "Creating 3D plot",
          "Creating 3D Line graph"
        ],
        "Data Visualization Using Seaborn in Python": [
          "Introduction to Seaborn",
          "Understanding Data Set",
          "Creating Swarm Plot",
          "Creating Violin Plot",
          "Creating Facet Grids",
          "Creating Heatmaps"
        ],
        "Projects": [
          "Create a Line Plot using Matplotlib.",
          "Create a Scatter Plot using Matplotlib",
          "Create a Bar Chart using Matplotlib",
          "Create a Histogram using Matplotlib"
        ],
        "Bonus Lecture": [
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Proficiency in Python: Since Matplotlib and Seaborn are Python libraries, a solid understanding of Python programming is necessary. Students should have a good grasp of Python syntax, variables, data structures (such as lists, dictionaries, and arrays), control flow (loops and conditionals), functions, and file I/O operations.",
        "Knowledge of data manipulation: It is beneficial to have some experience with data manipulation and cleaning techniques, such as loading data from different file formats (CSV, Excel, etc.), handling missing values, and performing basic data transformations (filtering, grouping, merging, etc.). This can be achieved through libraries like pandas."
      ],
      "description": "Welcome to the comprehensive course on \"Data Visualization in Tableau & Python with Matplotlib and Seaborn.\" In this course, you will learn how to create captivating and informative visualizations using two powerful tools: Tableau and Python libraries, Matplotlib and Seaborn. Whether you're a beginner or an experienced data analyst, this course will provide you with the necessary skills to effectively visualize data and communicate insights.\nCourse Features:\nPractical Approach: This course focuses on hands-on learning through practical exercises and real-world examples. You will work on various datasets, allowing you to apply the concepts and techniques learned directly to relevant scenarios.\nComprehensive Coverage: The course covers both Tableau and Python libraries, providing you with a well-rounded understanding of data visualization. You will learn the fundamentals of each tool and progressively advance to more advanced techniques, ensuring a thorough grasp of the subject matter.\nTableau Proficiency: You will gain proficiency in Tableau, a widely used data visualization tool. Starting with the basics, you will learn to create interactive dashboards, design captivating visualizations, and explore advanced functionalities for data analysis and storytelling.\nPython Visualization: Explore the capabilities of Python libraries, Matplotlib and Seaborn, for data visualization. You will learn to create static visualizations, customize plots, handle data manipulation, and leverage advanced statistical visualization techniques.\nData Preparation and Cleaning: An essential aspect of data visualization is data preparation. This course covers techniques for data cleaning, manipulation, and transformation to ensure high-quality data for visualization purposes.\nStorytelling and Communication: Learn how to tell compelling stories through data visualization. Discover effective techniques for communicating insights visually and creating impactful narratives that engage and persuade your audience.\nReal-World Projects: Apply your skills to real-world projects and datasets, allowing you to showcase your abilities and build a portfolio of impressive visualizations. Gain practical experience and confidence in creating visualizations that address real-world challenges.\nSupport and Resources: The course provides continuous support through Q&A sessions and a dedicated community forum, where you can interact with the instructor and fellow learners. Additional resources, such as code samples, datasets, and reference materials, will be provided to supplement your learning.\nLifetime Access: Gain lifetime access to the course materials, including updates and new content. You can revisit the course anytime to refresh your knowledge, access new resources, and stay up-to-date with the latest advancements in data visualization.\nCertificate of Completion: Upon completing the course, you will receive a certificate of completion, validating your skills in data visualization with Tableau and Python libraries.\nWhether you are a data analyst, data scientist, business professional, researcher, or anyone interested in mastering data visualization, this course will equip you with the necessary tools and knowledge to create impactful visualizations that drive insights and enhance data-driven decision-making.\nEnroll now and embark on a journey to become a proficient data visualization expert with Tableau, Matplotlib, and Seaborn!",
      "target_audience": [
        "Data Analysts: Professionals working in data analysis roles who want to enhance their data visualization skills. They may already have experience with data manipulation and analysis and want to effectively communicate their findings through visually engaging charts and graphs.",
        "Data Scientists: Individuals involved in data science who wish to add data visualization to their skill set. They understand the importance of effective data communication and want to leverage visualization techniques to convey complex insights to a non-technical audience.",
        "Business Intelligence Professionals: Those responsible for collecting, analyzing, and presenting data to support decision-making in an organization. They seek to develop expertise in creating interactive dashboards and visualizations using Tableau, Matplotlib, and Seaborn.",
        "Researchers and Academics: Researchers, scientists, and academics from various domains who deal with large datasets. They want to learn how to visually explore and communicate their data effectively, enabling them to present their findings in a visually compelling manner.",
        "Data Enthusiasts: Individuals who have a strong interest in data visualization and want to explore the power of Tableau, Matplotlib, and Seaborn to create impactful visualizations. They may come from diverse backgrounds and want to acquire practical skills to showcase data in a meaningful way.",
        "Students and Aspiring Data Professionals: Students pursuing degrees or courses in data-related fields, such as data science, analytics, or business intelligence. They aim to acquire a comprehensive understanding of data visualization techniques early in their careers to excel in their future roles."
      ]
    },
    {
      "title": "Machine Learning From Basic to Advanced",
      "url": "https://www.udemy.com/course/machine-learning-course/",
      "bio": "Learn to create Machine Learning Algorithms in Python Data Science enthusiasts. Code templates included.",
      "objectives": [
        "Master Machine Learning on Python",
        "Make accurate predictions",
        "Make robust Machine Learning models",
        "Use Machine Learning for personal purpose",
        "Have a great intuition of many Machine Learning models",
        "Know which Machine Learning model to choose for each type of problem",
        "Use SciKit-Learn for Machine Learning Tasks",
        "Make predictions using linear regression, polynomial regression, and multiple regression",
        "Classify data using K-Means clustering, Support Vector Machines (SVM), KNN, Decision Trees, Naive Bayes, etc."
      ],
      "course_content": {
        "Data Preprocessing": [
          "Data Preprocessing"
        ],
        "Classification": [
          "K-Nearest Neighbour",
          "Support Vector Machine (SVM)",
          "Kernel SVM"
        ],
        "Regresssion": [
          "Decision Tree Classification",
          "Random Forest Classification",
          "Simple Linear Regression",
          "Multiple Linear Regression",
          "Polynomial Linear Regression",
          "Decision Tree Regression",
          "Random Forest Regression",
          "Logistic Regression"
        ],
        "Bonus Material": [
          "Support Vector Regression(SVR)"
        ],
        "Clustering": [
          "K-Means Clustering",
          "Hierarchical Clustering"
        ]
      },
      "requirements": [
        "Some basic python programming experience.",
        "Basic understanding of python libraries like numpy, pasdas and matplotlib.",
        "Some high school mathematics."
      ],
      "description": "Are you ready to start your path to becoming a Machine Learning Engineer!\nThis comprehensive course will be your guide to learning how to use the power of Python to analyze data, create beautiful visualizations, and use powerful machine learning algorithms!\nData Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems!\nThis course is designed for both beginners with some programming experience or experienced developers looking to make the jump to Machine Learning as well as Data Scientist!\nInterested in the field of Machine Learning? Then this course is for you!\nThis course has been designed by Code Warriors the ML Enthusiasts so that we can share our knowledge and help you learn complex theories, algorithms, and coding libraries in a simple way.\nWe will walk you step-by-step into the World of Machine Learning. With every tutorial, you will develop new skills and improve your understanding of this challenging yet lucrative sub-field of Data Science.\nThis course is fun and exciting, but at the same time, we dive deep into Machine Learning. It is structured the following way:\nPart 1 - Data Preprocessing\nPart 2 - Regression: Simple Linear Regression, Multiple Linear Regression, Polynomial Regression, SVR, Decision Tree Regression, Random Forest Regression.\nPart 3 - Classification: Logistic Regression, K-NN, SVM, Kernel SVM, Naive Bayes, Decision Tree Classification, Random Forest Classification\nPart 4 - Clustering: K-Means, Hierarchical Clustering.\nAnd as a bonus, this course includes Python code templates which you can download and use on your own projects.",
      "target_audience": [
        "Anyone interested in Machine Learning.",
        "Students who have at least high school knowledge in math and who want to start learning Machine Learning.",
        "Any intermediate level people who know the basics of machine learning, including the classical algorithms like linear regression or logistic regression, but who want to learn more about it and explore all the different fields of Machine Learning.",
        "Any people who are not that comfortable with coding but who are interested in Machine Learning and want to apply it easily on datasets.",
        "Any students in college who want to start a career in Data Science.",
        "Any people who want to create added value to their business by using powerful Machine Learning tools."
      ]
    },
    {
      "title": "Apache Spark 3 - Real-time Stream Processing using Scala",
      "url": "https://www.udemy.com/course/apache-spark-streaming-in-scala/",
      "bio": "Learn to create Real-time Stream Processing applications using Apache Spark",
      "objectives": [
        "Real-time Stream Processing Concepts",
        "Spark Structured Streaming APIs and Architecture",
        "Working with File Streams",
        "Working With Kafka Source and Integrating Spark with Kafka",
        "State-less and State-full Streaming Transformations",
        "Windowing Aggregates using Spark Stream",
        "Watermarking and State Cleanup",
        "Streaming Joins and Aggregation",
        "Handling Memory Problems with Streaming Joins",
        "Creating Arbitrary Streaming Sinks"
      ],
      "course_content": {
        "Before you start": [
          "About the Course",
          "Course Prerequisite",
          "Source Code and Other Resources"
        ],
        "Setup your Environment": [
          "Spark Development Environment",
          "Spark Installation Prerequisites",
          "Installing Apache Spark",
          "Setup and test your IDE",
          "Install and run Apache Kafka"
        ],
        "Getting started with Spark Structured Streaming": [
          "Introduction to Stream Processing",
          "Spark Streaming APIs - Dstream Vs Structured Streaming",
          "Creating your first stream processing application",
          "Stream processing model in Spark",
          "Working with Files and Directories",
          "Streaming Sources, Sinks and Output Mode",
          "Fault Tolerance and Restarts"
        ],
        "Spark Streaming with Kafka": [
          "Streaming from Kafka Source",
          "Working with Kafka Sinks",
          "Multi-query Streams Application",
          "Kafka Serialization and Deserialization for Spark",
          "Creating Kafka AVRO Sinks",
          "Working with Kafka AVRO Source"
        ],
        "Windowing and Aggregates": [
          "Stateless Vs Statefull transformations",
          "Event time and Windowing",
          "Tumbling Window aggregate",
          "Watermarking your windows",
          "Watermark and output modes",
          "Sliding Window"
        ],
        "Stream Processing and Joins": [
          "Joining Stream to static source",
          "Joining Stream to another Stream",
          "Streaming Watermark",
          "Streaming Outer Joins"
        ],
        "Keep Learning": [
          "Final Word",
          "Bonus Lecture : Get Extra"
        ]
      },
      "requirements": [
        "Spark Fundamentals and exposure to Spark Dataframe APIs",
        "Kafka Fundamentals and working knowledge of Apache Kafka",
        "Programming Knowledge Using Scala Programming Language",
        "A Recent 64-bit Windows/Mac/Linux Machine with 8 GB RAM"
      ],
      "description": "About the Course\nI am creating Apache Spark 3 - Real-time Stream Processing using the Scala course to help you understand the Real-time Stream processing using Apache Spark and apply that knowledge to build real-time stream processing solutions. This course is example-driven and follows a working session like approach. We will be taking a live coding approach and explain all the needed concepts along the way.\nWho should take this Course?\nI designed this course for software engineers willing to develop a Real-time Stream Processing Pipeline and application using the Apache Spark. I am also creating this course for data architects and data engineers who are responsible for designing and building the organization’s data-centric infrastructure. Another group of people is the managers and architects who do not directly work with Spark implementation. Still, they work with the people who implement Apache Spark at the ground level.\nSpark Version used in the Course\nThis Course is using the Apache Spark 3.x. I have tested all the source code and examples used in this Course on Apache Spark 3.0.0 open-source distribution.",
      "target_audience": [
        "Software Engineers and Architects who are willing to design and develop a Bigdata Engineering Projects using Apache Spark",
        "Programmers and developers who are aspiring to grow and learn Data Engineering using Apache Spark"
      ]
    },
    {
      "title": "YOLOv8 & YOLO11: Custom Object Detection & Web Apps 2025",
      "url": "https://www.udemy.com/course/yolov8-the-ultimate-course-for-object-detection-tracking/",
      "bio": "Learn Custom Object Detection, Segmentation, Tracking, Pose Estimation & 17+ Projects with Web Apps in Python",
      "objectives": [
        "Introduction to YOLO",
        "Fundamentals of YOLOv8",
        "How to run a YOLOv8 program for Object Detection, Segmentation, Classification",
        "How to find the appropriate dataset",
        "Data annotation/labeling/ Automatic Dataset Splitting",
        "Train / Fine-Tune YOLOv8 Object Detection Model on Custom Dataset",
        "Visualize the Training Performances",
        "Multi-Object Tracking using YOLOv8",
        "Instance Segmentation using YOLOv8",
        "Train/ Fine-Tune YOLOv8 Instance Segmentation Model on a Custom Dataset",
        "YOLOv8 Instance Segmentation with Multiple Object Tracking",
        "Object Detection in the Browser using YOLOv8 and Flask",
        "Potholes Detection using YOLOv8",
        "Personal Protective Equipment Detection using YOLOv8",
        "Pen and Book Detection using YOLOv8",
        "Traffic Counting using YOLOv8 and DeepSORT Object Tracking",
        "Car Velocity Calculation + Traffic Counting",
        "Vehicles Counting (Entering and Leaving) using YOLOv8 & DeepSORT Object Tracking",
        "Car Velocity Calculation + Vehicles Counting (Entering and Leaving)",
        "Potholes Detection and Segmentation using YOLOv8",
        "Traffic Lights Detection and Color Recognition using YOLOv8",
        "Cracks Segmentation using YOLOv8.",
        "Helmet Detection and Segmentation using YOLOv8",
        "Traffic Watch: Automated Vehicle Direction Detection and Counting using YOLOv8",
        "License Plate Detection and Recognition using YOLOv8",
        "Face Detection, Gender Classification, Counting, Tracking, Alert and Analytics",
        "Object Blurring using YOLOv8 with Object Tracking",
        "Vehicles Segmentation and Tracking with Vehicles Counting and Speed Estimation",
        "Personal Protective Equipment (PPE) Detection Web App",
        "17 + Projects with YOLOv8: Unleashing the Power of YOLOv8",
        "Object Detection, Instance Segmentation, Pose Estimation and Image Classification using YOLO11",
        "Training / Fine-Tuning YOLO11 Models on Custom Dataset",
        "Multi-Object Tracking with Ultralytics YOLO",
        "Develop Streamlit Application for Object Detection with YOLO11.",
        "Car and License Plate Detection & Recognition with YOLO11 and PaddleOCR"
      ],
      "course_content": {
        "YOLOv8 Introduction": [
          "Introduction to YOLO",
          "Overview of CNN, RCNN, Fast RCNN, Faster RCNN, Mask R-CNN",
          "Introduction to YOLOv8",
          "Comparison of YOLOv8 and YOLOv7 considering the License Plate Detection Problem"
        ],
        "YOLOv8 Implementation": [
          "Running YOLOv8 on Windows",
          "Running YOLOv8 in Google Colab"
        ],
        "Training Custom YOLOv8": [
          "Potholes Detection Part1",
          "Potholes Detection Part2",
          "Personal Protective Equipment Detection| Google Colab| Part 1",
          "PPE Detection Part 2| Live Webcam Testing",
          "Pen and Book Detection Part 1",
          "Pen and Book Detection Part2",
          "Pen and Book Detection Part3"
        ],
        "YOLOv8 Object Tracking": [
          "Introduction to Multi-Object Tracking",
          "Implementing YOLOv8 with Multi-Object Tracker DeepSORT Algorithm",
          "Implementing YOLOv8 with Multi-Object Tracker DeepSORT Algorithm | Google Colab",
          "Traffic Counting using YOLOv8 and DeepSORT Object Tracking",
          "Car Velocity Calculation + Traffic Counting",
          "Vehicles Counting (Entering and Leaving) using YOLOv8 & DeepSORT Object Tracking",
          "Vehicles Counting Entering and Leaving Part1",
          "Vehicles Counting Entering and Leaving Part2",
          "Vehicles Counting Entering and Leaving with Speed Estimation"
        ],
        "YOLOv8 Object Segmentation and Tracking": [
          "YOLOv8 Segmentation",
          "Run YOLOv8 Segmentation on Google Colab",
          "YOLOv8 Segmentation with Tracking"
        ],
        "Training YOLOv8 Segmentation Model on Custom Dataset": [
          "YOLOv8 Segmentation on Custom Dataset | Potholes Segmentation | Part 1",
          "YOLOv8 Segmentation on Custom Dataset | Potholes Segmentation | Part 2",
          "YOLOv8 Segmentation on Custom Dataset | Potholes Segmentation | Part 3"
        ],
        "YOLOv8 Apps": [
          "Traffic Lights Detection and Color Recognition using YOLOv8 | Custom Dataset",
          "Cracks Segmentation using YOLOv8.",
          "Helmet Detection and Segmentation using YOLOv8",
          "Traffic Watch: Automated Vehicle Direction Detection and Counting using YOLOv8",
          "License Plate Detection and Recognition using YOLOv8",
          "Face Detection, Gender Classification, Counting, Tracking, Alert and Analytics",
          "Face Detection, Gender Classification, Counting, Tracking, Alert and Analytics",
          "Object Blurring using YOLOv8 with Object Tracking",
          "Vehicles Segmentation and Tracking with Vehicles Counting and Speed Estimation"
        ],
        "YOLOv8 WebApp Development": [
          "Installations",
          "Running YOLOv8",
          "YOLOv8 with Webcam",
          "YOLOv8 with Video",
          "Integrating YOLOv8 with Flask",
          "Testing on Live Webcam",
          "Creating a Complete Frontend WebApp",
          "Training YOLOv8 Custom Dataset",
          "App- Personal Protective Equipment (PPE) Detection"
        ],
        "YOLO11: New Object Detection Model": [
          "What's New in YOLO11?"
        ],
        "YOLO11 Implementation | Google Colab": [
          "Object Detection, Instance Segmentation, Pose Estimation & Image Classification"
        ]
      },
      "requirements": [
        "Mac / Windows / Linux - all operating systems work with this course!"
      ],
      "description": "YOLOv8 and YOLO11: Cutting-Edge Object Detection Models\nYOLOv8 and YOLO11 are the latest state-of-the-art object detection models from Ultralytics, surpassing previous versions in both speed and accuracy. These models build upon the advancements of earlier YOLO versions, introducing significant architectural and training improvements, making them versatile tools for a variety of computer vision tasks.\nThe YOLOv8 and YOLO11 models support a wide range of applications, including object detection, instance segmentation, image classification, pose estimation, and oriented object detection (OBB).\nCourse Structure\nThis course is divided into two parts:\nPart 1: YOLOv8\nIntroduction to YOLO\nOverview of CNN, RCNN, Fast RCNN, Faster RCNN, Mask R-CNN\nIntroduction to YOLOv8\nComparison of YOLOv8 and YOLOv7 with a focus on License Plate Detection\nRunning YOLOv8\nSetting up YOLOv8 on Windows\nUsing YOLOv8 in Google Colab\nDataset Preparation\nHow to find datasets\nData annotation, labeling, and automatic dataset splitting\nTraining YOLOv8\nTrain/ Fine-Tune YOLOv8 Model on a Custom Dataset\nCustom Projects:\nPotholes Detection\nPersonal Protective Equipment (PPE) Detection\nPen and Book Detection\nMulti-Object Tracking\nIntroduction to Multi-Object Tracking\nImplementing YOLOv8 with the DeepSORT algorithm\nRunning on Google Colab\nTraffic Analysis:\nVehicle counting and car velocity calculation\nVehicle entry and exit counting\nYOLOv8 Segmentation with Tracking\nAdvanced Applications\nPotholes Segmentation\nTraffic Lights Detection and Color Recognition\nCracks Segmentation\nHelmet Detection and Segmentation\nAutomated Vehicle Direction Detection and Counting\nFace Detection, Gender Classification, Crowd Counting, and Tracking\nLicense Plate Detection and Recognition with EasyOCR\nObject Blurring with Object Tracking\nVehicle Segmentation, Counting, and Speed Estimation\nWeb Integration\nIntegrating YOLOv8 with Flask\nCreating a Complete Web App for PPE Detection\nPart 2: YOLO11\nWhat's New in YOLO11\nKey updates and features in Ultralytics YOLO11\nUsing YOLO11 for Various Tasks\nObject Detection, Instance Segmentation, Pose Estimation, and Image Classification on Windows/Linux.\nModel Performance Evaluation\nTesting and analyzing YOLO11 performance\nTraining YOLO11\nTrain/ Fine-Tune YOLO11 Object Detection Model on a Custom Dataset for Personal Protective Equipment Detection.\nTrain/ Fine-Tune YOLO11 Instance Segmentation Model on a Custom Dataset for Potholes Detection.\nFine-Tune YOLO11 Pose Estimation Model on a Custom Dataset for Human Activity Recognition.\nTrain/ Fine-Tune YOLO11 Image Classification Model on a Custom Dataset for Plant Classification.\nAdvanced Multi-Object Tracking\nImplementing Multi-Object tracking with Bot-SORT and ByteTrack algorithms\nSpecialized Projects\nLicense Plate Detection & Recognition with YOLO11 and EasyOCR\nCar and License Plate Detection & Recognition with YOLO11 and PaddleOCR\nWeb Integration with YOLO11\nIntegrating YOLO11 with Flask to build a web app\nCreating a Streamlit Web App for object detection",
      "target_audience": [
        "Anyone who is interested in Computer Vision",
        "Anyone who study Computer Vision and want to know how to use YOLO for Object Detection, Instance Segmentation, Pose Estimation and Image Classification",
        "Anyone who aims to build Deep learning Apps with Computer Vision"
      ]
    },
    {
      "title": "R Programming: Advanced Analytics In R For Data Science",
      "url": "https://www.udemy.com/course/r-analytics/",
      "bio": "Take Your R & R Studio Skills To The Next Level. Data Analytics, Data Science, Statistical Analysis in Business, GGPlot2",
      "objectives": [
        "Perform Data Preparation in R",
        "Identify missing records in dataframes",
        "Locate missing data in your dataframes",
        "Apply the Median Imputation method to replace missing records",
        "Apply the Factual Analysis method to replace missing records",
        "Understand how to use the which() function",
        "Know how to reset the dataframe index",
        "Work with the gsub() and sub() functions for replacing strings",
        "Explain why NA is a third type of logical constant",
        "Deal with date-times in R",
        "Convert date-times into POSIXct time format",
        "Create, use, append, modify, rename, access and subset Lists in R",
        "Understand when to use [] and when to use [[]] or the $ sign when working with Lists",
        "Create a timeseries plot in R",
        "Understand how the Apply family of functions works",
        "Recreate an apply statement with a for() loop",
        "Use apply() when working with matrices",
        "Use lapply() and sapply() when working with lists and vectors",
        "Add your own functions into apply statements",
        "Nest apply(), lapply() and sapply() functions within each other",
        "Use the which.max() and which.min() functions"
      ],
      "course_content": {
        "Welcome To The Course": [
          "Welcome to the Advanced R Programming Course!",
          "Learning Paths",
          "Extra: Interview with Hadley Wickham",
          "Get the materials",
          "Prizes $$ for Learning"
        ],
        "Data Preparation": [
          "Welcome to this section. This is what you will learn!",
          "Project Brief: Financial Review",
          "Import Data into R",
          "What are Factors (Refresher)",
          "The Factor Variable Trap",
          "FVT Example",
          "gsub() and sub()",
          "Dealing with Missing Data",
          "What is an NA?",
          "An Elegant Way To Locate Missing Data",
          "Data Filters: which() for Non-Missing Data",
          "Data Filters: is.na() for Missing Data",
          "Removing records with missing data",
          "Reseting the dataframe index",
          "Replacing Missing Data: Factual Analysis Method",
          "Replacing Missing Data: Median Imputation Method (Part 1)",
          "Replacing Missing Data: Median Imputation Method (Part 2)",
          "Replacing Missing Data: Median Imputation Method (Part 3)",
          "Replacing Missing Data: Deriving Values Method",
          "Visualizing results",
          "Section Recap",
          "Data Preparation"
        ],
        "Lists in R": [
          "Welcome to this section. This is what you will learn!",
          "Project Brief: Machine Utilization",
          "Import Data Into R",
          "Handling Date-Times in R",
          "R programming: What is a List?",
          "Naming components of a list",
          "Extracting components of lists: [] vs [[]] vs $",
          "Adding and deleting components",
          "Subsetting a list",
          "Creating a Timeseries Plot",
          "Section Recap",
          "Lists in R"
        ],
        "\"Apply\" Family of Functions": [
          "Welcome to this section. This is what you will learn!",
          "Project Brief: Weather Patterns",
          "Import Data into R",
          "R programming: What is the Apply family?",
          "Using apply()",
          "Recreating the apply function with loops (advanced topic)",
          "Using lapply()",
          "Combining lapply() with []",
          "Adding your own functions",
          "Using sapply()",
          "Nesting apply() functions",
          "which.max() and which.min() (advanced topic)",
          "Section Recap",
          "\"Apply\" Family of Functions",
          "THANK YOU Video"
        ],
        "Congratulations!! Don't forget your Prize :)": [
          "Huge Congrats for completing the challenge!",
          "Bonus Lecture"
        ]
      },
      "requirements": [
        "Basic knowledge of R",
        "Knowledge of the GGPlot2 package is recommended",
        "Knowledge of dataframes",
        "Knowledge of vectors and vectorized operations"
      ],
      "description": "Ready to take your R Programming skills to the next level?\nWant to truly become proficient at Data Science and Analytics with R?\nThis course is for you!\nProfessional R Video training, unique datasets designed with years of industry experience in mind, engaging exercises that are both fun and also give you a taste for Analytics of the REAL WORLD.\nIn this course, you will learn:\nHow to prepare data for analysis in R\nHow to perform the median imputation method in R\nHow to work with date-times in R\nWhat Lists are and how to use them\nWhat the Apply family of functions is\nHow to use apply(), lapply() and sapply() instead of loops\nHow to nest your own functions within apply-type functions\nHow to nest apply(), lapply() and sapply() functions within each other\nAnd much, much more!\nThe more you learn, the better you will get. After every module, you will have a robust set of skills to take with you into your Data Science career.\n\n\nWe prepared real-life case studies.\nIn the first section, you will be working with financial data, cleaning it up, and preparing for analysis. You were asked to create charts showing revenue, expenses, and profit for various industries.\nIn the second section, you will be helping Coal Terminal understand what machines are underutilized by preparing various data analysis tasks.\nIn the third section, you are heading to the meteorology bureau. They want to understand better weather patterns and requested your assistance on that.",
      "target_audience": [
        "Anybody who has basic R knowledge and would like to take their skills to the next level",
        "Anybody who has already completed the R Programming A-Z course",
        "This course is NOT for complete beginners in R"
      ]
    },
    {
      "title": "PySpark - Apache Spark Programming in Python for beginners",
      "url": "https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/",
      "bio": "Master Apache Spark Programming in Python (PySpark) Using Free Databricks Community for Beginners with Capstone Project",
      "objectives": [
        "Apache Spark Foundation and Spark Architecture",
        "Data Engineering and Data Processing in Spark",
        "Working with Data Sources and Sinks",
        "Working with Data Frames and Spark SQL",
        "Using PyCharm IDE for Spark Development and Debugging",
        "Unit Testing, Managing Application Logs and Cluster Deployment"
      ],
      "course_content": {
        "Understanding Big Data and Data Lake": [
          "Section Overview",
          "What is Big Data and How it Started",
          "Hadoop Architecture, History, and Evolution",
          "What is Data Lake and How it works",
          "Introducing Apache Spark and Databricks Cloud"
        ],
        "Installing and Using Apache Spark": [
          "Section Overview",
          "Spark Development Environments",
          "How to create a Databricks Free Account",
          "Setup your Databricks Community Cloud Environment",
          "Introduction to Databricks Workspace",
          "Create your First Spark Application in Databricks Cloud",
          "Setup your Local Development IDE",
          "Mac Users - Setup your Local Development IDE",
          "Create your First Spark Application using IDE",
          "Source Code and Other Resources"
        ],
        "Getting Started with Apache Spark": [
          "Micro Project - Problem Statement",
          "Introduction to Spark Data Frames",
          "Creating Spark Dataframe",
          "Creating Spark Tables",
          "Common problem with Databricks Community",
          "Working with Spark SQL",
          "Dataframe Transformations and Actions",
          "Applying Transformations",
          "Querying Spark Dataframe",
          "More Dataframe Transformations"
        ],
        "Spark Execution Model and Architecture": [
          "Execution Methods - How to Run Spark Programs?",
          "Check your knowledge",
          "Spark Distributed Processing Model - How your program runs?",
          "Spark Execution Modes and Cluster Managers",
          "Check your knowledge",
          "Summarizing Spark Execution Models - When to use What?",
          "Working with PySpark Shell - Demo",
          "Installing Multi-Node Spark Cluster - Demo",
          "Working with Notebooks in Cluster - Demo",
          "Working with Spark Submit - Demo",
          "Section Summary",
          "Check your knowledge"
        ],
        "Spark Programming Model and Developer Experience": [
          "Creating Spark Project Build Configuration",
          "Configuring Spark Project Application Logs",
          "Check your knowledge",
          "Creating Spark Session",
          "Check your knowledge",
          "Configuring Spark Session",
          "Data Frame Introduction",
          "Data Frame Partitions and Executors",
          "Spark Transformations and Actions",
          "Spark Jobs Stages and Task",
          "Understanding your Execution Plan",
          "Unit Testing Spark Application",
          "Rounding off Summary"
        ],
        "Spark Structured API Foundation": [
          "Introduction to Spark APIs",
          "Introduction to Spark RDD API",
          "Working with Spark SQL",
          "Spark SQL Engine and Catalyst Optimizer",
          "Section Summary"
        ],
        "Spark Data Sources and Sinks": [
          "Spark Data Sources and Sinks",
          "Spark DataFrameReader API",
          "Reading CSV, JSON and Parquet files",
          "Creating Spark DataFrame Schema",
          "Spark DataFrameWriter API",
          "Writing Your Data and Managing Layout",
          "Spark Databases and Tables",
          "Working with Spark SQL Tables"
        ],
        "Spark Dataframe and Dataset Transformations": [
          "Introduction to Data Transformation",
          "Working with Dataframe Rows",
          "DataFrame Rows and Unit Testing",
          "Dataframe Rows and Unstructured data",
          "Working with Dataframe Columns",
          "Creating and Using UDF",
          "Misc Transformations"
        ],
        "Aggregations in Apache Spark": [
          "Aggregating Dataframes",
          "Grouping Aggregations",
          "Windowing Aggregations"
        ],
        "Spark Dataframe Joins": [
          "Dataframe Joins and column name ambiguity",
          "Outer Joins in Dataframe",
          "Internals of Spark Join and shuffle",
          "Optimizing your joins",
          "Implementing Bucket Joins"
        ]
      },
      "requirements": [
        "Programming Knowledge Using Python Programming Language",
        "A Recent 64-bit Windows/Mac/Linux Machine with 8 GB RAM"
      ],
      "description": "This course does not require any prior knowledge of Apache Spark or Hadoop. We have taken enough care to explain Spark Architecture and fundamental concepts to help you come up to speed and grasp the content of this course.\n\n\nAbout the Course\nI am creating PySpark - Apache Spark Programming in Python for beginners course to help you understand Spark programming and apply that knowledge to build data engineering solutions. This course is example-driven and follows a working session-like approach. We will be taking a live coding approach and explaining all the needed concepts along the way.\nWho should take this Course?\nI designed this course for software engineers willing to develop a Data Engineering pipeline and application using Apache Spark. I am also creating this course for data architects and data engineers who are responsible for designing and building the organization’s data-centric infrastructure. Another group of people is the managers and architects who do not directly work with Spark implementation. Still, they work with the people who implement Apache Spark at the ground level.\nSpark Version used in the Course\nThis Course is using the Apache Spark 3.5. I have tested all the source code and examples used in this Course on Apache Spark 3.5 in the Databricks environment.",
      "target_audience": [
        "Software Engineers and Architects who are willing to design and develop a Bigdata Engineering Projects using Apache Spark",
        "Programmers and developers who are aspiring to grow and learn Data Engineering using Apache Spark"
      ]
    },
    {
      "title": "Deep Learning and Computer Vision A-Z + Prizes",
      "url": "https://www.udemy.com/course/computer-vision-a-z/",
      "bio": "Become a Wizard of all the latest Computer Vision tools that exist out there. Detect anything and create powerful apps.",
      "objectives": [
        "Have a toolbox of the most powerful Computer Vision models",
        "Understand the theory behind Computer Vision",
        "Master OpenCV",
        "Master Object Detection",
        "Master Facial Recognition",
        "Create powerful Computer Vision applications"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Course!",
          "Recommended Workshops before we dive in!",
          "Learning Paths",
          "Get the materials",
          "Some Additional Resources!!",
          "This PDF resource will help you a lot!",
          "Prizes $$ for Learning"
        ],
        "Module 1 - Face Detection Intuition": [
          "Plan of attack",
          "Viola-Jones Algorithm",
          "Haar-like Features",
          "Integral Image",
          "Training Classifiers",
          "Adaptive Boosting (Adaboost)",
          "Cascading",
          "Face Detection Intuition"
        ],
        "Module 1 - Face Detection with OpenCV": [
          "Welcome to the Practical Applications",
          "Installations Instructions (once and for all!)",
          "Common Debug Tips",
          "Face Detection - Step 1",
          "Face Detection - Step 2",
          "Face Detection - Step 3",
          "Face Detection - Step 4",
          "Face Detection - Step 5",
          "Face Detection - Step 6",
          "Face Detection with OpenCV"
        ],
        "Homework Challenge - Build a Happiness Detector": [
          "Homework Challenge - Instructions",
          "Homework Challenge - Solution (Video)",
          "Homework Challenge - Solution (Code files)"
        ],
        "Module 2 - Object Detection Intuition": [
          "Plan of attack",
          "How SSD is different",
          "The Multi-Box Concept",
          "Predicting Object Positions",
          "The Scale Problem",
          "Object Detection Intuition"
        ],
        "Module 2 - Object Detection with SSD": [
          "Object Detection - Step 1",
          "Object Detection - Step 2",
          "Object Detection - Step 3",
          "Object Detection - Step 4",
          "Object Detection - Step 5",
          "Object Detection - Step 6",
          "Object Detection - Step 7",
          "Object Detection - Step 8",
          "Object Detection - Step 9",
          "Object Detection - Step 10",
          "Training the SSD",
          "Object Detection with SSD"
        ],
        "Homework Challenge - Detect Epic Horses galloping in Monument Valley": [
          "Homework Challenge - Instructions",
          "Homework Challenge - Solution (Video)",
          "Homework Challenge - Solution (Code files)"
        ],
        "Module 3 - Generative Adversarial Networks (GANs) Intuition": [
          "Plan of Attack",
          "The Idea Behind GANs",
          "How Do GANs Work? (Step 1)",
          "How Do GANs Work? (Step 2)",
          "How Do GANs Work? (Step 3)",
          "Applications of GANs",
          "Generative Adversarial Networks (GANs) Intuition"
        ],
        "Module 3 - Image Creation with GANs": [
          "GANs - Step 1",
          "GANs - Step 2",
          "GANs - Step 3",
          "GANs - Step 4",
          "GANs - Step 5",
          "GANs - Step 6",
          "GANs - Step 7",
          "GANs - Step 8",
          "GANs - Step 9",
          "GANs - Step 10",
          "GANs - Step 11",
          "GANs - Step 12",
          "Image Creation with GANs",
          "Special Thanks to Alexis Jacq",
          "THANK YOU Video"
        ],
        "Annex 1: Artificial Neural Networks": [
          "What is Deep Learning?",
          "Plan of Attack",
          "The Neuron",
          "The Activation Function",
          "How do Neural Networks work?",
          "How do Neural Networks learn?",
          "Gradient Descent",
          "Stochastic Gradient Descent",
          "Backpropagation"
        ]
      },
      "requirements": [
        "Only High School Maths",
        "Basic Python programming knowledge"
      ],
      "description": "You've definitely heard of AI and Deep Learning. But when you ask yourself, what is my position with respect to this new industrial revolution, that might lead you to another fundamental question: am I a consumer or a creator? For most people nowadays, the answer would be, a consumer.\nBut what if you could also become a creator?\nWhat if there was a way for you to easily break into the World of Artificial Intelligence and build amazing applications which leverage the latest technology to make the World a better place?\nSounds too good to be true, doesn't it?\nBut there actually is a way..\nComputer Vision is by far the easiest way of becoming a creator.\nAnd it's not only the easiest way, it's also the branch of AI where there is the most to create.\nWhy? You'll ask.\nThat's because Computer Vision is applied everywhere. From health to retail to entertainment - the list goes on. Computer Vision is already a $18 Billion market and is growing exponentially.\nJust think of tumor detection in patient MRI brain scans. How many more lives are saved every day simply because a computer can analyze 10,000x more images than a human?\nAnd what if you find an industry where Computer Vision is not yet applied? Then all the better! That means there's a business opportunity which you can take advantage of.\nSo now that raises the question: how do you break into the World of Computer Vision?\nUp until now, computer vision has for the most part been a maze. A growing maze.\nAs the number of codes, libraries and tools in CV grows, it becomes harder and harder to not get lost.\nOn top of that, not only do you need to know how to use it - you also need to know how it works to maximise the advantage of using Computer Vision.\nTo this problem we want to bring...\nComputer Vision A-Z.\nWith this new course you will not only learn how the most popular computer vision methods work, but you will also learn to apply them in practice!\nCan't wait to see you inside the class,\nKirill & Hadelin",
      "target_audience": [
        "Anyone interested in Computer Vision or Artificial Intelligence"
      ]
    },
    {
      "title": "Machine Learning Applied to Stock & Crypto Trading - Python",
      "url": "https://www.udemy.com/course/machine-learning-applied-to-stock-crypto-trading-python/",
      "bio": "Use Unsupervised, Supervised and Reinforcement Learning techniques to gain an edge in trading Stocks, Crypto, Forex...",
      "objectives": [
        "Understand hidden states and regimes for any market or asset using Hidden Markov Models",
        "Discover optimum assets for pairs trading in ETF's, Stocks, Forex or Crypto using K-Means Clustering",
        "Condense information from a vast array of indicators with PCA",
        "Make objective future predictions on financial data with XGBOOST",
        "Train an AI Reinforcement Learning agent to trade stocks with PPO",
        "Test for market efficiency on any given asset",
        "Become familiar with Python Libraries including Pandas, PyTorch (for deep learning) and sklearn"
      ],
      "course_content": {
        "Introduction": [
          "Welcome and Course Introduction",
          "Where to Ask Questions",
          "Resources Folder Overview",
          "Plan of Attack - Course Structure"
        ],
        "Resources and Disclaimer": [
          "Resources and Disclaimer",
          "Updated Resources (2023)",
          "2024 Update: StratManager"
        ],
        "Primer Theory": [
          "What is Machine Learning?",
          "A Brief Overview of Machine Learning",
          "Stage 1 - Data Ingestion",
          "Stage 2 - Feature Engineering",
          "Stage 3 - Model Selection and Training",
          "Stage 4 - Performance Evaluation",
          "Stage 5 - Model Deployment"
        ],
        "Environment Setup and Data Retrieval": [
          "Option 1 - Google Colab",
          "Option 1 - Google Colab Reading Existing Notebooks",
          "Option 1 - Google Colab Solving for Pandas Datareader (with YFinance)",
          "Option 2 - Notebooks Installing Python and Anaconda",
          "Option 2 - Notebooks Creating a Conda Environment",
          "Where to Get Data",
          "Bonus: Getting Poloniex and Binance Data"
        ],
        "Primer Practical": [
          "Python 101 - Variables and Arrays",
          "Python 101 - Dictionaries",
          "Python 101 - If Statements and Loops",
          "Python 101 - Functions and Classes",
          "Pandas 101 - Retrieve Data and Calculate Returns",
          "Pandas 101 - Structure Conditions and Iterations",
          "Pandas 101 - Value Extraction, Multiple Adj, Save and Load",
          "Backtesting 101 - Calculations and Strategy Returns",
          "Backtesting 101 - Metrics and Equity Curve",
          "Feature Engineering 101 - Data Preprocessing Part I",
          "Feature Engineering 101 - Data Preprocessing Part II",
          "Feature Engineering 101 - Applied Machine Learning",
          "Statistics - Testing for Market Efficiency Code Walkthrough"
        ],
        "Unsupervised Machine Learning - Hidden Markov Models": [
          "Theory - Unsupervised Machine Learning Introduction",
          "Theory - Hidden Markov Models Intuition",
          "HMM - Initial Data Structuring",
          "HMM - Model Training",
          "HMM - Viewing Hidden States",
          "HMM II - Data Structuring",
          "HMM lI - Model Predictions",
          "HMM II - Structuring Backtest",
          "HMM II - Initial Metrics",
          "HMM II - Making Use of Hidden States",
          "HMM II - Saving Outputs"
        ],
        "Unsupervised Machine Learning - K-Means Clustering": [
          "Theory - K-Means Clustering Intuition",
          "K-Means Setup",
          "K-Means Data Extraction",
          "K-Means Feature Engineering",
          "K-Means Applied and Visualized",
          "K-Means Removing Outliers",
          "Pairs Trading - Calculating Cointegrated Pairs",
          "K-Means - (Optional) - Visualizing TSNE Plot",
          "Pairs Trading - Calculating Spread and ZScore"
        ],
        "Unsupervised Learning - Principle Component Analysis": [
          "Theory - Principle Component Analysis",
          "PCA - Data Extraction",
          "PCA - Data Preprocessing - Handling Stationarity",
          "PCA - Train Test Split",
          "PCA - Completion with Visualization",
          "Random Forest Classification - Results",
          "Unsupervised Learning - Summary"
        ],
        "Supervised Machine Learning": [
          "Theory - Random Forests vs XGBOOST",
          "XGB Preprocessing - Data Ingestion",
          "XGB Preprocessing - Feature Expansion",
          "XGB Preprocessing - Stationarity",
          "XGB Preprocessing - Train Test Split",
          "XGB - Hyperparameter Optimization",
          "XGB - Initial Model Training",
          "XGB - Feature Selection",
          "XGB II - Train Test Split",
          "XGB II - Model Fitting",
          "XGB II - Model Evaluation - Measuring Loss and ROC",
          "XGB II - Model Evaluation - Performance Comparison",
          "XGB II - Model Evaluation - Summary Report",
          "XGB II - Model Evaluation - Confusion Matrix",
          "XGB II - Model Evaluation - View Tree"
        ],
        "Supervised Deep Learning - Basic Introduction": [
          "Theory - Deep Learning Neural Network Anatomy",
          "Deep Learning - Feature Engineering Part I",
          "Deep Learning - Feature Engineering Part II",
          "Deep Learning - Neural Net and Data Build",
          "Deep Learning - Model Training",
          "Deep Learning - (Optional Code Walkthrough) - LSTM Sequential Model"
        ]
      },
      "requirements": [
        "You should have some basic experience with Python",
        "You should be aware of trading related concepts like Pairs Trading",
        "You should have awareness of assets like ETF's, the VIX, Stocks and Crypto"
      ],
      "description": "Gain an edge in financial trading through deploying Machine Learning techniques to financial data using Python. In this course, you will:\n\n\nDiscover hidden market states and regimes using Hidden Markov Models.\nObjectively group like-for-like ETF's for pairs trading using K-Means Clustering and understand how to capitalise on this using statistical methods like Cointegration and Zscore.\nMake predictions on the VIX by including a vast amount of technical indicators and distilling just the useful information via Principle Component Analysis (PCA).\nUse one of the most advanced Machine Learning algorithms, XGBOOST, to make predictions on Bitcoin price data regarding the future.\nEvaluate performance of models to gain confidence in the predictions being made.\nQuantify objectively the accuracy, precision, recall and F1 score on test data to infer your likely percentage edge.\nDevelop an AI model to trade a simple sine wave and then move on to learning to trade the Apple stock completely by itself without any prompt for selection positions whatsoever.\nBuild a Deep Learning neural network for both Classification and receive the code for using an LSTM neural network to make predictions on sequential data.\nUse Python libraries such as Pandas, PyTorch (for deep learning), sklearn and more.\n\n\nThis course does not cover much in-depth theory. It is purely a hands-on course, with theory at a high level made for anyone to easily grasp the basic concepts, but more importantly, to understand the application and put this to use immediately.\nIf you are looking for a course with a lot of math, this is not the course for you.\nIf you are looking for a course to experience what machine learning is like using financial data in a fun, exciting and potentially profitable way, then you will likely very much enjoy this course.",
      "target_audience": [
        "Retail traders who are looking to gain an objective edge in the financial markets",
        "Enthusiasts who are looking for a practical and fun application of Machine Learning"
      ]
    },
    {
      "title": "R Ultimate 2024: R for Data Science and Machine Learning",
      "url": "https://www.udemy.com/course/r-ultimate-learn-r-for-data-science-and-machine-learning/",
      "bio": "R Basics, Data Science, Statistical Machine Learning models, Deep Learning, Shiny and much more (All R code included)",
      "objectives": [
        "learn all aspects of R from Basics, over Data Science, to Machine Learning and Deep Learning",
        "learn R basics (data types, structures, variables, and more)",
        "learn R programming (writing loops, functions, and more)",
        "data im- and export",
        "basic data manipulation (piping, filtering, aggregation of results, data reshaping, set operations, joining datasets)",
        "data visualisation (different packages are learned, e.g. ggplot, plotly, leaflet, dygraphs)",
        "advanced data manipulation (outlier detection, missing data handling, regular expressions)",
        "regression models (create and apply regression models)",
        "model evaluation (What is underfitting and overfitting? Why is data splitted into training and testing? What are resampling techniques?)",
        "regularization (What is regularization? How can you apply it?)",
        "classification models (understand different algorithms and learn how to apply logistic regression, decision trees, random forests, support vector machines)",
        "association rules (learn the apriori model)",
        "clustering (kmeans, hierarchical clustering, DBscan)",
        "dimensionality reduction (factor analysis, principal component analysis)",
        "Reinforcement Learning (upper confidence bound)",
        "Deep Learning (deep learning for multi-target regression, binary and multi-label classification)",
        "Deep Learning (learn image classification with convolutional neural networks)",
        "Deep Learning (learn about Semantic Segmentation)",
        "Deep Learning (Recurrent Neural Networks, LSTMs)",
        "More on Deep Learning, e.g. Autoencoders, pretrained models, ...",
        "R/Shiny for web application development and deployment"
      ],
      "course_content": {
        "Course Introduction": [
          "Course Overview",
          "R and RStudio (Overview and Installation)",
          "How to get the code",
          "How to get the code (alternative)",
          "RStudio Introduction / Project Setup",
          "File Formats",
          "Rmarkdown Lab",
          "Package Handling"
        ],
        "Data Types and -structures": [
          "Basic Data Types 101",
          "Basic Data Types Lab",
          "Matrices and Arrays Lab",
          "Lists",
          "Factors",
          "Dataframes",
          "Strings Lab",
          "Datetime"
        ],
        "R Programming": [
          "Operators",
          "Loops 101",
          "Loops Lab",
          "Functions 101",
          "Functions Lab (Intro)",
          "Functions Lab (Coding)"
        ],
        "Data Im- and Export": [
          "Data Import Lab",
          "Data Export Lab",
          "Web Scraping Intro",
          "Web Scraping Lab"
        ],
        "Basic Data Manipulation": [
          "Piping 101",
          "Filtering 101",
          "Filtering Lab",
          "Filtering Exercise",
          "Filtering Solution",
          "Data Aggregation 101",
          "Data Aggregation Lab",
          "Data Aggregation Exercise",
          "Data Aggregation Solution",
          "Data Reshaping 101",
          "Data Reshaping Lab",
          "Data Reshaping Exercise",
          "Data Reshaping Solution",
          "Set Operations 101",
          "Set Operations Lab",
          "Joining Datasets 101",
          "Joining Datasets Lab"
        ],
        "Data Visualisation": [
          "Visualisation Overview",
          "ggplot 101",
          "ggplot Lab",
          "plotly Lab (Intro)",
          "plotly Lab",
          "leaflet Lab (Intro)",
          "leaflet Lab",
          "dygraphs Lab (Intro)",
          "dygraphs Lab"
        ],
        "Advanced Data Manipulation": [
          "Outlier Detection 101",
          "Outlier Detection Lab (Intro)",
          "Outlier Detection Lab",
          "Outlier Detection Exercise",
          "Outlier Detection Solution",
          "Missing Data Handling 101",
          "Missing Data Handling Lab (Intro)",
          "Missing Data Handling Lab (1/1)",
          "Regular Expressions 101",
          "Regular Expressions Lab"
        ],
        "Machine Learning: Introduction": [
          "AI 101",
          "Machine Learning 101",
          "Models"
        ],
        "Machine Learning: Regression": [
          "Regression Types 101",
          "Univariate Regression 101",
          "Univariate Regression Interactive",
          "Univariate Regression Lab",
          "Univariate Regression Exercise",
          "Univariate Regression Solution",
          "Polynomial Regression 101",
          "Polynomial Regression Lab",
          "Multivariate Regression 101",
          "Multivariate Regression Lab",
          "Multivariate Regression Exercise",
          "Multivariate Regression Solution"
        ],
        "Machine Learning: Model Preparation and Evaluation": [
          "Underfitting / Overfitting 101",
          "Train / Validation / Test Split 101",
          "Train / Validation / Test Split Interactive",
          "Train / Validation / Test Split Lab",
          "Resampling Techniques 101",
          "Resampling Techniques Lab"
        ]
      },
      "requirements": [
        "no prior knowledge required - just be passionate to gain new skills"
      ],
      "description": "You want to be able to perform your own data analyses with R? You want to learn how to get business-critical insights out of your data? Or you want to get a job in this amazing field? In all of these cases, you found the right course!\nWe will start with the very Basics of R, like data types and -structures, programming of loops and functions, data im- and export.\nThen we will dive deeper into data analysis: we will learn how to manipulate data by filtering, aggregating results, reshaping data, set operations, and joining datasets. We will discover different visualisation techniques for presenting complex data. Furthermore find out to present interactive timeseries data, or interactive geospatial data.\nAdvanced data manipulation techniques are covered, e.g. outlier detection, missing data handling, and regular expressions.\nWe will cover all fields of Machine Learning: Regression and Classification techniques, Clustering, Association Rules, Reinforcement Learning, and, possibly most importantly, Deep Learning for Regression, Classification, Convolutional Neural Networks, Autoencoders, Recurrent Neural Networks, ...\nYou will also learn to develop web applications and how to deploy them with R/Shiny.\nFor each field, different algorithms are shown in detail: their core concepts are presented in 101 sessions. Here, you will understand how the algorithm works. Then we implement it together in lab sessions. We develop code, before I encourage you to work on exercise on your own, before you watch my solution examples. With this knowledge you can clearly identify a problem at hand and develop a plan of attack to solve it.\nYou will understand the advantages and disadvantages of different models and when to use which one. Furthermore, you will know how to take your knowledge into the real world.\nYou will get access to an interactive learning platform that will help you to understand the concepts much better.\nIn this course code will never come out of thin air via copy/paste. We will develop every important line of code together and I will tell you why and how we implement it.\nTake a look at some sample lectures. Or visit some of my interactive learning boards. Furthermore, there is a 30 day money back warranty, so there is no risk for you taking the course right now. Don’t wait. See you in the course.",
      "target_audience": [
        "R beginners interested in learning R",
        "data science practitioners who want to deepen their knowledge",
        "developers who want to learn different aspects of Machine Learning"
      ]
    },
    {
      "title": "Complete Guide to TensorFlow for Deep Learning with Python",
      "url": "https://www.udemy.com/course/complete-guide-to-tensorflow-for-deep-learning-with-python/",
      "bio": "Learn how to use Google's Deep Learning Framework - TensorFlow with Python! Solve problems with cutting edge techniques!",
      "objectives": [
        "Understand how Neural Networks Work",
        "Build your own Neural Network from Scratch with Python",
        "Use TensorFlow for Classification and Regression Tasks",
        "Use TensorFlow for Image Classification with Convolutional Neural Networks",
        "Use TensorFlow for Time Series Analysis with Recurrent Neural Networks",
        "Use TensorFlow for solving Unsupervised Learning Problems with AutoEncoders",
        "Learn how to conduct Reinforcement Learning with OpenAI Gym",
        "Create Generative Adversarial Networks with TensorFlow",
        "Become a Deep Learning Guru!"
      ],
      "course_content": {},
      "requirements": [
        "Some knowledge of programming (preferably Python)",
        "Some basic knowledge of math (mean, standard deviation, etc..)"
      ],
      "description": "Welcome to the Complete Guide to TensorFlow for Deep Learning with Python!\n\nThis course will guide you through how to use Google's TensorFlow framework to create artificial neural networks for deep learning! This course aims to give you an easy to understand guide to the complexities of Google's TensorFlow framework in a way that is easy to understand. Other courses and tutorials have tended to stay away from pure tensorflow and instead use abstractions that give the user less control. Here we present a course that finally serves as a complete guide to using the TensorFlow framework as intended, while showing you the latest techniques available in deep learning!\nThis course is designed to balance theory and practical implementation, with complete jupyter notebook guides of code and easy to reference slides and notes. We also have plenty of exercises to test your new skills along the way!\nThis course covers a variety of topics, including\nNeural Network Basics\nTensorFlow Basics\nArtificial Neural Networks\nDensely Connected Networks\nConvolutional Neural Networks\nRecurrent Neural Networks\nAutoEncoders\nReinforcement Learning\nOpenAI Gym\nand much more!\nThere are many Deep Learning Frameworks out there, so why use TensorFlow?\nTensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\n\nIt is used by major companies all over the world, including Airbnb, Ebay, Dropbox, Snapchat, Twitter, Uber, SAP, Qualcomm, IBM, Intel, and of course, Google!\nBecome a machine learning guru today! We'll see you inside the course!",
      "target_audience": [
        "Python students eager to learn the latest Deep Learning Techniques with TensorFlow"
      ]
    },
    {
      "title": "How to use Artificial Intelligence - A guide for everyone!",
      "url": "https://www.udemy.com/course/how-to-use-artificial-intelligence-a-guide-for-everyone/",
      "bio": "Benefit today from the technology of tomorrow : Start using Artificial Intelligence to boost your productivity!",
      "objectives": [
        "Introduce Artificial Intelligence Tools that can be used by everyone and their capabilities",
        "Provide non-technical, yet, hands-on experience using Artificial Intelligence to generate text, answer questions, and perform other language-related tasks",
        "Identify the potential applications and use cases of Artificial Intelligence and GPT-3, that everyone, without prior technical background can benefit from",
        "Discuss the limitations of Artificial Intelligence and potential challenges and risks associated with its use.",
        "Evaluate the future prospects of such Artificial Intelligence tooling"
      ],
      "course_content": {
        "Introduction": [
          "Quick Introduction",
          "Course Syllabus",
          "Disclaimers",
          "Resources Links"
        ],
        "Using Artifical Intelligence": [
          "Show me an example!",
          "Artificial Intelligence (AI) & GPT-3",
          "The adoption of ChatGPT and the path from GPT-3 to GPT-5",
          "Setting Up Your Account and Making a First Test",
          "OpenAI GPT Playground Setup",
          "Playground Access - Update",
          "Using Artifical Intelligence"
        ],
        "Using Artificial Intelligence - Use Cases": [
          "Having your own personal virtual assistant!",
          "Use Case 1 : Developer : Coding and Bug fixing",
          "Use Case 2 : Content Writing",
          "Use Case 3 : For Music Enthusiasts",
          "Use Case 4 : Teaching (or Explaining concepts and methods)",
          "Use Case 4 - Precision",
          "Use Case 5 : Legal, Compliance and Contract writing",
          "Use Case 6 : Project Management",
          "GPT Use Cases",
          "Quiz 3 - Is using ChatGPT free ?",
          "GPT 4o Update - New Use Cases",
          "Resources Links"
        ],
        "Prompt Engineering": [
          "Introducing Prompt Engineering",
          "ChatGPT Demo with Advanced Prompts"
        ],
        "Generating Images from Text": [
          "How to generate Images from Text descriptions using AI and Dall-E",
          "Generating Images from Text",
          "How to generate Images from Text descriptions using AI and Midjourney",
          "Generating Images from Text"
        ],
        "AI Tools : Overview and examples": [
          "AI Tools",
          "ChatGPT Plugins"
        ],
        "Caveats & Limits": [
          "Caveats & Limits discussion",
          "Caveats & Limitations"
        ],
        "Conclusion": [
          "What's Next ?",
          "What's Next : GPT-4",
          "Thank you!"
        ]
      },
      "requirements": [
        "No requirement! If you are willing to increase your productivity using available Artificial Intelligence Tools, this course is for you!"
      ],
      "description": "Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans.\nToday, it is a rapidly growing field that has the potential to revolutionize many aspects of our lives. In this course I am sharing an approach on how you can benefit from AI tools that you can have access to and the way you can use them to help you increase your productivity. Whether you are a Content Writer, a Teacher, a Graphical Designer, a Developer or in any field, there something AI can do for you and this is what this course is about!\nArtifical Intelligence can be applied today to a wide range of fields, including robotics, natural language processing, computer vision, and machine learning. The development of AI has the potential to revolutionize many industries and has sparked a great deal of debate about its potential impact on society.\nInstead of being afraid of AI, how can you use it to your advantage ?\nThis course is intended to be \"straight to the point\". It will not bother you with technical complexity. It will rather present you examples, but will also give you some information about the tools behind this, as well as its possible limitations and its future!\nThis course is for everyone! Learn how to use GPT-3, GPT-4, ChatGPT, Dall-E and Midjourney!\nNo prior experience in Programming or any technical field is needed to follow this course! Enroll now!",
      "target_audience": [
        "Everyone!",
        "Curious Professionals and Students",
        "Content Writers",
        "Graphical Designers",
        "Laywers",
        "Developers"
      ]
    },
    {
      "title": "Data Science : Master Machine Learning Without Coding",
      "url": "https://www.udemy.com/course/hands-on-machine-learning-without-writing-code/",
      "bio": "Learn Fundamentals Of Data Science & Machine Learning With Rapidminer (No Coding). Dataset & Solutions Included.",
      "objectives": [
        "Build predictive models using machine learning algorithms without writing a line of software code"
      ],
      "course_content": {},
      "requirements": [
        "Able to use a Windows computer and install software on it",
        "High school math"
      ],
      "description": "Learn To Master Data Science And Machine Learning Without Coding And Earn a 6-Figure Income\nWhy Data Science and Machine Learning are the Hottest and Most In-Demand Technology Jobs.\nData Scientist was recently dubbed “The Sexiest Job of the 21st Century” by Harvard Business Review, and for good reason!\nIf you’re looking for a fast and effective way to earn a 6-figure income without spending thousands of dollars in training, keep reading to learn about this revolutionary Udemy course.\nGlassdoor reports that Data Scientist was named the “Best Job in America for 2016,” which was based on the huge amount of career opportunities and 6-figure average salary.  Business media from Forbes to The New York Times also frequently report about the increasing demand for data scientists.\nWhy is this great news for you?\nThe sudden increase in demand for Data Scientists has created an incredible skills gap in the job market.  According to a McKinsey Report, by the end of 2018 the demand for them is expected to be 60% higher than the available talent!\nMachine Learning is the Key to Your High-Earning Future\nLeading companies understand that Machine Learning is the future, and are investing millions of dollars into Machine Learning Research.\n\n\nMachine Learning is the subset of Artificial Intelligence (AI) that enables computers to learn and perform tasks they haven’t been explicitly programmed to do.\nData Scientists and Machine Learning Engineers who are skilled in Machine Learning are even higher in demand across the entire employment spectrum.  Many diverse industries are searching for innovation in the field, and their need for Machine Learning experts and engineers is rapidly increasing.\nTraditional Machine Learning requires students to know software programming, which enables them to write machine learning algorithms.  But in this groundbreaking Udemy course, you’ll learn Machine Learning without any coding whatsoever.  As a result, it’s much easier and faster to learn!\nThere’s literally no other course on Udemy that teaches Machine Learning without the need for programming knowledge or coding, using free open source software!\nA Rare Opportunity to Quickly Learn Data Science and Machine Learning at an Affordable Cost… No Previous Knowledge of Programming Required!\nHappily, now you can shorten your learning curve and be on your way toward earning a 6-figure income with this groundbreaking Udemy training.\nMaster Machine Learning & Data Science Quickly!\nOne of the most common problems learners have when jumping into Machine Learning and Data Science is the steep learning curve, and when you add to this the complexity of learning programming languages like Python or R you can get demotivated and lose interest fast.\nA Different & More Effective Approach To Learning Data Science\nIn this groundbreaking course, you will learn the basic concepts of machine learning using a visual tool. Where you can just drag drop machine learning algorithms and all other functionality hiding the ugliness of code, making it much easier to grasp the fundamental concepts.\nWe’ll Build Several Machine Learning Algorithms Together.\nI’ll “hand-hold” you as we build from scratch several different types of machine learning algorithms used in the real world, across several industries and I will explain where and how they are used.\nLearn Both The Theory & Application Of Machine:\nThe course will teach you those fundamental concepts of machine learning by implementing practical exercises which are based on real world examples. You will learn the theory, but get hands on practice building these machine learning algorithms.\n\n\nYou’ll also get access to:\n·         The datasets used in all the exercises.\n·         The solution files of the completed exercises.\n·         Cheat sheets to help you remember the fundamental concepts.\nJoin the class now!",
      "target_audience": [
        "Software Programmers or Data Analysts Trying To Switch To A Data Science Career",
        "Business Analysts With No Programming Background Yet Want To Learn Machine Learning",
        "Students Trying to Understand the Basics of Machine Learning",
        "Anyone Who Wants To Understand the Fundamentals Behind Machine Learning"
      ]
    },
    {
      "title": "Master Data Engineering using GCP Data Analytics",
      "url": "https://www.udemy.com/course/master-data-engineering-using-gcp-data-analytics/",
      "bio": "Learn GCS for Data Lake, BigQuery for Data Warehouse, GCP Dataproc and Databricks for Big Data Pipelines",
      "objectives": [
        "Data Engineering leveraging Services under GCP Data Analytics",
        "Setup Development Environment using Visual Studio Code on Windows",
        "Building Data Lake using GCS",
        "Process Data in the Data Lake using Python and Pandas",
        "Build Data Warehouse using Google BigQuery",
        "Loading Data into Google BigQuery tables using Python and Pandas",
        "Setup Development Environment using Visual Studio Code on Google Dataproc with Remote Connection",
        "Big Data Processing or Data Engineering using Google Dataproc",
        "Run Spark SQL based applications as Dataproc Jobs using Commands",
        "Build Spark SQL based ELT Data Pipelines using Google Dataproc Workflow Templates",
        "Run or Instantiate ELT Data Pipelines or Dataproc Workflow Template using gcloud dataproc commands",
        "Big Data Processing or Data Engineering using Databricks on GCP",
        "Integration of GCS and Databricks on GCP",
        "Build and Run Spark based ELT Data Pipelines using Databricks Workflows on GCP",
        "Integration of Spark on Dataproc with Google BigQuery",
        "Build and Run Spark based ELT Pipeline using Google Dataproc Workflow Template with BigQuery Integration"
      ],
      "course_content": {},
      "requirements": [
        "A Computer with at least 8 GB RAM",
        "Programming Experience using Python is highly desired as some of the topics are demonstrated using Python",
        "SQL Experience is highly desired as some of the topics are demonstrated using SQL",
        "Nice to have Data Engineering Experience using Pandas or Pyspark",
        "This course is ideal for experienced data engineers to add GCP Analytics Services as key skills to their profile"
      ],
      "description": "Data Engineering is all about building Data Pipelines to get data from multiple sources into Data Lakes or Data Warehouses and then from Data Lakes or Data Warehouses to downstream systems. As part of this course, I will walk you through how to build Data Engineering Pipelines using GCP Data Analytics Stack. It includes services such as Google Cloud Storage, Google BigQuery, GCP Dataproc, Databricks on GCP, and many more.\nAs part of this course, first you will go ahead and setup environment to learn using VS Code on Windows and Mac.\nOnce the environment is ready, you need to sign up for Google Cloud Account. We will provide all the instructions to sign up for Google Cloud Account including reviewing billing as well as getting USD 300 Credit.\nWe typically use Cloud Object Storage as Data Lake. As part of this course, you will learn how to use Google Cloud Storage as Data Lake along with how to manage the files in Google Cloud Storage both by using commands as well as Python. It also covers, integration of Pandas with files in Google Cloud Storage.\nGCP provides RDBMS as service via Cloud SQL. You will learn how to setup Postgresql Database Server using Cloud SQL. Once the Database Server is setup, you will also take care of setting up required application database and user. You will also understand how to develop Python based applications by integrating with GCP Secretmanager to retrieve the credentials.\nOne of the key usage of Data is nothing but building reports and dashboards. Typically reports and dashboards are built using reporting tools pointing to Data Warehouse. As part of Google Data Analytics Services, BigQuery can be used as Data Warehouse. You will learn the features of BigQuery as a Data Warehouse along with key integrations using Python and Pandas.\nAt times, we need to process heavy volumes of data which also known as Big Data Processing. GCP Dataproc is a fully manage Big Data Service with Hadoop, Spark, Kafka, etc. You will not only learn how to setup the GCP Dataproc cluster, but also you will learn how to use single node Dataproc cluster for the development. You will setup development environment using VS Code with remote connection to the Dataproc Cluster.\nOnce you understand how to get started with Big Data Processing using Dataproc, you will take care of building end to end ELT Data Pipelines using Dataproc Workflow Templates. You will learn all key commands to submit Dataproc Jobs as well as Workflows. You will end up building ELT Pipelines using Spark SQL.\nWhile Dataproc is GCP Native Big Data Service, Databricks is another prominent Big Data Service available in GCP. You will also understand how to get started with Databricks on GCP.\nOnce you go through the details about how to get started with Databricks on GCP, you will take care of building end to end ELT Datapipelins using Databricks Jobs and Workflows.\nTowards the end of the course you should be fairly comfortable with BigQuery for Data Warehouse and GCP Dataproc for Data Processing, you will learn how to integrate these two key services by building end to end ELT Data Pipeline using Dataproc Workflow. You will also understand how to include Pyspark based application with Spark BigQuery connector as part of the Pipeline.\nIn the process of building Data Pipelines, you will also revise application development life cycle of Spark, troubleshooting issues related to the spark using relevant web interfaces such as YARN Timeline Server, Spark UI, etc.",
      "target_audience": [
        "Beginner or Intermediate Data Engineers who want to learn GCP Analytics Services for Data Engineering",
        "Intermediate Application Engineers who want to explore Data Engineering using GCP Analytics Services",
        "Data and Analytics Engineers who want to learn Data Engineering using GCP Analytics Services",
        "Testers who want to learn key skills to test Data Engineering applications built using GCP Analytics Services"
      ]
    },
    {
      "title": "Introduction to Time Series Analysis and Forecasting in R",
      "url": "https://www.udemy.com/course/time-series-analysis-and-forecasting-in-r/",
      "bio": "Work with time series and all sorts of time related data in R - Forecasting, Time Series Analysis, Predictive Analytics",
      "objectives": [
        "use R to perform calculations with time and date based data",
        "create models for time series data",
        "use models for forecasting",
        "identify which models are suitable for a given dataset",
        "visualize time series data",
        "transform standard data into time series format",
        "clean and pre-process time series",
        "create ARIMA and exponential smoothing models",
        "know how to interpret given models",
        "identify the best time series libraries for a given problem",
        "compare the accuracy of different models"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Course Introduction to Time Series Analysis and Forecasting in R",
          "Managing Expectations",
          "Basics of Time Series Analysis and Forecasting",
          "Method Selection in Forecasting",
          "Forecasting: Step by Step Guide",
          "Time Series Analysis and Forecasting Use Case: IT Store Staff Allocation",
          "Script for the Example",
          "Package Overview and the R Time Series Task View",
          "Datasets To Be Used",
          "Course Links",
          "Time Series Analysis Intro"
        ],
        "Working With Dates And Time In R": [
          "Welcome to this Section - What Is this Section About?",
          "Working with Different Date and Time Classes: POSIXt, Date and Chron",
          "Format Conversion from String to Date / Time - Function strptime",
          "The Lubridate Package",
          "Exercise: Using Lubridate on a Data Frame",
          "Date and Time Calculations with Lubridate",
          "Lubridate: Data Handling Exercise",
          "Section Script TD"
        ],
        "Time Series Data Pre-Processing and Visualization": [
          "Creating Time Series",
          "Exercise - Time Series Formatting",
          "Time Series Charts and Graphs",
          "Exercise: Seasonplot",
          "Importing Time Series Data From Excel or Other Sources",
          "Working with Irregular Time Series",
          "Working with Missing Data and Outliers",
          "Section Script TSPP",
          "Time Series Data Preparation"
        ],
        "Statistical Background For Time Series Analysis And Forecasting": [
          "Time Series Vectors and Lags",
          "Time Series Characteristics",
          "Basic Forecasting Models",
          "Model Comparison and Accuracy",
          "The Importance of Residuals in Time Series Analysis",
          "Stationarity",
          "Autocorrelation",
          "Functions acf() and pacf()",
          "Exercise: Forecast Comparison",
          "Section Script STAT",
          "Statistical Background"
        ],
        "Time Series Analysis And Forecasting": [
          "Selecting a Suitable Model - Quantitative Forecasting Models",
          "Seasonal Decomposition Intro",
          "Decomposition Demo",
          "Exercise: Decomposition",
          "Simple Moving Average",
          "Exponential Smoothing with ETS",
          "Judgmental Forecasts - Qualitative Forecasting Methods",
          "Section Script TSA"
        ],
        "ARIMA Models": [
          "What is Coming Up Next? ARIMA Models in Time Series Analysis",
          "Introduction to ARIMA Models",
          "Automated ARIMA Model Selection with auto.arima",
          "ARIMA Model Calculations",
          "Simulating Time Series Based on ARIMA",
          "Manual ARIMA Parameter Selection",
          "How to Indentify ARIMA Model Parameters",
          "ARIMA Forecasts",
          "ARIMA with Explanatory Variables - Adding a Second Variable to the Model",
          "Section Script ARIMA"
        ],
        "Multivariate Time Series Analysis": [
          "What is Coming Up Next? Multivariate Time Series Analysis in R",
          "Understanding Multivariate Time Series and Their Structure",
          "Multivariate Time Series Objects and Project Dataset",
          "Main R Packages for Multivariate Time Series Analysis",
          "Stationarity in Multivariate Time Series",
          "Vector Autoregressive Model Theory",
          "Implementing VAR Models in R",
          "Test for Residual Correlation and Model Diagnostics",
          "The Granger Test for Causality",
          "Forecasting a VAR Model",
          "Section Script"
        ],
        "Neural Networks in Time Series Analysis": [
          "What is Coming Up Next? Time Series Analysis Using Neural Networks",
          "Intro to Neural Networks for TSA",
          "Getting Familiar with the Dataset",
          "The Time Series Task View for Neural Nets - What is Available?",
          "Implementation of Neural Networks in R - Underlying Functions",
          "Practical Implementation of an Autoregressive Neural Net",
          "Implementing an External Regressor - Multivariate Neural Net",
          "Section Script",
          "Further Resources and Where to Go Next"
        ]
      },
      "requirements": [
        "computer with R and RStudio ready to use",
        "interest in statistics and programming",
        "time to solve the exercises",
        "basic knowledge of R (course R Base)",
        "NO advanced statistics or maths knowledge required"
      ],
      "description": "Understand the Now – Predict the Future!\nTime series analysis and forecasting is one of the key fields in statistical programming. It allows you to\nsee patterns in time series data\nmodel this data\nfinally make forecasts based on those models\nDue to modern technology the amount of available data grows substantially from day to day. Successful companies know that. They also know that decisions based on data gained in the past, and modeled for the future, can make a huge difference. Proper understanding and training in time series analysis and forecasting will give you the power to understand and create those models. This can make you an invaluable asset for your company/institution and will boost your career!\nWhat will you learn in this course and how is it structured?\nYou will learn about different ways in how you can handle date and time data in R. Things like time zones, leap years or different formats make calculations with dates and time especially tricky for the programmer. You will learn about POSIXt classes in R Base, the chron package and especially the lubridate package.\nYou will learn how to visualize, clean and prepare your data. Data preparation takes a huge part of your time as an analyst. Knowing the best functions for outlier detection, missing value imputation and visualization can safe your day.\nAfter that you will learn about statistical methods used for time series. You will hear about autocorrelation, stationarity and unit root tests.\nThen you will see how different models work, how they are set up in R and how you can use them for forecasting and predictive analytics. Models taught are: ARIMA, exponential smoothing, seasonal decomposition and simple models acting as benchmarks. Of course all of this is accompanied with plenty of exercises.\nWhere are those methods applied?\nIn nearly any quantitatively working field you will see those methods applied. Especially econometrics and finance love time series analysis. For example stock data has a time component which makes this sort of data a prime target for forecasting techniques. But of course also in academia, medicine, business or marketing techniques taught in this course are applied.\nIs it hard to understand and learn those methods?\nUnfortunately learning material on Time Series Analysis Programming in R is quite technical and needs tons of prior knowledge to be understood.\nWith this course it is the goal to make understanding modeling and forecasting as intuitive and simple as possible for you.\nWhile you need some knowledge in statistics and statistical programming, the course is meant for people without a major in a quantitative field like math or statistics. Basically anybody dealing with time data on a regular basis can benefit from this course.\nHow do I prepare best to benefit from this course?\nIt depends on your prior knowledge. But as a rule of thumb you should know how to handle standard tasks in R (course R Basics).\nWhat R you waiting for?",
      "target_audience": [
        "this course is for people working with time series data",
        "this course is for people interested in R",
        "this course is for people with some beginner knowledge in both R programming and statistics",
        "this course is for people working in various fields like (and not limited to): academia, marketing, business, econometrics, finance, medicine, engineering and science",
        "generally if you have time series data on your table and you do not know what to do with it, take this course!"
      ]
    },
    {
      "title": "The Complete Data Analyst Bootcamp From Basics To Advanced",
      "url": "https://www.udemy.com/course/data-analysis-with-pandas-a-complete-tutorial/",
      "bio": "Work With Pandas, Python For Data Science, ML & Data Analysis, Data Prep With EDA &100+ Exercises & Real Life Projects",
      "objectives": [
        "Build a Solid Foundation in Data Analysis with Python",
        "You will be able to work with the Pandas Data Structures: Series, DataFrame and Index Objects",
        "Learn hundreds of methods and attributes across numerous pandas objects",
        "You will be able to analyze a large and messy data files",
        "You can prepare real world messy data files for AI and ML",
        "Manipulate data quickly and efficiently",
        "You will learn almost all the Pandas basics necessary to become a 'Data Analyst'"
      ],
      "course_content": {
        "Getting Started": [
          "Course Introduction",
          "How To Get Most Out Of This Course",
          "Better To Know These Things",
          "How To Install Python IPython And Jupyter Notebook",
          "How To Install Anaconda For macOS And Linux Users",
          "How To Work With The Jupyter Notebook Part-1",
          "How To Work With The Jupyter Notebook Part-2"
        ],
        "Pandas Building Blocks": [
          "How To Work With The Tabular Data",
          "How To Read The Documentation In Pandas"
        ],
        "Pandas_Data Structures": [
          "Theory On Pandas Data Structures",
          "How To Construct The Pandas Series",
          "How To Construct The DataFrame Objects",
          "How To Construct The Pandas Index Objects",
          "Practice Part 01",
          "Practice Part 01 Solution"
        ],
        "Data Indexing And Selection": [
          "Theory On Data Indexing And Selection",
          "Data Selection In Series Part 1",
          "Data Selection In Series Part 2",
          "Indexers Loc And Iloc In Series",
          "Data Selection In DataFrame Part 1",
          "Data Selection In DataFrame Part 2",
          "Accessing Values Using Loc Iloc And Ix In DataFrame Objects",
          "Practice Part 02",
          "Practice Part 02 Solution"
        ],
        "Essential Functionalities": [
          "Theory On Essential Functionalities",
          "How To Reindex Pandas Objects",
          "How To Drop Entries From An Axis",
          "Arithmetic And Data Alignment",
          "Arithmetic Methods With Fill Values",
          "Broadcasting In Pandas",
          "Apply And Applymap In Pandas",
          "How To Sort And Rank In Pandas",
          "How To Work With The Duplicated Indices",
          "Summarising And Computing Descriptive Statistics",
          "Unique Values Value Counts And Membership",
          "Practice_Part_03",
          "Practice_Part_03 Solution"
        ],
        "Data Handling": [
          "Theory On Data Handling",
          "How To Read The Csv Files Part - 1",
          "How To Read The Csv Files Part - 2",
          "How To Read Text Files In Pieces",
          "How To Export Data In Text Format",
          "How To Use Python's Csv Module",
          "Practice_Part_04",
          "Practice_Part_04 Solution"
        ],
        "Data Cleaning And Preparation": [
          "Theory On Data Preprocessing",
          "How To Handle Missing Values",
          "How To Filter The Missing Values",
          "How To Filter The Missing Values Part 2",
          "How To Remove Duplicate Rows And Values",
          "How To Replace The Non Null Values",
          "How To Rename The Axis Labels",
          "How To Descretize And Bin The Data Part - 1",
          "How To Filter And Detect The Outliers",
          "How To Reorder And Select Randomly",
          "Converting The Categorical Variables Into Dummy Variables",
          "How To Use 'map' Method",
          "How To Manipulate With Strings",
          "Using Regular Expressions",
          "Working With The Vectorized String Functions",
          "Practice_Part_05",
          "Practice_Part_05 Solution"
        ],
        "Data Wrangling": [
          "Theory On Data Wrangling",
          "Hierarchical Indexing",
          "Hierarchical Indexing Reordering And Sorting",
          "Summary Statistics By Level",
          "Hierarchical Indexing With DataFrame Columns",
          "How To Merge The Pandas Objects",
          "Merging On Row Index",
          "How To Concatenate Along An Axis",
          "How To Combine With Overlap",
          "How To Reshape And Pivot Data In Pandas",
          "Practice_Part_06",
          "Practice_Part_06 Solution"
        ],
        "Data Grouping And Aggregation": [
          "Thoery On Data Groupby And Aggregation",
          "Groupby Operation",
          "How To Iterate Over Groupby Object",
          "How To Select Columns In Groupby Method",
          "Grouping Using Dictionaries And Series",
          "Grouping Using Functions And Index Level",
          "Data Aggregation",
          "Practice_Part_07",
          "Practice_Part_07 Solution"
        ],
        "Time Series Analysis": [
          "Theory On Time Series Analysis",
          "Introduction To Time Series Data Types",
          "How To Convert Between String And Datetime",
          "Time Series Basics With Pandas Objects",
          "Date Ranges Frequencies And Shifting",
          "Date Ranges Frequencies And Shifting Part - 2",
          "Time Zone Handling",
          "Periods And Period Arithmetic’s",
          "Practice_Part_08",
          "Practice_Part_08 Solution"
        ]
      },
      "requirements": [
        "Students must be willing to learn the Data Analysis with Python language",
        "If you know basics of Python that is well and good",
        "Basic Or intermediate experience with Microsoft Excel or another spreadsheet software, but not necessary",
        "Basic knowledge of data types (strings, integers, floating points, Booleans) etc, but not necessary",
        "Basic Programming knowledge Or knowing any other programming languages will also helps"
      ],
      "description": "Hi, dear learning aspirants welcome to “ Complete Data Analyst Bootcamp From Basics To Advanced | Python For Data Science A-Z: EDA With Real Exercises ” from beginner to advanced level. We love programming. Python is one of the most popular programming languages in today’s technical world. Python offers both object-oriented and structural programming features. Hence, we are interested in data analysis with Pandas in this course.\nThis course is for those who are ready to take their data analysis skill to the next higher level with the Python data analysis toolkit, i.e. \"Pandas\".\nThis tutorial is designed for beginners and intermediates but that doesn't mean that we will not talk about the advanced stuff as well. Our approach of teaching in this tutorial is simple and straightforward, no complications are included to make bored Or lose concentration.\nIn this tutorial, I will be covering all the basic things you'll need to know about the 'Pandas' to become a data analyst or data scientist.\nWe are adopting a hands-on approach to learn things easily and comfortably. You will enjoy learning as well as the exercises to practice along with the real-life projects (The projects included are the part of large size research-oriented industry projects).\nI think it is a wonderful platform and I got a wonderful opportunity to share and gain my technical knowledge with the learning aspirants and data science enthusiasts.\n\n\nWhat you will learn:\nYou will become a specialist in the following things while learning via this course\n“Data Analysis With Pandas”.\nYou will be able to analyze a large file\nBuild a Solid Foundation in Data Analysis with Python\nAfter completing the course you will have professional experience on;\nPandas Data Structures: Series, DataFrame and Index Objects\nEssential Functionalities\nData Handling\nData Pre-processing\nData Wrangling\nData Grouping\nData Aggregation\nPivoting\nWorking With Hierarchical Indexing\nConverting Data Types\nTime Series Analysis\nAdvanced Pandas Features and much more with hands-on exercises and practice works.\nStandard as like #Krish Naik and #KRISHAI\nSeries at a Glance\nSeries Methods and Handling\nIntroducing DataFrames\nDataFrames More In Depth\nWorking With Multiple DataFrames\nGoing MultiDimensional\nGroupBy And Aggregates\nReshaping With Pivots\nWorking With Dates And Time\nRegular Expressions And Text Manipulation\nVisualizing Data\nData Formats And I/O\n\n\nPandas and python go hand-in-hand which is why this bootcamp also includes a Pandas Coding In full length to get you up and running writing pythonic code in no time.\nThis is the ultimate course on one of the most-valuable skills today. I hope you commit to mastering data analysis with Pandas.\nSee you inside!\n\n\nRegards\nPruthviraja L\nTeam UpGraduate",
      "target_audience": [
        "Beginner Python developers - Curious to learn about Data Science Or Data Analysis",
        "Data Analysis Beginners",
        "Aspiring data scientists who want to add Python to their tool arsenal",
        "Students and Other Professionals",
        "AI and ML aspirants to upgrade their knowledge in Data Preprocessing before applying the machine learning algorithms to their projects",
        "Data Analyst job seekers who wants to update their Resume with Python's data analysis toolkit"
      ]
    },
    {
      "title": "Generative AI Guide: DALL-E, ChatGPT, and Creativity With AI",
      "url": "https://www.udemy.com/course/generative-ai-guide-dall-e-chatgpt-and-creativity-with-ai/",
      "bio": "Generative AI Techniques, Applications, and Tools to Harness the Power of AI-Driven Content Creation",
      "objectives": [
        "Principles and functioning of Generative AI models",
        "Create custom Generative AI apps",
        "Build conversational AI engines",
        "Building apps with AppSheet and integrating Generative AI",
        "Effective prompt engineering methods to achieve desired results",
        "Utilization of Google Cloud Gen AI Tools such as Gen AI Studio, Maker Suit, PaLM API, AppSheet, and Gen AI App Builder",
        "And SO much more!"
      ],
      "course_content": {
        "Introduction": [
          "Hi. I'm Prof. Reza!",
          "Welcome to Intro to Gen AI",
          "Take Full Advantage of This Course"
        ],
        "Traditional Artificial Intelligence": [
          "Overview of Traditional Artificial Intelligence",
          "Overview of Traditional Artificial Intelligence",
          "Demystifying Machine Learning",
          "Demystifying Machine Learning",
          "Unraveling Deep Learning",
          "Unraveling Deep Learning",
          "Distinguish Discriminative and Generative Models",
          "Distinguish Discriminative and Generative Models",
          "Overview of Traditional Artificial Intelligence",
          "Demystifying Machine Learning",
          "Unraveling Deep Learning",
          "Distinguish Discriminative and Generative Models"
        ],
        "Generative Artificial Intelligence": [
          "Introduction to Generative Artificial Intelligence",
          "Introduction to Generative Artificial Intelligence",
          "What are Transformers",
          "What are Transformers",
          "Prompt Engineering",
          "Prompt Engineering",
          "What are Foundation Models",
          "What are Foundation Models",
          "Types and Applications of Gen AI",
          "Types and Applications of Gen AI",
          "Introduction to Generative Artificial Intelligence",
          "What are Transformers",
          "Prompt Engineering",
          "What are Foundation Models",
          "Types and Applications of Gen AI"
        ],
        "Large Language Models": [
          "Introduction to Large Language Models",
          "Introduction to Large Language Models",
          "Benefits of Using LLMs",
          "Benefits of Using LLMs",
          "Examples of LLMs",
          "Examples of LLMs",
          "LLM Development",
          "LLM Development",
          "Importance of Tuning LLMs",
          "Importance of Tuning LLMs",
          "Introduction to Large Language Models",
          "Benefits of Using LLMs",
          "Examples of LLMs",
          "Development of LLMs",
          "Importance of Tuning LLMs"
        ],
        "Google Cloud Gen AI Tools": [
          "Generative AI Studio",
          "Generative AI Studio",
          "MakerSuite & PaLM API",
          "MakerSuite & PaLM API",
          "Gen App Builder",
          "Gen App Builder",
          "AppSheet",
          "AppSheet",
          "Generative AI Studio",
          "MakerSuite & PaLM API",
          "Gen App Builder",
          "AppSheet"
        ],
        "Demo: Building a no-code Gen AI App with AppSheet": [
          "AppSheet Demo"
        ]
      },
      "requirements": [
        "No programming experience needed - we'll teach you everything you need to know",
        "A computer with access to the internet",
        "No paid software required"
      ],
      "description": "Welcome to the Comprehensive Generative AI Course, your gateway to mastering the exciting world of Generative Artificial Intelligence. With this course, you'll gain the knowledge and skills needed to excel in this rapidly growing field which is expected to be valued at $100 billion over the next years.\nWith multiple hours of content, world class slides and resources, this course is the most detailed Generative AI course you will find. Even if you have zero programming experience, this course will take you from beginner to mastery. Here's why:\nThe course is taught by a PhD in computer science with multiple publications who has taught in multiple universities all over the world for years.\nThe course has been created to be 2023 ready and you'll be learning the latest tools and technologies used at large companies such as OpenAI.\nThis course doesn't cut any corners, there are beautiful detailed presentations, assignments, projects, downloadable resources, articles, and so much more.\nThe curriculum was developed over years while the instructor taught at the university level, with comprehensive student testing and feedback.\nWe've taught thousands of students how to code and many have gone on to change their lives by becoming professional developers or starting their own tech startup.\nThe course is constantly updated with new content, with new projects and modules determined by students - that's you!\nWe'll take you step-by-step through engaging video tutorials and teach you everything you need to know to understand Generative AI and succeed in the industry.\nThe course includes multiple hours of HD video tutorials and builds your knowledge while working on actual assignments.\nIn this comprehensive Generative AI course, you'll explore a variety of tools and technologies, including:\nPrompt design and its importance in generating desired outputs\nFoundation Models and their impact on Generative AI development\nBuilding apps with AppSheet and integrating Generative AI\nDifferent types of Generative AI models (text to text, text to image, etc.)\nCode generation using Generative AI models like Bard, ChatGPT 3.5, and GPT-4\nLarge Language Models (LLMs) and their advantages\nPrompt Engineering techniques in LLMs\nBuilding conversational AI engines\nDeep Learning and its significance in AI\nTask-specific Models available in the Model Garden\nModel training and deployment with Gen AI Studio and Maker Suit\nCreating custom Gen AI apps\nAnd SO much more!\nWith the knowledge gained throughout the course, you'll be ready to leverage Generative AI techniques to bring your creative visions to life and develop innovative solutions in various domains.\nSign up today, and look forward to:\nAnimated Video Lectures\n5+ Hours of University Professor Instruction\nGenerative AI Assignments and Projects\nQuizzes & Opportunities to Practice\nDownloadable Programming Resources and Cheatsheets\nTailor-made Generative AI Articles\n$1000+ worth of Generative AI course materials and course curriculum\nWe're excited to have you as part of Evergreen Programming. So what are you waiting for? Click the buy now button and join this world class Generative AI course.\nSee you inside.",
      "target_audience": [
        "If you want to understand the rapidly growing world of artificial intelligence with creative potential, then take this course.",
        "If you are looking to explore the fascinating world of Generative AI and its creative applications.",
        "If you want to leverage the power of Generative AI for any sort of business.",
        "If you want to take ONE COURSE and learn everything you need to know about Generative AI, take this course"
      ]
    },
    {
      "title": "From Zero to Pro Data Science & AI Advanced Full Course",
      "url": "https://www.udemy.com/course/data-science-mastery-complete-data-science-bootcamp-2025/",
      "bio": "Master Data Science, AI, and Machine Learning with hands-on projects in Python, Deep Learning, Big Data, and Analytics",
      "objectives": [
        "Understand Data Science Workflow: Master the end-to-end data science lifecycle, from data collection to model deployment.",
        "Data Collection Techniques: Learn to gather data from APIs, databases, and web scraping.",
        "Data Preprocessing: Clean and preprocess raw data for analysis and modeling.",
        "Exploratory Data Analysis (EDA): Uncover patterns and trends in datasets using visualization tools.",
        "Feature Engineering: Create and optimize features to improve model performance.",
        "Machine Learning Models: Build regression, classification, and clustering models using scikit-learn.",
        "Deep Learning Techniques: Train neural networks with TensorFlow and PyTorch.",
        "Model Deployment: Serve AI models using Flask, FastAPI, and Docker.",
        "Big Data Handling: Work with large datasets using tools like Hadoop and Spark.",
        "Ethical AI Practices: Understand data privacy, bias mitigation, and AI governance."
      ],
      "course_content": {
        "Data Science Modules - Introduction and Brief Overview": [
          "What Will We Cover",
          "Module 1: Data Collection – The Foundation of Data Science",
          "Mod 2: Data Cleaning and Preprocessing– Turning Raw Data into Usable Insights",
          "Module 3: Data Exploration and Analysis (EDA)",
          "Module 4: Feature Engineering – Transforming Data into Insights",
          "Module 5: Data Visualization – Communicating Insights Effectively",
          "Module 6: Machine Learning and Modeling – Building Intelligent Systems",
          "Module 7: Model Evaluation and Validation – Ensuring Reliable Predictions",
          "Module 8: Model Deployment –Bringing Machine Learning Models to Life",
          "Module 9: Big Data Technologies– Managing and Analyzing Massive Datasets",
          "Module 10: Data Ethics and Governance –Responsible AI and Data Practices",
          "Module 11: Business Understanding and Domain Expertise",
          "Mod 12: Communication and Storytelling– Turning Data into Impactful Narratives",
          "Whats Next: Bootcamp Deep Dive"
        ],
        "Week 1: Python Programming Basics": [
          "Introduction to Week 1 Python Programming Basics",
          "Day 1: Introduction to Python and Development Setup",
          "Day 2: Control Flow in Python",
          "Day 3: Functions and Modules",
          "Day 4: Data Structures (Lists, Tuples, Dictionaries, Sets)",
          "Day 5: Working with Strings",
          "Day 6: File Handling",
          "Day 7: Pythonic Code and Project Work",
          "Coding Exercise",
          "Resources for the Entire Course"
        ],
        "Week 2: Data Science Essentials": [
          "Introduction to Week 2 Data Science Essentials",
          "Day 1: Introduction to NumPy for Numerical Computing",
          "Day 2: Advanced NumPy Operations",
          "Day 3: Introduction to Pandas for Data Manipulation",
          "Day 4: Data Cleaning and Preparation with Pandas",
          "Day 5: Data Aggregation and Grouping in Pandas",
          "Day 6: Data Visualization with Matplotlib and Seaborn",
          "Day 7: Exploratory Data Analysis (EDA) Project"
        ],
        "Week 3: Mathematics for Machine Learning": [
          "Introduction to Week 3 Mathematics for Machine Learning",
          "Day 1: Linear Algebra Fundamentals",
          "Day 2: Advanced Linear Algebra Concepts",
          "Day 3: Calculus for Machine Learning (Derivatives)",
          "Day 4: Calculus for Machine Learning (Integrals and Optimization)",
          "Day 5: Probability Theory and Distributions",
          "Day 6: Statistics Fundamentals",
          "Day 7: Math-Driven Mini Project – Linear Regression from Scratch"
        ],
        "Week 4: Probability and Statistics for Machine Learning": [
          "Introduction to Week 4 Probability and Statistics for Machine Learning",
          "Day 1: Probability Theory and Random Variables",
          "Day 2: Probability Distributions in Machine Learning",
          "Day 3: Statistical Inference - Estimation and Confidence Intervals",
          "Day 4: Hypothesis Testing and P-Values",
          "Day 5: Types of Hypothesis Tests",
          "Day 6: Correlation and Regression Analysis",
          "Day 7: Statistical Analysis Project – Analyzing Real-World Data"
        ],
        "Week 5: Introduction to Machine Learning": [
          "Introduction to Week 5 Introduction to Machine Learning",
          "Day 1: Machine Learning Basics and Terminology",
          "Day 2: Introduction to Supervised Learning and Regression Models",
          "Day 3: Advanced Regression Models – Polynomial Regression and Regularization",
          "Day 4: Introduction to Classification and Logistic Regression",
          "Day 5: Model Evaluation and Cross-Validation",
          "More Than Accuracy: Communicating Model Performance to Non-Experts",
          "Day 6: k-Nearest Neighbors (k-NN) Algorithm",
          "Day 7: Supervised Learning Mini Project"
        ],
        "Week 6: Feature Engineering and Model Evaluation": [
          "Introduction to Week 6 Feature Engineering and Model Evaluation",
          "Day 1: Introduction to Feature Engineering",
          "Day 2: Data Scaling and Normalization",
          "Day 3: Encoding Categorical Variables",
          "Day 4: Feature Selection Techniques",
          "Why This, Not That: Explaining Feature Importance to Domain Experts",
          "Day 5: Creating and Transforming Features",
          "Day 6: Model Evaluation Techniques",
          "Day 7: Cross-Validation and Hyperparameter Tuning"
        ],
        "Week 7: Advanced Machine Learning Algorithms": [
          "Introduction to Week 7 Advanced Machine Learning Algorithms",
          "Day 1: Introduction to Ensemble Learning",
          "Day 2: Bagging and Random Forests",
          "Day 3: Boosting and Gradient Boosting",
          "Day 4: Introduction to XGBoost",
          "Day 5: LightGBM and CatBoost",
          "Day 6: Handling Imbalanced Data",
          "Day 7: Ensemble Learning Project – Comparing Models on a Real Dataset"
        ],
        "Week 8: Model Tuning and Optimization": [
          "Introduction to Week 8 Model Tuning and Optimization",
          "Day 1: Introduction to Hyperparameter Tuning",
          "Day 2: Grid Search and Random Search",
          "Day 3: Advanced Hyperparameter Tuning with Bayesian Optimization",
          "Smarter Search: Defending Hyperparameter Optimization Strategy",
          "Day 4: Regularization Techniques for Model Optimization",
          "Day 5: Cross-Validation and Model Evaluation Techniques",
          "Day 6: Automated Hyperparameter Tuning with GridSearchCV and RandomizedSearchCV",
          "Day 7: Optimization Project – Building and Tuning a Final Model"
        ],
        "Week 9: Neural Networks and Deep Learning Fundamentals": [
          "Introduction to Week 9 Neural Networks and Deep Learning Fundamentals",
          "Day 1: Introduction to Deep Learning and Neural Networks",
          "Day 2: Forward Propagation and Activation Functions",
          "Day 3: Loss Functions and Backpropagation",
          "Day 4: Gradient Descent and Optimization Techniques",
          "Day 5: Building Neural Networks with TensorFlow and Keras",
          "Day 6: Building Neural Networks with PyTorch",
          "Day 7: Neural Network Project – Image Classification on CIFAR-10",
          "Build with Intent: Justifying Your Neural Network Architecture"
        ]
      },
      "requirements": [
        "Basic Computer Skills: Familiarity with using computers, installing software, and navigating file systems.",
        "Fundamental Programming Knowledge (Optional): Basic understanding of programming concepts like variables, loops, and functions (Python preferred).",
        "Mathematics Fundamentals: High-school-level understanding of algebra, statistics, and basic probability.",
        "Logical Thinking: Ability to approach problems methodically and think critically.",
        "A Stable Computer Setup: A computer with at least 8GB RAM (16GB recommended), 50GB free storage, and the ability to install Python and relevant libraries.",
        "Curiosity and Passion for Learning: An eagerness to learn, experiment, and explore the exciting world of Data Science.",
        "Time Commitment: Willingness to dedicate 10-15 hours per week to lessons, exercises, and projects."
      ],
      "description": "Welcome to the Data Science Mastery: Complete Data Science Bootcamp 2025! This comprehensive Data Science Bootcamp is designed to equip you with end-to-end data science skills, empowering you to become a skilled Data Scientist ready to tackle real-world challenges. Whether you're an absolute beginner or looking to sharpen your expertise, this Data Science Bootcamp offers a structured, hands-on learning experience to guide you from fundamentals to advanced techniques.(AI)\nIn this Data Science Bootcamp 2025, you'll start with the core fundamentals of Data Science, including Python programming, data preprocessing, data visualization, and exploratory data analysis (EDA). As you progress, you'll explore advanced topics like machine learning algorithms, deep learning, natural language processing (NLP), and time series analysis. You'll also gain hands-on experience with industry-standard Data Science tools and libraries such as Pandas, NumPy, Scikit-learn, TensorFlow, and PyTorch.\nThis Data Science Bootcamp emphasizes practical learning, with real-world projects integrated into every module. You'll work with large datasets, optimize machine learning models, and learn to deploy data science solutions effectively.\nWhy Choose the Data Science Mastery Bootcamp?\nComprehensive Curriculum: Cover Python, Data Visualization, Machine Learning, and Deep Learning\nHands-On Projects: Real-world Data Science projects in every module\nMaster Data Science Tools: Learn Pandas, NumPy, Scikit-learn, TensorFlow, and PyTorch\nStructured Learning Path: Beginner-friendly to advanced Data Science techniques\nReal-World Applications: Solve real-world problems using Data Science solutions\nBy the end of the Data Science Mastery Bootcamp 2025, you'll have the confidence and hands-on experience to build Data Science models, analyze complex datasets, and drive data-driven decisions in any industry.\nWhether you're aiming to become a Data Scientist, a Machine Learning Engineer, or a leader in data-driven innovation, this Data Science Bootcamp is your gateway to success in the Data Science industry.\nJoin the Data Revolution Today – Enroll in the Data Science Mastery: Complete Data Science Bootcamp 2025 and take your first step towards becoming a Data Science expert!",
      "target_audience": [
        "Aspiring Data Scientists: Individuals who want to start a career in data science but don’t know where to begin.",
        "Students and Graduates: Learners from diverse educational backgrounds looking to add data science to their skill set.",
        "Professionals Seeking a Career Switch: Working professionals aiming to transition into data-centric roles like Data Analyst, Machine Learning Engineer, or AI Specialist.",
        "Tech Enthusiasts: Curious minds eager to understand how data can drive decisions and power intelligent systems.",
        "Business Professionals: Decision-makers and managers looking to leverage data insights to improve strategy and operations.",
        "Freelancers and Entrepreneurs: Individuals aiming to build data-driven solutions or AI-powered products."
      ]
    },
    {
      "title": "Machine Learning using Python",
      "url": "https://www.udemy.com/course/machine-learning-using-python-starttech/",
      "bio": "Linear & Logistic Regression, Decision Trees, XGBoost, SVM & other ML models in Python",
      "objectives": [
        "Learn how to solve real life problem using the Machine learning techniques",
        "Advanced Machine Learning models such as Decision trees, Random Forest, SVM etc.",
        "How to do basic statistical operations and run ML models in Python",
        "Understanding of basics of statistics and concepts of Machine Learning",
        "How to convert business problem into a Machine learning problem",
        "In-depth knowledge of data collection and data preprocessing for Machine Learning problem"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the course",
          "Course resources"
        ],
        "Setting up Python and Jupyter notebook": [
          "Installing Python and Anaconda",
          "This is a Milestone!",
          "Opening Jupyter Notebook",
          "Introduction to Jupyter",
          "Arithmetic operators in Python: Python Basics",
          "Quick coding exercise on arithmetic operators",
          "Strings in Python: Python Basics",
          "Quick coding exercise on String operations",
          "Lists, Tuples and Directories: Python Basics",
          "Quick coding exercise on Tuples",
          "Working with Numpy Library of Python",
          "Quick coding exercise on NumPy Library",
          "Working with Pandas Library of Python",
          "Quick coding exercise on Pandas Library",
          "Working with Seaborn Library of Python",
          "Python file for additional practice",
          "Quiz"
        ],
        "Integrating ChatGPT with Python": [
          "Integrating ChatGPT within JUPYTER notebook"
        ],
        "Basics of statistics": [
          "Types of Data",
          "Types of Statistics",
          "Describing data Graphically",
          "Measures of Centers",
          "Measures of Dispersion",
          "Quiz"
        ],
        "Introduction to Machine Learning": [
          "Introduction to Machine Learning",
          "Building a Machine Learning Model"
        ],
        "Data Preprocessing": [
          "Gathering Business Knowledge",
          "Data Exploration",
          "The Dataset and the Data Dictionary",
          "Importing Data in Python",
          "Univariate analysis and EDD",
          "EDD in Python",
          "Outlier Treatment",
          "Outlier Treatment in Python",
          "Missing Value Imputation",
          "Missing Value Imputation in Python",
          "Seasonality in Data",
          "Bi-variate analysis and Variable transformation",
          "Variable transformation and deletion in Python",
          "Non-usable variables",
          "Dummy variable creation: Handling qualitative data",
          "Dummy variable creation in Python",
          "Correlation Analysis",
          "Correlation Analysis in Python",
          "Quiz"
        ],
        "Linear Regression": [
          "The Problem Statement",
          "Basic Equations and Ordinary Least Squares (OLS) method",
          "Assessing accuracy of predicted coefficients",
          "Assessing Model Accuracy: RSE and R squared",
          "Simple Linear Regression in Python",
          "Multiple Linear Regression",
          "The F - statistic",
          "Interpreting results of Categorical variables",
          "Multiple Linear Regression in Python",
          "Test-train split",
          "Bias Variance trade-off",
          "Test train split in Python",
          "Regression models other than OLS",
          "Subset selection techniques",
          "Shrinkage methods: Ridge and Lasso",
          "Ridge regression and Lasso in Python",
          "Heteroscedasticity"
        ],
        "Introduction to the classification Models": [
          "Three classification models and Data set",
          "Importing the data into Python",
          "The problem statements",
          "Why can't we use Linear Regression?"
        ],
        "Logistic Regression": [
          "Logistic Regression",
          "Training a Simple Logistic Model in Python",
          "Result of Simple Logistic Regression",
          "Logistic with multiple predictors",
          "Training multiple predictor Logistic model in Python",
          "Confusion Matrix",
          "Creating Confusion Matrix in Python",
          "Evaluating performance of model",
          "Evaluating model performance in Python"
        ],
        "Linear Discriminant Analysis (LDA)": [
          "Linear Discriminant Analysis",
          "LDA in Python"
        ]
      },
      "requirements": [
        "Students will need to install Anaconda software but we have a separate lecture to guide you install the same"
      ],
      "description": "You're looking for a complete Machine Learning course in Python that can help you launch a flourishing career in the field of Data Science and Machine Learning, right?\nYou've found the right Machine Learning course!\nAfter completing this course, you will be able to:\n· Confidently build predictive Machine Learning models using Python to solve business problems and create business strategy\n· Answer Machine Learning related interview questions\n· Participate and perform in online Data Analytics competitions such as Kaggle competitions\nCheck out the table of contents below to see what all Machine Learning models you are going to learn.\nHow will this course help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning, Python and predictive modelling in Real world problems of business, this course will give you a solid base for that by teaching you the most popular techniques of machine learning, Python and predictive modelling.\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through linear regression. This course will give you an in-depth understanding of machine learning and predictive modelling techniques using Python.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques using R, Python, and we have used our experience to include the practical aspects of data analysis in this course.\nWe are also the creators of some of the most popular online courses - with over 1 million enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, machine learning, Python, predictive modelling, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts of machine learning, Python and predictive modelling. Each section contains a practice assignment for you to practically implement your learning on machine learning, Python and predictive modelling.\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 3 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of models - Fifth and sixth section cover Classification models and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey"
      ]
    },
    {
      "title": "LLMOps Masterclass 2025 - Generative AI - MLOps - AIOps",
      "url": "https://www.udemy.com/course/llmops-masterclass-generative-ai-mlops-aiops/",
      "bio": "Unlock the Future: Mastering Generative AI, MLOps, AIOps - LLMOps with Open AI and Hugging Face Models Deploy to Prod",
      "objectives": [
        "Gain a deep understanding of Generative AI, including its impact on daily life and real-world applications.",
        "Explore fundamental concepts such as AI levels, types, and the difference between generative and discriminative models.",
        "Learn about Prompt Engineering, including its architecture, components, and techniques for prompt generation.",
        "Understand the technical details of Language Model (LLM), its training process, and its enterprise applications.",
        "Develop hands-on experience by building LLM applications using ChatGPT and Hugging Face Library.",
        "Master the art of packaging and deploying AI applications using technologies such as FastAPI, Docker, and Kubernetes.",
        "Implement continuous integration and continuous deployment (CI/CD) pipelines using GitHub Actions, ensuring efficient project management.",
        "Explore monitoring techniques for LLM models in production, ensuring their reliability and performance.",
        "Acquire essential LLMOps basics, including version control systems, Git setup, and CICD demonstrations.",
        "Prepare for industry standards and best practices in AI development and operations, ensuring readiness for real-world challenges."
      ],
      "course_content": {
        "Introduction to Course": [
          "Welcome to MLOps Journey",
          "Source Code and Slides Access",
          "Introduction to LLM Ops with Prompt Engineering",
          "Connect with Instructor"
        ],
        "Navigating the Generative AI Tsunami": [
          "Impact of Generative AI in Day to Day Life",
          "Quiz - Impact of Generative AI in Day to Day Life",
          "How do you Propose to Increase the Sales for an E-Commerce Company ?",
          "What is AI ?",
          "Quiz - What is AI ?",
          "Man Vs Machines",
          "Levels of AI",
          "Levels of AI - Quiz",
          "Types of AI",
          "Example Use Case - Product for Each Type of AI System",
          "Knowledge Check"
        ],
        "Getting Started with Generative AI": [
          "What is Generative AI",
          "What is Generative AI ?",
          "Generative vs Discriminative Models",
          "Generative vs Discriminative Models",
          "Real World Applications of Generative AI",
          "Quiz - Real World Applications of Generative AI"
        ],
        "Prompt Engineering": [
          "Why World has the widespread attention ?",
          "Introduction to Prompt Engineering",
          "Quiz - Prompt Engineering",
          "Architecture and Component of a Prompt",
          "Strategies and Techniques of Prompt Generation"
        ],
        "Technical Details of LLM": [
          "What is LLM , and what is the idea of LLM ?",
          "How LLM is Trained ?",
          "How LLM is used in Enterprise"
        ],
        "Project 1 - Building LLM Application using ChatGPT": [
          "Source code",
          "Pre-Requisites",
          "Open AI Platform Quick Intro",
          "Create Open AI Assistant using Assistants API - Platform",
          "Create AI Assistant with Python"
        ],
        "Packaging the AI/ LLM Application": [
          "Introduction to FastAPI",
          "Packaging the AI Application",
          "Test with Postman",
          "Create Requirements.txt",
          "Introduction to Docker",
          "Docker Installation",
          "Docker Quickstart",
          "Build Docker Image for Project 1"
        ],
        "Deploying the Container Application with Kubernetes": [
          "Introducing Kubernetes",
          "Architecture of Kubernetes",
          "Installing Kubernetes",
          "Running the Application on Kubernetes",
          "Create Service Definition for Kubernetes",
          "Kubernetes Deployment and Deployment Controller",
          "Scaling the Application",
          "Performing the Rolling Update",
          "Config Maps",
          "Hands On - Config Maps",
          "Kubernetes Secrets",
          "Summary of Kubernetes Learning",
          "Implementing the Kubernetes Orchestration for our containers"
        ],
        "Github Actions": [
          "Introduction to GitHub Actions",
          "Quick Demo on github actions YAML file",
          "Understanding github Actions YAML file",
          "Create github Actions from Scratch",
          "Configure Workflow based on use case"
        ],
        "Setting Up Kubernetes on Google Cloud": [
          "Create Google Cloud Account",
          "Setting up the Google CLI",
          "Create Kubernetes cluster with GKE",
          "Testing the Deployment file with Kubernetes Config file on GKE Cluster",
          "Quick Word on Running in MacOS"
        ]
      },
      "requirements": [
        "Basic Understanding of Artificial Intelligence: Familiarity with fundamental concepts of artificial intelligence, including machine learning and deep learning, will provide a strong foundation for the course.",
        "Programming Proficiency: Proficiency in at least one programming language such as Python is highly recommended, as many practical exercises and projects will involve coding.",
        "Knowledge of Command Line Interface (CLI): Basic understanding of working with the command line interface will be beneficial for executing commands and managing applications throughout the course.",
        "Familiarity with Git: Understanding the basics of version control systems and Git operations will be helpful for managing project repositories and collaborating with peers.",
        "Cloud Computing Basics: A basic understanding of cloud computing concepts, particularly with platforms like Google Cloud Platform (GCP), will be beneficial for deploying applications and working with Kubernetes.",
        "While not mandatory, having these prerequisites will enhance the learning experience and ensure students can fully engage with the course materials and practical exercises. Additionally, a strong willingness to learn and explore new technologies is essential for success in the LLMOps Masterclass 2024."
      ],
      "description": "Unlock the potential of Generative AI with our comprehensive course, \"LLMOps - Generative AI - MLOps - AIOps Masterclass 2025\" From understanding the fundamentals to deploying advanced applications, this course equips you with the knowledge and skills to thrive in the era of artificial intelligence.\n\n\nHere's how your learning journey look like (Section wise) :\n\n\nIntroduction to Course: Dive into the world of LLM Ops with \"Introduction to LLM Ops with Prompt Engineering.\" Gain insights into the foundations of LLM Operations and the significance of Prompt Engineering.\nNavigating the Generative AI Tsunami: Explore the profound impact of Generative AI on everyday life. From understanding AI fundamentals to exploring its diverse applications, equip yourself with essential knowledge through modules such as \"Impact of Generative AI in Day to Day Life\" and \"Real World Applications of Generative AI.\"\nGetting Started with Generative AI: Delve deeper into Generative AI concepts with modules covering topics like \"Generative vs Discriminative Models\" and \"Real World Applications of Generative AI.\" Get hands-on experience and unlock the potential of this transformative technology.\nPrompt Engineering: Uncover the secrets behind Prompt Engineering and understand its widespread attention in the world. Learn about the architecture, components, strategies, and techniques of Prompt Generation through comprehensive modules tailored for practical implementation.\nTechnical Details of LLM: Gain a profound understanding of LLM and its underlying principles. Explore topics such as LLM training, enterprise applications, and the idea behind LLM through detailed modules designed to enhance your technical expertise.\nProject 1 - Building LLM Application using ChatGPT: Put your knowledge into action by embarking on a project to build an LLM application using ChatGPT. From prerequisites to deployment, this project will guide you through every step of the process, ensuring hands-on learning.\nPackaging the AI/ LLM Application: Learn to package and deploy AI applications efficiently with modules covering FastAPI, Docker, and more. Master the art of containerization and streamline your deployment process with industry-standard practices.\nDeploying the Container Application with Kubernetes: Discover the power of Kubernetes in deploying and orchestrating containerized applications. From installation to scaling, learn the ins and outs of Kubernetes deployment and enhance your proficiency in container management.\nGithub Actions: Explore the capabilities of GitHub Actions in automating workflows and enhancing collaboration. From introduction to implementation, master the art of configuring workflows tailored to your specific use cases.\nSetting Up Kubernetes on Google Cloud: Unlock the potential of Google Cloud Platform for Kubernetes deployment. From setting up your account to testing deployment files, gain practical insights into running applications on GKE clusters.\nImplement CI/CD with Github Actions - GKE: Optimize your development pipeline with continuous integration and continuous deployment. Learn to configure GitHub Secrets, adhere to industry standards, and streamline your deployment process for seamless project management.\nIntroducing Hugging Face Library: Discover the versatility of the Hugging Face Library in building AI applications. From text classification to finetuning models, explore the vast possibilities offered by this powerful toolkit.\nProject 2 - Building Generative AI App using Hugging Face: Put your Hugging Face skills to the test with a project focused on building a Generative AI application. From understanding text generation pipelines to setting up CI/CD pipelines, elevate your expertise in AI development.\nMonitoring of LLM Models in Production: Ensure the reliability and performance of LLM models in production with monitoring techniques. Explore platforms like WhyLabs and Langkit to gain insights into monitoring and optimizing LLM applications.\nLLMOps Basics: Master the basics of LLM Ops with modules covering version control systems, Git setup, and CICD demonstrations. Strengthen your foundation in LLM Ops and prepare yourself for advanced concepts.\nEmbark on your journey to mastering LLM Ops and stay ahead in the ever-evolving landscape of artificial intelligence. Join us today and unlock a world of endless possibilities.",
      "target_audience": [
        "AI Enthusiasts: Individuals passionate about artificial intelligence and eager to explore advanced topics such as Generative AI and MLOps will find this course valuable in expanding their expertise.",
        "Data Scientists and Machine Learning Engineers: Professionals working in data science and machine learning roles who seek to deepen their understanding of AI operations, including model deployment, monitoring, and optimization, will benefit from this course.",
        "Software Engineers: Developers interested in incorporating AI technologies into their applications and understanding the operational aspects of AI model deployment and management will find this course highly relevant.",
        "AI Researchers: Researchers aiming to enhance their understanding of practical AI deployment and operations, particularly in the context of Generative AI, will gain valuable insights from this course.",
        "IT Professionals and DevOps Engineers: Professionals involved in IT operations and DevOps who wish to expand their skill set to include AI Ops and cloud-native technologies like Kubernetes will find this course beneficial for career advancement.",
        "Entrepreneurs and Innovators: Individuals seeking to leverage AI technologies to innovate and develop new products and services will gain valuable knowledge and practical skills for building and deploying AI applications."
      ]
    },
    {
      "title": "Machine Learning : A Beginner's Basic Introduction",
      "url": "https://www.udemy.com/course/machine-learning-a-beginners-basic-introduction/",
      "bio": "Learn Machine Learning Basics with a Practical Example",
      "objectives": [
        "Install environment to test Machine learning",
        "Understand basic machine learning vocabulary",
        "Exposure to Machine Learning Frameworks",
        "Understand Supervised Machine Learning",
        "Create a basic home estimator calculator",
        "Load a Dataset",
        "Make Predictions from dataset"
      ],
      "course_content": {},
      "requirements": [
        "You should be able to use a PC at beginner level",
        "Basic knowledge of Python would help but not mandatory"
      ],
      "description": "Machine learning relates to many different ideas, programming languages, frameworks. Machine learning is difficult to define in just a sentence or two. But essentially, machine learning is giving a computer the ability to write its own rules or algorithms and learn about new things, on its own. In this course, we'll explore some basic machine learning concepts and load data to make predictions.\nValue estimation—one of the most common types of machine learning algorithms—can automatically estimate values by looking at related information. For example, a website can determine how much a house is worth based on the property's location and characteristics.\nIn this course, we will  use machine learning to build a value estimation system that can deduce the value of a home.   Although the tool  we will build in this course focuses on real estate, you can use the same approach to solve any kind of value estimation.\nWhat you'll learn include:\n\nBasic concepts in machine learning\nSupervised versus Unsupervised learning\nMachine learning frameworks\nMachine learning using Python and scikit-learn\nLoading sample dataset\nMaking predictions based on dataset\nSetting up the development environment\nBuilding a simple home value estimator\nThe examples in this course are basic but should give you a solid understanding of the power of machine learning and how it works.",
      "target_audience": [
        "Absolute beginners to Machine Learning"
      ]
    },
    {
      "title": "Unsupervised Machine Learning Hidden Markov Models in Python",
      "url": "https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python/",
      "bio": "HMMs for stock price analysis, language modeling, web analytics, biology, and PageRank.",
      "objectives": [
        "Understand and enumerate the various applications of Markov Models and Hidden Markov Models",
        "Understand how Markov Models work",
        "Write a Markov Model in code",
        "Apply Markov Models to any sequence of data",
        "Understand the mathematics behind Markov chains",
        "Apply Markov models to language",
        "Apply Markov models to website analytics",
        "Understand how Google's PageRank works",
        "Understand Hidden Markov Models",
        "Write a Hidden Markov Model in Code",
        "Write a Hidden Markov Model using Theano",
        "Understand how gradient descent, which is normally used in deep learning, can be used for HMMs"
      ],
      "course_content": {
        "Introduction and Outline": [
          "Introduction and Outline: Why would you want to use an HMM?",
          "Where to get the Code and Data",
          "How to Succeed in this Course"
        ],
        "Markov Models": [
          "The Markov Property",
          "Markov Models",
          "Probability Smoothing and Log-Space",
          "The Math of Markov Chains"
        ],
        "Markov Models: Example Problems and Applications": [
          "Example Problem: Sick or Healthy",
          "Example Problem: Expected number of continuously sick days",
          "Example application: SEO and Bounce Rate Optimization",
          "Example Application: Build a 2nd-order language model and generate phrases",
          "Example Application: Google’s PageRank algorithm",
          "Suggestion Box"
        ],
        "Hidden Markov Models for Discrete Observations": [
          "From Markov Models to Hidden Markov Models",
          "HMM - Basic Examples",
          "Parameters of an HMM",
          "The 3 Problems of an HMM",
          "The Forward-Backward Algorithm (part 1)",
          "The Forward-Backward Algorithm (part 2)",
          "The Forward-Backward Algorithm (part 3)",
          "The Viterbi Algorithm (part 1)",
          "The Viterbi Algorithm (part 2)",
          "HMM Training (part 1)",
          "HMM Training (part 2)",
          "HMM Training (part 3)",
          "HMM Training (part 4)",
          "How to Choose the Number of Hidden States",
          "Baum-Welch Updates for Multiple Observations",
          "Discrete HMM in Code",
          "The underflow problem and how to solve it",
          "Discrete HMM Updates in Code with Scaling",
          "Scaled Viterbi Algorithm in Log Space"
        ],
        "Discrete HMMs Using Deep Learning Libraries": [
          "Gradient Descent Tutorial",
          "Theano Scan Tutorial",
          "Discrete HMM in Theano",
          "Improving our Gradient Descent-Based HMM",
          "Tensorflow Scan Tutorial",
          "Discrete HMM in Tensorflow"
        ],
        "HMMs for Continuous Observations": [
          "Gaussian Mixture Models with Hidden Markov Models",
          "Generating Data from a Real-Valued HMM",
          "Continuous-Observation HMM in Code (part 1)",
          "Continuous-Observation HMM in Code (part 2)",
          "Continuous HMM in Theano",
          "Continuous HMM in Tensorflow"
        ],
        "HMMs for Classification": [
          "Unsupervised or Supervised?",
          "Generative vs. Discriminative Classifiers",
          "HMM Classification on Poetry Data (Robert Frost vs. Edgar Allan Poe)"
        ],
        "Bonus Example: Parts-of-Speech Tagging": [
          "Parts-of-Speech Tagging Concepts",
          "POS Tagging with an HMM"
        ],
        "Theano, Tensorflow, and Machine Learning Basics Review": [
          "(Review) Gaussian Mixture Models",
          "(Review) Theano Tutorial",
          "(Review) Tensorflow Tutorial"
        ],
        "Appendix / FAQ Finale": [
          "What is the Appendix?"
        ]
      },
      "requirements": [
        "Familiarity with probability and statistics",
        "Understand Gaussian mixture models",
        "Be comfortable with Python and Numpy"
      ],
      "description": "The Hidden Markov Model or HMM is all about learning sequences.\nA lot of the data that would be very useful for us to model is in sequences. Stock prices are sequences of prices. Language is a sequence of words. Credit scoring involves sequences of borrowing and repaying money, and we can use those sequences to predict whether or not you’re going to default. In short, sequences are everywhere, and being able to analyze them is an important skill in your data science toolbox.\nThe easiest way to appreciate the kind of information you get from a sequence is to consider what you are reading right now. If I had written the previous sentence backwards, it wouldn’t make much sense to you, even though it contained all the same words. So order is important.\nWhile the current fad in deep learning is to use recurrent neural networks to model sequences, I want to first introduce you guys to a machine learning algorithm that has been around for several decades now - the Hidden Markov Model.\nThis course follows directly from my first course in Unsupervised Machine Learning for Cluster Analysis, where you learned how to measure the probability distribution of a random variable. In this course, you’ll learn to measure the probability distribution of a sequence of random variables.\nYou guys know how much I love deep learning, so there is a little twist in this course. We’ve already covered gradient descent and you know how central it is for solving deep learning problems. I claimed that gradient descent could be used to optimize any objective function. In this course I will show you how you can use gradient descent to solve for the optimal parameters of an HMM, as an alternative to the popular expectation-maximization algorithm.\nWe’re going to do it in Theano and Tensorflow, which are popular libraries for deep learning. This is also going to teach you how to work with sequences in Theano and Tensorflow, which will be very useful when we cover recurrent neural networks and LSTMs.\nThis course is also going to go through the many practical applications of Markov models and hidden Markov models. We’re going to look at a model of sickness and health, and calculate how to predict how long you’ll stay sick, if you get sick. We’re going to talk about how Markov models can be used to analyze how people interact with your website, and fix problem areas like high bounce rate, which could be affecting your SEO. We’ll build language models that can be used to identify a writer and even generate text - imagine a machine doing your writing for you. HMMs have been very successful in natural language processing or NLP.\nWe’ll look at what is possibly the most recent and prolific application of Markov models - Google’s PageRank algorithm. And finally we’ll discuss even more practical applications of Markov models, including generating images, smartphone autosuggestions, and using HMMs to answer one of the most fundamental questions in biology - how is DNA, the code of life, translated into physical or behavioral attributes of an organism?\nAll of the materials of this course can be downloaded and installed for FREE. We will do most of our work in Numpy and Matplotlib, along with a little bit of Theano. I am always available to answer your questions and help you along your data science journey.\nThis course focuses on \"how to build and understand\", not just \"how to use\". Anyone can learn to use an API in 15 minutes after reading some documentation. It's not about \"remembering facts\", it's about \"seeing for yourself\" via experimentation. It will teach you how to visualize what's happening in the model internally. If you want more than just a superficial look at machine learning models, this course is for you.\n\nSee you in class!\n\n\n\"If you can't implement it, you don't understand it\"\nOr as the great physicist Richard Feynman said: \"What I cannot create, I do not understand\".\nMy courses are the ONLY courses where you will learn how to implement machine learning algorithms from scratch\nOther courses will teach you how to plug in your data into a library, but do you really need help with 3 lines of code?\nAfter doing the same thing with 10 datasets, you realize you didn't learn 10 things. You learned 1 thing, and just repeated the same 3 lines of code 10 times...\n\n\nSuggested Prerequisites:\ncalculus\nlinear algebra\nprobability\nBe comfortable with the multivariate Gaussian distribution\nPython coding: if/else, loops, lists, dicts, sets\nNumpy coding: matrix and vector operations, loading a CSV file\n\n\nWHAT ORDER SHOULD I TAKE YOUR COURSES IN?:\nCheck out the lecture \"Machine Learning and AI Prerequisite Roadmap\" (available in the FAQ of any of my courses, including the free Numpy course)",
      "target_audience": [
        "Students and professionals who do data analysis, especially on sequence data",
        "Professionals who want to optimize their website experience",
        "Students who want to strengthen their machine learning knowledge and practical skillset",
        "Students and professionals interested in DNA analysis and gene expression",
        "Students and professionals interested in modeling language and generating text from a model"
      ]
    },
    {
      "title": "Logistic Regression in Python",
      "url": "https://www.udemy.com/course/machine-learning-basics-classification-models-in-python/",
      "bio": "Logistic regression in Python tutorial for beginners. You can do Predictive modeling using Python after this course.",
      "objectives": [
        "Understand how to interpret the result of Logistic Regression model in Python and translate them into actionable insight",
        "Learn the linear discriminant analysis and K-Nearest Neighbors technique in Python",
        "Preliminary analysis of data using Univariate analysis before running classification model",
        "Predict future outcomes basis past data by implementing Machine Learning algorithm",
        "Indepth knowledge of data collection and data preprocessing for Machine Learning logistic regression problem",
        "Learn how to solve real life problem using the different classification techniques",
        "Course contains a end-to-end DIY project to implement your learnings from the lectures",
        "Basic statistics using Numpy library in Python",
        "Data representation using Seaborn library in Python",
        "Classification techniques of Machine Learning using Scikit Learn and Statsmodel libraries of Python"
      ],
      "course_content": {},
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Classification modeling course that teaches you everything you need to create a Classification model in Python, right?\nYou've found the right Classification modeling course!\nAfter completing this course you will be able to:\nIdentify the business problem which can be solved using Classification modeling techniques of Machine Learning.\nCreate different Classification modelling model in Python and compare their performance.\nConfidently practice, discuss and understand Machine Learning concepts\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning basics course.\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you the most popular Classification techniques of machine learning, such as Logistic Regression, Linear Discriminant Analysis and KNN\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem using classification techniques.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\nWhat is covered in this course?\nThis course teaches you all the steps of creating a classification model, which is the most popular Machine Learning model, to solve business problems.\nBelow are the course contents of this course on Classification Machine Learning models:\nSection 1 - Basics of Statistics\nThis section is divided into five different lectures starting from types of data then types of statistics\nthen graphical representations to describe the data and then a lecture on measures of center like mean\nmedian and mode and lastly measures of dispersion like range and standard deviation\nSection 2 - Python basic\nThis section gets you started with Python.\nThis section will help you set up the python and Jupyter environment on your system and it'll teach\nyou how to perform some basic operations in Python. We will understand the importance of different libraries such as Numpy, Pandas & Seaborn.\nSection 3 - Introduction to Machine Learning\nIn this section we will learn - What does Machine Learning mean. What are the meanings or different terms associated with machine learning? You will see some examples so that you understand what machine learning actually is. It also contains steps involved in building a machine learning model, not just linear models, any machine learning model.\nSection 4 - Data Pre-processing\nIn this section you will learn what actions you need to take a step by step to get the data and then prepare it for the analysis these steps are very important.\nWe start with understanding the importance of business knowledge then we will see how to do data exploration. We learn how to do uni-variate analysis and bi-variate analysis then we cover topics like outlier treatment and missing value imputation.\nSection 5 - Classification Models\nThis section starts with Logistic regression and then covers Linear Discriminant Analysis and K-Nearest Neighbors.\nWe have covered the basic theory behind each concept without getting too mathematical about it so that you\nunderstand where the concept is coming from and how it is important. But even if you don't understand\nit,  it will be okay as long as you learn how to run and interpret the result as taught in the practical lectures.\nWe also look at how to quantify models performance using confusion matrix, how categorical variables in the independent variables dataset are interpreted in the results, test-train split and how do we finally interpret the result to find out the answer to a business problem.\nBy the end of this course, your confidence in creating a classification model in Python will soar. You'll have a thorough understanding of how to use Classification modelling to create predictive models and solve business problems.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy\n\n\n------------\nBelow is a list of popular FAQs of students who want to start their Machine learning journey-\nWhat is Machine Learning?\nMachine Learning is a field of computer science which gives the computer the ability to learn without being explicitly programmed. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\nWhich all classification techniques are taught in this course?\nIn this course we learn both parametric and non-parametric classification techniques. The primary focus will be on the following three techniques:\nLogistic Regression\nLinear Discriminant Analysis\nK - Nearest Neighbors (KNN)\nHow much time does it take to learn Classification techniques of machine learning?\nClassification is easy but no one can determine the learning time it takes. It totally depends on you. The method we adopted to help you learn classification starts from the basics and takes you to advanced level within hours. You can follow the same, but remember you can learn nothing without practicing it. Practice is the only way to remember whatever you have learnt. Therefore, we have also provided you with another data set to work on as a separate project of classification.\nWhat are the steps I should follow to be able to build a Machine Learning model?\nYou can divide your learning process into 3 parts:\nStatistics and Probability - Implementing Machine learning techniques require basic knowledge of Statistics and probability concepts. Second section of the course covers this part.\nUnderstanding of Machine learning - Fourth section helps you understand the terms and concepts associated with Machine learning and gives you the steps to be followed to build a machine learning model\nProgramming Experience - A significant part of machine learning is programming. Python and R clearly stand out to be the leaders in the recent days. Third section will help you set up the Python environment and teach you some basic operations. In later sections there is a video on how to implement each concept taught in theory lecture in Python\nUnderstanding of  models - Fifth and sixth section cover Classification models and with each theory lecture comes a corresponding practical lecture where we actually run each query with you.\nWhy use Python for Machine Learning?\nUnderstanding Python is one of the valuable skills needed for a career in Machine Learning.\nThough it hasn’t always been, Python is the programming language of choice for data science. Here’s a brief history:\nIn 2016, it overtook R on Kaggle, the premier platform for data science competitions.\nIn 2017, it overtook R on KDNuggets’s annual poll of data scientists’ most used tools.\nIn 2018, 66% of data scientists reported using Python daily, making it the number one tool for analytics professionals.\nMachine Learning experts expect this trend to continue with increasing development in the Python ecosystem. And while your journey to learn Python programming may be just beginning, it’s nice to know that employment opportunities are abundant (and growing) as well.\nWhat is the difference between Data Mining, Machine Learning, and Deep Learning?\nPut simply, machine learning and data mining use the same algorithms and techniques as data mining, except the kinds of predictions vary. While data mining discovers previously unknown patterns and knowledge, machine learning reproduces known patterns and knowledge—and further automatically applies that information to data, decision-making, and actions.\nDeep learning, on the other hand, uses advanced computing power and special types of neural networks and applies them to large amounts of data to learn, understand, and identify complicated patterns. Automatic language translation and medical diagnoses are examples of deep learning.",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master classification machine learning techniques from Beginner to Advanced in short span of time"
      ]
    },
    {
      "title": "Support Vector Machines in Python: SVM Concepts & Code",
      "url": "https://www.udemy.com/course/machine-learning-adv-support-vector-machines-svm-python/",
      "bio": "Learn Support Vector Machines in Python. Covers basic SVM models to Kernel-based advanced SVM models of Machine Learning",
      "objectives": [
        "Get a solid understanding of Support Vector Machines (SVM)",
        "Understand the business scenarios where Support Vector Machines (SVM) is applicable",
        "Tune a machine learning model's hyperparameters and evaluate its performance.",
        "Use Support Vector Machines (SVM) to make predictions",
        "Implementation of SVM models in Python"
      ],
      "course_content": {},
      "requirements": [
        "Students will need to install Python and Anaconda software but we have a separate lecture to help you install the same"
      ],
      "description": "You're looking for a complete Support Vector Machines course that teaches you everything you need to create a Support Vector Machines model in Python, right?\nYou've found the right Support Vector Machines techniques course!\nHow this course will help you?\nA Verifiable Certificate of Completion is presented to all students who undertake this Machine learning advanced course.\nIf you are a business manager or an executive, or a student who wants to learn and apply machine learning in Real world problems of business, this course will give you a solid base for that by teaching you some of the advanced technique of machine learning, which are Support Vector Machines.\nWhy should you choose this course?\nThis course covers all the steps that one should take while solving a business problem through Decision tree.\nMost courses only focus on teaching how to run the analysis but we believe that what happens before and after running analysis is even more important i.e. before running analysis it is very important that you have the right data and do some pre-processing on it. And after running analysis, you should be able to judge how good your model is and interpret the results to actually be able to help your business.\nWhat makes us qualified to teach you?\nThe course is taught by Abhishek and Pukhraj. As managers in Global Analytics Consulting firm, we have helped businesses solve their business problem using machine learning techniques and we have used our experience to include the practical aspects of data analysis in this course\nWe are also the creators of some of the most popular online courses - with over 150,000 enrollments and thousands of 5-star reviews like these ones:\nThis is very good, i love the fact the all explanation given can be understood by a layman - Joshua\nThank you Author for this wonderful course. You are the best and this course is worth any price. - Daisy\nOur Promise\nTeaching our students is our job and we are committed to it. If you have any questions about the course content, practice sheet or anything related to any topic, you can always post a question in the course or send us a direct message.\nDownload Practice files, take Quizzes, and complete Assignments\nWith each lecture, there are class notes attached for you to follow along. You can also take quizzes to check your understanding of concepts. Each section contains a practice assignment for you to practically implement your learning.\n\n\nGo ahead and click the enroll button, and I'll see you in lesson 1!\n\n\nCheers\nStart-Tech Academy",
      "target_audience": [
        "People pursuing a career in data science",
        "Working Professionals beginning their Data journey",
        "Statisticians needing more practical experience",
        "Anyone curious to master SVM technique from Beginner to Advanced in short span of time"
      ]
    },
    {
      "title": "Data Science Projects: Data Analysis, ML, and Deployment",
      "url": "https://www.udemy.com/course/data-science-projects-3/",
      "bio": "Hands-On Data Science Projects: Streamlit & FastAPI for Real-World ML Model Deployment and Visualization",
      "objectives": [
        "Data Analysis and Visualization: Learn to analyze data and create visualizations with Pandas and Matplotlib",
        "Machine Learning Fundamentals: Build, train, and evaluate ML models, mastering algorithms and their real-world applications to solve data problems.",
        "Feature Engineering: Learn feature engineering to create impactful features, improving model performance and prediction accuracy effectively.",
        "Streamlit for Interactive Apps: Develop interactive web apps with Streamlit to deploy and visualize ML models, offering a user-friendly experience.",
        "FastAPI for Model Deployment: Create scalable APIs with FastAPI to deploy ML models, enabling real-time predictions and seamless integration.",
        "End-to-End Project Workflow: Experience the full data science lifecycle from data cleaning and exploration to model deployment with hands-on projects."
      ],
      "course_content": {
        "Salary Prediction Project - Streamlit": [
          "Dataset",
          "Salary Prediction Project Part 1",
          "Salary Prediction Project Part 2"
        ],
        "Scholarship Prediction Project - FastAPI": [
          "Dataset",
          "Scholarship Prediction Project"
        ],
        "Bike Purchasement Prediction Project - Streamlit": [
          "Dataset",
          "Bike Purchasement Prediction Project"
        ],
        "Bonus Section": [
          "bonus lecture"
        ]
      },
      "requirements": [
        "Basic Python Knowledge",
        "Understanding of Basic Data Science Fundamentals"
      ],
      "description": "Welcome to Data Science Projects - Hands-On Projects with Streamlit and FastAPI! This course is designed for aspiring data scientists who want to elevate their skills through practical, real-world projects. Dive into the exciting world of data science with three comprehensive projects that cover every essential aspect: data analysis, data cleaning, data visualization, feature engineering, and machine learning.\nIn this course, you will:\nAnalyze and Clean Data: Learn techniques for effective data cleaning and exploratory data analysis to uncover insights.\nVisualize Data: Create impactful visualizations to tell compelling data stories.\nEngineer Features: Understand feature engineering principles to enhance your models' performance.\nBuild and Evaluate ML Models: Develop, train, and evaluate machine learning models using popular algorithms.\nAdditionally, you will gain hands-on experience with:\nStreamlit: Build interactive web applications to showcase your machine learning models in two projects.\nFastAPI: Create a robust and scalable API for deploying your machine learning model in one project.\nBy the end of this course, you will have built three portfolio-worthy projects, demonstrating your ability to handle end-to-end data science processes and deploy machine learning models in real-time applications. Whether you are a beginner or an intermediate data scientist, this course will provide you with the practical skills needed to excel in the data science field. Join us and transform your data science expertise today!",
      "target_audience": [
        "Aspiring Data Scientists",
        "Intermediate Data Analysts",
        "Machine Learning Enthusiasts",
        "Python Developers",
        "Students in Data Science"
      ]
    },
    {
      "title": "A Crash Course In PySpark",
      "url": "https://www.udemy.com/course/a-crash-course-in-pyspark/",
      "bio": "Learn all the fundamentals of PySpark",
      "objectives": [
        "PySpark, Apache Spark, Big Data Analytics, Big Data Processing, Python"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "How is this course structured"
        ],
        "A Scenario To Get Us Started": [
          "Introduction to our development environment",
          "Introduction to our dataset & dataframes",
          "Latest Config Code",
          "Environment configuration code (latest code in downloadable file)",
          "Ingesting & Cleaning Data",
          "Answering our scenario questions"
        ],
        "Core Concepts": [
          "Bringing data into dataframes",
          "Inspecting A Dataframe",
          "Handling Null & Duplicate Values",
          "Selecting & Filtering Data",
          "Applying Multiple Filters",
          "Running SQL on Dataframes",
          "Adding Calculated Columns",
          "Group By And Aggregation",
          "Writing Dataframe To Files"
        ],
        "Challenge": [
          "Challenge Overview",
          "Challenge Solution"
        ],
        "Conclusion": [
          "Thanks for joining me to learn PySpark!"
        ]
      },
      "requirements": [
        "Python Familiarity, which can be learned through my 'No Nonsense Python' course"
      ],
      "description": "Spark is one of the most in-demand Big Data processing frameworks right now.\n\n\nThis course will take you through the core concepts of PySpark. We will work to enable you to do most of the things you’d do in SQL or Python Pandas library, that is:\nGetting hold of data\nHandling missing data and cleaning data up\nAggregating your data\nFiltering it\nPivoting it\nAnd Writing it back\nAll of these things will enable you to leverage Spark on large datasets and start getting value from your data.\nLet’s get started.",
      "target_audience": [
        "People wanting to leverage their big data with Spark"
      ]
    },
    {
      "title": "Statistics / Data Analysis in SPSS: Inferential Statistics",
      "url": "https://www.udemy.com/course/inferential-statistics-spss/",
      "bio": "Increase Your Data Analytic Skills – Highly Valued And Sought After By Employers",
      "objectives": [
        "In this course, you will gain proficiency in how to analyze a number of statistical procedures in SPSS.",
        "You will learn how to interpret the output of a number of different statistical tests",
        "Learn how to write the results of statistical analyses using APA format"
      ],
      "course_content": {},
      "requirements": [
        "Introduction to statistics course (either currently taking or already have completed) is recommended but not absolutely necessary",
        "Access to IBM SPSS Statistical software (strongly recommended)"
      ],
      "description": "November, 2019.\nJoin more than 1,000 students and get instant access to this best-selling content - enroll today!\nGet marketable and highly sought after skills in this course that will substantially increase your knowledge of data analytics, with a focus in the area of significance testing, an important tool for A/B testing and product assessment.\nMany tests covered, including three different t tests, two ANOVAs, post hoc tests, chi-square tests (great for A/B testing), correlation, and regression. Database management also covered!\nTwo in-depth examples provided of each test for additional practice.\nThis course is great for professionals, as it provides step by step instruction of tests with clear and accurate explanations. Get ahead of the competition and make these tests important parts of your data analytic toolkit!\nStudents will also have the tools needed to succeed in their statistics and experimental design courses.\n\nData Analytics is an rapidly growing area in high demand (e.g., McKinsey)\n\nStatistics play a key role in the process of making sound business decisions that will generate higher profits. Without statistics, it's difficult to determine what your target audience wants and needs.\nInferential statistics, in particular, help you understand a population's needs better so that you can provide attractive products and services.\nThis course is designed for business professionals who want to know how to analyze data. You'll learn how to use IBM SPSS to draw accurate conclusions on your research and make decisions that will benefit your customers and your bottom line.\nUse Tests in SPSS to Correctly Analyze Inferential Statistics\nUse the One Sample t Test to Draw Conclusions about a Population\nUnderstand ANOVA and the Chi Square\nMaster Correlation and Regression\nLearn Data Management Techniques\nAnalyze Research Results Accurately to Make Better Business Decisions\nWith SPSS, you can analyze data to make the right business decisions for your customer base. And by understanding how to use inferential statistics, you can draw accurate conclusions about a large group of people, based on research conducted on a sample of that population.\nThis easy-to-follow course, which contains illustrative examples throughout, will show you how to use tests to assess if the results of your research are statistically significant.\nYou'll be able to determine the appropriate statistical test to use for a particular data set, and you'll know how to understand, calculate, and interpret effect sizes and confidence intervals.\nYou'll even know how to write the results of statistical analyses in APA format, one of the most popular and accepted formats for presenting the results of statistical analyses, which you can successfully adapt to other formats as needed.\nContents and Overview\nThis course begins with a brief introduction before diving right into the One Sample t Test, Independent Samples t Test, and Dependent Samples t Test. You'll use these tests to analyze differences and similarities between sample groups in a population. This will help you determine if you need to change your business plan for certain markets of consumers.\nNext, you'll tackle how to use ANOVA (Analysis of Variance), including Post-hoc Tests and Levene's Equal Variance Test. These tests will also help you determine what drives consumer decisions and behaviors between different groups.\nWhen ready, you'll master correlation and regression, as well as the chi-square. As with all previous sections, you'll see illustrations of how to analyze a statistical test, and you'll access additional examples for more practice.\nFinally, you'll learn about data management in SPSS, including sorting and adding variables.\nBy the end of this course, you'll be substantially more confident in both IBM SPSS and statistics. You'll know how to use data to come to the right conclusions about your market.\nBy understanding how to use inferential statistics, you'll be able to identify consumer needs and come up with products and/or services that will address those needs effectively.\nJoin the over 1,000 students who have taken this best-selling course - enroll today!",
      "target_audience": [
        "Students seeking help with SPSS, especially how to analyze and interpret the results of statistical analyses",
        "Professionals desiring to augment their statistical skills",
        "Anyone seeking to increase their data analytic skills"
      ]
    },
    {
      "title": "Data Science : Complete Data Science & Machine Learning",
      "url": "https://www.udemy.com/course/complete-data-science-and-machine-learning-using-python/",
      "bio": "Learn and master the Data Science, Python for Machine Learning, Math for Machine Learning, Statistics for Data Science",
      "objectives": [
        "Learn Complete Data Science skillset required to be a Data Scientist with all the advance concepts",
        "Master Python Programming from Basics to advance as required for Data Science and Machine Learning",
        "Learn complete Mathematics of Linear Algebra, Calculus, Vectors, Matrices for Data Science and Machine Learning.",
        "Become an expert in Statistics including Descriptive and Inferential Statistics.",
        "Learn how to analyse the data using data visualization with all the necessary charts and plots",
        "Perform data Processing using Pandas and ScikitLearn",
        "Master Regression with all its parameters and assumptions",
        "Solve a Kaggle project and see how to achieve top 1 percentile",
        "Learn various classification algorithms such as Logistic Regression, Decision Tree, Random Forest, Support Vector Machines",
        "Get complete understanding of deep learning using Keras and Tensorflow",
        "Become the Pro by learning Feature Selection and Dimensionality Reduction"
      ],
      "course_content": {},
      "requirements": [
        "No prerequisites. I will teach right from basics in Python to Advanced Deep Learning",
        "Passion to deal with data analysis"
      ],
      "description": "Data Science and Machine Learning are the hottest skills in demand but challenging to learn. Did you wish that there was one course for Data Science and Machine Learning that covers everything from Math for Machine Learning, Advance Statistics for Data Science, Data Processing, Machine Learning A-Z, Deep learning and more?\nWell, you have come to the right place. This Data Science and Machine Learning course has 11 projects, 250+ lectures, more than 25+ hours of content, one Kaggle competition project with top 1 percentile score, code templates and various quizzes.\nWe are going to execute following real-life projects,\nKaggle Bike Demand Prediction from Kaggle competition\nAutomation of the Loan Approval process\nThe famous IRIS Classification\nAdult Income Predictions from US Census Dataset\nBank Telemarketing Predictions\nBreast Cancer Predictions\nPredict Diabetes using Prima Indians Diabetes Dataset\nToday Data Science and Machine Learning is used in almost all the industries, including automobile, banking, healthcare, media, telecom and others.\nAs the Data Science and Machine Learning practioner, you will have to research and look beyond normal problems, you may need to do extensive data processing. experiment with the data using advance tools and build amazing solutions for business. However, where and how are you going to learn these skills required for Data Science and Machine Learning?\nData Science and Machine Learning require in-depth knowledge of various topics. Data Science is not just about knowing certain packages/libraries and learning how to apply them. Data Science and Machine Learning require an indepth understanding of the following skills,\nUnderstanding of the overall landscape of Data Science and Machine Learning\nDifferent types of Data Analytics, Data Architecture, Deployment characteristics of Data Science and Machine Learning projects\nPython Programming skills which is the most popular language for Data Science and Machine Learning\nMathematics for Machine Learning including Linear Algebra, Calculus and how it is applied in Machine Learning Algorithms as well as Data Science\nStatistics and Statistical Analysis for Data Science\nData Visualization for Data Science\nData processing and manipulation before applying Machine Learning\nMachine Learning\nRidge (L2), Lasso (L1) and Elasticnet Regression/ Regularization for Machine Learning\nFeature Selection and Dimensionality Reduction for Machine Learning models\nMachine Learning Model Selection using Cross Validation and Hyperparameter Tuning\nCluster Analysis for unsupervised Machine Learning\nDeep Learning using most popular tools and technologies of today.\nThis Data Science and Machine Learning course has been designed considering all of the above aspects, the true Data Science and Machine Learning A-Z Course. In many Data Science and Machine Learning courses, algorithms are taught without teaching Python or such programming language. However, it is very important to understand the construct of the language in order to implement any discipline including Data Science and Machine Learning.\nAlso, without understanding the Mathematics and Statistics it's impossible to understand how some of the Data Science and Machine Learning algorithms and techniques work.\nData Science and Machine Learning is a complex set of topics which are interlinked. However, we firmly believe in what Einstein once said,\n\"If you can not explain it simply, you have not understood it enough.\"\nAs an instructor, I always try my level best to live up to this principle. This is one comprehensive course on Data Science and Machine Learning that teaches you everything required to learn Data Science and Machine Learning using the simplest examples with great depth.\nAs you will see from the preview lectures, some of the most complex topics are explained in a simple language.\nSome of the key skills you will learn,\nPython Programming\nPython has been ranked as the #1 language for Data Science and Machine Learning. It is easy to use and is rich with various libraries and functions required for performing various tasks for Data Science and Machine Learning. Moreover, it is the most preferred and default language of use for many Deep Learning frameworks including Tensorflow and Keras.\n\n\nAdvance Mathematics for Machine Learning\nMathematics is the very basis for Data Science in general and Machine Learning in particular. Without understanding the meanings of Vectors, Matrices, their operations as well as understanding Calculus, it is not possible to understand the foundation of the Data Science and Machine Learning. Gradient Descent which forms the very basis of Neural Network and Machine Learning is built upon the basics of Calculus and Derivatives.\n\n\nAdvance Statistics for Data Science\nIt is not enough to know only mean, median, mode etc. The advance techniques of Data Science and Machine Learning such as Feature Selection, Dimensionality Reduction using PCA are all based on advance inferential statistics of Distributions and Statistical Significance. It also helps us understanding the data behavior and then apply an appropriate machine learning technique to get the best result from various techniques of Data Science and Machine Learning.\n\n\nData Visualization\nAs they say, picture is worth a thousand words. Data Visualization is one of the key techniques of Data Science and Machine Learning and is used for Exploratory Data Analysis. In that, we visually analyse the data to identify the patterns and trends. We are going to learn how to create various plots and charts as well as how to analyse them for all the practical purposes. Feature Selection plays a key role in Machine Learning and Data Visualisation is key for it.\n\n\nData Processing\nData Science require extensive data processing. Data Science and Machine Learning practitioners spend more than 2/3rd of the time processing and analysing the data. Data can be noisy and is never in the best shape and form. Data Processing is one of the key disciplines of Data Science and Machine Learning to get the best results. We will be using Pandas which is the most popular library for data processing in Python and various other libraries to read, analyse, process and clean the data.\n\n\nMachine Learning\nThe heart and soul of Data Science is the predictive ability provided by the algorithms from Machine Learning and Deep Learning. Machine Learning takes the overall discipline of Data Science ahead of others. We will combine everything we would learn from the previous sections and build various machine learning models. The key aspects of the Machine Learning is not just about the algorithms but also understanding various parameters used by Machine Learning algorithms. We will understand all the key parameters and how their values impact the outcome so that you can build the best machine learning models.\n\n\nFeature Selection and Dimensionality Reduction\nIn case you wonder, what makes a good data scientists, then this section is the answer. A good Data Science and Machine Learning practitioner does not just use libraries and code few lines. She will analyse every feature of the data objectively and choose the most relevant ones based on statistical analysis. We will learn how to reduce the number of features as well as how we can retain the value in the data when we practice and build various machine learning models after applying the principles of Feature Selection and Dimensionality Reduction using PCA.\n\n\nDeep Learning\nYou can not become a good Data Science and Machine Learning practitioner, if you do not know how to build powerful neural network. Deep Learning can be said to be another kind of Machine Learning with great power and flexibility. After Learning Machine Learning, we are going to learn some key fundamentals of Deep Learning and build a solid foundation first. We will then use Keras and Tensorflow which are the most popular Deep Learning frameworks in the world.\n\n\nKaggle Project\nAs an aspiring Data Scientists, we always wish to work on Kaggle project for Machine Learning and achieve good results. I have spent huge effort and time in making sure you understand the overall process of performing a real Data Science and Machine Learning project. This is going to be a good Machine Learning challenge for you.\n\n\nYour takeaway from this course,\nComplete hands-on experience with huge number of Data Science and Machine Learning projects and exercises\nLearn the advance techniques used in the Data Science and Machine Learning\nCertificate of Completion for the most in demand skill of Data Science and Machine Learning\nAll the queries answered in shortest possible time.\nAll future updates based on updates to libraries, packages\nContinuous enhancements and addition of future Machine Learning course material\nAll the knowledge of Data Science and Machine Learning at fraction of cost\nThis Data Science and Machine Learning course comes with the Udemy's 30-Day-Money-Back Guarantee with no questions asked.\nSo what you are waiting for? Hit the \"Buy Now\" button and get started on your Data Science and Machine Learning journey without spending much time.\nI am so eager to see you inside the course.\n\n\nDisclaimer: All the images used in this course are either created or purchased/downloaded under the license from the provider, mostly from Shutterstock or Pixabay.",
      "target_audience": [
        "Beginners as well as advance programmers who want to make a career in Data Science and Machine Learning"
      ]
    },
    {
      "title": "Learn Hugging Face for Mastering Generative AI with LLMs",
      "url": "https://www.udemy.com/course/mastering-generative-ai-with-llms-a-hugging-face-guide/",
      "bio": "LLMs & Transformers in Hugging Face: Dive into Hugging Face, Fine-tuning, Tokenization, and Datasets",
      "objectives": [
        "How to build your own tokenizer using Hugging Face's Transformers library",
        "How to build a custom dataset in Hugging Face",
        "How to train a GPT-2 model from scratch using Hugging Face's Transformers library",
        "How to instruction fine-tune a LLM using PEFT"
      ],
      "course_content": {},
      "requirements": [
        "Beginner python experience"
      ],
      "description": "Welcome to \"Learn Hugging Face for Mastering Generative AI with LLMs\". In today's AI-driven world, Hugging Face has become a central platform for working with Large Language Models (LLMs), which have revolutionized generative AI by enabling machines to generate human-like text, answer questions, and even create original content. This course is meticulously designed to give you a deep understanding of these models and how to harness their power using Hugging Face.\nOur journey begins with a robust introduction to LLMs, exploring their intricacies and how to manage their compute requirements, all within the Hugging Face ecosystem. From there, we dive into the world of Hugging Face, which provides an extensive collection of pre-trained models that can be applied in a wide range of innovative applications.\nPractical knowledge is essential, so the course transitions into a deep dive into Transformers, a key technology behind LLMs, with a special focus on Hugging Face implementations. You'll get hands-on experience with Hugging Face tools, manipulating datasets, building custom models, and mastering tokenization.\nFinally, we emphasize training, fine-tuning, and quantization, with models downloaded from Hugging Face. Learn how to adjust LLMs to your needs, whether for summarization or text generation. With techniques like Instruction Fine-tuning and PEFT, you'll master the art of fine-tuning models. We’ll even show you how to train a GPT-2 from scratch using Hugging Face to generate text from a custom dataset. Then finally, we will show you how to quantize your models so that they take up less memory.",
      "target_audience": [
        "Data Scientists learning to build their own models in Hugging Face",
        "Generative AI enthusiasts that want to learn how to build & use their own custom models"
      ]
    },
    {
      "title": "Improving data quality in data analytics & machine learning",
      "url": "https://www.udemy.com/course/dataqc_x/",
      "bio": "Learn why, when, and how to maximize the quality of your data to optimize data-based decisions",
      "objectives": [
        "Strategies for increasing data quality",
        "Ways to assess data quality",
        "Interpreting data visualizations",
        "How to spot problems in data"
      ],
      "course_content": {
        "Introduction": [
          "Is this course right for you?"
        ],
        "Download course materials (Python code)": [
          "Download the code"
        ],
        "Why data quality matters": [
          "Section summary",
          "Is data or are data??",
          "On the origins and quality of data",
          "Potential problems with data",
          "GIGO (garbage in, garbage out)",
          "Data quality influences data-driven decisions"
        ],
        "Ensuring high data quality": [
          "Section summary",
          "Data management",
          "Data documentation",
          "Data audits",
          "What to include in data documentation",
          "Data cleaning phases",
          "Improve quality before getting data",
          "Improve quality during data collection",
          "Improve quality after data collection",
          "Improve quality during data analysis",
          "Risks of biased results",
          "When to maximize data quality"
        ],
        "Assessing data quality": [
          "Section summary",
          "Qualitative vs. quantitative quality assessments",
          "Evaluating data quality by eye and by algorithm",
          "Qualitative assessments via visual inspection",
          "Code: Visualizing data distributions",
          "Variance assessments",
          "Correlations and correlation matrices",
          "Data error rates",
          "Sample sizes",
          "Code: Measuring data quality"
        ],
        "Data transformations": [
          "Section summary",
          "Z-score scaling",
          "Min/max scaling",
          "Binning (rounding)",
          "Unit normalization",
          "Rank transform",
          "Nonlinear transformations",
          "Code: Transforming data"
        ],
        "Outliers and missing data": [
          "Section summary",
          "What are outliers?",
          "The z-score method",
          "The modified z-score method",
          "Dealing with missing data",
          "Code: Dealing with bad or missing data"
        ],
        "Be a high-quality data scientist": [
          "Section summary",
          "Keeping up with data science developments",
          "Can you know everything?",
          "What data scientists want"
        ],
        "Bonus": [
          "Bonus material"
        ]
      },
      "requirements": [
        "Interest in working with data",
        "Interest in knowing more about data quality",
        "Some Python skills are useful for the optional coding videos"
      ],
      "description": "All of our decisions are based on data. Our sense organs gather data, our memories are data, and our gut-instincts are data. If you want to make good decisions, you need to have high-quality data.\n\n\nThis course is about data quality: What it means, why it's important, and how you can increase the quality of your data.\n\n\nIn this course, you will learn:\nHigh-level strategies for ensuring high data quality, including terminology, data documentation and management, and the different research phases in which you can check and increase data quality.\nQualitative and quantitative methods for evaluating data quality, including visual inspection, error rates, and outliers. Python code is provided to see how to implement these visualizations and scoring methods using pandas, numpy, seaborn, and matplotlib.\nSpecific data methods and algorithms for cleaning data and rejecting bad or unusual data. As above, Python code is provided to see how to implement these procedures using pandas, numpy, seaborn, and matplotlib.\n\n\nThis course is for\nData practitioners who want to understand both the high-level strategies and the low-level procedures for evaluating and improving data quality.\nManagers, clients, and collaborators who want to understand the importance of data quality, even if they are not working directly with data.",
      "target_audience": [
        "Data science practitioners",
        "Data scientist students",
        "Managers or colleagues who work with data practitioners"
      ]
    },
    {
      "title": "R Programming for Statistics and Data Science",
      "url": "https://www.udemy.com/course/r-programming-for-statistics-and-data-science/",
      "bio": "R Programming for Data Science & Data Analysis. Applying R for Statistics and Data Visualization with GGplot2 in R",
      "objectives": [
        "Learn the fundamentals of programming in R",
        "Work with R’s conditional statements, functions, and loops",
        "Build your own functions in R",
        "Get your data in and out of R",
        "Learn the core tools for data science with R",
        "Manipulate data with the Tidyverse ecosystem of packages",
        "Systematically explore data in R",
        "The grammar of graphics and the ggplot2 package",
        "Visualise data: plot different types of data & draw insights",
        "Transform data: best practices of when and how",
        "Index, slice, and subset data",
        "Learn the fundamentals of statistics and apply them in practice",
        "Hypothesis testing in R",
        "Understand and carry out regression analysis in R",
        "Work with dummy variables",
        "Learn to make decisions that are supported by the data!",
        "Have fun by taking apart Star Wars and Pokemon data, as well some more serious data sets"
      ],
      "course_content": {
        "Introduction": [
          "Ten Things You Will Learn in This Course"
        ],
        "Getting started": [
          "Intro",
          "Downloading and installing R & RStudio",
          "Quick guide to the RStudio user interface",
          "RStudio's GUI",
          "Changing the appearance in RStudio",
          "Installing packages in R and using the library"
        ],
        "The building blocks of R": [
          "Creating an object in R",
          "Exercise 1 Creating an object in R",
          "Data types in R - Integers and doubles",
          "Data types in R - Characters and logicals",
          "Objects and Data Types",
          "Exercise 2 Data types in R",
          "Coercion rules in R",
          "Exercise 3 Coercion rules in R",
          "Functions in R",
          "Exercise 4 Using functions in R",
          "Functions and arguments",
          "Exercise 5 The arguments of a function",
          "Building a function in R (basics)",
          "Objects and Functions",
          "Exercise 6 Building a function in R",
          "Using the script vs. using the console"
        ],
        "Vectors and vector operations": [
          "Intro",
          "Introduction to vectors",
          "Vector recycling",
          "Exercise 7 Vector recycling",
          "Naming a vector in R",
          "Exercise 8 Vector attributes - names",
          "Introduction to vectors",
          "Getting help with R",
          "Getting Help with R",
          "Slicing and indexing a vector in R",
          "Extracting elements from a vector",
          "Exercise 9 Indexing and slicing a vector",
          "Changing the dimensions of an object in R",
          "Exercise 10 Vector attributes - dimensions"
        ],
        "Matrices": [
          "Creating a matrix in R",
          "Faster code: creating a matrix in a single line of code",
          "Creating a matrix",
          "Exercise 11 Creating a matrix in R",
          "Do matrices recycle?",
          "Indexing an element from a matrix",
          "Slicing a matrix in R",
          "Exercise 12 Indexing and slicing a matrix",
          "Matrix arithmetic",
          "Exercise 13 Matrix arithmetic",
          "Matrix operations in R",
          "Matrix operations",
          "Exercise 14 Matrix operations",
          "Categorical data",
          "Creating a factor in R",
          "Factors in R",
          "Exercise 15 Creating a factor in R",
          "Lists in R",
          "Exercise: Lists in R",
          "Completed 33% of the course"
        ],
        "Fundamentals of programming with R": [
          "Relational operators in R",
          "Logical operators in R",
          "Vectors and logicals operators",
          "Relational and Logical operators in R",
          "Exercise Logical operators",
          "If, else, else if statements in R",
          "Exercise If, else, else if statements in R",
          "If, else, else if statements - Keep-In-Mind's",
          "For loops in R",
          "Exercise: For Loops in R",
          "While loops in R",
          "Exercise: While loops in R",
          "Repeat loops in R",
          "Loops in R",
          "Building a function in R 2.0",
          "Building a function in R 2.0 - Scoping",
          "Exercise Scoping",
          "Completed 50% of the course"
        ],
        "Data frames": [
          "Intro",
          "Creating a data frame in R",
          "Exercise 16 Creating a data frame in R",
          "The Tidyverse package",
          "Data import in R",
          "Importing a CSV in R",
          "Data export in R",
          "Exercise 17 Importing and exporting data in R",
          "Creating data frames",
          "Getting a sense of your data frame",
          "Indexing and slicing a data frame in R",
          "Data frame operations",
          "Extending a data frame in R",
          "Exercise 18 Data frame operations",
          "Dealing with missing data in R"
        ],
        "Manipulating data": [
          "Intro",
          "Data transformation with R - the Dplyr package - Part I",
          "Data transformation with R - the Dplyr package - Part II",
          "Sampling data with the Dplyr package",
          "Using the pipe operator in R",
          "Manipulating data",
          "Exercise 19 Data transformation with Dplyr",
          "Tidying data in R - gather() and separate()",
          "Tidying data in R - unite() and spread()",
          "Tidying data",
          "Exercise 20 Data tidying with Tidyr"
        ],
        "Visualizing data": [
          "Intro",
          "Intro to data visualization",
          "Intro to ggplot2",
          "Variables: revisited",
          "Building a histogram with ggplot2",
          "Exercise 21 Building a histogram with ggplot2",
          "Building a bar chart with ggplot2",
          "Exercise 22 Building a bar chart with ggplot2",
          "Building a box and whiskers plot with ggplot2",
          "Exercise 23 Building a box plot with ggplot2",
          "Building a scatterplot with ggplot2",
          "Exercise 24 Building a scatterplot with ggplot2"
        ],
        "Exploratory data analysis": [
          "Population vs. sample",
          "Mean, median, mode",
          "Skewness",
          "Exercise 25 Determining Skewness",
          "Variance, standard deviation, and coefficient of variability",
          "Covariance and correlation",
          "Exercise 26 Practical example with real estate data"
        ]
      },
      "requirements": [
        "You’ll need to install R Studio. We will show you how to do it in one of the first lectures of the course",
        "All software and data used in the course are free."
      ],
      "description": "R Programming for Statistics and Data Science 2023\nR Programming is a skill you need if you want to work as a data analyst or a data scientist in your industry of choice. And why wouldn't you?  Data scientist is the hottest ranked profession in the US.\n\nBut to do that, you need the tools and the skill set to handle data. R is one of the top languages to get you where you want to be. Combine that with statistical know-how, and you will be well on your way to your dream title.\nThis course is packing all of this, and more, in one easy-to-handle bundle, and it’s the perfect start to your journey.\nSo, welcome to R for Statistics and Data Science!\nR for Statistics and Data Science is the course that will take you from a complete beginner in programming with R to a professional who can complete data manipulation on demand. It gives you the complete skill set to tackle a new data science project with confidence and be able to critically assess your work and others’.\nLaying strong foundations\nThis course wastes no time and jumps right into hands-on coding in R. But don’t worry if you have never coded before, we start off light and teach you all the basics as we go along! We wanted this to be an equally satisfying experience for both complete beginners and those of you who would just like a refresher on R.\nWhat makes this course different from other courses?\nWell-paced learning.\nReceive top class training with content which we’ve built - and rigorously edited - to deliver powerful and efficient results.\nEven though preferred learning paces differ from student to student, we believe that being challenged just the right amount underpins the learning that sticks.\nIntroductory guide to statistics.\nWe will take you through descriptive statistics and the fundamentals of inferential statistics.\nWe will do it in a step-by-step manner, incrementally building up your theoretical knowledge and practical skills.\nYou’ll master confidence intervals and hypothesis testing, as well as regression and cluster analysis.\nThe essentials of programming – R-based.\nPut yourself in the shoes of a programmer, rise above the average data scientist and boost the productivity of your operations.\nData manipulation and analysis techniques in detail.\nLearn to work with vectors, matrices, data frames, and lists.\nBecome adept in ‘the Tidyverse package’ - R’s most comprehensive collection of tools for data manipulation – enabling you to index and subset data, as well as spread(), gather(), order(), subset(), filter(), arrange(), and mutate() it.\nCreate meaning-heavy data visualizations and plots.\nPractice makes perfect.\nReinforce your learning through numerous practical exercises, made with love, for you, by us.\nWhat about homework, projects, & exercises?\nThere is a ton of homework that will challenge you in all sorts of ways. You will have the chance to tackle the projects by yourself or reach out to a video tutorial if you get stuck.\nYou: Is there something to show for the skills I will acquire?\nUs: Indeed, there is – a verifiable certificate.\nYou will receive a verifiable certificate of completion with your name on it. You can download the certificate and attach it to your CV and even post it on your LinkedIn profile to show potential employers you have experience in carrying out data manipulations & analysis in R.\nIf that sounds good to you, then welcome to the classroom :)",
      "target_audience": [
        "Aspiring data scientists",
        "Beginners to programming",
        "People interested in statistics and data analysis",
        "Anyone who wants to learn how to code and apply their skills in practice"
      ]
    },
    {
      "title": "Machine Learning Regression Masterclass in Python",
      "url": "https://www.udemy.com/course/machine-learning-regression-masterclass-in-python/",
      "bio": "Build 8+ Practical Projects and Master Machine Learning Regression Techniques Using Python, Scikit Learn and Keras",
      "objectives": [
        "Master Python programming and Scikit learn as applied to machine learning regression",
        "Understand the underlying theory behind simple and multiple linear regression techniques",
        "Apply simple linear regression techniques to predict product sales volume and vehicle fuel economy",
        "Apply multiple linear regression to predict stock prices and Universities acceptance rate",
        "Cover the basics and underlying theory of polynomial regression",
        "Apply polynomial regression to predict employees’ salary and commodity prices",
        "Understand the theory behind logistic regression",
        "Apply logistic regression to predict the probability that customer will purchase a product on Amazon using customer features",
        "Understand the underlying theory and mathematics behind Artificial Neural Networks",
        "Learn how to train network weights and biases and select the proper transfer functions",
        "Train Artificial Neural Networks (ANNs) using back propagation and gradient descent methods",
        "Optimize ANNs hyper parameters such as number of hidden layers and neurons to enhance network performance",
        "Apply ANNs to predict house prices given parameters such as area, number of rooms..etc",
        "Assess the performance of trained Machine learning models using KPI (Key Performance indicators) such as Mean Absolute error, Mean squared Error, and Root Mean Squared Error intuition, R-Squared intuition, Adjusted R-Squared and F-Test",
        "Understand the underlying theory and intuition behind Lasso and Ridge regression techniques",
        "Sample real-world, practical projects"
      ],
      "course_content": {},
      "requirements": [
        "Machine Learning basics",
        "PC with Internet connetion"
      ],
      "description": "Artificial Intelligence (AI) revolution is here! The technology is progressing at a massive scale and is being widely adopted in the Healthcare, defense, banking, gaming, transportation and robotics industries.\nMachine Learning is a subfield of Artificial Intelligence that enables machines to improve at a given task with experience. Machine Learning is an extremely hot topic; the demand for experienced machine learning engineers and data scientists has been steadily growing in the past 5 years. According to a report released by Research and Markets, the global AI and machine learning technology sectors are expected to grow from $1.4B to $8.8B by 2022 and it is predicted that AI tech sector will create around 2.3 million jobs by 2020.\nThe purpose of this course is to provide students with knowledge of key aspects of machine learning regression techniques in a practical, easy and fun way. Regression is an important machine learning technique that works by predicting a continuous (dependant) variable based on multiple other independent variables. Regression strategies are widely used for stock market predictions, real estate trend analysis, and targeted marketing campaigns.\nThe course provides students with practical hands-on experience in training machine learning regression models using real-world dataset. This course covers several technique in a practical manner, including:\n· Simple Linear Regression\n· Multiple Linear Regression\n· Polynomial Regression\n· Logistic Regression\n· Decision trees regression\n· Ridge Regression\n· Lasso Regression\n· Artificial Neural Networks for Regression analysis\n· Regression Key performance indicators\nThe course is targeted towards students wanting to gain a fundamental understanding of machine learning regression models. Basic knowledge of programming is recommended. However, these topics will be extensively covered during early course lectures; therefore, the course has no prerequisites, and is open to any student with basic programming knowledge. Students who enroll in this course will master machine learning regression models and can directly apply these skills to solve real world challenging problems.",
      "target_audience": [
        "Data Scientists who want to apply their knowledge on Real World Case Studies",
        "Machine Learning Enthusiasts who look to add more projects to their Portfolio"
      ]
    },
    {
      "title": "Deployment of Machine Learning Models",
      "url": "https://www.udemy.com/course/deployment-of-machine-learning-models/",
      "bio": "Learn how to integrate robust and reliable Machine Learning Pipelines in Production",
      "objectives": [
        "Build machine learning model APIs and deploy models into the cloud",
        "Send and receive requests from deployed machine learning models",
        "Design testable, version controlled and reproducible production code for model deployment",
        "Create continuous and automated integrations to deploy your models",
        "Understand the optimal machine learning architecture",
        "Understand the different resources available to productionise your models",
        "Identify and mitigate the challenges of putting models in production"
      ],
      "course_content": {},
      "requirements": [
        "A Python installation",
        "A Git installation",
        "Confidence in Python programming, including familiarity with Numpy, Pandas and Scikit-learn",
        "Familiarity with the use of IDEs, like Pycharm, Sublime, Spyder or similar",
        "Familiarity with writing Python scripts and running them from the command line interface",
        "Knowledge of basic git commands, including clone, fork, branch creation and branch checkout",
        "Knowledge of basic git commands, including git status, git add, git commit, git pull, git push",
        "Knowledge of basic CLI commands, including navigating folders and using Git and Python from the CLI",
        "Knowledge of Linear Regression and model evaluation metrics like the MSE and R2"
      ],
      "description": "Welcome to Deployment of Machine Learning Models, the most comprehensive machine learning deployments online course available to date. This course will show you how to take your machine learning models from the research environment to a fully integrated production environment.\n\n\nWhat is model deployment?\nDeployment of machine learning models, or simply, putting models into production, means making your models available to other systems within the organization or the web, so that they can receive data and return their predictions. Through the deployment of machine learning models, you can begin to take full advantage of the model you built.\n\n\nWho is this course for?\nIf you’ve just built your first machine learning models and would like to know how to take them to production or deploy them into an API,\nIf you deployed a few models within your organization and would like to learn more about best practices on model deployment,\nIf you are an avid software developer who would like to step into deployment of fully integrated machine learning pipelines,\nthis course will show you how.\n\n\nWhat will you learn?\nWe'll take you step-by-step through engaging video tutorials and teach you everything you need to know to start creating a model in the research environment, and then transform the Jupyter notebooks into production code, package the code and deploy to an API, and add continuous integration and continuous delivery. We will discuss the concept of reproducibility, why it matters, and how to maximize reproducibility during deployment, through versioning, code repositories and the use of docker. And we will also discuss the tools and platforms available to deploy machine learning models.\nSpecifically, you will learn:\nThe steps involved in a typical machine learning pipeline\nHow a data scientist works in the research environment\nHow to transform the code in Jupyter notebooks into production code\nHow to write production code, including introduction to tests, logging and OOP\nHow to deploy the model and serve predictions from an API\nHow to create a Python Package\nHow to deploy into a realistic production environment\nHow to use docker to control software and model versions\nHow to add a CI/CD layer\nHow to determine that the deployed model reproduces the one created in the research environment\nBy the end of the course you will have a comprehensive overview of the entire research, development and deployment lifecycle of a machine learning model, and understood the best coding practices, and things to consider to put a model in production. You will also have a better understanding of the tools available to you to deploy your models, and will be well placed to take the deployment of the models in any direction that serves the needs of your organization.\n\n\nWhat else should you know?\nThis course will help you take the first steps towards putting your models in production. You will learn how to go from a Jupyter notebook to a fully deployed machine learning model, considering CI/CD, and deploying to cloud platforms and infrastructure.\nBut, there is a lot more to model deployment, like model monitoring, advanced deployment orchestration with Kubernetes, and scheduled workflows with Airflow, as well as various testing paradigms such as shadow deployments that are not covered in this course.\n\n\nWant to know more? Read on...\nThis comprehensive course on deployment of machine learning models includes over 100 lectures spanning about 10 hours of video, and ALL topics include hands-on Python code examples which you can use for reference and re-use in your own projects.\nIn addition, we have now included in each section an assignment where you get to reproduce what you learnt to deploy a new model.\nSo what are you waiting for? Enroll today, learn how to put your models in production and begin extracting their true value.",
      "target_audience": [
        "Data scientists who want to deploy their first machine learning model",
        "Data scientists who want to learn best practices model deployment",
        "Software developers who want to transition into machine learning"
      ]
    },
    {
      "title": "The Beginner's Guide to AI - Unity 6 Compatible",
      "url": "https://www.udemy.com/course/artificial-intelligence-in-unity/",
      "bio": "A practical guide to programming non-player characters for games in the Unity Game Engine with C#",
      "objectives": [
        "Design and program NPCs with C# in Unity",
        "Explain how AI is applied in computer games",
        "Implement AI-related Unity Asset plugins into existing projects",
        "Work with a variety of AI techniques for developing navigation and decision making abilities in NPCs"
      ],
      "course_content": {
        "Introduction": [
          "Welcome to the Course",
          "Introduction to Artificial Intelligence",
          "Join the H3D Student Community",
          "FAQs",
          "Not So Scary Vector Mathematics",
          "Vector Mathematics Basics Cheat Sheet",
          "Test your vector mathematics knowledge",
          "Setting up Pathfinding in Unity",
          "Updating to Unity 6"
        ],
        "The Mathematics of AI": [
          "The Cartesian plane",
          "Vectors Part 1",
          "Vectors Part 2",
          "Vectors Part 3",
          "Calculating the Dot Product",
          "Calculating the Angle Between Vectors",
          "A Simple AI Pet Challenge Project"
        ],
        "The Physics of AI": [
          "Time",
          "Normalising Movement with Time",
          "Velocity",
          "Predicting Future Locations of Game Objects Part 1",
          "Predicting Future Locations of Game Objects Part 2",
          "Acceleration Part 1",
          "Acceleration Part 2",
          "Acceleration Part 3",
          "Trajectories Part 1",
          "Trajectories Part 2",
          "Trajectories Part 3"
        ],
        "The A* Algorithm": [
          "The A* Pathfinding Algorithm Part 1",
          "The A* Pathfinding Algorithm Part 2",
          "The A* Pathfinding Algorithm Part 3",
          "The A* Pathfinding Algorithm Part 4",
          "The A* Pathfinding Algorithm Part 5",
          "The A* Pathfinding Algorithm Part 6"
        ],
        "Waypoints and Graphs": [
          "Waypoints",
          "Slerping to the Direction of Travel",
          "Following a Circuit",
          "Following a Tracker",
          "Using A* with Waypoints Part 1",
          "A Simple Graph API Part 1",
          "A Simple Graph API Part 2",
          "A Simple Graph API Part 3",
          "Using A* with Waypoints Part 2",
          "Traversing a Path",
          "Giving Commands to Pathfind"
        ],
        "Vehicles": [
          "Setting up Wheel Physics",
          "Forces on Wheels",
          "Constructing a Simple Car",
          "Turning the Steering Wheel",
          "Creating A Circuit with Waypoints",
          "Automatically Driving a Circuit Part 1",
          "Braking",
          "Driving Forces",
          "Improved Driving Tactics",
          "Adding a Progress Tracker",
          "Adding Antiroll Stabilising",
          "Reconfiguring for Car Setting Testing",
          "Avoiding Other Drivers",
          "Improving Avoidance and Reversing"
        ],
        "Navigation Meshes": [
          "Navigation Mesh Introduction",
          "From Waypoints to Navigation Meshes",
          "NavMesh Agents Part 1",
          "NavMesh Agents Part 2",
          "NavMesh Agents Part 3",
          "Following a Player on a NavMesh"
        ],
        "Finite State Machines": [
          "Finite State Machines",
          "Creating a State Class",
          "Patrolling",
          "Building the AI Class",
          "Chasing the Player Part 1",
          "Chasing the Player Part 2",
          "FSM Challenge"
        ],
        "Autonomously Moving Agents": [
          "Seek and Flee",
          "Pursuit",
          "Evade",
          "Wander",
          "Hide Part 1",
          "Hide Part 2",
          "Hide Part 3",
          "Complex Behaviours",
          "Behaviour Challenge"
        ],
        "Crowd Simulation": [
          "Moving As One",
          "Creating a City Crowd Part 1",
          "Creating a City Crowd Part 2",
          "Fleeing Part 1",
          "Fleeing Part 2",
          "Flocking Part 1",
          "Flocking Part 2",
          "Flocking Part 3",
          "Flocking Part 4",
          "Crowd Challenge Project",
          "Flock Challenge Project"
        ]
      },
      "requirements": [
        "You should be familiar with C# and the Unity Game Development Engine."
      ],
      "description": "Do your non-player characters (NPCs) lack drive and ambition?  Are they slow, stupid and constantly banging their heads against the wall? Then this course is for you.  Join Penny as she explains, demonstrates and assists you in creating your very own NPCs in Unity with C#. All you need is a sound knowledge of Unity, C# and the ability to add two numbers together.\nThis course uses Unity Version 2021.3 LTS but all project has been tested and work in Unity 6.\nIn this course, Penny reveals the most popular AI techniques used for creating believable character behaviour in games using her internationally acclaimed teaching style and knowledge from over 30 years working with games, graphics and having written two award winning books on games AI. Throughout, you will follow along with hands-on workshops designed to teach you about the fundamental AI techniques used in today's games.  You'll join in as NPCs are programmed to chase, patrol, shoot, race, crowd and much more.\nLearn how to program and work with:\nvectors\nwaypoints\nnavmeshes\nthe A* algorithm\ncrowds\nflocks\nanimated characters\nvehicles\nand industry standard techniques such as goal-oriented action learning and behaviour trees.\nContents and Overview\nThe course begins with a detailed examination of vector mathematics that sits at the very heart of programming the movement of NPCs. Following this, systems of waypoints will be used to move characters around in an environment before examining the Unity waypoint system for car racing with AI controlled cars.  This leads into an investigation of graph theory and the A* algorithm before we apply these principles to developing navmeshes and developing NPCs who can find their way around a game environment.  Before an aquarium is programmed complete with autonomous schooling fish, crowds of people will be examined from the recreation of sidewalk traffic, to groups of people fleeing from danger. Having examined the differing ways to move NPCs around in a game environment, their thinking abilities will be discussed with full explanations and more hands-on workshops using finite state machines and behaviour trees.\nThe follow-along workshops included in the course come with starter Unity asset files and projects complete with solutions.  Throughout, there are also quizzes and challenge exercises to reinforce your learning and guide you to express your newfound knowledge.\nAt the completion of this course you will have gained a broad understanding of what AI is in games, how it works and how you can use it in your own projects.  It will equip you with a toolset to examine any of the techniques presented in more depth to take your game environments to the next level.\nWhat students are saying about this course:\nThis has been my favourite Udemy-Unity course so far. It took me from literally 0% knowledge of how game AI is achieved, and took me to a whole new level. Waypoints, pathfinding, state machines, etc etc etc are all covered in-depth and will reveal the magic (spoiler alert: it isn't magic) behind making your computer characters seem like they really have a mind of their own.\nOh My God. I love her way of teaching things. I haven’t finished this course yet. But all i can say is that it is another brilliant course from her. Artificial intelligence by itself is a tricky thing to do. And before starting this course i never thought that i will understand anything in it. But i was wrong. With her style of teaching, you will learn how to move your characters in an ”intelligent“ way. This course is perfectly sliced and the pace is wonderful.",
      "target_audience": [
        "Anyone interested in learning how to program their own non-player characters (NPCs).",
        "Anyone interested in seeing how artificial intelligence is applied in computer games."
      ]
    },
    {
      "title": "Data Engineering with Google Dataflow and Apache Beam on GCP",
      "url": "https://www.udemy.com/course/data-engineering-with-google-dataflow-and-apache-beam/",
      "bio": "First steps to Extract, Transform and Load data using Apache Beam and Deploy Pipelines on Google Dataflow",
      "objectives": [
        "Apache Beam",
        "ETL",
        "Python",
        "Google Cloud",
        "DataFlow",
        "Google Cloud Storage",
        "Big Query"
      ],
      "course_content": {
        "Apache Beam Concepts": [
          "2.1 What is Apache Beam ?",
          "2.2 Apache Beam Architecture Overview",
          "2.3 Apache Beam Pipeline Flow"
        ],
        "Apache Beam Main Functions": [
          "3.1 Using Apache Beam In Google Colab",
          "3.2 Read Inputs",
          "3.3 Write Outputs",
          "3.4 beam.Map / beam.FlatMap",
          "3.5 beam.Filter",
          "3.6 beam.Flatten",
          "3.7 beam.CombinePerKey",
          "3.8 beam.combiners.Count.PerKey()",
          "3.9 beam.CoGroupByKey",
          "3.10 ParDo – Customizing Beam Functions"
        ],
        "Apache Beam + GCP = Dataflow": [
          "4.1 GCP Setup",
          "4.2 Creating Service Account and a Bucket on GCP",
          "4.3 Setup Apache Beam Local (SDK)",
          "4.4 Direct Runner Execution + Saving to GCS",
          "4.5 Creating a Dataflow Template",
          "4.6 Executing Batch Job in Dataflow",
          "4.7 Creating Dataflow Template to write in Big Query",
          "4.8 Executing Batch Job to write in Big Query"
        ]
      },
      "requirements": [
        "Basic Python",
        "Free GCP Account"
      ],
      "description": "This course wants to introduce you to the Apache Foundation's newest data pipeline development framework: The Apache Beam, and how this feature is becoming popular in partnership with Google Dataflow. In a summary, we want to cover the following topics:\n\n\n1. Understand your inner workings\n2. What are your benefits\n3. Explain how to use on your local machine without installation via Google Colab for development\n4. Its main functions\n5. Configure Apache Beam python SDK locallyvice\n6. How to deploy this resource on Google Dataflow to a Batch pipeline\n\n\nThis course is dynamic, you will be receiving updates whenever possible.\nIt is important to remember that this course does not teach Python, but uses it. So, get comfortable with knowing Python basics, defining a function, creating objects and data types.\nAlso, if you are interested in learning section 4, which consists of deploying a pipeline on Google Dataflow, you will need to have a free counter in GCP. It's a simple process, but it requires a credit card!\n\n\nI kindly ask you you to consider all the efforts to put this course together and give a nice rate at the end of the course, even tough the course is simple, it was made with all good intent to share knowledge for cheap price. Thanks and hope you enjoy!\n___________________________________________________________________________________________________________\n\n\nRequirements:\n· Basic knowledge of Python\n· Have Python 3.7 or greater installed locally (from section 4)\n· Free account at GCP (from section 4)\n\n\nSchedule:\n· Section 2 – Concepts\n· Section 3 – Main Functions\n· Section 4 – Apache Beam on Google Dataflow",
      "target_audience": [
        "Data Engineers",
        "Data Analysts",
        "Business Intelligence Professionals",
        "Open Source Fans",
        "ETL Engineers"
      ]
    },
    {
      "title": "Mastering Artificial Intelligence",
      "url": "https://www.udemy.com/course/mastering-artificial-intelligence/",
      "bio": "Your Complete Guide to Artificial Intelligence",
      "objectives": [
        "Students will develop a solid understanding of the definition and historical evolution of AI.",
        "Students will see how AI technologies are transforming industries and solving complex problems.",
        "Students will explore the responsibilities associated with AI technologies, including topics like bias, fairness, transparency, and privacy.",
        "This course will equip students with essential machine learning skills, including supervised learning techniques for prediction and classification.",
        "Students will gain a comprehensive understanding of deep learning, including the principles and techniques behind artificial neural networks.",
        "Students will learn how feedforward neural networks work and how they can be used for various AI tasks.",
        "Students will explore different activation functions used in neural networks and understand their role in modeling complex data.",
        "Students will learn how CNNs are used for image recognition and processing.",
        "Students will understand how RNNs can be applied to tasks involving sequences, such as natural language processing and time series analysis.",
        "Students will engage in a hands-on capstone project. This project will require them to apply the concepts they have learned to solve real-world AI challenges."
      ],
      "course_content": {
        "Introduction to Artificial Intelligence": [
          "Introduction to AI",
          "History and Types of AI",
          "Application of AI",
          "Ethical Considerations in AI"
        ],
        "Machine Learning Fundamentals": [
          "Machine Learning fundamentals",
          "ML Data Pre Processing-Lab session",
          "Evaluation Metrics",
          "Machine Learning Types & Supervised machine learning algorithms",
          "Model Based on Linear Regression- Lab Session",
          "Supervised machine learning algorithms",
          "Comparing Classification Algorithms",
          "Unsupervised learning",
          "K-Means clustering Model"
        ],
        "Deep learning": [
          "Deep Learning and Neural Networks Introduction",
          "Deep Learning and Neural Networks Introduction Part-2",
          "Artificial Neural Networks Model",
          "Convolutional neural networks",
          "CNN for Image Classification",
          "Recurrent neural networks",
          "RNN Sun Spot Model"
        ],
        "Natural Language Processing (NLP)": [
          "Natural Language Processing Introduction",
          "Text Pre Processing",
          "Text Pre-processing Model",
          "Sentiment Analysis",
          "Language modeling and generation",
          "Sentiment analysis Model"
        ],
        "Computer Vision": [
          "Computer Vision Introduction",
          "Image Processing",
          "Feature Extraction",
          "Object Detection",
          "Image Segmentation",
          "Image Segmentation Process & Image Classification",
          "Face Mask Detection Model"
        ],
        "Reinforcement Learning": [
          "Introduction to Reinforcement learning",
          "Reinforcement Learning Algorithms",
          "Reinforcement Learning Model"
        ],
        "Capstone Project": [
          "Capstone Project -Heart Disease Prediction Model"
        ]
      },
      "requirements": [
        "Learners should be comfortable using a computer and common software applications.",
        "While not mandatory, a basic understanding of algebra and problem-solving skills can be helpful in grasping some AI and machine learning concepts.",
        "Familiarity with programming basics is beneficial, but the course will provide introductory materials for those with limited programming experience.",
        "Participants should have access to a computer or laptop with an internet connection to access course materials, online resources, and programming tools.",
        "A code editor or integrated development environment (IDE) for Python, such as Jupyter Notebook or Visual Studio Code, is recommended.",
        "A modern web browser is necessary for accessing online resources and course materials.",
        "The course is designed to be accessible to beginners, and participants will receive guidance and support to help them build the necessary skills and knowledge."
      ],
      "description": "** Mastering Artificial Intelligence: Your Ultimate Guide to AI **\n\n\nWelcome to Selfcode Academy's comprehensive AI course! Whether you're a beginner or a seasoned tech enthusiast, this program is designed to empower you with the skills and knowledge needed to excel in the exciting world of Artificial Intelligence.\n\n\n\n\nWho Can Benefit:\nBeginners: No prior AI experience required! This course is perfect for those embarking on their AI journey.\nStudents: Whether you're in high school, college, or pursuing graduate studies, this course complements your academic pursuits.\nProfessionals: Looking to boost your career prospects? Transitioning to AI from another field? No problem! This course is tailored for you.\nTech Innovators: Entrepreneurs and visionaries, get ready to turn your AI concepts into reality.\nData Enthusiasts: If you're passionate about data and want to harness its potential with AI, this course is your gateway.\nLifelong Learners: Stay updated with the latest AI advancements and become part of the tech-savvy community.\n\n\n\n\nCourse Highlights:\nModule 1: Introduction to AI\nDefine AI and uncover its fascinating history.\nExplore AI's impact in healthcare, finance, and various industries.\nDelve into crucial ethical considerations in AI.\nModule 2: Machine Learning Fundamentals\nLearn to make predictions and categorize data through supervised learning.\nDiscover patterns in unlabeled data using unsupervised learning.\nEvaluate model performance using essential metrics like accuracy, precision, and recall.\nModule 3: Deep Learning and Neural Networks\nGrasp the intricacies of artificial neural networks (ANNs).\nDive into feedforward neural networks, activation functions, and their applications.\nGet hands-on experience with convolutional and recurrent neural networks (CNNs and RNNs).\nModule 4: Natural Language Processing (NLP)\nPrepare text data for analysis with advanced preprocessing techniques.\nPerform sentiment analysis and text classification.\nGenerate coherent text using cutting-edge AI models.\nModule 5: Computer Vision\nMaster image processing techniques and feature extraction.\nDetect objects and segment images effectively.\nLeverage CNNs for image classification.\nModule 6: Reinforcement Learning\nExplore the foundations of reinforcement learning and Markov Decision Processes (MDPs).\nImplement Q-learning and value iteration algorithms to solve complex problems.\nModule 7: Capstone Project\nApply your newfound AI expertise to solve real-world challenges.\n\n\nEmbark on this thrilling AI adventure with us! Our hands-on projects, clear explanations, and supportive community will boost your confidence to tackle AI challenges and contribute to the future of technology. Start your journey to AI mastery today!\n\n\n* All the resource files are added in video 1 of section 1.",
      "target_audience": [
        "Beginners in AI: Individuals who are entirely new to the field of AI and want to explore its fundamental concepts and practical applications will benefit from this course. We provide a solid foundation for those with no prior experience.",
        "Students: High school students, college undergraduates, and postgraduates pursuing degrees in computer science, data science, engineering, or related fields can use this course to supplement their academic knowledge and gain hands-on experience in AI.",
        "Professionals Seeking Career Advancement: Working professionals looking to transition into AI-related roles or enhance their existing skill set will find this course valuable. It provides practical insights and hands-on experience applicable to various industries.",
        "Entrepreneurs and Innovators: Individuals interested in creating AI-powered solutions, startups, or tech innovations will gain a strong foundation in AI concepts and techniques to bring their ideas to life.",
        "Data Enthusiasts: Anyone passionate about data analysis, machine learning, or data-driven decision-making can use this course to acquire the skills needed to work with AI and machine learning technologies.",
        "AI Enthusiasts: Those with a general interest in AI, robotics, and automation can satisfy their curiosity by gaining a comprehensive understanding of AI's principles and real-world applications.",
        "Lifelong Learners: Individuals who are lifelong learners eager to explore cutting-edge technologies and stay up-to-date with the latest advancements in AI and machine learning will find this course engaging and informative.",
        "This course is designed to be inclusive, with content structured to accommodate learners at different levels of expertise. Whether you're a complete novice or someone looking to deepen your knowledge in AI, our course provides a flexible learning path to meet your needs and interests."
      ]
    },
    {
      "title": "Data Science Mastery with Python: Comprehensive course",
      "url": "https://www.udemy.com/course/data-science-with-python-complete-course/",
      "bio": "Transform Data into Insights with Python, Machine Learning, and Advanced Analytics – Gateway to a Data-Driven Career",
      "objectives": [
        "Perform high-level mathematical and technical computing using the NumPy and SciPy packages and data analysis with the Pandas package",
        "Gain an in-depth understanding of Data Science processes: data wrangling, data exploration, data visualization, hypothesis building, and testing",
        "Master the essential concepts of Python programming, including data types, tuples, lists, dicts, basic operators, and functions.",
        "Apply knowledge and actionable insights from data across a broad range of application domains."
      ],
      "course_content": {
        "Introduction": [
          "Getting Started with Data Science"
        ],
        "Basic Maths Required for Data Science": [
          "Let's Start with Statistics",
          "Data Quality Issues",
          "Types of Statistics",
          "Measures of Spread",
          "Measures of Shapes",
          "Plots Visualisation",
          "Inferential Statistics",
          "Probability",
          "Conditional Probability",
          "Random Variables",
          "Normal Probability Distribution",
          "Central Limit Theorem",
          "Hypothesis Testing for Decision Making"
        ],
        "Python for Data Science": [
          "Python for Data Science",
          "Python Installation - Google Collab",
          "Python Basics",
          "Identifiers in Python",
          "Comments in Python",
          "Python Indentation",
          "Python Statements",
          "Variables in Python",
          "Data Types & Related Stuffs in Python",
          "Conversion of Data Types in Python",
          "Python I/O functions",
          "Output Formatting",
          "User Input in Python",
          "Operators in Python",
          "Control Flow in Python",
          "Functions in Python",
          "Types of Functions in Python",
          "Recursive Functions in Python",
          "Argument in a Function",
          "Lambda or Anonymous Functions in Python"
        ],
        "Advance Python": [
          "Advance Programming in Python",
          "Advance Programming in Python: Part 2",
          "Data Visualisations",
          "Bivariate Plotting",
          "Multivariate Plotting"
        ],
        "Let's dig deeper": [
          "EDA",
          "EDA on Mc'donalds Data Set",
          "Exploratory Data Analysis"
        ],
        "Let's Explore in to Machine Learning": [
          "Introduction: Machine Learning",
          "Unsupervised Learning",
          "Reinforement Learning"
        ],
        "Module Seven": [
          "Linear Regression",
          "How to use Linear Regression",
          "Logistic Regression",
          "Logistic Regression on Titanic Data Set",
          "Decision Tree",
          "Algorithms used in Decision Treee",
          "Gini Index",
          "Issues with Decision Tree",
          "Applications of Decision Tree",
          "Working on Titanic Data Set",
          "Random Forest",
          "Types of Random Forest",
          "Why Random Forest",
          "Application of Random Forest",
          "Random Forest Implementation on Titanic Data Set",
          "Model Evaluation Technique",
          "Concept of R-Squared",
          "Linear Regression",
          "Classification",
          "Confusion Matrix",
          "Recall / Sensitivity / True Rate of Positive",
          "FB score",
          "AUC/ ROC curve",
          "Model Evaluation recall Curve"
        ],
        "Module Eight": [
          "Data Analysis using R",
          "Data Analysis using R: part 2",
          "All about R Language"
        ],
        "Featured Topics in Java": [
          "Big Data",
          "Intro to Hadoop",
          "Intro to Tableu",
          "Intro to Business Analytics"
        ],
        "Project: Telecom Churn Production": [
          "Project: Part 1: Let's get our system ready",
          "Project: part 2",
          "Project: Part 3",
          "Project: part 4",
          "Project: Let's Finalise it"
        ]
      },
      "requirements": [
        "An understanding of the fundamentals of Python programming",
        "Basic knowledge of statistics"
      ],
      "description": "Today Data Science and Machine Learning are used in almost every industry, including automobiles, banks, health, telecommunications, telecommunications, and more.\n\n\nAs the manager of Data Science and Machine Learning, you will have to research and look beyond common problems, you may need to do a lot of data processing. test data using advanced tools and build amazing business solutions. However, where and how will you learn these skills required in Data Science and Machine Learning?\nDATA SCIENCE COURSE-OVERVIEW\n\n\nGetting Started with Data Science\nDefine Data\nWhy Data Science?\nWho is a Data Scientist?\nWhat does a Data Scientist do?\nThe lifecycle of Data Science with the help of a use case\nJob trends\nData Science Components\nData Science Job Roles\nMath Basics\nMultivariable Calculus\nFunctions of several variables\nDerivatives and gradients\nStep function, Sigmoid function, Logit function, ReLU (Rectified Linear Unit) function\nCost function\nPlotting of functions\nMinimum and Maximum values of a function\nLinear Algebra\nVectors\nMatrices\nTranspose of a matrix\nThe inverse of a matrix\nThe determinant of a matrix\nDot product\nEigenvalues\nEigenvectors\nOptimization Methods\nCost function/Objective function\nLikelihood function\nError function\nGradient Descent Algorithm and its variants (e.g., Stochastic Gradient Descent Algorithm)\nProgramming Basics\nR Programming for Data Science\nHistory of R\nWhy R?\nR Installation\nInstallation of R Studio\nInstall R Packages.\nR for business\nFeatures of R\nBasic R syntax\nR programming fundamentals\nFoundational R programming concepts such as data types, vectors arithmetic, indexing, and data frames\nHow to perform operations in R including sorting, data wrangling using dplyr, and data visualization with ggplot2\nUnderstand and use the various graphics in R for data visualization.\nGain a basic understanding of various statistical concepts.\nUnderstand and use hypothesis testing method to drive business\ndecisions.\nUnderstand and use linear, non-linear regression models, and\nclassification techniques for data analysis.\nWorking with data in R\nMaster R programming and understand how various statements are executed in R.\nPython for Data Science\nIntroduction to Python for Data Science\nIntroduction to Python\nPython Installation\nPython Environment Setup\nPython Packages Installation\nVariables and Datatypes\nOperators\nPython Pandas-Intro\nPython Numpy-Intro\nPython SciPy-Intro\nPython Matplotlib-Intro\nPython Basics\nPython Data Structures\nProgramming Fundamentals\nWorking with data in Python\nObject-oriented programming aspects of Python\nJupyter notebooks\nUnderstand the essential concepts of Python programming such as data types, tuples, lists, dicts, basic operators and functions\nPerform high-level mathematical computing using the NumPy package and its vast library of mathematical functions\nPerform scientific and technical computing using the SciPy package and its sub-packages such as Integrate, Optimize, Statistics, IO, and Weave\nPerform data analysis and manipulation using data structures and tools provided in the Pandas package\nGain an in-depth understanding of supervised learning and unsupervised learning models such as linear regression, logistic regression, clustering, dimensionality reduction, K-NN and pipeline\nUse the matplotlib library of Python for data visualization\nExtract useful data from websites by performing web scraping using\nPython\nIntegrate Python with MapReduce\nData Basics\nLearn how to manipulate data in various formats, for example, CSV file, pdf file, text file, etc.\nLearn how to clean data, impute data, scale data, import and export data, and scrape data from the internet.\nLearn data transformation and dimensionality reduction techniques such as covariance matrix plot, principal component analysis (PCA), and linear discriminant analysis (LDA).\nProbability and Statistics Basics\nImportant statistical concepts used in data science\nDifference between population and sample\nTypes of variables\nMeasures of central tendency\nMeasures of variability\nCoefficient of variance\nSkewness and Kurtosis\nInferential Statistics\nRegression and ANOVA\nExploratory Data Analysis\nData visualization\nMissing value analysis\n\n\nIntroduction to Big Data\nIntroduction to Hadoop\nIntroduction to Tableau\nIntroduction to Business Analytics\nIntroduction to Machine Learning Basics\nSupervised vs Unsupervised\nTime Series Analysis\nText Mining\nData Science Capstone Project\n\n\nScience and Mechanical Data require in-depth knowledge on a variety of topics. Scientific data is not limited to knowing specific packages/libraries and learning how to use them. Science and Mechanical Data requires an accurate understanding of the following skills,\n\n\nUnderstand the complete structure of Science and Mechanical Data\n\n\nDifferent Types of Data Analytics, Data Design, Scientific Data Transfer Features and Machine Learning Projects\n\n\nPython Programming Skills which is the most popular language in Science and Mechanical Data\n\n\nMachine Learning Mathematics including Linear Algebra, Calculus and how to apply it to Machine Learning Algorithms and Science Data\n\n\nMathematics and Mathematical Analysis of Data Science\n\n\nData Science Data Recognition\n\n\nData processing and deception before installing Learning Machines\n\n\nMachine learning\n\n\nRidge (L2), Lasso (L1), and Elasticnet Regression / Regularization for Machine Learning\n\n\nSelection and Minimization Feature for Machine Learning Models\n\n\nSelection of Machine Learning Model using Cross Verification and Hyperparameter Tuning\n\n\nAnalysis of Machine Learning Materials Groups\n\n\nIn-depth learning uses the most popular tools and technologies of today.\n\n\nThis Data Science and Machine Learning course is designed to consider all of the above, True Data Science and Machine Learning A-Z Course. In most Data Science and Machine Learning courses, algorithms are taught without teaching Python or this programming language. However, it is very important to understand language structure in order to apply any discipline including Data Science and Mechanical Learning.\n\n\nAlso, without understanding Mathematics and Statistics it is impossible to understand how other Data Science and Machine Learning algorithms and techniques work.\n\n\nScience and Mechanical Data is a set of complex linked topics. However, we strongly believe in what Einstein once said,\n\n\n\"If you can't explain it easily, you didn't understand it well enough.\"\n\n\nAs a teacher, I constantly strive to reach my goal. This is one comprehensive course in Science and Mechanical Data that teaches you everything you need to learn Science and Mechanical Data using simple examples with great depth.\n\n\nAs you will see from the preview talks, some of the more complex topics are explained in simple language.\n\n\nSome important skills you will learn,\n\n\nPython Programming\n\n\nPython is listed as the # 1 language for Data Science and Mechanical Data. It is easy to use and rich with various libraries and functions required to perform various Data Science and Machine Learning activities. In addition, it is the most widely used and automated language for the use of many Deep Learning frameworks including Tensorflow and Keras.\n\n\n\n\n\n\nAdvanced Mathematics Learning Machine\n\n\nMathematics is the foundation of Data Science in general and Learning Machines in particular. Without understanding the meanings of Vectors, Matrices, their operations and understanding Calculus, it is impossible to understand the basics of Data Science and Machine Learning. The Gradient Declaration of Basic Neural Network and Mechanical Learning is built on the foundations of Calculus and Derivatives.\n\n\n\n\n\n\nPrevious Statistics for Data Science\n\n\nIt is not enough to know only what you are saying, in the middle, the mode, etc. Advanced Techniques for Science and Mechanical Data such as feature selection, size reduction using PCA are all based on previous Distribution and Statistical Significance calculations. It also helps us to understand the operation of the data and use the appropriate machine learning process to get the best results from various Data Science and Mechanical Learning techniques.\n\n\n\n\n\n\nData recognition\n\n\nAs they say, the picture costs a thousand words. Data identification is one of the most important methods of Data Science and Mechanical Data and is used for Analytical Data Analysis. In that, we analyze the data visually to identify patterns and styles. We will learn how to create different sites and charts and how to analyze them for all practical purposes. Feature Selection plays an important role in Machine Learning and Visualization Data is its key.\n\n\n\n\n\n\nData processing\n\n\nScientific Data requires extensive data processing. Data Science and Machine Learning specialists spend more than 2/3 of their time analyzing and analyzing data. Data can be noisy and never in good condition. Data processing is one of the most important ways for Data Science and Mechanics to learn to get the best results. We will be using Pandas which is a well-known Python data processing library and various other libraries for reading, analyzing, processing and cleaning data.\n\n\n\n\n\n\nMachine learning\n\n\nHeart and Soul Data Science is a guessing skill provided by algorithms from the Deep Learning and Learning Machines. Machine learning takes the complete discipline of Data Science ahead of others. We will integrate everything we have learned in previous sections and build learning models for various machines. The key features of Machine Learning are not only ingenuity but also understanding of the various parameters used by Machine Learning algorithms. We will understand all the key parameters and how their values affect the outcome in order to build the best machine learning models.",
      "target_audience": [
        "For Complete Beginners to Data Sciecne, which will make you Hero in the Data Science Field."
      ]
    },
    {
      "title": "Generate and visualize data in Python and MATLAB",
      "url": "https://www.udemy.com/course/suv-data-mxc/",
      "bio": "Learn how to simulate and visualize data for data science, statistics, and machine learning in MATLAB and Python",
      "objectives": [
        "Understand different categories of data",
        "Generate various datasets and modify them with parameters",
        "Visualize data using a multitude of techniques",
        "Generate data from distributions, trigonometric functions, and images",
        "Understand forward models and how to use them to generate data",
        "Improve MATLAB and Python programming skills"
      ],
      "course_content": {
        "Introductions": [
          "Following along in Python, MATLAB, or Octave",
          "Overall goals of this course",
          "Why and how to simulate data",
          "What is \"signal\" and what is \"noise\"?",
          "The importance of visualization"
        ],
        "Descriptive statistics and basic visualizations": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Mean, median, standard deviation, variance",
          "Histogram",
          "Interquartile range",
          "Violin plot"
        ],
        "Data distributions": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Normal and uniform distributions",
          "QQ plot",
          "Poisson distribution",
          "Log-normal distribution",
          "Measures of distribution quality (SNR and Fano factor)",
          "Cohen's d for separating distributions"
        ],
        "Time series signals": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Sharp transients",
          "Smooth transients",
          "Repeating: sine, square, and triangle waves",
          "Multicomponent oscillators",
          "Dipolar and multipolar chirps"
        ],
        "Time series noise": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Seeded reproducible normal and uniform noise",
          "Pink noise (aka 1/f aka fractal)",
          "Brownian noise (aka random walk)",
          "Multivariable correlated noise"
        ],
        "Image signals": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Lines and edges",
          "Sine patches and Gabor patches",
          "Geometric shapes",
          "Rings"
        ],
        "Image noise": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Image white noise",
          "Checkerboard patterns and noise",
          "Perlin noise in 2D",
          "Filtered 2D-FFT noise"
        ],
        "Data clustering in space": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Clusters in 2D",
          "Clusters in N-D"
        ],
        "Spatiotemporal structure using forward models": [
          "Course materials for this section (reader, MATLAB code, Python code)",
          "Forward model: 2D sheet",
          "Mixed overlapping forward models",
          "Example: Simulate human brain (EEG) data"
        ],
        "Bonus section": [
          "Bonus lecture"
        ]
      },
      "requirements": [
        "Interest in data",
        "High-school math",
        "Basic programming familiarity (MATLAB or Python)",
        "Familiarity with power spectra from the Fourier transform"
      ],
      "description": "Data science is quickly becoming one of the most important skills in industry, academia, marketing, and science. Most data-science courses teach analysis methods, but there are many methods; which method do you use for which data? The answer to that question comes from understanding data. That is the focus of this course.\n\nWhat you will learn in this course:\n\nYou will learn how to generate data from the most commonly used data categories for statistics, machine learning, classification, and clustering, using models, equations, and parameters. This includes distributions, time series, images, clusters, and more. You will also learn how to visualize data in 1D, 2D, and 3D.\nAll videos come with MATLAB and Python code for you to learn from and adapt!\n\n\nThis course is for you if you are an aspiring or established:\nData scientist\nStatistician\nComputer scientist (MATLAB and/or Python)\nSignal processor or image processor\nBiologist\nEngineer\nStudent\nCurious independent learner!\nWhat you get in this course:\n>6 hours of video lectures that include explanations, pictures, and diagrams\npdf readers with important notes and explanations\nExercises and their solutions\nMATLAB code and Python code\nWith >4000 lines of MATLAB and Python code, this course is also a great way to improve your programming skills, particularly in the context of data analysis, statistics, and machine learning.\n\n\nWhat do you need to know before taking this course?\nYou need some experience with either Python or MATLAB programming. You don't need to be an expert coder, but if you are comfortable working with variables, for-loops, and basic plotting, then you already know enough to take this course!",
      "target_audience": [
        "Data scientists who want to learn how to generate data",
        "Statisticians who want to evaluate and validate methods",
        "Someone who wants to improve their MATLAB skills",
        "Someone who wants to improve their Python skills",
        "Scientists who want a better understanding of data characteristics",
        "Someone looking for tools to better understand data",
        "Anyone who wants to learn how to visualize data"
      ]
    },
    {
      "title": "MySQL - Statistics for Data Science & Business Analytics",
      "url": "https://www.udemy.com/course/ureadup_sds/",
      "bio": "SQL - MySQL for Data Analytics - Beginners - Statistics for Data Science - MySQL for Data Analysis - with 25 projects",
      "objectives": [
        "SQL - MySQL for Data science.",
        "Write complex SQL queries across multiple tables.",
        "Relational databases versus non relational databases.",
        "Learn how to code in SQL",
        "Sampling distribution with practical simulation apps and answering of important technical questions.",
        "Confidence level and Confidence interval.",
        "Distinguish and work with different types of distributions .",
        "Inferential and Descriptive statistics with collection of important quizzes and examples .",
        "One sample mean t test .",
        "Two sample means t test .",
        "How to calculate P value using manual and direct method ?",
        "What is after data analysis ?",
        "Null hypothesis and alternative hypothesis .",
        "Understand What is P value ?",
        "Data types and Why we need to study data types ?",
        "What is Type one error ?",
        "Relationship between Type one error and Alpha ( non confident probability )",
        "Is Normal distribution and t distributions are cousins ?",
        "Projects like Estimation of goals in premier league ( using confidence interval ) , and more .. and more to learn it",
        "What is \"double edged sword of statistics\" ?",
        "Practical significance versus statistical significance , and more and more to learn it"
      ],
      "course_content": {},
      "requirements": [
        "Definitely no experience is required",
        "I will start from level ZERO and gradually step by step i will make you in advanced level .",
        "All i need from you is to be patient when learning and i'm sure you will love SQL and statistics for data science ."
      ],
      "description": "300+ Lectures/Articles lectures include real life practical projects and examples for people need to learn SQL - MySQL  for data science and statistics\nEnjoy practice  25 Apps/Projects in SQL - MySQL for data analytics and learn Statistics  for data science .\n\nThis is the Complete 2021 Bootcamp , Two courses in ONE COURSE .\nDo you like jobs in MySQL for data science ?\nDo you like jobs in Machine learning ?\nDo you like jobs in Marketing analyst ?\nDo you like jobs in Programming for data science ?\nDo you like jobs in Business analysis and business intelligence ?\n\nIf the answer is yes , then all of the above needs MySQL and Statistics for data science .\n\n\nWho prepared this course material ?\nThis course material is prepared from highly experienced engineers worked in a leader companies like Microsoft , Facebook and google .\nAfter hard working from five months ago we created 300+ Lectures/Articles to cover everything related to SQL - MySQL  & statistics for data science  .\nIn no time with simple and easy way you will learn and love  SQL for data science and statistics .\nWe stress in this course to make it very spontaneous to make all students love SQL and statistics for data science .\n\n\nWho's teaching you in this course ?\nI'm senior developer and chief data science engineer  . i worked for many projects related to expert systems and artificial intelligence .\nAlso i worked as Tutor and consultant trainer with a leader international companies located in USA and UK .\nI spent over five  months  of hard working to create 300+ Lectures/Articles  in super high quality  to make all students enjoy and love  SQL for data science and statistics.\nI'm sharing a lot of practical experience from my own work with you in this  course .\n\n\n\nWhy learn MySQL  or SQL ?\nMySQL is a popular database platform for businesses because it is extremely easy to use. It is commonly used in combination with web development and data science . You hear “it’s easy to work with” a lot in relation to computer languages, but MySQL truly is simple.\nFor instance, someone with little to no knowledge of MySQL can easily establish a database .\nOf course, a lot of hosting providers make this process even simpler by handling all the necessary tasks for new website administrators, but it doesn’t detract from the point that MySQL is relatively easy to use.\nI could not imagine data science without databases .\n\nWhat is my final goal after my students enroll in this course ?\nMy final goal is to make all students and engineers  love SQL for data science and statistics .\nMy big challenge in this course is to make it professional course at the same time it should be very easy and simple for all People .\nTherefore you will notice that i used a lot of  graphics and imaginary ideas to make you LOVE SQL for data science and statistics\n\nSo, what are you waiting for? Click the “Take this course” button, and let’s begin this journey together!\n\n\nWhat is course contents ?\nStarting introduction to data science and data analysis .\nUnderstand programming basics for data science\nlearn SQL - MySQL basics .\nWrite all the SQL joins\nCreate Foreign key and primary key in MySQL databases .\nLearn how to start and stop MySQL server .\nAnalyze data using Aggregate Functions .\nRead and import external CSV files into MySQL database .\nExport MySQL database table contents into CSV file .\nAwesome Projects and examples like :\nHow Mr. Genie helped us to find all fishes in the sea ?\nMr. Genie power versus statistics power\nThe double edged sword of statistics\nHelp fisherman to catch Tuna using sampling distribution\nIce Cola example with student's t distribution .\nEstimation of goals in premier league ( using confidence interval ) .\nTAKE YOUR BREATH BEFORE HYPOTHESIS TESTING\nOne sample mean t test .\nTwo sample mean t test .\nNull hypothesis and alternative hypothesis .\nWhat is P value ?\nHow to calculate P value using manual and direct method ?\nTwo mini stories for TWO PROJECTS  related to hypothesis testing\nProject one is how Sarah used \"one sample mean  t test\" for Ice Cola factory  to prove that her brother Ibrahim is innocent ?\nProject two how Sarah used \"two sample mean t test\" for Ice Cola factory to help her brother Ibrahim to increase Ice Cola sales in winter ?\n... more and more\n.... and more and more course contents\n\n\n#SQL #MySQL #MySQL-for-data-science #SQL-for-Data-science",
      "target_audience": [
        "People interested to learn programming and SQL for data science",
        "Beginners in programming",
        "Beginners in data science",
        "Beginners in MySQL databases for data science",
        "Anyone needs to learn statistics from beginner to advanced level .",
        "Statistics for data science",
        "Statistics for data analysis",
        "Marketing analyst",
        "AP statistics",
        "beginner in business intelligence"
      ]
    },
    {
      "title": "Decision Tree - Theory, Application and Modeling using R",
      "url": "https://www.udemy.com/course/decision-tree-theory-application-and-modeling-using-r/",
      "bio": "Analytics/ Supervised Machine Learning/ Data Science: CHAID / CART / Random Forest etc. workout (Python demo at the end)",
      "objectives": [
        "Get Crystal clear understanding of decision tree",
        "Understand the business scenarios where decision tree is applicable",
        "Become comfortable to develop decision tree using R statistical package",
        "Understand the algorithm behind decision tree i.e. how does decision tree software work",
        "Understand the practical way of validation, auto validation and implementation of decision tree"
      ],
      "course_content": {
        "Introduction to decision tree": [
          "Welcome Note",
          "Section Overview",
          "Need of a decision tree",
          "Anatomy of a Decision Tree",
          "Gain From a Decision Tree",
          "KS of a decision tree",
          "Business Application of a Decison tree",
          "Defintions related with Objective segmentation",
          "Decision Tree vs Logistic Regression",
          "Check basic understanding of decision tree",
          "FAQ: for Introduction section",
          "Section PDF"
        ],
        "1 A : Model Design - Ensure actionable data for modeling": [
          "Section Overview",
          "Model Design in Principal",
          "Model Design Precautions",
          "Model Design Outcome",
          "Performance Window Design",
          "Check basic understanding of model design",
          "Data Audit n Treatment Guideline and section PDF"
        ],
        "Demo of Decision Tree development using R": [
          "Section Overview",
          "Understand The data for Demo",
          "View resource to download files",
          "How to download excel files, R program etc?",
          "Install R and R Studio",
          "First Decision Tree in R",
          "Second Decision Tree in R",
          "About New additions in the course work",
          "Practical_Usage_of_classification_tree - demo",
          "Practical_Usage_of_classification_tree - assignment",
          "Practical_Usage_of_classification_tree - assignment solution",
          "Section PDF"
        ],
        "Algorithm behind decision tree": [
          "Section Overview",
          "Intutive Understanding of Numeric Variable Split",
          "GINI Index of a node",
          "GINI Index of a Split",
          "CART in action : Decide which variable n its value for the split",
          "Practical approach of Decision Tree Development",
          "Some practical situation of decision tree model validation",
          "Implementing decision tree model",
          "Auto Pruning Technique of decision tree development part 1",
          "K Fold Cross Validation",
          "Auto Pruning Using R.",
          "Developing Regression Tree Using R",
          "Interpret Regression Tree Output",
          "Another Regression Tree Using R",
          "Practical_Usage_of_Regression_tree - demo part 1",
          "Practical_Usage_of_Regression_tree - demo part 2",
          "Practical_Usage_of_Regression_tree - assignment",
          "Practical_Usage_of_Regression_tree - assignment solution",
          "Linear Regression vs Regression tree",
          "CHAID Algorithm",
          "CHAID vs CART",
          "Check basic understanding of algorithm behind decision tree",
          "FAQ - for algorithm behind decision tree section",
          "Section PDF",
          "Appendix Content - Chi Square Statistic",
          "Appendix Content - Feel The Chi Square Statistic",
          "Appendix content - Degree of freedom of a cross tab",
          "Appendix content - Chi Square Distribution",
          "Appendix content - PDF"
        ],
        "Other algorithm of decision tree development": [
          "Section Overview",
          "Entropy of a Node",
          "Entropy of a Split",
          "ID3 Method",
          "Random Forest Method",
          "R syntax for Random Forest",
          "Ensemble Learning - Bagging and Bossting",
          "Check basic understanding of algorithm behind decision tree -01",
          "Section FAQ - Other algorithm of decision tree",
          "Introduction to Gradient Boosting",
          "FAQ - For other algorithm of decision tree",
          "Section PDF",
          "Bonus topic - Decision Tree using Python",
          "Bonus Topic - Analytics / Data Science / Machine Learning Interview questions",
          "Closure Note"
        ]
      },
      "requirements": [
        "The course is fairly simple but it will help if they understand how to read excel formula"
      ],
      "description": "What is this course?\nDecision Tree Model building is one of the most applied technique in analytics vertical. The decision tree model is quick to develop and easy to understand. The technique is simple to learn. A number of business scenarios in lending business / telecom / automobile etc. require decision tree model building.\nThis course ensures that student get understanding of\nwhat is the decision tree\nwhere do you apply decision tree\nwhat benefit it brings\nwhat are various algorithm behind decision tree\nwhat are the steps to develop decision tree in R\nhow to interpret the decision tree output of R\nCourse Tags\nDecision Tree\nCHAID\nCART\nObjective segmentation\nPredictive analytics\nID3\nGINI\nMaterial in this course\nthe videos are in HD format\nthe presentation used to create video are available to download in PDF format\nthe excel files used is available to download\nthe R program used is also available to download\nHow long the course should take?\nIt should take approximately 8 hours to internalize the concepts and become comfortable with the decision tree modeling using R\nThe structure of the course\nSection 1 – motivation and basic understanding\nUnderstand the business scenario, where decision tree for categorical outcome is required\nSee a sample decision tree – output\nUnderstand the gains obtained from the decision tree\nUnderstand how it is different from logistic regression based scoring\nSection 2 – practical (for categorical output)\nInstall R - process\nInstall R studio - process\nLittle understanding of R studio /Package / library\nDevelop a decision tree in R\nDelve into the output\nSection 3 – Algorithm behind decision tree\nGINI Index of a node\nGINI Index of a split\nVariable and split point selection procedure\nImplementing CART\nDecision tree development and validation in data mining scenario\nAuto pruning technique\nUnderstand R procedure for auto pruning\nUnderstand difference between CHAID and CART\nUnderstand the CART for numeric outcome\nInterpret the R-square meaning associated with CART\nSection 4 – Other algorithm for decision tree\nID3\nEntropy of a node\nEntropy of a split\nRandom Forest Method\nWhy take this course?\nTake this course to\nBecome crystal clear with decision tree modeling\nBecome comfortable with decision tree development using R\nHands on with R package output\nUnderstand the practical usage of decision tree",
      "target_audience": [
        "Data Mining professionals",
        "Analytics professionals",
        "People seeking job in analytics industry"
      ]
    },
    {
      "title": "Data Science and Machine Learning Fundamentals [2025]",
      "url": "https://www.udemy.com/course/data-science-and-machine-learning-fundamentals-one/",
      "bio": "Learn to master Data Science and Machine Learning Fundamentals with Python and Pandas",
      "objectives": [
        "Knowledge about Data Science and Machine Learning theory, algorithms, methods, best practices, and tasks",
        "Deep hands-on knowledge about Data Science and Machine Learning, and know how to do common Data Science and Machine Learning tasks",
        "The ability to handle common Data Science and Machine Learning tasks with confidence",
        "Master Python for Data Handling",
        "Master Pandas for Data Handling",
        "Knowledge and practical hands-on knowledge of Scikit-learn, Statsmodels, Matplotlib, Seaborn, and many other Python libraries",
        "Detailed and deep, Master knowledge of Regression, Regression Analysis, Prediction, Classification, and Cluster analysis",
        "Advanced knowledge of A.I. prediction models and automatic model creation",
        "Advanced Knowledge of Text Mining, Text Mining Tasks, and Emotion Mining",
        "Cloud computing: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources"
      ],
      "course_content": {
        "Introduction": [
          "Course introduction",
          "Setup of the Anaconda Cloud Notebook",
          "Download and installation of the Anaconda Distribution (optional)",
          "The Conda Package Management System (optional)"
        ],
        "Master Python for data handling": [
          "Overview",
          "Python Integers",
          "Python Floats",
          "Python Strings",
          "Python String Methods",
          "Python Strings and DateTime Objects",
          "Python Data Storage Overview",
          "Python Set",
          "Python Tuple",
          "Python Dictionary",
          "Python List",
          "Data Transformers and Functions Overview",
          "Python While Loop",
          "Python For Loop",
          "Python Conditional Code Branching and Logic Operators",
          "Python Function Theory",
          "Python Functions: create your own functions",
          "Python Object Oriented Programming: Some Theory",
          "Python Object Oriented Programming II: OOP",
          "Python Object Oriented Programming III: Files and Tables",
          "Python Object Oriented Programming IV: Recap and More"
        ],
        "Master Pandas for Data Handling": [
          "Master Pandas for Data Handling: Overview",
          "Pandas Theory and Terminology",
          "Creating a DataFrame from scratch",
          "Pandas File Handling: Overview",
          "Pandas File Handling: The .csv file format",
          "Pandas File Handling: The .xlsx file format",
          "Pandas File Handling: SQL-database files",
          "Pandas Operations & Techniques: Overview",
          "Pandas Operations & Techniques: Object Inspection",
          "Pandas Operations & Techniques: DataFrame Inspection",
          "Pandas Operations & Techniques: Column Selections",
          "Pandas Operations & Techniques: Row Selections",
          "Pandas Operations & Techniques: Conditional Selections",
          "Pandas Operations & Techniques: Scalers and Standardization.",
          "Pandas Operations & Techniques: Concatenate DataFrames",
          "Pandas Operations & Techniques: Joining DataFrames",
          "Pandas Operations & Techniques: Merging DataFrames",
          "Pandas Operations & Techniques: Transpose & Pivot Functions",
          "Pandas Data Preparation: Overview & workflow",
          "Pandas Data Preparation II: Edit DataFrame labels",
          "Pandas Data Preparation III: Duplicates",
          "Pandas Data Preparation IV: Missing Data & Imputation",
          "Pandas Data Preparation V: Data Binnings [Extra Video]",
          "Pandas Data Preparation VI: Indicator Features [Extra Video]",
          "Pandas Data Description: Overview",
          "Pandas Data Description II: Sorting and Ranking",
          "Pandas Data Description III: Descriptive Statistics",
          "Pandas Data Description IV: Crosstabulations & Groupings",
          "Pandas Data Visualization: Overview",
          "Pandas Data Visualization II: Histograms",
          "Pandas Data Visualization III: Boxplots",
          "Pandas Data Visualization IV: Scatterplots",
          "Pandas Data Visualization V: Pie Charts",
          "Pandas Data Visualization VI: Line plots"
        ],
        "Master Regression and Prediction with Machine Learning models": [
          "Regression, Prediction, and Supervised Learning. Section Overview (I)",
          "The Traditional Simple Regression Model (II)",
          "The Traditional Simple Regression Model (III)",
          "Some practical and useful modelling concepts (IV)",
          "Some practical and useful modelling concepts (V)",
          "Linear Multiple Regression model (VI)",
          "Linear Multiple Regression model (VII)",
          "Multivariate Polynomial Multiple Regression models (VIII)",
          "Multivariate Polynomial Multiple Regression models (VIIII)",
          "Regression Regularization, Lasso and Ridge models (X)",
          "Decision Tree Regression models (XI)",
          "Random Forest Regression (XII)",
          "Voting Regression (XIII)"
        ],
        "Master Classification with Machine Learning models": [
          "Classification and Supervised Learning, overview",
          "Logistic Regression Classifier",
          "The Naive Bayes Classifier",
          "K-Nearest Neighbor Classifier (KNN) [Extra Video]",
          "The Decision Tree Classifier",
          "The Random Forest Classifier",
          "Linear Discriminant Analysis (LDA) [Extra Video]",
          "The Voting Classifier"
        ],
        "Master Cluster Analysis and Unsupervised Learning": [
          "Overview",
          "K-Means Cluster Analysis",
          "Auto-updated K-Means Cluster Analysis, introduction and simulation",
          "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
          "Four Hierarchical Clustering algorithms",
          "Principal Component Analysis (PCA)"
        ],
        "Advanced Machine Learning models and tasks": [
          "Overview",
          "Artificial Neural Networks, Feedforward Networks, and the Multi-Layer Perceptron",
          "Feedforward Multi-Layer Perceptrons for Classification tasks",
          "Feedforward Multi-Layer Perceptrons for Prediction tasks"
        ],
        "Text Mining and NLP": [
          "Text Mining and NLP introduction and overview",
          "Text Mining Setup",
          "Text Mining Tasks",
          "Text Mining Process",
          "Text Indexing Process",
          "The Tokenization Process",
          "Spelling correction and stop words",
          "Lemmatization and Stemming",
          "The Bag of Words Data Structure and some models",
          "The TF-IDF Data Structure and some models",
          "The N-grams Data Structure",
          "Attention-based models and Generative Pre-trained Transformer models",
          "Emotion Mining and Sentiment Analysis"
        ]
      },
      "requirements": [
        "The four ways of counting (+-*/)",
        "Everyday experience with Windows, Linux, or MacOS"
      ],
      "description": "This course is an exciting hands-on view of the fundamentals of Data Science and Machine Learning\nData Science and Machine Learning are developing on a massive scale. Everywhere you look in society, the world wide web, or in technology, you will find Data Science and Machine Learning algorithms working behind the scenes to analyze and optimize all aspects of our lives, businesses, and our society. Data Science and Machine Learning with Artificial Intelligence are some of the hottest and fastest-developing areas right now.\nThis course will teach you the fundamentals of Data Science and Machine Learning. This course has exclusive content that will teach you many new things regardless of if you are a beginner or an experienced Data Scientist, and aspires to be one of the best Udemy courses in terms of education and value.\nYou will learn about\nRegression and Prediction with Machine Learning models using supervised learning. This course has the most complete and fundamental master-level regression analysis content packages on Udemy, with hands-on, useful practical theory, and automatic Machine Learning algorithms for model building, feature selection, and artificial intelligence. You will learn about models ranging from linear regression models to advanced multivariate polynomial regression models.\nClassification with Machine Learning models using supervised learning. You will learn about the classification process, classification theory, and visualizations as well as some useful classifier models, including the very powerful Random Forest Classifier Ensembles and Voting Classifier Ensembles.\nCluster Analysis with Machine Learning models using unsupervised learning. In this part of the course, you will learn about unsupervised learning, cluster theory, artificial intelligence, explorative data analysis, and seven useful Machine Learning clustering algorithms ranging from hierarchical cluster models to density-based cluster models.\nThe fundamentals of Data Science and Machine Learning. This course gives a very solid foundation and knowledge base for Data Science and Machine Learning jobs or studies.\nAdvanced A.I. prediction models and automatic model creation. This video course includes videos where the use of very powerful algorithms for automatic model creation is taught.\nAdvanced Text Mining and Automation. You will learn to mine text data and the fundamentals of Text and Emotion Mining such as Tokenization, text data preparation, spell checking, lemmatization, stemming, and classification of text data.\nMastering Python for data handling.\nMastering Pandas for data handling.\nThis course includes\na comprehensive and easy-to-follow teaching package for Mastering Python and Pandas for data handling, which makes anyone able to learn the course contents regardless of beforehand knowledge of programming, tabulation software, Python, Pandas, Data Science, or Machine Learning.\nLearn to use Cloud computing: Use the Anaconda Cloud Notebook (Cloud-based Jupyter Notebook). Learn to use Cloud computing resources\nan optional easy-to-follow guide for downloading, installing, and setting up the Anaconda Distribution, which makes anyone able create a local installation of a Python Data Science and Machine Learning environment.\ncontent that will teach you many new things, regardless of if you are a beginner or an experienced Data Scientist.\na large collection of unique content, and will teach you many new things that only can be learned from this course on Udemy.\nA complete masterclass package for Data Science and Machine Learning.\nA course structure built on a proven and professional framework for learning.\nA compact course structure and no killing time.\nIs this course for you?\nThis course is for you, regardless if you are a beginner or an experienced Data Scientist.\nThis course is for you, regardless if you have no education or are experienced with a Ph.D.\nCourse requirements\nThe four ways of counting (+-*/)\nBasic everyday experience with either Windows, Linux, Mac OS, or similar operating systems\nAfter completing this course, you will have\nKnowledge about Data Science and Machine Learning theory, algorithms, methods, best practices, and tasks.\nDeep hands-on knowledge of Data Science and Machine Learning, and know how to do common Data Science and Machine Learning tasks.\nThe ability to handle common Data Science and Machine Learning tasks with confidence.\nKnowledge to Master Python for Data Handling.\nKnowledge to Master Pandas for Data Handling.\nKnowledge and practical hands-on knowledge of Scikit-learn, Stats models, Matplotlib, Seaborn, and many other Python libraries.\nDetailed and deep Master knowledge of Regression Prediction, Classification, and Cluster Analysis.\nAdvanced knowledge of A.I. prediction models and automatic model creation.\nAdvanced Knowledge of Text Mining, Text Mining Tasks, and Emotion Mining.",
      "target_audience": [
        "This course is for you, regardless if you are a beginner or experienced Data Scientist, regardless if you have a Ph.D., or no education or experience at all."
      ]
    },
    {
      "title": "Python Data Visualization: Matplotlib & Seaborn Masterclass",
      "url": "https://www.udemy.com/course/python-data-visualization-matplotlib-seaborn/",
      "bio": "Learn Python's Matplotlib + Seaborn libraries for data analytics & business intelligence, w/ a top Python instructor!",
      "objectives": [
        "Master Python's Matplotlib & Seaborn libraries, two of Python's most powerful data visualization packages",
        "Design and format 20+ chart types using Matplotlib & Seaborn, including line charts, bar charts, scatter plots, histograms, violin plots, heatmaps and more",
        "Learn advanced customization options like subplots, gridspec, style sheets and parameters",
        "Apply best practices for Python data visualization, storytelling, formatting and visual design",
        "Build powerful, practical skills for modern analytics, data science and business intelligence"
      ],
      "course_content": {
        "Getting Started": [
          "Course Structure & Outline",
          "READ ME: Important Notes for New Students",
          "DOWNLOAD: Course Resources",
          "Introducing the Course Project",
          "Setting Expectations",
          "Jupyter Installation & Launch"
        ],
        "Intro to Data Visualization": [
          "Why Visualize Data?",
          "3 Key Questions",
          "Essential Visuals",
          "Chart Formatting & Storytelling",
          "Common Visualization Mistakes",
          "Key Takeaways",
          "QUIZ: Intro to Data Visualization"
        ],
        "Matplotlib Fundamentals": [
          "Intro to Matplotlib",
          "Plotting Methods",
          "Plotting DataFrames",
          "ASSIGNMENT: Plotting DataFrames",
          "SOLUTION: Plotting DataFrames",
          "Anatomy of a Matplotlib Figure",
          "Chart Titles & Font Sizes",
          "Chart Legends",
          "Line Styles",
          "Axis Limits",
          "Figure Sizes",
          "Custom Axis Ticks",
          "Vertical Lines",
          "Adding Text",
          "PRO TIP: Text Annotations",
          "Removing Borders",
          "ASSIGNMENT: Formatting Charts",
          "SOLUTION: Formatting Charts",
          "Line Charts",
          "Stacked Line Charts",
          "Dual Axis Charts",
          "ASSIGNMENT: Dual Axis Line Charts",
          "SOLUTION: Dual Axis Line Charts",
          "Bar Charts",
          "ASSIGNMENT: Bar Charts",
          "SOLUTION: Bar Charts",
          "Stacked Bar Charts",
          "Grouped Bar Charts",
          "Combo Charts",
          "ASSIGNMENT: Advanced Bar Charts",
          "SOLUTION: Advanced Bar Charts",
          "Pie & Donut Charts",
          "ASSIGNMENT: Pie & Donut Charts",
          "SOLUTION: Pie & Donut Charts",
          "Scatterplots & Bubble Charts",
          "Histograms",
          "ASSIGNMENT: Scatterplots & Histograms",
          "SOLUTION: Scatterplots & Histograms",
          "Key Takeaways",
          "QUIZ: Matplotlib Fundamentals"
        ],
        "PROJECT #1: Analyzing the Global Coffee Market": [
          "Project #1 Introduction",
          "Project #1 Solution Walkthrough"
        ],
        "Advanced Customization": [
          "Intro to Advanced Customization",
          "Subplots",
          "ASSIGNMENT: Subplots",
          "SOLUTION: Subplots",
          "GridSpec",
          "ASSIGNMENT: GridSpec",
          "SOLUTION: GridSpec",
          "Color Options",
          "Color Palettes",
          "ASSIGNMENT: Colors",
          "SOLUTION: Colors",
          "Style Sheets",
          "ASSIGNMENT: Style Sheets",
          "SOLUTION: Style Sheets",
          "rcParameters",
          "Saving Figures & Images",
          "Key Takeaways",
          "QUIZ: Advanced Customization"
        ],
        "PROJECT #2: Visualizing Global Coffee Production": [
          "Project #2 Introduction",
          "Project #2 Solution Walkthrough"
        ],
        "Visualization with Seaborn": [
          "Intro to Seaborn",
          "Basic Formatting Options",
          "Bar Charts & Histograms",
          "ASSIGNMENT: Bar Charts & Histograms",
          "SOLUTION: Bar Charts & Histograms",
          "Box & Violin Plots",
          "ASSIGNMENT: Box & Violin Plots",
          "SOLUTION: Box & Violin Plots",
          "Linear Relationship Charts",
          "Jointplots",
          "PairPlots",
          "ASSIGNMENT: Linear Relationship Charts",
          "SOLUTION: Linear Relationship Charts",
          "Heatmaps",
          "ASSIGNMENT: Heatmaps",
          "SOLUTION: Heatmaps",
          "FacetGrid",
          "Matplotlib Integration",
          "Key Takeaways",
          "QUIZ: Visualization with Seaborn"
        ],
        "PROJECT #3: Analyzing Used Car Sales": [
          "Project #3 Introduction",
          "Project #3 Solution Walkthrough"
        ],
        "BONUS LESSON": [
          "BONUS LESSON"
        ]
      },
      "requirements": [
        "We'll use Anaconda & Jupyter Notebooks (a free, user-friendly coding environment)",
        "Familiarity with base Python is strongly recommended, but not a strict prerequisite"
      ],
      "description": "This is a hands-on, project-based course designed to help you learn two of the most popular Python packages for data visualization and business intelligence: Matplotlib & Seaborn.\n\n\nWe'll start with a quick introduction to Python data visualization frameworks and best practices, and review essential visuals, common errors, and tips for effective communication and storytelling.\n\n\nFrom there we'll dive into Matplotlib fundamentals, and practice building and customizing line charts, bar charts, pies & donuts, scatterplots, histograms and more. We'll break down the components of a Matplotlib figure and introduce common chart formatting techniques, then explore advanced customization options like subplots, GridSpec, style sheets and parameters.\n\n\nFinally we'll introduce Python's Seaborn library. We'll start by building some basic charts, then dive into more advanced visuals like box & violin plots, PairPlots, heat maps, FacetGrids, and more.\n\n\nThroughout the course you'll play the role of a Consultant at Maven Consulting Group, a firm that provides strategic advice to companies around the world. You'll practice applying your skills to a range of real-world projects and case studies, from hotel customer demographics to diamond ratings, coffee prices and automotive sales.\n\n\nCOURSE OUTLINE:\n\n\nIntro to Python Data Visualization\nLearn data visualization frameworks and best practices for choosing the right charts, applying effective formatting, and communicating clear, data-driven stories and insights\n\n\nMatplotlib Fundamentals\nExplore Python's Matplotlib library and use it to build and customize several essential chart types, including line charts, bar charts, pie/donut charts, scatterplots and histograms\n\n\nPROJECT #1: Analyzing the Global Coffee Market\nRead data into Python from CSV files provided by a major global coffee trader, and use Matplotlib to visualize volume and price data by country\n\n\nAdvanced Formatting & Customization\nApply advanced customization techniques in Matplotlib, including multi-chart figures, custom layout and colors, style sheets, gridspec, parameters and more\n\n\nPROJECT #2: Visualizing Global Coffee Production\nContinue your analysis of the global coffee market, and leverage advanced data visualization and formatting techniques to build a comprehensive report to communicate key insights\n\n\nData Visualization with Seaborn\nVisualize data with Python's Seaborn library, and build custom visuals using additional chart types like box plots, violin plots, joint plots, pair plots, heatmaps and more\n\n\nPROJECT #3: Analyzing Used Car Sales\nUse Seaborn and Matplotlib to explore, analyze and visualize automotive auction data to help your client identify the best deals on used service vehicles for the business\n\n\nJoin today and get immediate, lifetime access to the following:\n\n\n7.5 hours of high-quality video\nPython Matplotlib & Seaborn PDF ebook (150+ pages)\nDownloadable project files & solutions\nExpert support and Q&A forum\n30-day Udemy satisfaction guarantee\n\n\nIf you're a data analyst. data scientist, business intelligence professional or data engineer looking to add Matplotlib & Seaborn to your Python data analysis and visualization skill set, this is the course for you!\n\n\nHappy learning!\n-Chris Bruehl (Python Expert & Lead Python Instructor, Maven Analytics)\n\n\n__________\nLooking for our full business intelligence stack? Search for \"Maven Analytics\" to browse our full course library, including Excel, Power BI, MySQL, Tableau and Machine Learning courses!\n\n\nSee why our courses are among the TOP-RATED on Udemy:\n\n\n\"Some of the BEST courses I've ever taken. I've studied several programming languages, Excel, VBA and web dev, and Maven is among the very best I've seen!\" Russ C.\n\n\n\"This is my fourth course from Maven Analytics and my fourth 5-star review, so I'm running out of things to say. I wish Maven was in my life earlier!\" Tatsiana M.\n\n\n\"Maven Analytics should become the new standard for all courses taught on Udemy!\" Jonah M.",
      "target_audience": [
        "Analysts or business intelligence professionals looking to learn data visualization with Matplotlib and Seaborn",
        "Aspiring data scientists who want to build or strengthen their Python data visualization skills",
        "Anyone interested in learning one of the most popular open source programming languages in the world",
        "Students looking to learn powerful, practical skills with unique, hands-on projects and course demos"
      ]
    },
    {
      "title": "Machine Learning Model Deployment with Streamlit",
      "url": "https://www.udemy.com/course/machine-learning-model-deployment-with-streamlit/",
      "bio": "Deploy ML models with Streamlit and share your data science work with the world",
      "objectives": [
        "Understand the core concepts and features of Streamlit",
        "Build interactive data-driven web applications to deploy your model",
        "Master the advanced features and integrations in Streamlit",
        "Apply the best practices and optimization techniques for Streamlit",
        "Connect your Streamlit app to data sources",
        "Deploy your Streamlit app for free"
      ],
      "course_content": {
        "Introduction to Streamlit": [
          "Welcome!",
          "Installation and setup",
          "Overview of Streamlit and its features",
          "Creating a basic Streamlit app"
        ],
        "Streamlit fundamentals": [
          "Text elements in Streamlit",
          "Data display elements",
          "Charting elements",
          "Input widgets - Part 1",
          "Input widgets - Part 2",
          "Forms in Streamlit",
          "Customize the layout",
          "Capstone project - Build an interactive dashboard",
          "Capstone project - Build an interactive dashboard - Solution"
        ],
        "Caching": [
          "Basics of caching in Streamlit",
          "Code - Basics of caching",
          "Refactor our dashboard with caching",
          "Advanced caching in Streamlit",
          "Capstone project - Deploy a classification model with caching",
          "Improving our last capstone"
        ],
        "Session state management": [
          "Basics of state mangement",
          "Code - State management",
          "Advanced state management",
          "Code - Advanced state management",
          "Build a temperature conversion calculator",
          "Capstone project - Deploy a regression model with state management"
        ],
        "Multipage applications": [
          "Basics of multipage applications",
          "Code - Build your first multipage app",
          "Widget state mangement in multipage apps",
          "Code - Implement a workaround for multipage apps",
          "Capstone project - Train and rank different classification models"
        ],
        "Authentication": [
          "Basic authentication",
          "Code - Basic authentication",
          "Streamlit-Authenticator",
          "Code - Streamlit-Authenticator",
          "Capstone project - Clustering for a marketing campaign"
        ],
        "Connect to data sources": [
          "Connect to data sources",
          "Code - Connect to a database (Supabase)",
          "Code - Make API calls",
          "Capstone project - Deploy a demand forecasting model"
        ],
        "Deploy to production": [
          "Deployment process",
          "Deploy a Streamlit app",
          "Advanced deployment concepts",
          "Deploy a Streamlit app with secrets",
          "Next steps"
        ]
      },
      "requirements": [
        "A working knowledge of Python and machine learning is required.",
        "This course focuses only on deploying models using Streamlit. We will not spend time explaining how the models work or how they are developed and trained.",
        "A computer with Anaconda installed.",
        "Your favourite text editor installed (I use VSCode)"
      ],
      "description": "The complete course to deploy machine learning models using Streamlit. Build web applications powered by ML and AI and deploy them to share them with the world.\n\n\nThis course will take you from the basics to deploying scalable applications powered by machine learning. To put your knowledge to the test, I have designed more than six capstone projects with full guided solutions.\n\n\nThis course covers:\n\n\nBasics of Streamlit\nAdd interactive elements, like buttons, forms, sliders, input elements, etc.\nDisplay charts\nCustomize the layout of your application\nCapstone project: build an interactive dashboard\nCaching\nPerformance enhancement with caching\nBasic and advanced usage of caching\nCapstone project: deploy a classification model\nSession state management\nAdd more interactivity and boost performance with session state management\nBasic and advanced usage of session state\nCapstone project: deploy a regression model\nMultipage applications\nBuild large apps with multiple pages\nCapstone project: train and rank classification models\nAuthentication\nAdd a security layer with authentication\nAdd login/logout components\nAdvanced authentication with user management, reset password, etc.\nCapstone project: deploy a clustering model for marketing\nConnect to data sources\nConnect to databases\nAccess data through APIs\nCapstone project: Deploy a sales demand model\nDeployment\nDeploy a Streamlit app for free\nAdvanced deployment process with secrets management and environment variables",
      "target_audience": [
        "Data scientists and machine learning engineers looking to deploy ML models and dashboards."
      ]
    }
  ]
}