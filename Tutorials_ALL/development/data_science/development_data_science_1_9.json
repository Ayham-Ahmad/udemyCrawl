{
  "courses": [
    {
      "title": "Introdução à Ciência de Dados na Prática com Python",
      "url": "https://www.udemy.com/course/introducao-a-ciencia-de-dados-aprenda-na-pratica-com-python/",
      "bio": "O primeiro passo para você se tornar um Cientista de Dados! Explicação detalhada com códigos de exemplo.",
      "objectives": [
        "Criar modelos de predição com Deep Learning e Machine Learning",
        "Aprenderá a fazer Transfer Learning e Fine Tuning com CNNs",
        "Explorar conjuntos de dados e criar visualizações",
        "Criar seus próprios Datasets de voz, vídeo e imagem",
        "Fazer pré-processamento nos dados",
        "Aprenderá as metodologias de avaliação estatística mais utilizadas"
      ],
      "course_content": {
        "Introdução": [
          "Introdução do curso"
        ],
        "Python e suas Bibliotecas": [
          "Introdução da Unidade 1",
          "Apresentação da IDE",
          "Preparação do ambiente"
        ],
        "Google Colaboratory": [
          "Introdução da Unidade 2",
          "Apresentação do Ambiente",
          "Preparação do ambiente",
          "Integração com GD",
          "Modos de utilização"
        ],
        "Pré-Processamento": [
          "Introdução da Unidade 3",
          "Como carregar um sinal de áudio",
          "Implementação do filtro da média em áudio",
          "Transformada Rápida de Fourier (FFT) em áudio",
          "Conversão de áudio em espectrograma",
          "Salvar em arquivo após pré-processamento",
          "Captura e carregamento da imagem",
          "Redimensionamento da imagem",
          "Conversão de cores e Separação de canais",
          "Implementação de filtro da média na imagem",
          "Transformada Rápida de Fourier (FFT) em uma imagem",
          "Realce de bordas na imagem",
          "Realce com equalização de histograma",
          "Extração de frames de um vídeo"
        ],
        "Análise Exploratória": [
          "Introdução da Unidade 4",
          "Estrutura de dados: Série",
          "Estrutura de dados: Dataframe",
          "Gráfico de barras",
          "Gráfico de pizza",
          "Histograma",
          "Gráfico de densidade com seaborn",
          "Correlação e gráfico de dispersão",
          "Gráfico de dispersão para mais de duas variáveis",
          "Séries temporais e gráficos de linha",
          "Gráfico de dispersão categórico",
          "Gráfico de caixa",
          "Gráfico violino",
          "Distribuições bivariadas",
          "Distribuições conjuntas par a par"
        ],
        "Metodologias de Avaliação": [
          "Introdução da Unidade 5",
          "Validação hold-out",
          "Validação cruzada",
          "Busca em grade",
          "Acurácia, Sensibilidade, Especificidade, Recall, F-score",
          "Matriz de confusão e sua interpretação",
          "Métricas de similaridade"
        ],
        "Aprendizagem de Máquina": [
          "Introdução da Unidade 6",
          "Criando um dataset de voz, vogal sustentada",
          "Extração de característica de áudio com FFT",
          "Classificação supervisionada da voz",
          "Criando um dataset de imagem, gestos com as mãos",
          "Extração de característica da imagem com histograma",
          "Classificação supervisionada da imagem",
          "Aprendizado não supervisionado com k-means"
        ],
        "Deep Learning - CNNs": [
          "Introdução da Unidade 7",
          "Coleta de dados",
          "Organização da base de dados",
          "Criar uma CNN genérica",
          "Predição",
          "Gerar resultados",
          "Transferência de aprendizagem",
          "Ajuste fino"
        ],
        "Material extra": [
          "Denoising",
          "Deblurring",
          "Dehazing"
        ],
        "Exercícios sugeridos": [
          "Apresentação dos exercícios"
        ]
      },
      "requirements": [
        "Familiaridade com Informática e escrita de código"
      ],
      "description": "Ciência de dados (em inglês: data science) é uma área interdisciplinar, que localiza-se em uma interface entre a estatística e a ciência da computação, que utiliza o método científico; processos, algoritmos e sistemas, para extrair conhecimento e tomar decisões a partir de dados dos diversos tipos, sendo eles ruidosos, nebulosos, estruturados ou não-estruturados. Sendo assim uma área voltada para o estudo e a análise organizada de dados científicos e mercadológicos, financeiros, sociais, geográficos, históricos, biológicos, psicológicos, dentre muitos outros. Visa, desde modo, a extração de conhecimento, detecção de padrões e/ou obtenção de insights para possíveis tomadas de decisão. Cientistas de Dados podem trabalhar no setor privado, por exemplo, transformando grande quantidade de dados brutos em insights de negócios, auxiliando empresas em tomadas de decisões para atingir melhores resultados ou na academia e terceiro setor como pesquisadores quantitativos interdisciplinares. Há uma forte relação da área da ciência de dados com a inteligência artificial, uma vez que o principal profissional que lida com o desenvolvimento, manutenção e fiscalização de inteligências artificiais e machine learning são cientistas de dados. O curso é voltado para o público em geral que queira iniciar na área de ciência de dados. Todas as aulas foram elaboradas utilizando a linguagem Python e as bibliotecas mais populares na área. A ideia foi introduzir o assunto em sete módulos com aulas e exemplos práticos.",
      "target_audience": [
        "Interessados em entrar para o mundo da Ciência de Dados"
      ]
    },
    {
      "title": "【AI 자막】 머신 러닝 : Python에서의 자연어 처리 마스터하기! (V2)",
      "url": "https://www.udemy.com/course/natural-language-processing-in-python-korean/",
      "bio": "자연어 처리 (NLP) : Python에서 Markov 모델, NLTK, 인공 지능(AI), 딥 러닝, 머신 러닝 및 데이터 과학 사용하기",
      "objectives": [
        "CountVectorizer, TF-IDF, word2vec 및 GloVe를 사용하여 텍스트를 벡터로 변환하는 방법",
        "문서 retrieval 시스템 / 검색 엔진 / 유사도 검색 / 벡터 유사도 구현 방법",
        "확률 모델, 언어 모델 및 Markov 모델(Transformers, BERT 및 GPT-3의 선행 학습 요건)",
        "유전 알고리즘과 언어 모델링을 사용하여 암호 해독 알고리즘을 구현하는 방법",
        "스팸 탐지 기능을 구현하는 방법",
        "감정 분석을 구현하는 방법",
        "문서 스피너를 구현하는 방법",
        "텍스트 요약 구현 방법",
        "잠재 시맨틱 인덱싱을 구현하는 방법",
        "LDA, NMF 및 SVD로 토픽 모델링을 구현하는 방법",
        "머신 러닝(나이브 베이즈, 로지스틱 회귀, PCA, SVD, 잠재 디리클레 할당)",
        "딥 러닝(ANN, CNN, RNN, LSTM, GRU) (BERT 및 GPT-3의 더 중요한 선행 학습 요건)",
        "허깅 페이스 트랜스포머(VIP 전용)",
        "Python, Scikit-Learn, Tensorflow 등에 NLP에 사용하는 방법",
        "텍스트 전처리, 토큰화, 스톱워드, 표제어 추출, 어간 추출",
        "품사(POS, Parts-of-speech) 태깅 및 명명된 엔티티 인식(NER)",
        "OpenAI ChatGPT, GPT-4, DALL-E, Midjourney 및 Stable Diffusion을 위한 중요한 기초사항 이해"
      ],
      "course_content": {
        "강의 소개": [
          "강의 소개 및 개요",
          "초급, 중급, 상급이신가요? 모두 괜찮습니다!"
        ],
        "설정하기": [
          "직접 실습해봅시다: 실용적인 코딩 경험, 데이터 링크",
          "깃허브 사용 방법 및 추가 코딩 팁(선택 사항)",
          "코드, 노트북, 데이터를 찾는 곳",
          "이 강의를 성공적으로 이수하는 방법",
          "Temporary 403 오류"
        ],
        "벡터 모델 및 텍스트 전처리": [
          "벡터 모델 및 텍스트 전처리 개요",
          "NLP의 기본 정의",
          "벡터란 무엇인가요?",
          "단어의 가방",
          "Count Vectorizer (이론)",
          "토큰화",
          "Stopwords",
          "어간 추출 및 표제어 추출",
          "어간 추출 및 표제어 - 데모",
          "Count Vectorizer (코드)",
          "벡터 유사도",
          "TF-IDF (이론)",
          "(실습형) Recommender 연습 프롬프트",
          "TF-IDF (코드)",
          "단어-인덱스 매핑",
          "TF-IDF를 처음부터 구축하는 방법",
          "신경망 단어 임베딩",
          "신경망 단어 임베딩 - 데모",
          "벡터 모델 및 텍스트 전처리 섹션 마무리",
          "텍스트 요약 - 예습 맛보기",
          "다른 언어로 NLP를 수행하는 방법",
          "제안 상자"
        ],
        "확률 모델 (소개)": [
          "확률 모델(소개)"
        ],
        "마르코프 모델 (중급)": [
          "마르코프 모델 섹션 소개",
          "마르코프 속성",
          "마르코프 모델",
          "확률 스무싱 및 로그 확률",
          "텍스트 분류기 구축하기(이론)",
          "텍스트 분류기 구축하기(연습 프롬프트)",
          "텍스트 분류기 구축하기(코드 파트 1)",
          "텍스트 분류기 구축하기(코드 파트 2)",
          "언어 모델(이론)",
          "언어 모델(연습 프롬프트)",
          "언어 모델(코드 파트 1)",
          "언어 모델(코드 파트 2)",
          "마르코프 모델 섹션 마무리"
        ],
        "아티클 스피너 (중급)": [
          "아티클 스피닝 : 문제 설명",
          "아티클 스피닝 : N-그램 접근법",
          "아티클 스피닝 연습 프롬프트",
          "파이썬의 아티클 스피너 (1부)",
          "파이썬의 아티클 스피너 (2부)",
          "사례 연구 : 잘못된 아티클 스피너"
        ],
        "암호 해독 (고급)": [
          "섹션 소개",
          "암호 (Ciphers)",
          "언어 모델 (복습)",
          "유전자 알고리즘",
          "코드 준비",
          "코드 - 1부",
          "코드 - 2부",
          "코드 - 3부",
          "코드 - 4부",
          "코드 - 5부",
          "코드 - 6부",
          "암호 해독 - 추가적인 토론",
          "섹션 마무리"
        ],
        "머신 러닝 모델 (소개)": [
          "머신 러닝 모델 (소개)"
        ],
        "스팸 탐지": [
          "스팸 탐지 : 문제 설명",
          "나이브 베이즈 직관",
          "스팸 탐지 : 연습 프롬프트",
          "추가: 클래스 불균형, ROC, AUC 및 F1 스코어 - 1부",
          "추가: 클래스 불균형, ROC, AUC 및 F1 스코어 - 2부",
          "Python에서 스팸 탐지"
        ],
        "감정 분석": [
          "감정 분석 : 문제 설명",
          "로지스틱 회귀 직관 - 1부",
          "다중 클래스 로지스틱 회귀 - 2부",
          "로지스틱 회귀 훈련 및 해석 - 3부",
          "감정 분석 : 연습 프롬프트",
          "Python에서의 감정 분석 - 1부",
          "Python에서의 감정 분석 - 2부"
        ]
      },
      "requirements": [
        "파이썬을 설치하세요. (무료입니다!)",
        "괜찮은 수준의 파이썬 프로그래밍 기술",
        "선택 사항: 수학 부분을 이해하려면 선형 대수학과 확률이 도움이 됩니다."
      ],
      "description": "[꼭 읽어주세요] 한글 AI 자막 강의란?\n유데미의 한국어 [자동] AI 자막 서비스로 제공되는 강의입니다.\n강의에 대한 질문사항은 Lazy Programmer 강사님이 확인하실 수 있도록 Q&A 게시판에 영어로 남겨주시기 바랍니다.\n\n\nOpenAI ChatGPT, GPT-4, DALL-E, 미드저니, 스테이블 디퓨전과 같은 AI 기술이 실제로 어떻게 작동하는지 궁금한 적이 있나요? 이 과정에서는 이러한 획기적인 애플리케이션의 기초를 배우게 됩니다.\n\n\n안녕하세요!\n【AI 자막】 머신 러닝 : Python에서의 자연어 처리 마스터하기! (V2) 강의에 오신 것을 환영합니다.\n\n\n이 강의는 다음 내용을 다루는 방대한 4-in-1 과정입니다:\n1) 벡터 모델과 텍스트 전처리 방법\n2) 확률 모델과 Markov 모델\n3) 머신 러닝 메소드\n4) 딥러닝 및 신경망 메소드\n\n\n벡터 모델과 텍스트 전처리 방법을 다루는 1부에서는 데이터 과학과 인공 지능에서 벡터가 왜 필수적인지 알아봅니다. CountVectorizer 와 TF-IDF 등 텍스트를 벡터로 변환하는 다양한 기법에 대해 알아보고, word2vec 와 GloVe 같은 신경 임베딩 방법의 기초를 배우게 됩니다.\n그런 다음 배운 내용을 다음과 같은 다양한 작업에 적용하게 됩니다:\n텍스트 분류\n문서 검색/검색 엔진\n텍스트 요약\n\n\n이 과정에서 토큰화, 어간 추출, 표제어 추출과 같은 중요한 텍스트 전처리 단계도 배우게 됩니다.\n품사 태깅 과 같은 고전적인 NLP 작업도 간략하게 소개합니다.\n\n\n확률 모델과 Markov 모델을 다루는 2부에서는 지난 100년 동안 데이터 과학과 머신 러닝에서 가장 중요한 모델 중 하나인 확률 모델에 대해 알아봅니다. 이 모델은 NLP 외에도 금융, 생물 정보학, 강화 학습 등 다양한 분야에 적용되고 있습니다.\n이 강의에서는 이러한 확률 모델의 다양한 활용법을 살펴봅니다:\n\n\n텍스트 분류기 구축하기\n아티클 스피닝\n텍스트 생성(시 생성)\n\n\n중요한 것은 이러한 방법들이 BERT와 GPT-3 같은 최신 Transformer모델의 작동 방식을 이해하기 위한 필수 전제 조건이라는 점입니다. 특히, BERT 및 GPT의 사전 교육 목표에 해당하는 두 가지 중요한 작업에 대해 알아볼 것입니다.\n머신 러닝 방법을 다루는 3부에서는 다음과 같은 고전적인 NLP 작업에 대해 자세히 알아보세요:\n\n\n스팸 탐지\n감정 분석\n잠재 시맨틱 분석(혹은 잠재 시맨틱 인덱싱)\n토픽 모델링\n이 섹션에서는 이론 중심이 아닌 애플리케이션 중심으로 진행되므로, 다양한 머신러닝 알고리즘의 세부 사항을 배우는 데 대부분의 시간을 할애하는 대신 위의 작업에 어떻게 적용할 수 있는지에 초점을 맞출 것입니다.\n물론 무슨 일이 일어나고 있는지 이해하기 위해서는 해당 알고리즘에 대한 학습이 필요합니다. 다음 알고리즘이 사용됩니다:\n\n\n나이브 베이즈\n로지스틱 회귀\n주성분 분석 (PCA) / 특이값 분해 (SVD)\n잠재 디리클레 할당 (LDA)\n이러한 알고리즘은 머신러닝/인공지능의 지엽적인 알고리즘이 아닌, NLP의 기본이 되는 알고리즘이므로 모든 NLP 과정의 필수적인 부분입니다.\n딥러닝 방법을 다루는 4부에서는 NLP 작업을 해결하는 데 적용할 수 있는 최신 신경망 아키텍처에 대해 알아봅니다. 뛰어난 성능과 유연성 덕분에 신경망은 앞서 언급한 모든 작업을 해결하는 데 사용할 수 있습니다.\n다음에 대해 배우게 됩니다:\n\n\n피드포워드 인공 신경망 (ANNs)\n임베딩\n컨볼루션 신경망 (CNNs)\n순환 신경망 (RNNs)\nRNN에 대한 연구에는 언어 번역, 음성 인식 및 텍스트 음성 변환과 같은 어려운 작업에 Google, Amazon, Apple, Facebook 등에서 널리 사용되고 있는 LSTM 및 GRU와 같은 최신 아키텍처가 포함됩니다.\n물론 BERT와 GPT-3 같은 최신 Transformer 는 심층 신경망의 예이므로 이 과정은 Transformer 를 이해하기 위한 필수 전제 조건입니다.\n\n\n이 강의의 장점 3줄 요약:\n모든 코드 라인에 대한 자세한 설명 - 동의하지 않는 부분이 있으면 언제든지 이메일을 보내주세요.\n다른 강의처럼 키보드에 \"타이핑\"하느라 시간 낭비 없음 - 솔직히 말해서 처음부터 단 20분 만에 배울 만한 코드를 작성할 수 있는 사람은 아무도 없습니다.\n대학 수준의 수학이 두렵지 않음 - 다른 강의에서 다루지 않는 알고리즘에 대한 중요한 세부 정보를 얻을 수 있습니다.\n\n\n읽어주셔서 감사드리며 강의에서 뵙기를 바랍니다!",
      "target_audience": [
        "자연어 처리(NLP)를 배우고자 하는 분",
        "인공지능, 머신러닝, 딥러닝 또는 데이터 과학에 관심이 있는 분",
        "Udemy의 흔한 초보자 레벨의 강의 수준을 넘어서서 심화 내용을 학습하고 싶은 분"
      ]
    },
    {
      "title": "YOLOv8目標檢測實戰：Android手機部署",
      "url": "https://www.udemy.com/course/yolov8android/",
      "bio": "YOLOv8 Object Detection :Android Phone Deployment",
      "objectives": [
        "掌握YOLOv8目標檢測的Android手機部署方法",
        "學習YOLOv8目標檢測的Android手機部署代碼",
        "學習模型的onnx和ncnn模型匯出方法",
        "學習Android的JNI機制"
      ],
      "course_content": {},
      "requirements": [
        "熟悉Python和PyTorch"
      ],
      "description": "YOLOv8 目標檢測基於先前 YOLO 版本的成功，引入了新功能和改進，進一步提升了性能和靈活性。\n本課程在Windows上手把手演示YOLOv8（YOLOv8n和YOLOv8s）目標檢測在Android（安卓）手機進行部署的過程。內容包括：安裝軟體環境、安裝PyTorch，克隆和安裝YOLOv8，匯出onnx模型，onnx轉換成NCNN檔，安裝Android Studio，準備Android專案檔案(下載專案檔案、放置ncnn模型檔、放置ncnn和opencv的android檔)，手機連接電腦並編譯軟體(安裝投屏軟體、手機連接電腦配置、編譯和調試、匯出簽名apk)，自己資料集訓練模型的部署，專案代碼解析（安卓的JNI機制、C++代碼、Java代碼）。",
      "target_audience": [
        "希望學習YOLOv8目標檢測技術的學員和從業者"
      ]
    },
    {
      "title": "Percorso completo di Data Analysis",
      "url": "https://www.udemy.com/course/corso-completo-data-analysis/",
      "bio": "TUTTO ciò che ti serve per diventare un Data Analyst",
      "objectives": [
        "Interagire con i database tramite linguaggi SQL e NoSQL",
        "Programmare in Python per l'analisi dei dati",
        "Integrare l'Intelligenza artificiale nel processi di analisi dei dati",
        "Creare Dashboard con Power BI e Tableau",
        "Utilizzare in modo professionale Excel e Access per l'analisi dei dati"
      ],
      "course_content": {},
      "requirements": [
        "Nessun prerequisito in particolare"
      ],
      "description": "Con oltre 60 ore di video lezioni imparerai a gestire tutti i principali step dell'analisi dei dati, dall'acquisizione degli input, alle analisi con i linguaggi di programmazione più richiesti, passando per la creazione di Dashboard comunicative e professionali, fino a utilizzare professionalmente l'intelligenza artificiale e gli LLM tramite le API di Python.\n\n\nIl corso tratta tutte le principali aree tematiche della Data Analysis\nSQL\nExcel\nPython e Pandas\nStatistica\nMachine Learning con Python\nIntelligenza Artificiale e LLM\nMongoDB\nAccess\nPower BI Desktop\nTableau\nSQL Server\nOracle\nData Management\n\n\nGli argomenti sono trattati al 100% in modo pratico, con tantissimi esempi pratici. Durante il corso ti proporrò anche una serie di progetti da portare a termine, in modo da guidarti nella costruzione di un portfolio personale da Data Analyst,\n\n\nTutte le lezioni sono corredate da tantissimo materiale didattico, che nel complesso va a costituire un vero e proprio insieme di manuali delle varie tecnologie utilizzate.\n\n\nQuesto corso è pensato per guidarti passo dopo passo, anche se stai muovendo i primi passi nell'analisi dei dati. Attraverso un approccio pratico, potrai sviluppare fin da subito competenze utili nel mondo professionale.\n\n\nDevo dirti però che affronteremo argomenti che richiedono impegno e dedizione. Potrebbe essere necessario rivedere le lezioni più volte o metterle in pausa per eseguire gli esercizi sul tuo PC. Il mio obiettivo è fornirti una preparazione concreta, evitando di limitarmi a semplici presentazioni teoriche che danno la sola illusione di aver capito.\n\n\nSe hai poca familiarità con il computer e incontri difficoltà nelle operazioni di base, come la gestione delle email o il copia e incolla di file, ti suggerisco di partire da un corso di informatica più introduttivo.\n\n\nPer seguire le lezioni è necessario avere una connessione Internet funzionante e un PC personale per l’installazione dei software. La procedura d’installazione dei software oggetto del corso rientra nel programma solo per il sistema operativo Windows. Alcuni moduli didattici richiedono che sul proprio PC siano già installati Excel e Access. Ricorda di utilizzare sempre un PC personale per l’installazione di qualsiasi software. È sempre sconsigliato usare un PC aziendale senza la preventiva autorizzazione del reparto IT.\n\n\nPS. Questo corso non è la piattaforma didattica La Scuola dei Dati",
      "target_audience": [
        "Chiunque voglia intraprendere o dare uno slancio alle sue competenze di Data Analysis"
      ]
    },
    {
      "title": "P30 Preparation Course 2025",
      "url": "https://www.udemy.com/course/p30-preparation-course-2025/",
      "bio": "Project ,Program and Portfolio management offices",
      "objectives": [
        "Understand the key principles of P3O – Students will grasp the core concepts, principles, and frameworks behind the Portfolio, Programme, and Project Offices",
        "Comprehend the differences between portfolios, programs, and projects – Students will be able to differentiate between these concepts and their relationship",
        "Learn how to establish and tailor a P3O model – Students will understand how to customize a P3O model according to organizational needs and objectives.",
        "Prepare for the P3O Foundation Certificate exam – The course will equip students with the knowledge necessary to pass the P3O Foundation Certificate exam."
      ],
      "course_content": {
        "Introduction": [
          "P3O Study Plan",
          "Course Introduction",
          "P30 Introduction part one",
          "P3O Introduction Part Two",
          "Introduction"
        ],
        "2- Why have a P3O?": [
          "Chapter 2 Part one",
          "Chapter 2 Part two",
          "2- Why have a P3O?"
        ],
        "3 - Designing a P3O model": [
          "3 - Designing a P3O model Part one",
          "3 - Designing a P3O model Part two",
          "3 - Designing a P3O model"
        ],
        "4 How to implement or re-energize a P3O": [
          "4 How to implement or re-energize a P3O Part one",
          "4 How to implement or re-energize a P3O Part two",
          "4 How to implement or re-energize a P3O"
        ],
        "5 How to operate a P3O": [
          "5 How to operate a P3O",
          "5 How to operate a P3O"
        ],
        "P3M3": [
          "P3M3 Summary"
        ],
        "P3O Course Summary": [
          "P3O Summary"
        ],
        "Case study": [
          "PMO Case study"
        ],
        "P3O Practice test": [
          "P3O Full Practice test 2025"
        ],
        "خطوات التقدم للاختبار الدولى": [
          "خطوات التقدم للاختبار الدولى"
        ]
      },
      "requirements": [
        "No prior requirements – The course is suitable for anyone interested in learning about P3O.",
        "Basic project management knowledge – It would be helpful but not mandatory, as the course covers fundamental P3O concepts from scratch.",
        "No specific tools or software required – All required tools and techniques will be introduced during the course."
      ],
      "description": "1. Introduction\nThis intensive course is designed to equip professionals with the knowledge and tools needed to establish and develop effective Project, Programme, and Portfolio Offices (PMOs) aligned with the P3O® (Portfolio, Programme, and Project Offices) model. It blends strategic theory, governance, and real-world best practices.\n2. Course Objectives\nBy the end of this course, participants will be able to:\nUnderstand the P3O® model, components, principles, and lifecycle.\nDifferentiate between portfolio, programme, and project management and their support structures.\nDesign tailored PMO models (centralized, decentralized, virtual, etc.).\nBuild the business case and blueprint for implementing PMOs.\nIdentify and apply key tools, techniques, and services in operating a PMO.\nAlign PMOs with strategy and organizational governance.\nEstablish roles, responsibilities, and reporting lines in PMO environments.\nPrepare for practical implementation and continual development of PMOs.\n\n\n2. Target Audience\nPMO Directors & Managers\nPortfolio/Programme/Project Managers\nStrategy and Planning Professionals\nConsultants in PPM/Change Management\nHeads of Departments and Operational Managers\n3. Daily Course Content and Activities\nFoundations of P3O and the Value of PMOs\nTopics Covered:\nIntroduction to P3O®: Definitions and Concepts\nOverview of Portfolio, Programme, and Project Management\nWhy Have a P3O? Value Proposition and Drivers\nP3O Principles and Governance Alignment\nCase Study:\nWhy We Need a P3O? (P3O Vision and Strategic Drivers)\nWorkshop:\nStakeholder identification and drafting a PMO vision statement\nP3O Models, Functions & Services\nTopics Covered:\nCentralized, Decentralized, Virtual and Hybrid Models\nFunctional Areas: Strategic Planning, Delivery Support, COE\nDesigning PMO Structures based on Organizational Needs\nTailoring Services and Tools\nCase Study:\nP3O Value Matrix: Aligning functions with strategic objectives\nWorkshop:\nDesign a customized P3O model for a given organizational scenario\nOperating PMOs – Tools, Techniques, and Integration\nTopics Covered:\nTools and Techniques (Dashboards, Prioritization, Dashboards)\nCapacity Planning and Knowledge Management\nFacilitated Workshops and Business Process Swimlanes\nIntegrating PMO with BAU and Corporate Functions\nCase Study:\nReporting Challenges in a European Public Sector Organization\nWorkshop:\nDevelop a PMO toolset for a hypothetical organization\nImplementation Lifecycle and Business Case\nTopics Covered:\nP3O Lifecycle: Identify → Define → Deliver → Close\nBuilding the Blueprint, Benefits Map, and Business Case\nP3M3 Maturity Model and Capability Assessments\nCritical Success Factors and Risk Mitigation\nCase Study:\nBlueprint and Vision Planning\nWorkshop:\nDraft a Business Case & Benefits Map for a PMO implementation\nGovernance, Roles, and Continuous Development\nTopics Covered:\nP3O Governance Structures\nRoles and Responsibilities (Sponsor, Head of P3O, Analysts)\nReporting Lines and Escalation Paths\nRe-energizing an Existing PMO\nEstablishing Communities of Practice\nCase Study:\nGovernance Conflict and Resolutions in PMO Setup\nWorkshop:\nDevelop an action plan to establish or re-energize a PMO\nCapstone Presentation:\nGroup presentations of PMO establishment plans (Vision + Model + Tools)\n4. Deliverables Provided\nCourse slides and handouts\nSample business case and blueprint templates\nCase study library\nExam-style practice questions\nParticipation certificate",
      "target_audience": [
        "• PMO Directors & Managers",
        "• Portfolio/Programme/Project Managers",
        "• Strategy and Planning Professionals",
        "• Consultants in PPM/Change Management",
        "• Heads of Departments and Operational Managers"
      ]
    },
    {
      "title": "Inteligência Artificial descomplicada para todos",
      "url": "https://www.udemy.com/course/inteligencia-artificial-descomplicada/",
      "bio": "Aprenda tudo de forma simples, sem \"tecniquês\" complicado.",
      "objectives": [
        "Introdução à IA: Visão geral, importância dos dados e termos técnicos.",
        "Projetos e Equipes de IA: Fluxo de ideias, colaboração e trabalho em equipe.",
        "Cultura de IA nas Empresas: Como adotar IA e evitar armadilhas.",
        "Limitações e Impactos da IA: Questões éticas, impactos na economia e empregos.",
        "Como Máquinas Aprendem: Desde algoritmos básicos até redes neurais.",
        "IA Generativa: Incluindo modelos de linguagem avançados, como LLM."
      ],
      "course_content": {
        "Introdução à Inteligência Artificial (IA)": [
          "Introdução ao curso",
          "Visão geral da área de Inteligência Artificial (IA)",
          "Aprendizado de máquina",
          "Importância dos dados",
          "Alguns termos técnicos",
          "A cultura de IA nas empresas",
          "O que a IA pode e não pode fazer?",
          "Aprendizado Profundo (Redes Neurais)"
        ],
        "Projetos e equipes de IA": [
          "Transformando ideias em projetos",
          "Fluxo de trabalho em projetos de IA",
          "Impactos nas diferentes funções de trabalho",
          "Seleção de projetos de IA",
          "O trabalho com equipes de IA",
          "Ferramentas essenciais em projetos de IA"
        ],
        "Desenvolvendo a cultura de IA nas empresas": [
          "Estimulando o uso de IA nas empresas",
          "Funções e responsabilidades de uma equipe de IA",
          "Etapas para estimular uma cultura voltada à IA nas empresas",
          "Principais armadilhas em projetos de IA",
          "Dando os primeiros passos com a IA",
          "Exemplos de aplicações de IA"
        ],
        "Limitações e Impactos gerais da IA": [
          "Impactos da IA",
          "Limites da IA",
          "IA: discriminação e preconceitos",
          "Ataques adversários",
          "Usos nocivos da IA",
          "IA como uma ferramenta de desenvolvimento econômico",
          "IA e os empregos"
        ],
        "Como as máquinas aprendem": [
          "Dos dados ao conhecimento (Algoritmos Clássicos)",
          "Árvores de decisão",
          "Regras de associação",
          "Redes neurais"
        ],
        "IA Generativa": [
          "O que é a Inteligência Artificial Generativa",
          "LLM (Large Language Model)"
        ],
        "Encerramento do curso": [
          "Mensagem final"
        ]
      },
      "requirements": [
        "Não há pré-requisitos para o curso. A ideia do curso é possibilitar que profissionais de qualquer área ou função, desde CEO's, líderes de empresas ou mesmo curiosos sobre o tema tenham a oportunidade de compreender conceitos básicos de IA sem aquele \"tecniquês\" complicado."
      ],
      "description": "Domine a Inteligência Artificial e esteja à frente na era digital!\nVivemos em um mundo onde a Inteligência Artificial (IA) não é mais uma opção, mas uma necessidade vital. Em cada canto do nosso dia a dia, essa tecnologia se manifesta. Cada vez mais, profissionais de todas as áreas estão encontrando IA em suas rotinas diárias, seja em sistemas de recomendação, automações, atendimento ao cliente, ou análise de dados avançada. Não dá para ignorar esta área revolucionária e correr o risco de ficar obsoleto.\nPor que este curso?\nDescomplicado: Diga adeus ao \"tecniquês\" e mergulhe em conceitos de IA de forma simples, direta e acessível.\nUniversal: Projetado não só para os aficionados por tecnologia, mas também para líderes, CEO's, profissionais liberais, curiosos e qualquer um que queira estar alinhado com as tendências disruptivas do mercado moderno.\nFoco nos Fundamentos: Você vai entender profundamente os pilares que sustentam a IA.\nNão fique para trás. Em um mundo em constante evolução tecnológica, a compreensão da IA é o diferencial crítico que pode colocá-lo à frente de seus concorrentes, impulsionar sua carreira e inovar em seus projetos.\nA Inteligência Artificial está moldando nosso futuro. Você está pronto para dominá-la e ser parte dessa revolução?",
      "target_audience": [
        "Profissionais de qualquer área e curiosos que desejam aprender sobre Inteligência Artificial."
      ]
    },
    {
      "title": "Análise de Sentimentos com Python e ChatGPT",
      "url": "https://www.udemy.com/course/analise-de-sentimentos-com-python-e-chatgpt/",
      "bio": "Criando uma API em Flask para Análise de Sentimentos com OpenAI",
      "objectives": [
        "Entender como funciona a API da OpenAI",
        "Como utilizar o ChatGPT",
        "Fazer uma requisição à API da OpenAI",
        "Usar o modelo de análise de texto/sentimento da OpenAI",
        "Ler e tratar dados de uma planilha",
        "Criar uma API com Flask",
        "Criar uma solução completa de Análise de Sentimentos"
      ],
      "course_content": {
        "Apresentação do Curso": [
          "Introdução",
          "Dinâmica do Curso",
          "Estrutura do Curso: Módulos",
          "Ferramentas",
          "Dicas e Contato"
        ],
        "Introdução à Análise de Sentimentos": [
          "Definição",
          "Importância",
          "Exemplos"
        ],
        "Jupyter Notebook": [
          "Definição",
          "Instalação",
          "Visão Geral",
          "Executando Códigos"
        ],
        "Introdução à OpenAI e ao ChatGPT": [
          "Definição",
          "Uso do ChatGPT na prática",
          "Modelos",
          "Criação do token para acessar a API",
          "Aula Prática 1: Conectando à API com Python"
        ],
        "Projeto": [
          "Aula Prática 2: Criação da função de processamento do arquivo",
          "Aula Prática 3: Criação da API em Python com Flask",
          "Aula Prática 4: Criação de link de download do resultado",
          "Transforme em um negócio!"
        ]
      },
      "requirements": [
        "Básico de Python/Pandas/Flask",
        "Força de Vontade",
        "Interesse por Inteligência Artificial"
      ],
      "description": "CURSO PRÁTICO\nCOM CERTIFICADO\n\n\nSOBRE O CURSO\n\n\nEsse NÃO é mais um curso complicado, sem explicações claras ou exemplos práticos.\n\n\nEsse curso É um jeito simples de você aprender como utilizar a API da OpenAI e Linguagem Python para construir sua própria API de Análise de Sentimentos.\n\n\nO objetivo final desse curso é um projeto no qual criaremos uma API com Flask (website) que permitirá o upload de um arquivo Excel e retornará na tela os resultados da Análise de Sentimento dos textos contidos nessa planilha e também os disponibilizará para download.\n\n\nVocê não precisa ter ampla experiência na área de Dados ou em Desenvolvimento ou exatas para acompanhar todo o curso, que foi pensado com didática simples e módulos progressivos para você avançar com segurança.\n\n\nComece hoje a explorar a Área de Dados e Empreendedorismo. Mesmo que já esteja na área, essa é a oportunidade para você melhorar suas habilidades e ter ideias de aplicação com ChatGPT.\n\n\nRelacionar suas habilidades de linguagens como Python, HTML e outras do mundo de desenvolvimento pode ser uma ótima maneira de você ser criativo no mercado de trabalho na hora de resolver problemas de negócio.\n\n\nVeja também outros cursos que temos disponíveis:\nSQL para Análise de Dados: do Básico ao Avançado (2024)\nCriação com Dashboards com Looker Studio\nPython: Manipulação de Dados com Pandas\nManipulação de Dados com Linguagem R\nMachine Learning: Clusterização com Linguagem Python\nMachine Learning: Classificação com Linguagem Python\nMachine Learning: Regressão com Linguagem Python\nCriação de Dashboards com Metabase\n\n\nSOBRE O INSTRUTOR\nMe chamo Caio Avelino, e o conhecimento que vou dividir com você nesse curso foi adquirido, principalmente, com minha experiência no mercado de trabalho. Atuo nas áreas de Business Intelligence, Ciência de Dados e Inteligência Artificial há anos e tive a oportunidade de desenvolver minhas habilidades em diversas startups.\n\n\nVocê sairá deste curso pronto para criar sua aplicação de análise de sentimentos com OpenAI. Estarei online e sempre à disposição para esclarecer dúvidas e melhorar sua experiência profissional com o seu aprendizado.\n\n\nAté mais!",
      "target_audience": [
        "Profissionais de dados (analistas, cientistas, engenheiros, etc)",
        "Desenvolvedores que desejam criar aplicações com ChatGPT",
        "Qualquer profissional que queira aprender código e criar aplicações"
      ]
    },
    {
      "title": "빅데이터분석기사 필기 올인원: 3주에 끝내는 완벽 대비",
      "url": "https://www.udemy.com/course/maso-ds-bigdata-onc2007/",
      "bio": "국가 공인 자격증 빅데이터 분석기사 필기 시험 개념과 최신 기출 문제로 완성하는 완벽한 합격전략",
      "objectives": [
        "빅데이터 이해와 분석 계획 수립 방법",
        "필수 이론 및 요점 정리를 통한 완벽한 자격 검정 시험 준비",
        "데이터 수집 및 저장 계획의 설계",
        "유형별 최신 기출 복원 문제 풀이로 완성하는 자격증 합격 전략"
      ],
      "course_content": {
        "빅데이터분석기사 필기 소개": [
          "빅데이터분석기사 필기 소개"
        ],
        "빅데이터 개요": [
          "데이터의 이해",
          "데이터베이스의 이해",
          "데이터베이스의 활용",
          "기출변형문제"
        ],
        "빅데이터의 특징": [
          "빅데이터의 특징",
          "빅데이터의 유형",
          "빅데이터 출현 배경",
          "기출변형문제"
        ],
        "빅데이터의 가치": [
          "빅데이터의 가치",
          "빅데이터의 위기요인"
        ],
        "데이터 산업의 이해": [
          "데이터 산업의 이해",
          "공공 데이터"
        ],
        "빅데이터 조직 및 인력": [
          "빅데이터 조직",
          "데이터 사이언스",
          "빅데이터 인력",
          "기출변형문제"
        ],
        "빅데이터 플랫폼": [
          "빅데이터 플랫폼",
          "하둡(Hadoop)",
          "하둡 에코시스템",
          "API Gateway",
          "기출변형문제"
        ],
        "빅데이터와 인공지능": [
          "인공지능",
          "빅데이터와 인공지능"
        ],
        "개인정보 법·제도": [
          "데이터 3법",
          "해외 개인정보 보호 법제"
        ],
        "개인정보 활용": [
          "개인정보 활용",
          "기출변형문제"
        ]
      },
      "requirements": [
        "데이터 분석에 대한 사전 지식은 필요 없습니다. 통계 지식이 있으면 도움이 되나, 필수 사항은 아닙니다."
      ],
      "description": "\"어제보다 성장하겠습니다. 그리고, 어제보다 성장하려는 사람을 돕겠습니다.\"\n\n\n마소캠퍼스의 진심과 소망을 담은 Actionable Content로,\n2013년부터 온오프라인으로 함께해 온 누적 강의 1억시간!\n이 소중한 경험과 시간은 언제나 마소캠퍼스와 수강생 모두의 성장의 원천입니다.\n\n\n-\n빅데이터 시대에 나아갈 준비되셨나요?\n최신 빅데이터 분석기사 필시 시험 기출 문제를 통한 완벽한 대비가 시작됩니다!\n단순한 이론 암기를 넘어 이론을 활용한 실전 문제 풀이는 필수적인 준비 항목입니다.\n이제 여러분도 마소캠퍼스와 함께 빅데이터 분석기사 마스터할 수 있습니다.\n\n\n빅데이터 분석기사 필기 합격을 위해 최근 시험의 기출 문제를 철저히 분석하는 것은 시험 준비에 필수적입니다.\n“빅데이터분석기사 필기 올인원: 3주에 끝내는 완벽 대비” 강의는 실제 시험에서 요구되는 다양한 문제 유형과 해결 전략을 제공하는 여러분의 시험 준비 길라잡이입니다!\n\n\n본 강의는 총 4과목의 빅데이터 분석기사 필기 시험 범위 모두를 커버하며,\n각 과목별로 주요 문제들을 선별해 심층 분석을 진행합니다.\n1과목에서는 빅데이터의 기본 개념과 구조에 대한 문제를, 2과목에서는 데이터 처리 기술과 관련된 기술적 문제와 통계적 접근 방법을, 3과목에서는 분석 모형 설계와 다양한 분석 기법 적용, 그리고 4과목에서는 분석 모형 평가부터 분석 결과 활용에 대한 문제를 다룹니다.\n\n\n빅데이터 분석기사 필기, 합격할 수 있습니다!\n본 강의를 통해 수강생들은 각 과목의 핵심적인 이론과 문제 해결 기법을 학습할 수 있으며, 이는 빅데이터 분석기사 시험을 통과하는 데 있어 큰 도움이 됩니다.\n실제 시험과 유사한 형태의 문제를 다루면서 시험에 대한 자신감도 키워 보세요!\n\n\n이제, 빅데이터 시대의 주인공이 되어보세요.\n데이터 드리븐 시대의 주인공이 되기를 희망하는 분들에게 마소캠퍼스의 \"빅데이터 분석기사 필기 최신기출변형\" 강의는 실질적인 도움을 제공합니다.\n빅데이터 분석기사 시험 준비에 있어 이론뿐만 아니라 실제 문제 해결 능력까지 갖출 수 있는 최고의 선택을 하세요!\n\n\n\n\n빅데이터분석기사 필기 올인원: 3주에 끝내는 완벽 대비 강의는 1과목부터 4과목까지,\n복잡한 데이터 분석 이론을 실전 문제 풀이로 깊이 있게 다루는 과정입니다.\n\n\n1과목부터 4과목까지 전반적인 커버리지!\n1과목의 기출 문제에서 시작해 4과목의 문제까지, 모든 과목을 체계적으로 다룹니다. 이를 통해 수강생들은 데이터 분석의 전체 프로세스를 이해하고, 각 과목의 중요 포인트를 파악할 수 있습니다.\n\n\n빅데이터 분석의 시작과 끝!\n비전공자나 분석 초보자도 이해하기 쉽도록 각 과목별 필수 이론을 명확하고 간결하게 설명합니다. 또한, 기출 문제를 통해 이론이 실제 어떻게 적용되는지를 직접 보고 배울 수 있습니다.\n\n\n실전처럼 문제 풀이로 실력 점검!\n각 과목별 대표적인 기출 문제를 선정하여 실전처럼 문제 풀이를 진행합니다.\n이 과정을 통해 시험에 대한 실질적인 준비와 자신감을 높일 수 있으며, 합격을 위한 체계적인 접근 방법을 마스터할 수 있습니다.\n\n\n주요 기출 문제로 실전 준비 완성!\n핵심 이론 설명과 더불어 대표 유형 기출 문제를 같이 풀어보며\n합격을 위한 확실한 준비 과정을 지원합니다.\n\n\n빅데이터분석기사 필기 최신기출변형 강의를 수료한 후, 수강생들은 빅데이터 분석 시험의 최신 트렌드와 실제 기출 문제에 대한 심층적인 이해를 갖추게 됩니다. 이 강의는 시험을 준비하는 학습자는 물론, 처음 시험 유형을 접하는 입문자에게도 깊이 있는 지식을 제공합니다.\n- 최신 시험 경향 파악\n- 문제 해결 능력 향상\n- 통합적인 데이터 분석 역량 강화\n- 시험 특정 요소 마스터\n이 강의를 통해 빅데이터 분석 전문가로 성장하고\n데이터 시대의 주인공이 되어보세요.\n\n\n-\n[ 강 사 소 개 ]\n\n\n이 상 미\n現 마소캠퍼스 데이터 분석/소프트웨어 강사\n現 세종사이버대학교 교수\n컴퓨터/데이터 분야 자격증 교재 다수 출간\n\n\nADP 및 ADsP, 빅데이터 프로그래밍, 데이터 사이언스, 그리고 컴퓨터 활용 능력 자격증 강의를 다년간 진행해 온 전문가입니다. 성균관대학교 컴퓨터 공학과 석사 학위를 보유하고 있으며, 1990년부터 30년 이상 IT 교육 분야에서 강의하였습니다. 대학과 자격증 전문 기관을 넘나들며 현직자 및 대학생을 대상으로 수많은 수강생을 가르쳤으며, 이러한 풍부한 강의 경험을 바탕으로 컨텐츠 개발에 열정을 쏟고 있습니다. 특히, 수년 간의 강의 경험을 통해 저의 노하우를 바탕으로 저술한 자격증 관련 도서들은 많은 응시생과 강사들에게 큰 도움이 되어드리고자 합니다.",
      "target_audience": [
        "단기간에 빅데이터 분석 기사 자격증 취득을 목표로 하는 분",
        "빅데이터 분석 분야에 자신이 없지만 시험 합격을 원하는 분",
        "빅데이터 분석 기사 필기 시험에 불합격했던 분",
        "IT업계로 입사/직무전환/리스킬을 꿈꾸는 분"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第 30 部 AI 缺陷 - 如何欺騙人工智能",
      "url": "https://www.udemy.com/course/generative_ai_30/",
      "bio": "關於人工智能可解釋性，對抗性樣本，Adversarial example，Word Embeddings Optimization，Inverse Text Generation， Pytorch，GPT",
      "objectives": [
        "深入瞭解什麼是機器學習可解釋性",
        "深入瞭解如何生成圖像分類模型的對抗性樣本",
        "深入瞭解如何尋找和輸入embedding 相類似的tokens",
        "深入瞭解如何逆向優化輸入來誘導語言模型說出指定詞語"
      ],
      "course_content": {
        "課程準備": [
          "課程工具準備",
          "如何使用uv 作為包管理器和項目管理工具"
        ],
        "什麼是機器學習可解釋性 & 正向傳播與反向傳播的的過程": [
          "什麼是機器學習可解釋性 & 正向傳播與反向傳播的的過程"
        ],
        "如何欺騙 AI 圖像分類模型": [
          "如何欺騙 AI 分類模型 & 如何生成對抗性樣本",
          "如何生成圖像分類模型的對抗性樣本"
        ],
        "如何誘導語言模型生成我們想要的輸出": [
          "如何誘導 GPT 語言模型生成我們想要的輸出",
          "如何實現正規化 & 如何尋找和輸入embedding 相類似的tokens",
          "如何用 Pytorch 實現逆向優化輸入來誘導語言模型說出指定詞語",
          "如何實現基於目標輸出的語言模型輸入優化"
        ]
      },
      "requirements": [
        "一臺電腦"
      ],
      "description": "本課程從深入探討如何操控語言模型\nTokenization (分詞)：了解文字如何被分解成模型能理解的單元（tokens）。\nWord Embeddings (詞嵌入)：理解詞彙如何被轉換成數值向量，以及這些向量如何捕捉詞語的語義關係。\n模型生成機制：親身體驗模型如何根據給定的輸入來預測和生成文字。\nLogits 與 Loss Functions (對數幾率與損失函數)：這些是模型內部評估其預測好壞的數學工具，理解它們對於掌握模型行為至關重要。\n\n\n掌握進階的 AI 優化技術\n這不僅僅是訓練一個模型，更是訓練如何為模型找到最佳輸入\nGradient Descent (梯度下降)：這是深度學習中最核心的優化演算法之一。\nRegularization (正則化)：您將了解如何引導優化過程，使其結果不僅有效，而且符合特定約束（例如，生成的輸入詞向量要「像」實際的詞）。\n超參數調優：專案中的 epochs、lr、batch_size 等都是超參數。通過調整它們並觀察結果，您將學會這些參數如何影響優化過程的效率和效果。\n\n\n擴展 AI 應用視野\n展示了 AI 不僅可以從輸入到輸出，還可以從輸出「反推」輸入。這種「逆向」思維在許多領域都有潛在應用",
      "target_audience": [
        "AI 工程師",
        "資料科學家",
        "深度學習研究人員",
        "想從理論進入實作，追求數學 + 程式雙重能力提升者"
      ]
    },
    {
      "title": "YOLOv8目標檢測實戰：訓練自己的數據集",
      "url": "https://www.udemy.com/course/yolov8-od/",
      "bio": "YOLOv8 Object Detection : Train Custom Dataset",
      "objectives": [
        "掌握YOLOv8目標檢測訓練自己的資料集方法",
        "掌握圖像標注方法",
        "掌握自動劃分資料集的方法",
        "學習YOLOv8網路架構"
      ],
      "course_content": {
        "课程介绍": [
          "课程介绍"
        ],
        "目标检测基础知识": [
          "目标检测-任务说明",
          "目标检测-常用数据集",
          "目标检测-性能指标"
        ],
        "YOLOv8目标检测网络": [
          "YOLO目标检测系列技术发展史",
          "YOLOv8网络架构"
        ],
        "YOLOv8目标检测项目实战（Windows）": [
          "安装软件环境(Nvidia驱动，CUDA和cuDNN)",
          "安装PyTorch",
          "安装YOLOv8",
          "使用labelImg标注自己的数据集",
          "准备自己的数据集",
          "修改配置文件",
          "修改設定檔（更新）",
          "训练自己的数据集",
          "测试训练出的网络和性能统计"
        ],
        "YOLOv8目标检测项目实战（Ubuntu）": [
          "安装软件环境（Nvidia显卡驱动、cuda和cudnn）",
          "安装PyTorch",
          "安装YOLOv8",
          "使用labelImg标注自己的数据集",
          "准备自己的数据集",
          "修改配置文件",
          "修改設定檔（更新）",
          "训练自己的数据集",
          "测试训练出的网络模型和性能统计"
        ]
      },
      "requirements": [
        "熟悉Python和PyTorch"
      ],
      "description": "Ultralytics YOLOv8 基於先前 YOLO 版本的成功，引入了新功能和改進，進一步提升性能和靈活性。YOLOv8 支援目標檢測與跟蹤、實例分割、圖像分類和姿態估計任務。YOLOv8使用 PyTorch開發，設計了更高效的具有豐富梯度流的骨幹網路和Neck。採用了Anchor-free無錨范式、解耦頭、Task Aligned正負樣本分配策略和CIoU+DFL損失等前沿技術。\n本課程將手把手地教大家使用labelImg標注和使用YOLOv8訓練自己的資料集，完成一個多目標檢測實戰專案，可檢測圖像和視頻中的足球和梅西兩個目標類別。\n本課程分別在Windows和Ubuntu系統上做專案演示。包括：安裝軟體環境（Nvidia顯卡驅動、cuda和cudnn）、安裝PyTorch、安裝YOLOv8、使用labelme標注自己的資料集、準備自己的資料集（自動劃分訓練集和驗證集）、修改設定檔、訓練自己的資料集（合適的命令參數選擇）、測試訓練出的網路模型和性能統計。",
      "target_audience": [
        "希望學習YOLOv8目標檢測技術的學員和從業者"
      ]
    },
    {
      "title": "ChatGPT提示词工程师指南",
      "url": "https://www.udemy.com/course/chatgpt-yk/",
      "bio": "人工智能提示词工程入门",
      "objectives": [
        "快速入门提示次词技术，获得竞争优势",
        "了解提示词工程师的职业内容",
        "掌握ChatGPT提示词的撰写与优化技巧",
        "熟悉不同类型提示词及其应用"
      ],
      "course_content": {
        "介绍": [
          "课程介绍",
          "讲师介绍",
          "课程大纲",
          "引入讲解"
        ],
        "课程内容": [
          "第一章 指令、角色、种子与标准提示词",
          "第二章 少样本、分布思考与一致性提示词",
          "第三章 知识生成及整合、多项选择与可解释变量提示词",
          "第四章 控制生成、问答与概述提示词",
          "第五章 对话、对抗性与聚类提示词",
          "第六章 强化学习、情感分析与命名实体识别提示词"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "ai从业人员或者兴趣学员"
      ],
      "description": "当今科技迅猛发展的时代，以ChatGPT聊天机器人为代表引发了一场前所未有的技术革命，不仅带来了智能对话的全新体验，同时为了追求智能化、个性化、高效的用户体验，对提示词工程师的期望日益增加。\n为此，三节课邀请了《ChatGPT大师班》和《AI绘画大师班》的作者黄豆奶爸带来本次课程。\n课程深入介绍人工智能技术的最新发展，重点关注以ChatGPT聊天机器人为代表的提示词工程师技术。通过学习本课程，你将掌握如何合理设置提示词来实现更加智能、准确的对话交互，通过实际的操作掌握提示词工程师技术具体的方法和技巧，提高工作效率，同时为生活带来便利、创造力。",
      "target_audience": [
        "AI技术爱好者",
        "企业家、管理者、创业者",
        "对新兴职业感兴趣或想转型的学员"
      ]
    },
    {
      "title": "Python DataScience - Veri Bilimi (Pandas, Numpy, Matplotlib)",
      "url": "https://www.udemy.com/course/python-mucizesi-veri-bilimi-pandas-numpy-ve-matplotlib/",
      "bio": "Pandas ve Numpy kütüphaneleri. Bir eğitim alana ikinci eğitim ücretsizdir.",
      "objectives": [
        "Bu eğitimde Python ile Veri Bilimi anlatılmaktadır.",
        "Data Science",
        "Pandas",
        "Numpy",
        "Matplotlib",
        "Veri Analizi",
        "Bid Data"
      ],
      "course_content": {
        "Giriş": [
          "Giriş",
          "Python Ortamının Kurulumu"
        ],
        "Pandas Kütüphanesi": [
          "DataFrame İşlemleri",
          "DataFrame Adres İşlemleri",
          "DataFrame Örnek Uygulamaları",
          "Veriler Üzerinde Değişiklik Yapmak (Silmek, Eklemek, Dönüştürmek)",
          "Filtreleme İşlemleri",
          "Filtre Uygulamaları",
          "Metin Dosyası Analiz İşlemleri",
          "Excel İşlemleri",
          "CSV İşlemleri",
          "XML İşlemleri",
          "DataFrame Tekrar ve Uygulamaları",
          "SERIES İşlemleri",
          "Index ve Tablo Yapısı İşlemleri",
          "Veri Değiştirme İşlemleri",
          "Aritmetik İşlemler",
          "Aritmetik İşlemler ve Veri Uygulamaları",
          "Metin (String) İşlemleri",
          "Tablo Birleştirme İşlemleri",
          "Tablo Eşleştirme Kuralları",
          "Tablo Eşleştirme İşlemleri",
          "Çoklu Tablo Eşleştirme",
          "Pandas Fonksiyonları",
          "Veri Gruplama ve İstatistiki Analizler",
          "Pivot Tablo",
          "Pivot Tabloda Çoklu Satır ve Sütun",
          "Pivot Tablo Uygulamaları",
          "Grafik Çizim İşlemleri"
        ],
        "Numpy Kütüphanesi": [
          "Numpy Kütüphanesi Genel Görünümü",
          "Dizilerin Yapısı",
          "Dizi (ARRAY) Tanımlama İşlemleri",
          "Dizi Üreteci Parametreleri",
          "Numpy Veri Türleri",
          "Dizilerde Sıralama İşlemi",
          "Dizi ile İlgili Bilgi Alma",
          "Adresleme İşlemleri",
          "Filtreleme İşlemleri",
          "Dizi Verilerini Değiştirmek",
          "Dizilerde Birleştirme İşlemi",
          "Dizileri Alt Dizilere Bölmek",
          "Temel Matematiksel İşlemler",
          "Trigonometrik ve Logaritmik İşlemler",
          "Numpy Uygulamaları",
          "Bir Dizideki Benzersiz Elemanları Listelemek",
          "Dizileri Ters Çevirmek",
          "Çok Boyutlu Dizileri Sadeleştirmek",
          "Numpy ile Veri İşleme ve Yapılandırılmış Diziler"
        ],
        "Grafik İşlemleri (Matplotlib Kütüphanesi)": [
          "Temel Grafik İşlemleri",
          "Grafik Bileşenleri",
          "Grafik Parametre Uygulamaları",
          "Grafiğin Belli Bir Alanını Göstermek",
          "Eksen Etiketlerini Düzenlemek",
          "Tek Grafik İçerisinde Çoklu Çizim",
          "Çoklu Grafikleri Farklı Bölgelerde Göstermek",
          "Çoklu Grafik Uygulamaları",
          "Grafik Çeşitleri",
          "Çubuk (Bar) Grafik",
          "Çubuk Grafik Parametreleri",
          "Eksen Değerlerini Değiştirmek",
          "Üst Üste Çubuk Grafikler",
          "Farklı Kaynakları Grafiğe Çevirmek",
          "Grafiğe Tablo Eklemek",
          "İki Boyutlu Verilerde Tablo Ekleme",
          "Çubuk Grafik Uygulamaları",
          "Grafiklere Veri Etiketi Eklemek",
          "Dağılım (Scatter) Grafiği",
          "Pasta (Pie) Grafik",
          "Alan (Area) Grafiği",
          "Histogram Grafik"
        ],
        "Sonuç": [
          "Sonuç"
        ]
      },
      "requirements": [
        "Kursu almak için lise mezunu olmanız yeterlidir."
      ],
      "description": "Python Mucizesi – Veri Bilimi eğitimine hoş geldiniz.\nTemel Gereksinimler\nÖncelikle Python biliyor olmanız gerekmektedir. Bu kapsamda Python’ı temelden uzman seviyesine kadar öğrenebileceğiniz Python Mucizesi - Temel, Orta ve İleri Düzey eğitimini ücretsiz talep edebilirsiniz. Nasıl yapılacağı size gelen açılış mesajında olacaktır.\nBu eğitim, detaylı olup diğer eğitimlerde sadece 1-2 saat gibi kısa sürelerde anlatılmıştır. Biz ise çok daha uzman ve detaylı seviyede anlatıyoruz. Başka eğitimlerde Veri Bilimi adı altında verilen eğitimlerde Python baştan itibaren anlatılmakta olup bu kapsamda süre israfı olmaktadır. Bu eğitim, rakip eğitimlere kıyaslandığında 1-2 saatte anlatılan eğitimleri 20 saat olarak çok detaylı ve bol örnekle anlatmaktadır. Yine diğer eğitimlerde Veri Bilimi adı altında Python baştan sona anlatılmakta ve böyle sanki veri bilimi bu süre uzunluğunda anlatılmış gibi yapılmakla birlikte bu eğitimde ise sadece veri bilimi anlatılmış ve gereksiz yere Python baştan anlatılmamıştır. Eğer Python dilini baştan almak isterseniz de diğer eğitimimiz olan Python Mucizesi - Temel, Orta ve İleri Düzey eğitimini zaten ücretsiz veriyoruz.\nBu eğitimde 3 ana kütüphane anlatılmaktadır:\nPandas\nNumpy\nMatplotlib\nPandas\nPandas kütüphanesi öyle güçlüdür ki Excel ile yapabildiğiniz analizleri yapabiliyorsunuz. Fakat neden bunu seçesiniz!\nPandas çok güçlü bir kütüphanedir. Excel’de verilerde satır sınırı olurken Pandas da aynı işlemleri yapabildiği halde bu tarz sınırlar yoktur.\nPandas kütüphanesi çok hızlıdır. Excel’de uzun süren ve kilitlenmelere yol açan işlemler burada çok kıs sürede çözülür.\nAlgoritması ve formül kurma işlemleri çok basit ve çok anlaşılırdır.\nGelişmeler önce Pandas kütüphanesinde üretilmiş olup daha sonra Excel ise bunları kopyalayıp aynısını yapmaya çalışmıştır. Fakat yine de çok altında kalmaktadır. Örneğin Excel’in son versiyonlarında sürekli yeni özellikler gelmekle birlikte aslında bu özellikler zaten çok önceden Pandas’da yapılmış ve kullanılmış olmaktadır. Yine bir çok özellik henüz Excel’e eklenememiştir.\nNumpy\nPandas ile birlikte neden Numpy?\nBirincisi, Pandas da tek başına tüm işinizi görür ve yapmak istediğiniz tüm işlemleri yapabilirsiniz. Yani Pandas öğrendiğinizde Numpy bilmek zorunda değilsiniz. Tüm işleri Pandas yapar.\nFakat Numpy da öğrenmek avantajlıdır. Numpy kütüphanesi, Pandas’ta yapılabilenleri yapabildiği gibi ayrıca simülasyon, istatistiki analizler vb. konularda çok daha basit kod yapısına sahip olup basit algoritmalar kullanarak çok karmaşık yapılar üretebilirsiniz.\nAyrıca siz mesela Excel veya başka ortamlarda bir formül yazdığınızda tüm satırlar için bu formülü tekrar çalıştırırsınız. Numpy ise eğitimde de göreceğiniz gibi tek formülle tüm satırlara aynı işlemi yapabilir.\nAyrıca Numpy kütüphanesi grafik çizme ve görsel analizler için de oldukça uygun bir kütüphanedir. Grafik çizmek amacıyla kullanılan kütüphaneler ön şart olarak Numpy kullanmaktadır.\nMatplotlib\nBu kütüphane ise görsel analizler ve grafikler için oldukça güçlüdür.\nNeden bu eğitimi alalım?\nÖncelikle her konuda yeterli ve çok sayıda örnek veriyoruz. Diğer eğitimlerdeki gibi her şey hazır ve önceden belirlenmiş örnekler ile gitmiyoruz. Tamamı gerçek hayattan örnekler ve dolu dolu, bol…\nGerçek hayattan örnekler olduğu için gerçek hayatın karmaşık yapısı ile gidiyoruz.\nMüfredat\nPandas\nDataFrame İşlemleri\nDataFrame Adres İşlemleri\nDataFrame Örnek Uygulamaları\nVeriler Üzerinde Değişiklik Yapmak (Silmek, Eklemek, Dönüştürmek)\nFiltreleme İşlemleri\nFiltre Uygulamaları\nMetin Dosyası Analiz İşlemleri\nExcel İşlemleri\nCSV İşlemleri\nXML İşlemleri\nDataFrame Tekrar ve Uygulamaları\nSERIES İşlemleri\nIndex ve Tablo Yapısı İşlemleri\nVeri Değiştirme İşlemleri\nAritmetik İşlemler\nAritmetik İşlemler ve Veri Uygulamaları\nMetin (String) İşlemleri\nTablo Birleştirme İşlemleri\nTablo Eşleştirme Kuralları\nTablo Eşleştirme İşlemleri\nÇoklu Tablo Eşleştirme\nPandas Fonksiyonları\nVeri Gruplama ve İstatistiki Analizler\nPivot Tablo\nPivot Tabloda Çoklu Satır ve Sütun\nPivot Tablo Uygulamaları\nGrafik Çizim İşlemleri\nNumpy\nNumpy Kütüphanesi Genel Görünümü\nDizilerin Yapısı\nDizi (ARRAY) Tanımlama İşlemleri\nDizi Üreteci Parametreleri\nNumpy Veri Türleri\nDizilerde Sıralama İşlemi\nDizi ile İlgili Bilgi Alma\nAdresleme İşlemleri\nFiltreleme İşlemleri\nDizi Verilerini Değiştirmek\nDizilerde Birleştirme İşlemi\nDizileri Alt Dizilere Bölmek\nTemel Matematiksel İşlemler\nTrigonometrik ve Logaritmik İşlemler\nNumpy Uygulamaları\nBir Dizideki Benzersiz Elemanları Listelemek\nDizileri Ters Çevirmek\nÇok Boyutlu Dizileri Sadeleştirmek\nNumpy ile Veri İşleme ve Yapılandırılmış Diziler\nMatplotlib\nTemel Grafik İşlemleri\nGrafik Bileşenleri\nGrafik Parametre Uygulamaları\nGrafiğin Belli Bir Alanını Göstermek\nEksen Etiketlerini Düzenlemek\nTek Grafik İçerisinde Çoklu Çizim\nÇoklu Grafikleri Farklı Bölgelerde Göstermek\nÇoklu Grafik Uygulamaları\nGrafik Çeşitleri\nÇubuk (Bar) Grafik\nÇubuk Grafik Parametreleri\nEksen Değerlerini Değiştirmek\nÜst Üste Çubuk Grafikler\nFarklı Kaynakları Grafiğe Çevirmek\nGrafiğe Tablo Eklemek\nİki Boyutlu Verilerde Tablo Ekleme\nÇubuk Grafik Uygulamaları\nGrafiklere Veri Etiketi Eklemek\nDağılım (Scatter) Grafiği\nPasta (Pie) Grafik\nAlan (Area) Grafiği\nHistogram Grafik",
      "target_audience": [
        "Bu kurs, Python kullanarak veri bilimi öğrenmek isteyenlere yöneliktir."
      ]
    },
    {
      "title": "Spring AI c Kotlin Spring Boot ChatGPT MCP Claude e DeepSeek",
      "url": "https://www.udemy.com/course/spring-ai-kotlin-spring-boot-mcp-openai-chatgpt-ollama-deepseek-gemini/",
      "bio": "Domine Inteligência Artificial no Kotlin com Spring AI, MCP, Spring Boot, OpenAI(ChatGPT), Ollama, Open WebUI e DeepSeek",
      "objectives": [
        "10% Teoria e 90% Prática",
        "Integrar Inteligência Artificial com Kotlin, Spring Boot e Spring AI",
        "Criar API's RESTful inteligentes conectadas ao ChatGPT",
        "Prompt Engineering e Prompt Templating para gerar conteúdo dinâmico",
        "Gerar imagens com o DALL·E a partir de textos",
        "Transcrever áudio em texto de forma eficiente",
        "Construir interfaces modernas com React JS",
        "Aplicar boas práticas de código limpo e bem estruturado com Spring Boot e Spring AI",
        "Configurar e integrar APIs da OpenAI como GPT-4 e DALL·E",
        "Entender fundamentos de IA, GPTs e modelos de LLM",
        "Desenvolver projetos completos para o seu portfólio",
        "Aprender com exercícios práticos e guias de revisão",
        "Criar APIs inteligentes seguindo as melhores práticas",
        "Criar aplicações inteligentes e alinhadas com o que há de mais novo no mercado"
      ],
      "course_content": {
        "Introdução": [
          "Introdução"
        ],
        "ChatGPT e OpenAI Explicados: Conceitos Essenciais para Começar a Usar a IA": [
          "0201 Apresentação da Seção - ChatGPT e OpenAI Explicados",
          "0202 O que é o ChatGPT?",
          "0203 Criando uma Conta na OpenAI(ChatGPT)",
          "0204 O que são Prompts?",
          "0205 Escrevendo Prompts"
        ],
        "Conhecendo a API da OpenAI (ChatGPT)": [
          "0301 Apresentação da Seção - Conhecendo a API da OpenAI (ChatGPT)",
          "0302 Introdução à OpenAI",
          "0303 O que são API's?",
          "0304 O que é um GPT?",
          "0305 O que são Modelos LLM?",
          "0306 Conhecendo o OpenAI Playground",
          "0305 O que são Tokens?",
          "0306 Obtendo o Máximo do ChatGPT e da OpenAI",
          "0307 Preços e Rate Limits da OpenAI (ChatGPT)"
        ],
        "Tudo o que Você Precisa Saber Antes de Começar a Codificar": [
          "0401 Apresentação da Seção - O que Você Precisa Saber Antes de Começar a Codar",
          "0402 Como Preparar seu Ambiente de Desenvolvimento: Ferramentas e Recomendações",
          "0403 Como Acessar o Repositório com o Código das Aulas do Curso no Github?",
          "0404 Como Consultar o Código das Aulas?"
        ],
        "Integrando uma Aplicação Spring Boot com Spring AI à OpenAI e ChatGPT": [
          "0501 Apresentação da Seção - Integrando uma App Spring Boot com Spring AI à Open",
          "0502 O que é Spring AI e como Incorporar Inteligência Artificial às suas Apps Sp",
          "0503 Conhecendo o Primeiro Projeto com Spring AI e ReactJS que Iremos Construir",
          "0504 Criando a App com Spring AI na Spring Initializr e Executando-o na IntelliJ",
          "0505 Criando uma API key para Consumir a API do ChatGPT",
          "0506 Criando uma Variável de Ambiente para Armazenar a Secret do ChatGPT OpenAI",
          "0507 Criando a Variável de Ambiente p. Armazenar a Secret do ChatGPT na IntelliJ",
          "0508 Ajustando a Aplicação para se Comunicar com a Open AI"
        ],
        "[PROJETO 1] Geração de Receitas e Imagens com Spring AI, OpenAI e ChatGPT": [
          "0601 Apresentação da Seção - Geração de Receitas e Imagens com Spring AI, OpenAI",
          "0602 Fazendo a Primeira integração com Spring AI, OpenAI e ChatGPT",
          "0603 Testando a Primeira integração com Spring AI, OpenAI e ChatGPT no Postman",
          "0604 Implementando o Serviço Responsável por consumir a API da OpenAI e ChatGPT",
          "0605 Implementando o Controller Responsável por consumir a API da OpenAI e ChatG",
          "0606 Testando a Integração c Spring Boot Spring AI, OpenAI e ChatGPT no Postman",
          "0607 Criando o Serviço Responsável p criar Receitas c Spring AI e OpenAI(ChatGPT",
          "0608 Criando o Controller Responsável p criar Receitas c Spring AI e OpenAI(Chat",
          "0609 Testando a Integração do Gerador de Receitas através do Postman",
          "0610 Criando o Serviço Responsável por Gerar Imagens c Spring AI e OpenAI(ChatGP",
          "0611 Criando o Controller Responsável por Gerar Imagens c Spring AI e OpenAI(Cha",
          "0612 Testando a Integração do Gerador de Imagens através do Postman",
          "0613 Habilitando CORS pra Integração com o Client React JS"
        ],
        "[PROJETO 1] Client React JS para consumir a API com Spring AI, OpenAI e ChatGPT": [
          "0701 Apresentação da Seção - Client React JS p a API c Spring AI, OpenAI e ChatG",
          "0702 Instalando o Node.JS",
          "0703 Instalando o Visual Studio Code",
          "0704 Criando o Client React para consumir a API com Spring AI, OpenAI e ChatGPT",
          "0705 Iniciando a Aplicação que vai consumir a API c Spring AI, OpenAI e ChatGPT",
          "0706 Implementando o Layout da Tela de Chat no React JS",
          "0707 Aplicando o CSS ao Layout do Client em React JS",
          "0708 Criando os Primeiros Componentes do Client em React JS",
          "0709 Trabalhando com State e Finalizando o Componente de Chat",
          "0710 Integrando o Layout do Componente de Chat à API com Axios",
          "0711 Implementando o Layout da Tela de Geração de Receitas no React JS",
          "0712 Integrando o Layout da Tela de Geração de Receitas no React JS à API",
          "0713 Ajustando o Layout para Renderizar Markdown como HTML",
          "0714 Implementando o Layout da Tela de Geração de Imagens",
          "0715 Integrando o Layout da Tela de Geração de Imagens no React JS à API"
        ],
        "[PROJETO 2] Transcrição de Voz em Texto com Spring AI, OpenAI e ChatGPT": [
          "0801 Apresentação da Seção - Transcrição de Voz em Texto com Spring AI, OpenAI",
          "0802 O Segundo Projeto com Spring AI e ReactJS que Iremos Construir",
          "0803 Criando e Executando o Projeto com Spring AI na IntelliJ",
          "0804 Criando o Serviço Responsável por Transcrever Áudio usando OpenAI e ChatGPT",
          "0805 Criando o Controller Responsável p Transcrever Áudio com OpenAI e ChatGPT",
          "0806 Testando o Endpoint de Transcrição de Áudio com OpenAI e ChatGPT no Postman"
        ],
        "[PROJETO 2] Criando o Client React JS p 2° Projeto c Spring AI e ChatGPT(OpenAI)": [
          "0901 Apresentação da Seção - Criando o Client React JS pro 2° Projeto c SpringAI",
          "0902 Iniciando a Integração com a API com ReactJS",
          "0903 Implementando o Layout da Tela de Transcrição de Áudios no React JS",
          "0904 Integrando o Layout do Componente de Transcrição de Áudio à API com Axios",
          "0905 Testando a Tela de Transcrição de Áudios no React JS"
        ],
        "DeepSeek do 0 à Integração c. Spring AI, Spring Boot, Kotlin e React JS": [
          "1001 Apresentação da Seção - DeepSeek do 0 à Integração com Spring AI e React JS",
          "1002 O que é a DeepSeek?",
          "1003 Criando uma conta na DeepSeek",
          "1004 Conhecendo os Modelos LLM da DeepSeek",
          "1005 Conhecendo o Playground da DeepSeek",
          "1006 Preços e Rate Limits",
          "1007 Criando uma API key para Consumir a API da DeepSeek",
          "1008 Criando uma Variável de Ambiente para Armazenar a Secret da DeepSeek",
          "1009 Ajustando a Aplicação para se Comunicar com a DeepSeek",
          "1010 Adicionando Créditos à Conta da Deep Seek",
          "1011 Testando a Integração com a DeepSeek no Postman",
          "1012 Integrando o Client React JS à API que Consome a DeepSeek"
        ]
      },
      "requirements": [
        "Conhecimentos básicos de Kotlin",
        "Familiaridade com Spring Boot e desenvolvimento de REST APIs",
        "Noções básicas de Orientação a Objetos",
        "Conhecimentos básicos de REST API's",
        "Experiência prévia com ferramentas SPA como React, Angular ou Vue.js (não obrigatório, mas ajuda a acelerar o aprendizado)",
        "Vontade de aprender e explorar o mundo da Inteligência Artificial"
      ],
      "description": "O curso ensina a integração da Inteligência Artificial com Kotlin, utilizando Spring Boot 3.4.4 e o revolucionário Spring AI, combinando teoria e prática de forma equilibrada. Você aplicará esse conhecimento na implementação de duas aplicações poderosas que exploram os modelos LLM mais avançados da OpenAI.\nIniciaremos com os conceitos fundamentais de IA, OpenAI, GPT's, Modelos LLM, Tokens e claro, o Spring AI. Vamos preparar seu ambiente de desenvolvimento com Kotlin, Spring Boot 3.4.4, Visual Studio Code, IntelliJ e Postman, além de configurar a conexão com as APIs da OpenAI.\nA prática começa cedo! No primeiro projeto, você criará uma API RESTful inteligente, capaz de se conectar diretamente com o ChatGPT para fornecer respostas personalizadas. Em seguida, mergulharemos em Prompt Engineering e Prompt Templating para construir um gerador de receitas personalizadas com base em parâmetros fornecidos pelo usuário, unindo criatividade e tecnologia de ponta. Para fechar esse projeto com chave de ouro, você integrará o DALL·E, permitindo a geração de imagens a partir de descrições textuais. E para garantir a robustez, tudo será testado e validado no Postman e apresentado em um client moderno em React JS.\nNo segundo projeto, exploraremos outro cenário fascinante: a transcrição de áudio em texto. Imagine criar uma aplicação capaz de transformar áudios em textos de maneira prática e eficiente, ampliando a acessibilidade e a usabilidade do sistema. Assim como no projeto anterior, você desenvolverá também um client em React JS para tornar a interação ainda mais intuitiva.\nE não para por aí. Neste curso, você também vai integrar modelos LLM de outras plataformas além da OpenAI, como a DeepSeek, uma alternativa poderosa e cada vez mais reconhecida. Você vai aprender a configurar autenticação, fazer chamadas à API, testar tudo no Postman e consumir essas respostas com fluidez no frontend.\nAlém disso, vamos explorar o uso de modelos locais com o Ollama. Com ele, você poderá rodar LLM's diretamente na sua máquina, inclusive alguns da própria DeepSeek. Isso amplia sua autonomia, garante mais privacidade e permite que você trabalhe offline, sem depender de chamadas externas.\nPara garantir que você desenvolva aplicações sólidas, o curso cobre boas práticas de desenvolvimento, aplicando padrões do Spring Boot e conceitos de código limpo. Além disso, trabalharemos com variáveis de ambiente para proteger suas chaves de acesso às APIs da OpenAI.\nTodos os principais arquivos de configuração estarão disponíveis para download, e você contará com guias básicos em PDF que reforçam os principais conceitos abordados.\nAo final do curso, você terá criado dois projetos robustos, prontos para turbinar seu portfólio e destacar seu perfil no mercado.\nNesse treinamento, você aprenderá a trabalhar com tecnologias como:\nSpring AI;\nIntegração com a API da OpenAI (ChatGPT);\nSpring Boot 3.4.4 (última versão estável);\nKotlin (última versão LTS);\nPrompt Engineering e Prompt Templating;\nGeração de imagens com DALL·E;\nTranscrição de áudio em texto;\nCriação de APIs RESTful inteligentes;\nTestes e validação de APIs com Postman;\nBoas práticas de desenvolvimento e código limpo;\nCriação de interfaces modernas com React JS.\nEsses conhecimentos te colocarão à frente no mercado, capacitando você a desenvolver soluções reais, inovadoras e de alto impacto. Mas o mais importante: você dominará as integrações do Spring AI com a OpenAI, estando sempre preparado para criar aplicações inteligentes, escaláveis e alinhadas com as demandas atuais do mercado.\nA Inteligência Artificial está evoluindo rapidamente, e profissionais que dominam essas tecnologias são cada vez mais valorizados. Pensando nisso, este curso adota uma abordagem estratégica: projetos menores e poderosos, permitindo atualizações frequentes sem comprometer a fluidez do aprendizado. Dessa forma, você sempre terá acesso às práticas mais recentes, dominando as novidades do mundo da IA de forma dinâmica e eficiente.\nSe você quer se destacar e criar aplicações inteligentes com Inteligência Artificial, Spring AI, Kotlin, Spring Boot 3 e OpenAI, inscreva-se agora e vamos juntos transformar ideias em projetos incríveis!\nTe vejo no curso!",
      "target_audience": [
        "Desenvolvedores Kotlin que desejam integrar Inteligência Artificial em suas aplicações",
        "Desenvolvedores back-end com experiência em Spring Boot e que querem explorar o potencial da IA com Spring AI",
        "Desenvolvedores full-stack que desejam expandir suas habilidades para criar aplicações inteligentes",
        "Profissionais de tecnologia que já trabalham com REST APIs e querem aprender a integrar modelos de IA como GPT-4 e DALL·E",
        "Desenvolvedores front-end que querem aprender a consumir APIs inteligentes",
        "Quem já tentou outros tutoriais sobre IA e descobriu que eram muito superficiais, desatualizados ou difíceis de seguir",
        "Profissionais que buscam se destacar no mercado dominando tecnologias de ponta como Inteligência Artificial, Spring AI e OpenAI"
      ]
    },
    {
      "title": "Pythonで体系的に学ぶデータサイエンスとAIの初歩 Vol.4 ネットワーク基礎・ブロックチェーンのしくみと要素技術",
      "url": "https://www.udemy.com/course/pythonai-vol4/",
      "bio": "ネットワークの基礎知識（進数変換、IPアドレス、公開鍵暗号、デジタル署名、SQL、Web API）、ブロックチェーンのしくみと要素技術（ビットコイン、楕円曲線暗号、ハッシュ関数）、シンプルなブロックチェーン（二重支払問題、PoW）",
      "objectives": [
        "Pythonの基礎知識を得て、簡単なプログラムを作成できるようになります。",
        "データサイエンスやAIの学習に必要な数学・統計などの初歩的基礎知識を体系的に得ることができます。",
        "データサイエンスとAIを本格的に学ぶにあたって必要な初歩的事項を身に付けることができます。",
        "本Vol.4では特に「ネットワークの基礎知識」、「ブロックチェーンのしくみと要素技術」、「シンプルなブロックチェーン」についての基礎知識を得ることができます。"
      ],
      "course_content": {
        "コースの紹介": [
          "コースの紹介"
        ],
        "Anacondaのインストール方法": [
          "Anacondaのインストール方法 Windows向け",
          "Anacondaのインストール方法 Mac向け"
        ],
        "テキスト": [
          "テキスト"
        ],
        "ネットワークの基礎知識": [
          "テンプレートプログラム",
          "テンプレートプログラムの使い方 Windows向け",
          "テンプレートプログラムの使い方 Mac向け"
        ],
        "ITの基礎": [
          "2進数・10進数・16進数と進数変換",
          "ビット演算1",
          "ビット演算2",
          "ビット演算3",
          "ビット演算4",
          "バイト型データとエンコード・デコード1",
          "バイト型データとエンコード・デコード2"
        ],
        "インターネット": [
          "Webサイトが見えるしくみ",
          "ネットワークの種類",
          "プライベートIPアドレス1",
          "プライベートIPアドレス2"
        ],
        "セキュリティ": [
          "公開鍵暗号1",
          "公開鍵暗号2",
          "デジタル署名1",
          "デジタル署名2",
          "デジタル証明書",
          "Webサイトへの攻撃"
        ],
        "データベース": [
          "基本操作 CRUD",
          "データベースの作成・接続とテーブルの作成1",
          "データベースの作成・接続とテーブルの作成2",
          "SQL文による操作1",
          "SQL文による操作2",
          "SQL文による操作3"
        ],
        "Webサイト": [
          "JSONデータ形式",
          "Web APIの利用1",
          "Web APIの利用2",
          "Web APIの利用3",
          "簡単な Web API の作成1 Flask",
          "簡単な Web API の作成2 HTMLファイルの表示",
          "簡単な Web API の作成3 GETメソッド",
          "簡単な Web API の作成4 GETメソッド",
          "簡単な Web API の作成5 クエリパラメーター",
          "簡単な Web API の作成6 パスパラメーター",
          "簡単な Web API の作成7 to-do リストアプリ",
          "簡単な Web API の作成8 to-do リストアプリ",
          "簡単な Web API の作成9 to-do リストアプリ",
          "簡単な Web API の作成10 to-do リストアプリの操作",
          "簡単な Web API の作成11 urllibによる操作"
        ],
        "サンプルプログラム": [
          "サンプルプログラム"
        ]
      },
      "requirements": [
        "簡単なプログラミングの経験があった方が望ましいです。",
        "中学生レベル、できれば高校生レベルの数学的素養があると理解が早いです。"
      ],
      "description": "■「Pythonで体系的に学ぶデータサイエンスとAIの初歩」シリーズ全4巻ではデータサイエンスとAIの初歩を学ぶのに必要な数学、統計学などの初歩的基礎知識を体系的に学んでいきます。高校の新学習指導要領の「情報I」および「情報II」の内容も多く含んでいます。\n■高校生程度の数学の知識があれば理解できるように説明しているので、いきなりデータサイエンスやAIのコースは荷が重いという方にお勧めです。実際に中高生向けのプログラミング教室での実績があります。\n■Anacondaをインストールして、Jupyter Notebookでプログラミングを行なっていきます。\n■学習内容\nVol.1ではPythonの基礎、数と暗号、関数と微分\nVol.2では確率と統計、ベイズ統計、データサイエンスとAIの初歩\nVol.3では線形代数、主成分分析、自然言語処理\nVol.4ではネットワークの基礎知識、ブロックチェーン\nについて学んでいきます。\n■4コースともPDF形式のテキスト（約100ページ）が付属しています。\nまた、タイピングが大変だったり時間がないという方のためにある程度コードを記載したテンプレートプログラムをつけてあります。完成したサンプルプログラムもついています。\n■Vol.4の内容は以下の通りです。\nネットワークの基礎知識\nブロックチェーンのしくみと要素技術\nシンプルなブロックチェーン",
      "target_audience": [
        "データサイエンスやAIに関心を持つが、いきなり本格的なAIプログラミングは困難なPythonプログラミング初心者。",
        "高校の新学習指導要領「情報I」、「情報II」レベルのプログラミングを学びたい方。",
        "高校の新学習指導要領「情報I」、「情報II」レベルのプログラミングを教えたい教師。"
      ]
    },
    {
      "title": "Her Yönüyle Uygulamalı SQL",
      "url": "https://www.udemy.com/course/her-yonuyle-uygulamali-sql/",
      "bio": "SQL'i baştan sona uygulamalarla detaylı bir şekilde öğreneceksiniz.",
      "objectives": [
        "MSSQL öğrenerek veritabanlarında veri sorgulaması yapabileceksiniz.",
        "Veri tanımlama, sorgulama ve manipülasyonu kapsamında select, insert, update, create, delete ve truncate gibi komutların kullanımına hakim olacaksınız.",
        "İlişkisel Veri Tabanını öğreneceksiniz.",
        "Case When, Join, SubQuery, Not exists gibi yapıların kullanımlarını öğrenip, karmaşık kullanımlar için hakim olacaksınız."
      ],
      "course_content": {
        "Giriş": [
          "Giriş"
        ],
        "VERİ ve VERİTABANI NEDİR?": [
          "VERİ ve VERİTABANI NEDİR?",
          "RDBMS - İLİŞKİSEL VERİTABANI YÖNETİM SİSTEMİ",
          "MICROSOFT SQL SERVER KURULUMU",
          "VERİTABANI ve TABLO OLUŞTURMA"
        ],
        "SQL KOMUTLARI": [
          "SQL KOMUTLARI",
          "SQL'e CSV IMPORT ETME",
          "DQL SQL KOMUTLARI - SELECT ve TOP",
          "DML SQL KOMUTLARI - INSERT",
          "DML SQL KOMUTLARI - UPDATE",
          "DML SQL KOMUTLARI - DELETE",
          "DDL SQL KOMUTLARI - CREATE ve DROP",
          "DDL SQL KOMUTLARI - TEMPORARY TABLE ve GLOBAL TEMPORARY TABLE",
          "DDL SQL KOMUTLARI - TRUNCATE ve ALTER",
          "CREATE INSERT SELECT ALIŞTIRMA"
        ],
        "SQL WHERE ve AGGREGATE FONKSİYONLAR": [
          "VERİ TİPLERİ",
          "WHERE KOMUTU İLE KOŞUL BELİRLEME",
          "AND ve OR OPERATÖRLERİ",
          "ORDER BY KOMUTU",
          "SUBQUERY",
          "SUBQUERY ve WHERE ALIŞTIRMA",
          "AGGREGATE FONKSİYONLAR ve GROUP BY",
          "AGGREGATE FONKSİYONLAR ALIŞTIRMA"
        ],
        "JOIN YAPILARI": [
          "VERİTABANI RESTORE ETME",
          "INNER JOIN ve ALIAS",
          "LEFT JOIN ve RIGHT JOIN",
          "FULL OUTER JOIN",
          "JOIN ALIŞTIRMA"
        ],
        "PRATİK": [
          "PRATİK",
          "CASE WHEN",
          "COMMON TABLE EXPRESSION (CTE)"
        ],
        "SON": [
          "SON"
        ]
      },
      "requirements": [
        "Sıfırdan başlayan bir eğitim olduğu için daha öncesinde SQL bilginizin olması gerekli değildir."
      ],
      "description": "Bu kursta, veri analistleri ve veri bilimcileri için önemli olan SQL başlangıç seviyesinden ileri düzeye kadar kapsamlı bir eğitim sunulmaktadır. SQL, ilişkisel veritabanlarıyla etkili bir şekilde etkileşim kurmanın temel yoludur ve bu eğitim, katılımcıların SQL’i ustalıkla kullanmalarını hedeflemektedir.\nEğitimin ilk bölümünde, veri tabanlarının temel kavramları ve ilişkisel veritabanları hakkında bilgi verilmektedir. DDL (Data Definition Language), DML (Data Manipulation Language) ve DQL (Data Query Language) kapsamında temel SQL komutlarını öğrenerek, veritabanı yönetimi konusunda sağlam bir altyapı kazanacaksınız.\nWHERE yapısının kullanımı ve operatörlerle veri filtreleme yöntemlerini öğrenecek, ardından JOIN yapıları sayesinde farklı tablolar arasında veri birleştirme becerisi kazanacaksınız. Aggregate fonksiyonları ile veri toplama ve analiz yapma yetkinliğinizi artıracak, temporary tablolar ve Common Table Expressions (CTE) oluşturarak veri manipülasyonuna dair pratik deneyim edineceksiniz.\nBu kurs, SQL konusundaki bilgi ve becerilerinizi geliştirerek kariyerinize önemli katkılar sağlamayı amaçlamaktadır. Eğitim boyunca çeşitli uygulamalar ve örnek projeler ile bilgilerinizi pekiştirecek, gerçek dünya senaryolarında nasıl SQL kullanacağınızı öğrenerek profesyonel becerilerinizi geliştireceksiniz. Hem teorik hem de uygulamalı içeriklerle dolu olan bu eğitim, veri odaklı projelerde daha etkili olmanıza yardımcı olacaktır. Kurs sonunda, edindiğiniz yetkinliklerle iş hayatında bir adım öne çıkma fırsatı bulacaksınız. Ayrıca, sektördeki en son trendleri takip etme ve güncel yöntemleri öğrenme şansını da elde edeceksiniz. Bu sayede, işverenlerin aradığı yetenekleri kazanarak rekabet avantajı elde edebilirsiniz.",
      "target_audience": [
        "SQL öğrenmek isteyenler",
        "Kariyerinde Veri Analistliği, İş Zekası, Veri Bilimi alanında çalışmak isteyenler",
        "Bilgisayar mühendisliği ve yazılım alanındaki öğrenciler"
      ]
    },
    {
      "title": "파이썬 웹서비스API 실전 프로젝트 - 돈 버는 디지털마케팅",
      "url": "https://www.udemy.com/course/webservice-api/",
      "bio": "네이버쇼핑, 쿠팡파트너스 등 돈 버는 디지털마케팅은 꾸준함도 필요하지만, 과학적인 분석과 자동화로 쉽고 효과적으로 접근하세요. 인기상품, 키워드, 블로그, 쇼핑 정보를 수집하고 분석하는 핵심 비법을 알려드립니다.",
      "objectives": [
        "파이썬 웹서비스API",
        "웹 스크래핑, 크롤링",
        "Open API 사용하기",
        "플라스크 웹서비스",
        "아이오닉 웹앱",
        "데이터 수집 자동화",
        "데이터 분석 자동화",
        "디지털마케팅의 기초와 프로그래밍"
      ],
      "course_content": {
        "디지털마케팅 이해하기": [
          "[특별강의] 디지털마케팅, 돈 버는 방법",
          "[특별강의] 제휴마케팅, 수익창출 방법",
          "[특별강의] 웹 스크랩핑 Web Scraping 이란?"
        ],
        "웹스크래핑, API로 데이터 수집 자동화": [
          "Marketing Master 기능소개",
          "Marketing Master 웹앱 만들기",
          "네이버쇼핑 Best100 정보 스크래핑",
          "네이버 Open API로 블로그, 쇼핑정보 수집",
          "네이버 광고API 연관키워드 검색",
          "쿠팡파트너스 API로 인기상품 정보수집"
        ],
        "파이썬 플라스크 웹서비스": [
          "플라스크 첫 프로그램",
          "네이버쇼핑 Best100 웹서비스",
          "네이버 광고API 연관키워드 웹서비스",
          "네이버 API 블로그, 쇼핑정보 웹서비스",
          "쿠팡파트너스 API 인기상품 웹서비스"
        ],
        "아이오닉 웹앱 완성하기": [
          "웹앱 클라스, 컴퍼넌트 제작하기",
          "아이오닉 웹앱 환경설정",
          "초기 Home 페이지 제작",
          "검색Search 페이지 UI 제작",
          "검색Search Form 제작",
          "검색Search 웹서비스API 연동",
          "검색Search 키워드,블로그, 쇼핑 API 연동",
          "쿠팡파트너스 상품검색 UI 제작",
          "쿠팡파트너스 웹서비스 API 연동"
        ]
      },
      "requirements": [
        "파이썬 100분 핵심강의",
        "아이오닉 100분 핵심강의",
        "열심히 하고자 하는 마음"
      ],
      "description": "웹스크래핑, Open API, 플라스크 기술로 효과적인 디지털마케팅에 도전하세요!\n\n\n디지털마케팅을 아시나요?\n네이버 블로그, 쿠팡파트너스로 자면서도 돈을 버는 방법을 찾고 있나요?\n\n\n그런데, 무작정 열심히 상품 찾아 블로그에 소개하면 될까요?\n인기있는 상품, 효과적인 키워드, 많이 찾는 블로그는 모두 이유가 있습니다.\n인기상품, 키워드, 블로그, 쇼핑정보를 수집하고 분석하는\n파이썬 웹서비스API 프로그램을 직접 만들어 보세요.\n\n\n디지털마케팅 개념과 원하는 프로그램을 만드는 비법을 알려드립니다.\n\n\n-강의소개\n'Marketing Master'라는 디지털마케팅 프로그램을 만들면서 파이썬(Python), 플라스크(Flask), 아이오닉(Ionic) 프로그램 능력을 키우는 데 만족하지 말고 디지털마케팅을 이용해 돈을 버는 일에 도전해 보세요!\n디지털마케팅, 너무 광범위하고 어려운 용어인가요? 주변에서 블로그를 하거나 유튜브 광고 또는 제휴마케팅을 하시는 분들이 키워드, SEO(Serch Engine Optimization) 등을 잘 이용해서 검색순위를 높이고 수입을 올린다는 말을 합니다. 과연 어떻게 하는 건지 궁금합니다. 이런 일을 도와주는 프로그램도 주변에 있습니다.\n이런 프로그램은 어떻게 만들 수 있을까요? 내가 원하는 이런 기능을 넣어서 만들 수는 없을까요? 여러분 모두 할 수 있습니다. 디지털 마케팅을 하기 위해 가장 핵심이 되는 기능을 직접 만들어보도록 과정을 구성했습니다.\n\n\n네이버쇼핑 Best100 상품정보를 자동으로 찾아줍니다!\n\n네이버광고 API로 인기있는 연관검색어를 추천해줍니다.\n\n검색어와 관련된 인기 블로그와 판매상품은 무엇일까요?\n\n원하는 쿠팡파트너스 상품링크를 모아줘요. 홍보해서 돈 벌어야지요.\n\n\n재미있는 과제를 이론과 함께 한 단계씩 배워나갈 수 있도록 강의를 구성했습니다.\n과정을 마치고 나면 여러분의 'Marketing Master' 프로젝트를 꿈꾸게 될 것입니다.\n나만의 'Marketing Master' 프로젝트를 만들어서 직접 돈을 벌어보세요!\n\n\n-프로젝트 소개\n가장 많이 찾는 인기상품은 어떤 걸까요?\n웹스크롤링 기술을 이용해서 네이버쇼핑 Best100상품 정보를 자동으로 가져와서 인기있는 상품을 분석해보세요.\n\n\n많이 조회되면서 경쟁은 적은 연관 검색어는 어떤 것들인가요?\n네이버광고 API를 사용하면 상품과 연관된 효과적인 검색어를 찾아 광고효과를 높일 수 있어요.\n\n\n검색어와 연관된 인기 블로그와 판매 상품을 찾아줍니다. 인기가 있는데는 이유가 있겠지요?\n네이버 Open API를 사용하여 검색어와 연관된 인기 블로그와 판매 상품을 살펴보고 홍보 계획을 세워봐요.\n\n\n쿠팡파트너스에서 잘 팔리는 상품링크를 자동으로 가져와 보세요.\n쿠팡파트너스 API로 원하는 카테고리별 인기상품을 찾아 그 링크를 이용해서 쉽게 홍보하세요.\n\n\n특별강의\n프로그램을 만드는 방법을 소개하는 내용뿐 아니라 디지털마케팅, 웹스크래핑, SEO(Searh Engine Optimization), 키워드검색, 제휴마케팅 등 업무 지식을 배울 수 있는 특별강의를 포함했습니다. 여러분이 만드는 프로그램을 이용해서 직접 돈을 버는 데도 도전해 보세요!\n\n\n- 어떤 툴을 사용하나요?\n\n\n이 강의는 웹스크래핑과 Open API를 다루기 위해 파이썬을 사용합니다.\n가져와서 가공한 데이터를 새로운 API로 웹에서 서비스하기 위해서 플라스크를 썼습니다.\n그리고 웹앱으로 멋지게 화면을 만들기 위해서 아이오닉을 이용했습니다.\n이 외에도 몇가지 유용한 소프트웨어를 설치하는데 강의 속에서 하나씩 설명드리겠습니다.\n\n-궁금해요!\nQ. 이 강의는 어떤 특징을 가지고 있나요?\nA. 네이버 블로그와 쿠팡파트너스 등 디지털마케팅을 효과적으로 하기위해 인기상품, 검색어, 블로그, 쇼핑정보를 찾아야 합니다. 이 과정은 디지털마케팅 관련된 이론 설명 뿐 아니라 Open API, 웹스크래핑을 이용해서 필요한 인기상품, 검색어, 블로그, 쇼핑정보를 찾아 웹앱서비스를 하는 실전 프로젝트를 배우게 됩니다.\nQ. 비전공자도 들을 수 있나요?\nA. 데이터 과학은 꼭 전산을 전공한 분만 할 수 있는 분야가 아닙니다. 파이썬, 플라스크, 아이오닉 등 프로그램을 배우는 것도 중요하지만 디지털마케팅을 이해하는 것도 중요합니다. 여러분의 열정만 있다면 충분히 배우고 활용할 수 있는 내용입니다.",
      "target_audience": [
        "파이썬 웹서비스를 배우고 싶은 분",
        "Open API를 이용하고 만들고 싶은 분",
        "웹스크래핑, 크롤링을 배우고 싶은 분",
        "데이터 수집과 분석을 자동화 하고 싶은 분",
        "네이버, 쿠팡 등 쇼핑 정보를 분석하고 싶은 분",
        "쿠팡파트너스, 네이버 블로그 프로그램을 만드시는 분",
        "디지털마케팅 프로그램으로 수익을 올리려는 분"
      ]
    },
    {
      "title": "오렌지(Orange)를 활용한 코딩 없는 AI 데이터 분석 - Lv.4 분류 머신러닝 분석",
      "url": "https://www.udemy.com/course/maso-ds-orange-onc46/",
      "bio": "복잡한 파이썬(python), 느린 엑셀(excel) 데이터 분석은 이제 그만! 노 코드 분석 도구 Orange로 쉽게 하는 분류 머신러닝(Machine Learning) 분석",
      "objectives": [
        "노코딩 분석 도구 Orange를 활용한 로지스틱 회귀 분석",
        "분류 모델 평가 지표의 이해와 적용",
        "앙상블 기법을 통한 분석 능력 향상",
        "다양한 분류 모델의 실무 데이터 적용 역량 강화"
      ],
      "course_content": {
        "둘로 나누는 이진분류와 보정학습": [
          "AAC081 이진분류의 대명사 – 로지스틱 회귀분석",
          "AAC082 이진분류의 대명사 – 로지스틱 회귀분석 실습",
          "AAC091 Calibrated Learner 보정 학습자"
        ],
        "분류가 잘 되었는지 어떻게 알까": [
          "AAC101 분류 모델의 평가 지표 – 혼동 행렬",
          "AAC102 분류 모델의 평가 지표 – 혼동 행렬 실습",
          "AAC103 분류 모델의 평가 지표 – ROC 곡선과 AUC",
          "AAC104 분류 모델의 평가 지표 – ROC 곡선과 AUC 실습",
          "AAC105 ROC 분석으로 예측 확률 임계치 정하기",
          "AAC106 분류 모델의 평가 지표 – Gain chart"
        ],
        "초평면을 찾는 서포트 벡터 머신": [
          "AAC111 서포트 벡터 머신(SVM)",
          "AAC112 커널 트릭"
        ],
        "잘 모를 때는 옆사람 보고 따라하는 KNN": [
          "AAC121 최근접이웃을 따라하는 KNN"
        ],
        "독립된 두 사건의 확률 곱하기 나이브 베이즈": [
          "AAC131 조건부 확률에 기반한 나이브 베이즈"
        ],
        "나무와 숲을 흉내낸 분류모델 Tree와 랜덤포레스트": [
          "AAC141 Decision Tree 의사결정 나무 모형",
          "AAC142 불순도와 정보 이득",
          "AAC143 무작위성을 높인 랜덤 포레스트"
        ],
        "데이터에서 맞춤형 분석 모델 만들기": [
          "AAC151 분류 모형의 분류와 Curve fit"
        ],
        "여러 모델을 결합해서 분석하는 앙상블 기법": [
          "AAC161 분석 모델의 중첩 – 앙상블 기법",
          "AAC162 다수결로 결정하는 배깅(Bagging)",
          "AAC163 부스팅 – AdaBoost와 Gradient Boosting, XG Boost",
          "AAC164 여러 모델의 결과를 메타 분석하는 스태킹(stacking)"
        ]
      },
      "requirements": [
        "본 강의는 인공지능에 관심이 생긴 누구나 바로 들어 실무에 활용할 수 있는 역량 제공을 목표로 설계된 강의이나, 오렌지(Orange)를 활용한 코딩 없는 AI 데이터 분석 - Lv.3 예측 머신러닝 분석의 첫 걸음 강의를 먼저 수강하시는 것을 추천드립니다.",
        "실습 위주의 강의이기 때문에 강의 화면과 실습 화면을 분리할 수 있는 듀얼 모니터 또는 여분 기기를 함께 준비해주시면 좋습니다.",
        "오렌지는 무료로 배포되고 있는 소프트웨어이며, 다운로드부터 설치까지 하나하나 가르쳐 드리기 때문에 누구나 손쉽게 인공지능 데이터분석 환경을 구축 가능합니다. Portable 버전을 사용하면 외부 인터넷 연결 없이도 사용 가능하여, 보안 수준이 높은 근무 환경에서도 사용 가능합니다."
      ],
      "description": "\"어제보다 성장하겠습니다. 그리고, 어제보다 성장하려는 사람을 돕겠습니다.\"\n\n\n마소캠퍼스의 진심과 소망을 담은 Actionable Content로,\n2013년부터 온오프라인으로 함께해 온 누적 강의 1억시간!\n이 소중한 경험과 시간은 언제나 마소캠퍼스와 수강생 모두의 성장의 원천입니다.\n\n\n-\n데이터 분석 개념부터 AI도구 활용 실습까지!\n개발자 없이도 고급 데이터 분석? 물론 가능합니다!\n“코딩 없는 AI 데이터 분석 Lv.4 분류 머신러닝 분석”에서는 머신 러닝의 복잡성을 걷어내고,\n데이터 분석에 관심있는 모든 분이 접근할 수 있는 방식으로 데이터 분석을 다룹니다.\n\n\n고급 데이터 분석이 막막하신가요?\n‘코딩 없는 AI 데이터 분석 Lv.4 분류 머신러닝 분석’에서는 기본적인 이진 분류부터 모델의 평가,\n여러 모델을 결합하여 분석하는 앙상블 기법까지 여러 기법들을 배울 수 있습니다.\n단순한 데이터 분석을 넘어 실질적이고 효과적은 분류 모델을 구축하는 방법을 마스터할 수 있습니다.\n\n\n이 모든 것을 가능하게 하는 도구는 바로 “Orange”입니다.\n복잡한 코딩 없이도, 드래그 앤 드롭만으로 데이터를 불러오고, 처리하며, 분석할 수 있습니다.\n개발자와 동등한 수준의 데이터 사이언스 작업을 수행하여\n개발자에 버금가는 가치를 창출할 수 있습니다.\n\n\n데이터 사이언스 분야가 궁금하셨나요? 이제 그 궁금증을 해결할 시간입니다.\n마소캠퍼스의 강의와 함께 라면 누구나 고급 데이터 분석도 쉽게 가능합니다.\n여러분도 이제 데이터 사이언스 분야에서 높은 연봉을 목표로 할 수 있습니다.\n\n\n마소캠퍼스의 “코딩 없는 AI 데이터 분석 Lv.4 분류 머신러닝 분석” 강의를 통해\n데이터의 가치를 마음껏 활용해 보세요.\n여러분이 직접 데이터 분석가가 되어, 실제 데이터를 분석하는 경험을 할 수 있습니다.\n\n\n\n\n강의 특징\n본 강의는 분류 모델을 심도 있게 다루면서도, 코딩 지식 없이도 데이터 분석을 수행할 수 있는 구조로 설계되었습니다. 복잡한 코딩을 배우지 않고도, 누구나 쉽게 분석 모델을 구축하고 평가할 수 있도록 도와드립니다.\n\n\n1. 오렌지를 활용하여 누구나 가능한 데이터 분석!\nOrange는 데이터 분석 초보자부터 전문가까지 모두 사용할 수 있는 도구로, 복잡한 코딩 없이도 직관적인 인터페이스를 통해 분류 모델을 효과적으로 학습하고 적용할 수 있습니다.\n\n\n\n\n2. 분류 모델의 구축과 평가\n이 강의에서는 이진 분류, 서포트 벡터 머신(SVM), 랜덤 포레스트 등 다양한 분류 알고리즘의 구축 방법을 배우며, ROC 곡선과 혼동 행렬 같은 평가 지표를 통해 모델의 성능을 정밀하게 평가하는 방법을 학습합니다.\n\n\n\n\n3. 여러 분석 모델 다루기\n이 강의를 통해 여러 모델을 결합하는 앙상블 기법에 집중하며, 단일 모델로는 달성하기 어려운 예측 정밀도를 실현할 수 있습니다. 이를 통해 모델의 정확성을 극대화하는 방법을 배우며 복잡한 데이터를 분석하는 능력을 강화할 수 있습니다.\n\n\n\n\n4. 고급 데이터 분석 기법의 실전 적용\n다양한 데이터를 바탕으로 분류 모델 적용을 통해 실무에 필요한 데이터 분석 기법을 배울 수 있습니다. 이 과정에서 데이터 사이언티스트의 실질적인 역량을 강화할 수 있습니다.\n\n\n더욱 쉽고 간편한 AI 활용 데이터 분석 강의를 듣고 나면\n마소캠퍼스의 “코딩 없는 AI 데이터 분석 Lv.4 분류 머신러닝 분석” 강의는\n고급 데이터 분석 및 데이터 사이언스 분야에 관심있는 모든 분들에게 적합합니다.\n\n\n분류 모델의 기본 개념과 심화 개념 이해\n모델 평가 기법 습득\n실제 데이터셋에 모델 적용\n고급 분석 기법 마스터\n\n\n오렌지와 함께 데이터 사이언스의 전문 지식과 실무 능력을 키워 보세요.\n이제 코딩의 부담 없이 데이터 분석을 마스터할 수 있습니다!\n\n\n-\n\n\n[ 강 사 소 개 ]\n\n\n최 정 아\n現 마소캠퍼스 콘텐츠랩 이사\n연세대학교 경영학 석사\nYSCEC의 웹마스터로서 연세-게이오(日)-릿쿄(日)-푸단(中) 대학의 YKLP 사업에 초기부터 합류해 성공적으로 론칭시킨 국제 원격교육 전문가입니다. 이후 플레이포럼 편집장으로 자리를 옮겨 MAU 238만 명의 커뮤니티를 7년간 운영하면서 최대 900만 뷰를 달성한 디지털 콘텐츠를 제작했습니다. 언어학, 정보학, 경영학 학위를 소지한 다재다능한 디지털마케팅 전문가로서 데이터를 활용해 디지털 플랫폼에서 최고의 퍼포먼스를 이뤄냈습니다. 효과적인 데이터 마케팅 방법을 다룬 도서를 다수 출간하여 모두 경제경영 분야 베스트셀러에 오른 검증된 지식을 공유하고 있습니다.",
      "target_audience": [
        "데이터 기반 의사 결정이 필요한 분들",
        "복잡한 코딩 없이 고급 분석 기법을 마스터하고 싶으신 분",
        "데이터 분석 역량을 강화하고 싶은 분",
        "커리어 전환을 고려하는 IT 및 비IT 분야 종사자"
      ]
    },
    {
      "title": "Séries Temporais com Redes Neurais Artificiais em Python",
      "url": "https://www.udemy.com/course/series-temporais-com-redes-neurais-artificiais-em-python/",
      "bio": "Uso das Redes Neurais Aplicadas em Séries Temporais para Ciência de Dados, Finanças, Economia, Estatística...",
      "objectives": [
        "Séries Temporais",
        "Redes Neurais Artificiais",
        "Redes Neurais aplicadas em séries temporais",
        "Redes FeedForward (FF)",
        "Redes MLP",
        "Redes N-Beats",
        "Redes Recorrentes (RNN)",
        "Redes Recorrentes (LSTM)",
        "Redes Recorrentes (GRU)",
        "Redes Convolucionais Temporais (TCN)",
        "Redes Transformers",
        "Estatística Básica",
        "Conceitos de Python"
      ],
      "course_content": {
        "Introdução": [
          "Boas vindas e apresentação do instrutor",
          "Apresentação do curso e da plataforma de estudos"
        ],
        "Fundamentos da linguagem Python": [
          "A Linguagem Python",
          "Conhecendo o Google Colaboratory",
          "Instalação do Anaconda Python",
          "Conhecendo o Jupyter Notebook",
          "Primeiros passos",
          "Operadores Matemáticos",
          "Importações de bibliotecas e pacotes",
          "Estrutura condicional",
          "Estrutura de Repetição",
          "Listas, Tuplas e Dicionários",
          "Criação de Funções",
          "Função Lambda e função map",
          "List Comprehensions",
          "Vetores (arrays) e matrizes"
        ],
        "Fundamentos de Séries Temporais": [
          "Conceitos de séries temporais",
          "Criação de séries temporais no Python parte 1",
          "Criação de séries temporais no Python parte 2",
          "Covariância",
          "Processo estocástico",
          "Estacionaridade",
          "Testes de Estacionaridade no Python",
          "Passeio aleatório",
          "Passeio aleatório no Python",
          "Autocorrelação e Ruído Branco",
          "Autocorrelação no Python",
          "Decomposição e filtragem",
          "Decomposição no Python",
          "Transformação e Diferenciação",
          "Transformação e Diferenciação no Python",
          "Diferenciação no Python",
          "Suavização por Média móvel",
          "Média móvel no Python",
          "Métricas de desempenho"
        ],
        "Redes Neurais Artificiais – Conceitos Gerais": [
          "Neurônio Biológico e neurônio artificial.",
          "Evolução das redes neurais",
          "Perceptron",
          "Funções de ativação",
          "Aprendizagem nas redes neurais.",
          "Aprendizagem com descida do gradiente",
          "Topologias das redes neurais artificiais",
          "Algoritmo Backpropagation",
          "Definição dos parâmetros"
        ],
        "Séries Temporais com Redes Neurais Artificiais": [
          "Tipos de Redes Neurais Artificiais para Séries Temporais",
          "Redes FeedForward (FF)",
          "Redes Neurais N-BEATS (Neural Basis Expansion Analysis for Time Series)",
          "Séries Temporais com redes N-BEATS em Python",
          "Redes Neurais Recorrentes (RNN, LSTM e GRU)",
          "Séries Temporais com Redes RNN em Python",
          "Séries Temporais com Redes LSTM em Python",
          "Séries Temporais com Redes GRU em Python",
          "Redes Convolucionais Temporais (TCN)",
          "Séries Temporais com Redes Convolucionais Temporais (TCN) em Python",
          "Redes Transformers para séries temporais",
          "Transformers para séries temporais em Python"
        ],
        "Projeto de Séries Temporais com Redes Neurais Artificiais": [
          "Conhecendo o dataset",
          "Tratamento dos Dados",
          "Análise da série",
          "Projeto Pluviometria N-Beats",
          "Projeto Pluviometria RNN",
          "Projeto Pluviometria LSTM",
          "Projeto Pluviometria GRU",
          "Projeto Pluviometria TCN",
          "Projeto Pluviometria Transformers",
          "Projeto Pluviometria: Previsão"
        ],
        "Finalização do curso": [
          "Encerramento"
        ],
        "Referências Bibliográficas": [
          "Referências e links úteis gratuitos"
        ]
      },
      "requirements": [
        "Não há pré-requisitos"
      ],
      "description": "Com este curso você irá dominar as principais técnicas de análises de séries temporais utilizando os algoritmos mais atuais de Redes Neurais Artificiais, tanto teoricamente como na prática, em Python, uma das ferramentas mais poderosa e popular em análises estatísticas.\nO diferencial desse curso é que iremos obter o embasamento teórico das séries temporais e Redes Neurais Artificiais, faremos na prática todos os tratamentos e testes estatísticos necessários e aplicaremos os conhecimentos em dados reais. Não é um curso onde somente serão apresentados os comandos utilizados, tudo será explicado detalhadamente.\nPara atender a todos os alunos, sem importar a área e o nível de conhecimento, as duas primeiras seções são referentes aos fundamentos da Linguagem Python e fundamentos de Redes Neurais Artificiais.\nO curso é apresentado no sistema operacional Windows, mas usuários do Linux e Mac acompanham tranquilamente.\nTodas as aulas são explicadas passo a passo, de forma clara e objetiva. A análise de série temporal, além de ser um estudo sensacional, está cada dia mais presente no mercado de trabalho e em pesquisas científicas. Diversas áreas que trabalham com análise de dados, necessitam de análises de séries temporais com o objetivo de previsão e entendimento dos dados, e aplicar corretamente as séries temporais é fundamental para obter as melhores previsões.\nTenho certeza que a sua visão sobre séries temporais irá mudar após esse curso.",
      "target_audience": [
        "Cientista de Dados",
        "Estatístico",
        "Economista",
        "Administrador",
        "Engenheiro de Dados",
        "Pesquisador",
        "Alunos de Graduação",
        "Alunos de Pós Graduação",
        "Profissionais da Área de Finanças",
        "Profissionais de Instituições Financeiras"
      ]
    },
    {
      "title": "Modelagem multivariada com R",
      "url": "https://www.udemy.com/course/modelagem-multivariada-r/",
      "bio": "Extraia padrões informativos de diferentes bancos de dados!",
      "objectives": [
        "Análises de bancos de dados com múltiplas variáveis.",
        "Extrair e demonstrar padrões de distribuições de dados.",
        "Interpretar dados complexos em múltiplas dimensões.",
        "Apresentação gráfica de resultados."
      ],
      "course_content": {
        "Apresentação e materiais do curso": [
          "Apresentação e instruções",
          "Materiais do curso"
        ],
        "Fundamentos e pressupostos de métodos multivariados": [
          "Métodos multivariados: princípios e aplicações",
          "Definições de pressupostos multivariados",
          "Testes diagnósticos dos pressupostos"
        ],
        "Análises de correlações lineares": [
          "Coeficientes de correlação de Pearson e Spearman"
        ],
        "Comparações multivariadas de médias e variâncias": [
          "Teste T² de Hotelling: comparações de médias de matrizes",
          "MANOVA: Análise multivariada da variância (paramétrica)",
          "PERMANOVA: Análise multivariada da variância (não paramétrica)"
        ],
        "Análises de agrupamento": [
          "Análise de agrupamento hierárquica (Clustering)",
          "Análise de agrupamento não-hierárquica (K-means)"
        ],
        "Análises de ordenação irrestritas": [
          "Funções dos autovalores e autovetores em ordenações",
          "PCA (Análise de componentes principais)",
          "FA (Análise Fatorial)",
          "Análise de correspondência padrão (CA) e sem tendência (DCA)",
          "PCoA (Análise de coordenadas principais)",
          "NMDS (Escalonamento multidimensional não-métrico)",
          "ANOSIM (Análise de similaridade)",
          "Adição de variáveis suplementares"
        ],
        "Análises canônicas (ordenações restritivas)": [
          "RDA (Análise de Redundância)",
          "CCA (Análise de correspondência canônica)",
          "VP (Partição de variância)",
          "Avaliação de inflação de variância (VIF) em CCA e RDA",
          "LDA (Análise de discriminantes lineares)",
          "Seleção de variáveis"
        ],
        "Correlação entre matrizes e ordenações": [
          "Teste de Mantel",
          "Análise de Procrustes"
        ]
      },
      "requirements": [
        "É desejável conhecimento básico de R (ex: instalação, uso de pacotes). Porém, o conteúdo é acessível a todos os públicos."
      ],
      "description": "No mundo atual o volume de dados gerados nos diversos contextos e setores está cada vez maior. Para analisar esses dados e extrair informações valiosas de padrões e previsões a partir deles é necessário empregar métodos estatísticos que consideram suas múltiplas dimensões. Conjuntos de dados compostos por diversas variáveis distribuem as informações em mais do que apenas duas dimensões facilmente visíveis. Para lidar com a análise das relações entre variáveis em múltiplas dimensões, os métodos multivariados são os mais adequados, robustos e informativos para esse propósito. Independe de sua área profissional de atuação (ex: ciências naturais, engenharias, marketing, finanças, etc), você pode ter um aliado importante nas análises multivariadas para te ajudar a obter padrões a partir de múltiplos dados visando soluções de problemas e tomadas de decisão.\nNeste curso, nós usamos uma abordagem objetiva e focada em executar as análises multivariadas de dados no R ambiente de programação do R. Para isso, explicações e bases teóricas são explicadas em cima das demonstrações práticas no R. Com essa abordagem, você se tornará independente nas aplicações e interpretações dos métodos mostrados no curso nas mais diversas situações. Ao final, você terá adquirido um conjunto importante de conhecimentos analíticos que agregaram valor significativo na sua jornada, visando o seu sucesso na obtenção e permanência em bons cargos profissionais.",
      "target_audience": [
        "Pesquisadores, professores, pós-graduandos, analistas de dados, analistas de negócio, estatísticos. Empresas e profissionais em geral que desejam enriquecer o currículo."
      ]
    },
    {
      "title": "Dijital İşletmeniz İçin Potansiyel Müşterilerinizi Bulun A-Z",
      "url": "https://www.udemy.com/course/dijital-isletmeniz-icin-potansiyel-musterilerinizi-bulun-a-z/",
      "bio": "E-Mail ve Phone Scraping İle Sıfırdan Zirveye - Müşterilerinizin İletişim Bilgilerine Ulaşarak Satışlarınızı Arttırın!",
      "objectives": [
        "Dijital İşletmeniz İçin Hedef Potansiyel Müşterilerinize Ulaşmayı Öğreneceksiniz.",
        "İşbirliği Yapmak İstediğiniz Markadaki Kişilerin İletişim Bilgilerine (Telefon,E-posta) Ulaşmayı Öğrenecekseniz.",
        "Uygulamanızı Daha Fazla Kişiye Tanıtmak&Ulaştırmak İçin Hedef Kitlenizin İletişim Bilgilerini Elde Edeceksiniz.",
        "Hayalinizi Süsleyen Firmanın İletişim Bilgilerini Kolaylıkla Elde Edeceksiniz.",
        "Satışlarınızı/Web Sayfası Tıklanmalarınızı/Uygulama Ziyaretlerinizi Her Geçen Gün Arttıracaksınız.",
        "E-mail Scraping & Phone Scraping",
        "Linkedin Ağınızdaki Kişiler İle İşbirliği Yapmak İçin Ağınızdaki Kişilerin İletişim Bilgilerini Elde Edeceksiniz.",
        "Universitenizin Kampüs Etkinliklerine Konuşmacı Arıyorsunuz Alanında Uzman Kişilerin İletişim Bilgilerini Nasıl Elde Edebileceğinizi Öğreneceksiniz.",
        "Hayalinizdeki Mesleği Yapan Kişilere Kariyerle İlgili Sorunuz Varsa Alanında Uzman Kişilerin İletişim Bilgilerini Nasıl Elde Edebileceğinizi Öğreneceksiniz."
      ],
      "course_content": {
        "Giriş": [
          "Giriş"
        ],
        "Hedef Müşteri Adaylarınıza Nasıl Ulaşırsınız ?": [
          "Hedef Müşteri Adaylarınıza Nasıl Ulaşırsınız ?"
        ],
        "E- posta arama": [
          "E-Posta Yakalama Siteleri",
          "Google Üzerinden E-Posta Araması Nasıl Yapılır ?",
          "Arama sonuçlarını kaydetme"
        ],
        "Müşterilerimizin E-Posta Ve Telefon Numaralarını Kod Yazarak Nasıl Bulabiliriz ?": [
          "Müşterilerimizin e-posta bilgilerini bulalım - 1",
          "Müşterilerimizin telefon bilgilerini bulalım - 2"
        ],
        "Hiç Kod Bilmeden Program Kullanarak Müşterilerinizin Bilgilerine Ulaşın": [
          "Program Kurulumu",
          "İnstagram ve Web Sayfası Üzerinden E-posta ve telefon numaralarına ulaşın",
          "Twitter ve Web Sayfası Üzerinden E-posta ve telefon numaralarına ulaşın",
          "Toplanılan e-posta adreslerinin doğrulanması ve onaylanması",
          "Toplanan verileri dışarıya (excel,csv) aktarma"
        ]
      },
      "requirements": [
        "Temel Bilgisayar Kullanım Becerisi"
      ],
      "description": "E-mail Scraping ve Phone Number Scraping bugünlerde en çok konuşulan konulardan biri haline geldi, piyasada her zaman bir tüketici olarak işlevleriyle sınırlı kalacağınız için işlerin nasıl yapıldığını size hiçbir şey göstermeyen birçok ücretli araç var.\nAğınızdaki kişiler ile iş birliği mi yapmak istiyorsunuz ? veya Universitenizin kampüs etkinliklerine konuşmacı mı arıyorsunuz ?yoksa hayalinizdeki mesleği yapan kişilere kariyerle ilgili sorunuz mu var ?\nEğer cevabınız \"EVET\" ise bağlantılarınızın e-mail adreslerini ve telefon numaralarını öğrenmelisiniz .\nBu eğitim süresince hem kod yazarak hem kod yazmadan sizlere pazar-market araştırmalarınız için nasıl veri çekileceğini göstereceğim.\n\n\nPeki bu eğitim kapsamında hangi projeleri gerçekleştireceğiz ?\n1. Web üzerinden e-posta ve telefon numarası araması gerçekleştirerek hedef kitlemizin iletişim bilgilerine ulaşacağız.\n2. İnstagram üzerinden e-posta ve telefon numarası araması gerçekleştirerek hedef kitlemizin iletişim bilgilerine ulaşacağız.\n3. Twitter üzerinden e-posta ve telefon numarası araması gerçekleştirerek hedef kitlemizin iletişim bilgilerine ulaşacağız.\n4. Facebook üzerinden e-posta ve telefon numarası araması gerçekleştirerek hedef kitlemizin iletişim bilgilerine ulaşacağız.\n5. Telegram üzerinden e-posta ve telefon numarası araması gerçekleştirerek hedef kitlemizin iletişim bilgilerine ulaşacağız.\n6. İşletmesini/girişimini büyütmek isteyenler için iş ortağı araştırması gerçekleştireceğiz.\n\n\nBu kursu diğerlerinden farklı kılan nedir ve neden kayıt olmalısınız?\n· İlk olarak, bu en güncel kurstur. Python 3.7,Google Chrome Extension kullanacaksınız\n· Türkçe kaynak eksikliği çok fazla.Türkçe kaynak eğitim arayanlar bu kurs tam size göre :)\n· Nasıl profesyonel bir web kazıyıcı olunacağına dair adım adım ayrıntılı bir kılavuza sahip olacaksınız.\no JavaScript web sitelerini sıyırmak için BeautifulSoup nasıl kullanacağınızı öğreneceksiniz ve sizi temin ederim ki, bu kursta yapacağım gibi Web Scraping işlemini gerçekten nasıl kullanacağınızı öğreten herhangi bir öğretici bulamazsınız.\n· Udemy'den 30 gün para iade garantisi\nBu nedenle , araç setine web kazıma eklemek isteyen bir veri analisti veya yapılandırılmamış HTML web sayfalarından yapılandırılmamış verilerin nasıl çıkarılacağını öğrenmek isteyen bir başkası olun ve ardından bazı veri analizlerini uygulamak için bu verileri yapılandırılmış bir şekilde geri depolayın. o zaman bu kursa katılabilirsiniz.",
      "target_audience": [
        "Girişimini/Markasını Büyütmek İsteyenler",
        "Satışlarım Her Geçen Gün Düşüyor Diyenler",
        "Web Sitesi Üzerinden Veri Çekmek İsteyenler"
      ]
    },
    {
      "title": "פיתוח של רשתות נוירונים בפייתון בעזרת machine learning",
      "url": "https://www.udemy.com/course/practical-neural-networks-with-keras-heb/",
      "bio": "קורס ללמידה עמוקה ופיתוח של רשתות נוירונים בפייתון בעזרת הספריות tensorflow & keras- machine learning",
      "objectives": [
        "מטרת הקורס היא לבנות מודלים של רשתות נוירונים בעזרת הספריות",
        "Tensorflow + Keras",
        "בקורס נראה מהי רשת נוירונים וכיצד היא בנויה.",
        "שני הפרקים הראשונים הם קצת יותר מאתגרים משאר הקורס בהם נלמד את הרקע המתמטי העומד מאחורי רשתות נורונים, על מנת לקבל הבנה עמוקה של נושא זה.",
        "במסגרת פרקים אלו נראה כיצד לבצע פעולות נדרשות במטריצות וכן נלמד את האלגוריתמים",
        "SGD- Stochastric Gradient Descent and Backpropagation",
        "בפרקים אלו ניצור רשת נוירונים בעזרת פיתון בלבד ללא עזרה של ספריות על מנת להפנים את העיקרון של עבודת רשתות נוירונים",
        "לאחר פרקים אלו נלמד כיצד לעבוד עם הספריות ובעזרתם ליצור רשת שתזהה אותיות כתב יד.",
        "נושאים נוספים שבהם נעסוק יהיו",
        "overfitting & hyperparameters",
        "Regularization",
        "Dropout",
        "Cross entropy loss function & softmax",
        "Grid search hyperparameters",
        "לאחר מכן נראה כיצד ליצור רשתות מבוססות קונבולוצי. נלמד את כל השכבות המרכיבות אותה ונראה כיצד לזהות קטגוריות של תמונות צבעוניות",
        "נושא נוסף שנראה הוא כיצד ליצור רשת לחישוב רגרסיה שבעזרת נבצע הערכת שווי של בתים על פי פרמטרים של גודל, שכונה וכו.",
        "נלמד כיצד לטפל בקובצי קול ואיך לזהות את סוג המוזיקה של הקובץ, אם הוא קובץ רוק או קלסי לדוגמא.",
        "נלמד כיצד לבנות רשתות מורכבות אם דילוגים על שכבות ומספר כניסות שמאחדים בעזרת",
        "Functional API",
        "לבסוף נראה כיצד להשתמש ברשתות קיימות שכבר מאומנות לצורך ספציפי בעזרת השיטות של",
        "Transfer learning + feature extractions",
        "במסגרת הקורס נראה דוגמאות פרקטיות רבות שכתובות בפיתון כך שבסיום הקורס יוכלו המשתתפים להגיע לרמה שבה יוכלו לפתח בעצמם רשתות לצורך פתרון בעיות בתחום"
      ],
      "course_content": {
        "מהי רשת נוירונים וכיצד היא פועלת": [
          "מהו נוירון וכיצד הוא בנוי",
          "פונקציית אקטיבציה",
          "חישובי הרשת בעזרת מטריצות",
          "mnist database and quadratic loss function",
          "Gradient Descent Algorithm"
        ],
        "Backpropagation algorithm": [
          "הגדרת הסמנטיקה של רשת נוירונים",
          "הכרת הנוסחאות - backpropagation",
          "ניסוח האלגוריתם והבנת פעולתו",
          "כתיבת תוכנית בפיתון שממשת את האלגוריתם ללא עזרת ספריות",
          "Feed forward",
          "אימון הרשת",
          "סיכום האלגוריתמים - Backpropagation + Gradient Descent"
        ],
        "הכרת הספריות - keras + Tensorflow": [
          "התקנת - Tensorflow + Keras",
          "הכרת הספריות - keras + Tensorflow",
          "יצירה ואימון רשת נוירונים בעזרת הספריות",
          "הבנת הפרמטרים של מודל הרשת שראינו"
        ],
        "Overfitting & hyperparameters": [
          "שמירת ארכיטקטורת הרשת וכן המשקלים בקובץ",
          "טעינת רשת מקובץ ושימוש בה לצורך זיהוי - Predict",
          "overfitting",
          "הכרת הפונקציות - Cross entropy + Softmax",
          "Grid Search Hyperparameters"
        ],
        "רשתות נוירונים מבוססות ארכיטקטורת קונבולוציה": [
          "Vanishing gradient problem",
          "רשתות מבוססות ארכיטקטורת קונבולוציה",
          "מימוש מעשי של רשת קונבולוציה ופתרון בעית cifar10",
          "ניתוח הרשת שבנינו ושימוש בשיטת Data augmentation"
        ],
        "בניית רשת נוירונים לפתרון בעיות רגרסיה וכן רשתות לטיפול בקובצי קול": [
          "פתרון בעית רגרסיה לחישוב שווי של בתים",
          "הבנת המבנה של קובצי קול והכרת תרשים של MelSpectogram",
          "יצירת מל-ספקטוגראם בעזרת ספרית librosa",
          "בנית מודל לסיווג של קובצי קול על פי סוג המוזיקה"
        ],
        "Functional API, callbacks & checkpoints": [
          "functional API",
          "callbacks and checkpoints",
          "שימוש בנקודות השחזור לצורך המשך אימון או חיזוי"
        ],
        "Transfer learning": [
          "יצירת בסיס נתונים לצורך למידה",
          "בנית רשת נוירונים על בסיס VGG16",
          "שימוש ברשת לצורך חיזוי",
          "feature extraction"
        ]
      },
      "requirements": [
        "דרישות הקדם לקורס הם ידע וניסיון בשפת פיתון. אין צורך בידע נוסף.",
        "הספריות הנדרשות יילמדו בקורס עצמו"
      ],
      "description": "הקורס מלמד הן את הרקע המתמטי של רשתות נוריונים והן את ההיבטים הפרקטים של שימוש בספריות:\nבמהלך הקורס יוצגו  דוגמאות רבות של מודלים לזיהוי תמונות, זיהוי קובצי קול, הערכת שווי של בתים ושימושים נוספים של רשתות.\nיוסברו טכניקות של רשתות מבוססות ארכיטקטורת קונבולוציה, רגרסיה ושיטות רבות לאופטימיזציה.\nכמו כן יודגמו שימושים בהביטם שונים של הספריה לשמירה של הרשת, שימוש ברשת קיימת לחיזוי, אופנים שונים של בנית שכבות ועוד.\nדרישת הקדם היחידה לקורס היא ידע בתכנות בפייתון.\n\n\nSGD- Stochastic Gradient Descent\nBackpropagation\nOverfitting & Hyperparameters\nL1 & L2 Regularization\nDropout\nCross entropy loss function & softmax activation function\nCNN - Convolutional Neural Network\nCNN layers ,Conv2D, pooling, strides, padding, channels\nRegression\nmelspectogram\nData augmentation\nBatch Normalization\nFunctional API\nCheckpoints and Callbacks\nTransfer learning\nFeature extractions",
      "target_audience": [
        "סטודנטים בעלי ידע בפיתון שמתעניינים בתחם של בינה מלאכותית ורוצים ללמוד בנית מודלים של רשתות נוירונים"
      ]
    },
    {
      "title": "Intelligence artificielle : Algorithme Minimax",
      "url": "https://www.udemy.com/course/intelligence-artificielle-minimax/",
      "bio": "Implémentation de l'algorithme Minimax en Python",
      "objectives": [
        "Le principe de l'algorithme Minimax",
        "L'implémentation de l'algorithme Minimax en Python",
        "Le principe de l'algorithme Alpha-Beta pruning",
        "L'implémentation de l'algorithme Alpha-Beta pruning en Python",
        "L'introduction d'intelligence artificielle dans les jeux vidéos",
        "L'amélioration de ses connaissances en Python, par la pratique"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Algorithme Minimax",
          "Généralité : API"
        ],
        "Minimax : implémentation": [
          "Prise en main",
          "Module Minimax",
          "Module game state",
          "Tests",
          "Module UI",
          "Minimax : récupération des mouvements",
          "Finalisation"
        ],
        "Alpha–beta pruning": [
          "Alpha–beta pruning"
        ]
      },
      "requirements": [
        "Connaissances basiques en programmation"
      ],
      "description": "Dans ce cours, nous allons implémenter l'algorithme d'intelligence artificielle Minimax ainsi que sa version optimisée, l'algorithme Alpha Beta pruning.\n\n\nNous allons appliquer l'algorithme au jeu de morpion, créant ainsi une intelligence artificielle qu'il ne sera pas possible de battre. L'algorithme sera implémenté de façon générique et pourra donc être appliqué simplement à d'autres jeux.\n\n\nCe cours s'adresse aux développeurs qui aimeraient ajouter de l'intelligence artificielle dans leurs jeux, à ceux qui aimeraient implémenter l'algorithme minimax ainsi qu'aux passionnés d'intelligence artificielle. Ce cours a également l'objectif d'être un tremplin vers des cours plus avancé d'intelligence artificielle, de machine learning et deep learning.\n\n\nCe cours, enseigné en utilisant le langage de programmation Python, requiert des connaissances de base en programmation. Si vous n'avez pas les bases requises, je vous recommande de vous mettre à jour en suivant un cours accéléré de programmation (si vous le désirez, je propose sur Udemy un cours de programmation accéléré en Python).\n\n\nConcepts abordés :\nL'algorithme Minimax et son implémentation en Python\nL'algorithme Alpha Beta pruning et son implémentation en Python\nL'intelligence artificielle dans les jeux vidéos\nLa création de modules et de frameworks d'intelligence artificielle\nLe concept de fonction heuristique\n\n\nN'attendez plus avant de vous lancer dans le monde de l'intelligence artificielle!",
      "target_audience": [
        "À ceux qui veulent apprendre l'algorithme Minimax",
        "Aux développeurs qui veulent introduire de l'intelligence artificielle dans leurs jeux",
        "À ceux qui sont intéressés par l'intelligence artificielle"
      ]
    },
    {
      "title": "Apache Spark 3+ pour les débutants: la base du big data !",
      "url": "https://www.udemy.com/course/apache-spark-en-francais/",
      "bio": "Spark en Java, même principe que Scala/python. Création dataframe, RDD, sparkSQL, streaming et utilisation de databricks",
      "objectives": [
        "Comprendre l'architecture des applications Spark.",
        "Manipuler des données via des DataFrames, RDD, Datasets et vérifier vos connaissances via des exercices.",
        "Lire et écrire des données dans tout type de format ( CSV, Parquet, ORC ...).",
        "Comprendre l'IHM web SparkUI.",
        "Utiliser les actions et les transformations de base.",
        "Développer des UDFs (User Defined Functions)",
        "Utiliser des fonctions avancées de Spark (Persistance, Accumulators, ...)",
        "Développer des tests unitaires dans un contexte Spark."
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Introduction à Spark": [
          "Qu'est-ce qu'Apache Spark ?",
          "Architecture Spark"
        ],
        "Installation et configuration": [
          "Installer Spark",
          "Initialisation d'un programme Spark"
        ],
        "Concept fondamental : Les RDD": [
          "Explication du concept de RDD dans Spark.",
          "Création et chargement de RDD",
          "Transformation des RDD",
          "Actions sur les RDD",
          "Exercice",
          "Correction et Pair RDD",
          "Gestion des partitions"
        ],
        "Concept fondamental : Les Datasets et Dataframes": [
          "Introduction aux Datasets et Dataframes",
          "Création de Dataframes et Dataframes",
          "Opérations sur les Dataframes",
          "Ecriture des Dataframes",
          "Relational Grouped Dataset et Window functions",
          "Exercice",
          "Réponse exercice",
          "Typage fort et sécurité des Datasets"
        ],
        "Exercice Quiz théorie Spark et Dataframe": [
          "Questions sur la theorie et les fonctions dataframes"
        ],
        "Concept fondamental : Spark SQL": [
          "Introduction à Spark SQL",
          "Intégration avec les Datasets et les DataFrames",
          "Fonctionnalités de Spark SQL et UDF",
          "Fontions SQL et Dataframe",
          "Exercices"
        ],
        "Concept fondamental : Spark Streaming": [
          "Introduction au Structured Streaming",
          "Structured Streaming et SparkSQL",
          "Spark stream: rétablissement après une panne",
          "Regroupement sur le temps et watermark"
        ],
        "Excercice Spark Streaming": [
          "Exercice : Kafka stream",
          "Présentation Kafka (optionnel)",
          "Topic et partition (optionnel)",
          "Installer Zookeeper (optionnel)",
          "installation kafka (optionnel)",
          "Créer un topic kafka (optionnel)",
          "Publier un message dans notre topic (optionnel)",
          "Correction exercice: Kafka stream"
        ],
        "Spark UI": [
          "Présentation de Spark UI"
        ]
      },
      "requirements": [
        "Aucune expérience en Spark ou big data n'est requise."
      ],
      "description": "Bienvenue dans la formation en français de apache Spark 3.5 !\nSaviez-vous que des centaines de grandes entreprises utilisent apache Spark ? Parmi les contributeur nous retrouvons intel, Facebook, IBM, Netflix...\nSi vous souhaitez apprendre Spark, étape par étape, vous êtes dans à la bonne formation. Aucune connaissance Spark ou Scala n'est préalablement requise, nous allons voir ensemble la théorie pour ensuite la mettre en pratique.\nA la fin de la formation vous connaitrez :\nLes concepts importants : DataFrame, Distribution, Transformations, Actions, Plan d’exécution...\nComment installer et tester nos développement Spark en local.\nManipuler les DataFrame en utilisant les fonctions de base et aussi avancés.\nLire et écrire des fichiers de tout type: CSV, JSON, parquet, ORC...\nUtiliser Spark SQL.\nComprendre l'UI Spark.\nUtiliser le structured streaming (Spark streaming)\nUtiliser Spark sur Databricks\n\n\nJ'intervient moi-même dans des grandes entreprises en tant qu'expert Spark depuis plusieurs années,  cela me permet d'être critique sur les bonnes pratiques à avoir et d'enrichir la formation avec mes propres expériences.\n\n\nPS: Le cours utilise Java comme langage de programmation principal, mais n'ayez aucune inquiétude, tous les concepts abordés dans ce cours sont aussi applicables aux autres langages de programmation: Scala et Python\n\n\nVous souhaitez utiliser Kafka ou Nifi ? d'autres de mes formations en Français sont disponibles (code de réduction disponible sur mon site internet) :\nKafka : Apache Kafka pour débutant\nNifi 1.0 : Apache Nifi De A à Z - le Guide complet\nNifi 2.0 : Apache Nifi 2 : de Zéro à Héros",
      "target_audience": [
        "Développeurs intéressés par Apache Spark",
        "Data engineer",
        "Data scientist intéressés par Apache Spark",
        "Développeurs big data"
      ]
    },
    {
      "title": "【普段のエクセル作業時間をショートカット】xlwingsでエクセルとPythonを合体",
      "url": "https://www.udemy.com/course/xlwingspython/",
      "bio": "Pythonをやりたい、VBAもやりたい、そんな方に両取りの便利ツールをご紹介。エクセルVBAからPythonを操作し、 レポート作成を自動化しよう。より複雑な処理もPythonに任せて、操作が簡単なエクセルから実行する方法を解釈します。",
      "objectives": [
        "ビジネス担当者が自ら作業効率化を実現",
        "繰り返し作業をプログラミングに任せる",
        "具体的なエクセルの使用方法をPythonで置き換え",
        "自分の業務に置き換えて適用できるスキルを身につける"
      ],
      "course_content": {
        "講座の紹介": [
          "目的・対象者・レクチャー内容"
        ],
        "xlwingsの操作基本編①": [
          "初めての操作：ライブラリーのインポートからシート名の取得",
          "シートの処理",
          "シートの処理（続き）、Pandasデータの連携、セルの名称"
        ],
        "xlwingsの操作基本編：図の操作": [
          "図の挿入 その１",
          "図の挿入 その２"
        ],
        "ケーススタディーの紹介": [
          "システム図"
        ],
        "ケーススタディー１：リスク計測": [
          "開発タブ（Developerタブ）、xlwingsの設定",
          "xlwingsがエクセルタブに出ない場合のトラブルシューティング",
          "平均シミュレーションその１",
          "平均シミュレーションその２",
          "平均シミュレーションその３",
          "平均シミュレーションその４",
          "平均シミュレーションその５",
          "平均シミュレーションその６"
        ],
        "RunMainとRunPythonの比較": [
          "RunMainとRunPythonの比較"
        ],
        "ケーススタディー２：アウトルックのメール自動作成": [
          "VBAの追加設定",
          "メール送信自動化その１",
          "メール送信自動化その２",
          "メール送信自動化その３"
        ],
        "ケーススタディー３：アウトルックの予定確認": [
          "予定確認その１",
          "予定確認その２",
          "予定確認その３",
          "予定確認その４"
        ]
      },
      "requirements": [
        "エクセルの使用経験がある",
        "Pythonの使用経験がある（ただし、初心者でもOKです）",
        "VBAの使用経験がある（ただし、初心者でもOKです）"
      ],
      "description": "エクセルの集計・アウトルック操作・レポート業務をプログラミング言語Pythonで行えるようになるを目的にした2022年４月中旬で唯一のxlwingsを日本語で紹介する講座です。\n\n\n対象者：\n①エクセルで繰り返しの業務、かつ、複雑な処理をエクセルで行っているビジネス担当者\n②Pythonの基礎を普段使っているものの、説明先の担当者がエクセルを使っているため、データ主導の意思決定が浸透しにくいと感じるデータサイエンティスト\n③メールの送信を自動化したいなと思っているプログラミングが少しわかる営業担当者\n\n\n講座内容：\n１．Pythonからエクセルを操作する基本的な機能の紹介\n・エクセルブック・シート操作\n・値の入力・セルの命名\n・図の作成・挿入\n２．ケーススタディー①：正規分布を用いた平均のシミュレーション実行\n３．ケーススタディー②：アウトルックメールの作成\n４．ケーススタディー③：アウトルックの予定確認（2022年4月20日追加）\n\n\n※対象にしたい自動化業務のアイデアがあれば、Q&Aに順次投稿してください\n\n\nぜひ、ご受講の上で、業務効率化を推進していきましょう。",
      "target_audience": [
        "Python初心者で普段の業務はエクセルで行っている方",
        "データサイエンティストでビジネス担当者に結果を説明したいと思っている方",
        "アウトルックのメール送信を自動化したい方"
      ]
    },
    {
      "title": "金融业务增长秘籍之：核心指标体系的搭建与拆解",
      "url": "https://www.udemy.com/course/zxaylrnf/",
      "bio": "推动业务高效开展",
      "objectives": [
        "1.了解什么是增长及增长痛点",
        "2.掌握核心指标拆分五部曲",
        "3.了解金融业务模式",
        "4.掌握金融行业如何进行指标拆解，搭建核心指标体系"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "课程内容": [
          "1.核心指标拆分五部曲-助力业务增行",
          "2.金融业务模式介绍",
          "3.金融行业如何建立指标拆分体系"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "有一定业务数据分析经验，了解数据分析"
      ],
      "description": "【课程介绍】\nB端产品有非常大的可能都要接触到数据报表的需求。试想一下，在没有自动化报表的情况下，老板想要根据数据情况了解公司经营状况，需要等各业务负责人同步数据；业务负责人想要监控业务数据，以调整对应的运营策略，只能等待数据部门提数；而对于运营来说，如果想了解一日的运营效果以及时调整方案，则需要到不同系统导出不同环节的各项数据，再凑成数据报表进行分析。为了解决这些问题，并提高效率，自动化仪表盘的设计成为了不可或缺的一环。本节课将带你深入了解用户仪表盘、业务仪表盘及其构成要素，并教会你如何围绕核心指标，完成仪表盘的搭建。\n本节课程是由授课老师与三节课合作制作的。在此，要特别感谢老师的辛苦付出！经历了课程立项、设计、开发中的众多环节，我们才能最终为你呈现现在的这门课程。无论是授课老师还是三节课团队，都希望这门课程能够让你有所收获，希望同学们结合个人工作情况，学以致用。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "1.想要提升专业技能的0-1岁产品经理或数据PM",
        "2.正在搭建自动化报表的PM",
        "3.想要转型产品的互联网从业者"
      ]
    },
    {
      "title": "[2024년 개정] 데이터분석 전문가(ADP) 자격증 핵심정리 - 실기편",
      "url": "https://www.udemy.com/course/2024-adp-b/",
      "bio": "[강의 교안 제공] 데이터분석 전문가(ADP) 자격증 실기 시험 핵심 정리편",
      "objectives": [
        "ADP 실기 개요",
        "데이터 전처리",
        "선형회귀, 로지스틱회귀",
        "예측분석_SVM_의사결정나무",
        "분류분석_KNN_SVM_의사결정나무",
        "앙상블분석, 군직분석, 시계열 분석, 통계 분석",
        "불균형데이터처리",
        "문제풀이"
      ],
      "course_content": {
        "데이터분석 전문가(ADP) 자격증 따기 - 실기": [
          "1 ADP 실기 개요 및 전처리(1)",
          "2 데이터 전처리(2)",
          "3 데이터 전처리(3)",
          "4 선형회귀(1)",
          "5 선형회귀(2)",
          "6 예측분석_SVM_의사결정나무",
          "7 로지스틱회귀(1)",
          "8 로지스틱회귀(2)",
          "9 분류분석_KNN_SVM_의사결정나무",
          "10 앙상블분석",
          "11 불균형데이터처리",
          "12 군집분석",
          "13 시계열분석",
          "14 통계분석(1)",
          "15 통계분석(2)",
          "16 문제풀이(31회 기출)"
        ]
      },
      "requirements": [
        "누구나 수강할 수 있습니다."
      ],
      "description": "안녕하세요, ITGO 입니다.\n\n\n본 강의는 [2024년 개정] 데이터분석 전문가(ADP) 자격증 핵심정리 - 실기편입니다.\n\n\n본 과정을 통해 수강생은 데이터분석 전문가(ADP) 자격증 실기 시험에 합격하기 위한 내용을 학습할 수 있습니다.\n\n\n본 강의는 실기 시험 대비 중심으로 진행됩니다.\n\n\n\n\n누구를 위한 강의인가요?\n\n\n데이터분석전문가(ADP) 자격증을 취득하고자 하는 분\n\n\n실무 데이터 분석에 필요한 이론 및 지식을 이해하고 싶은 분\n\n\n데이터분석 관련 분야 취업을 준비하시는 분\n\n\n\n\n무엇을 배우나요?\n\n\nADP 실기 개요\n\n\n데이터 전처리\n\n\n선형회귀, 로지스틱회귀\n\n\n예측분석_SVM_의사결정나무\n\n\n분류분석_KNN_SVM_의사결정나무\n\n\n앙상블분석, 군직분석, 시계열 분석, 통계 분석\n\n\n불균형데이터처리\n\n\n문제풀이\n\n\n\n\n[2024년 개정] 데이터분석 전문가(ADP) 자격증 핵심정리 - 실기편 강의에 입문해봅시다~!\n\n\n그럼 강의에서 만나요 :)",
      "target_audience": [
        "데이터분석전문가(ADP) 자격증을 취득하고자 하는 분",
        "실무 데이터 분석에 필요한 이론 및 지식을 이해하고 싶은 분",
        "데이터분석 관련 분야 취업을 준비하시는 분"
      ]
    },
    {
      "title": "世界10万人が学んだ講師が教えるLLM開発(日本語字幕)",
      "url": "https://www.udemy.com/course/llm-engineering-master-ai-and-large-language-models-japanese/",
      "bio": "8週間で8つのアプリを開発。生成AI未経験から、大規模言語モデル、RAG、LoRA、AIエージェントをマスター。",
      "objectives": [
        "プロジェクト1：企業のウェブサイトをインテリジェントにスクレイピングし、ナビゲートするAI搭載のパンフレット生成ツールを作成します",
        "プロジェクト2：UIと関数呼び出し機能を備えた、航空会社向けのマルチモーダルなカスタマーサポートエージェントを構築します",
        "プロジェクト3：オープンソースモデルとクローズドソースモデルの両方を使用して、音声から議事録とアクションアイテムを作成するツールを開発します",
        "プロジェクト4：Pythonコードを最適化されたC++に変換し、パフォーマンスを60,000倍向上させるAIを作成します",
        "プロジェクト5：検索拡張生成（RAG）を使用して、企業関連のあらゆる事柄に関する専門家となるAIナレッジワーカーを構築します",
        "プロジェクト6：最終課題パートA – フロンティアモデルを使用して、短い説明文から製品価格を予測します",
        "プロジェクト7：最終課題パートB – 価格予測でフロンティアモデルと競合するために、ファインチューニングされたオープンソースモデルを実行します",
        "プロジェクト8：最終課題パートC – モデルと協調してお買い得品を見つけ、特別セールを通知する自律型マルチエージェントシステムを構築します",
        "検索拡張生成（RAG）、ファインチューニング、エージェントワークフローなど、LLMソリューションのパフォーマンスを向上させるための最新技術を比較対照します",
        "主要なフロンティアモデル10種とオープンソースモデル10種を比較検討し、特定のタスクに最適な選択ができるようになります"
      ],
      "course_content": {},
      "requirements": [
        "Pythonの基礎知識。このコースではPythonの基本は解説せず、すべてPythonで完結します",
        "インターネットに接続されたPC（Mac（Linux）またはWindows）が必要です",
        "フロンティアモデルを扱うために、API費用として5ドル程度の予算を確保することをお勧めします。ただし、ご希望であればオープンソースモデルを使用してコースを完了することも可能です"
      ],
      "description": "字幕の表示は、視聴画面下部の再生バーにある字幕アイコンをクリックしてください。\n\n\n生成AIとLLMをマスターする：8週間の実践的なハンズオン講座\n\n\n業界のベテランEd Donnerが指導する、実践的で現実的なプロジェクトを通じて、あなたのAIキャリアを加速させましょう。高度な生成AI製品を構築し、20以上の画期的なモデルを試し、検索拡張生成（RAG）、QLoRA、エージェントといった最先端の技術をマスターします。\n\n\nこのコースで得られること\n\n\n• 最先端のモデルとフレームワークを用いて、高度な生成AI製品を構築します。\n• フロンティアモデルやオープンソースモデルを含む、20以上の画期的なAIモデルを試します。\n• Hugging Face (ハギングフェイス)、LangChain (ラングチェーン)、Gradio (グラディオ) といったプラットフォームの習熟度を高めます。\n• 検索拡張生成（RAG）、QLoRAによるファインチューニング、エージェントといった最先端の技術を実装します。\n• 以下のような、実社会で役立つAIアプリケーションを作成します：\n• テキスト、音声、画像と対話するマルチモーダルなカスタマーサポートアシスタント。\n• 共有ドライブに基づいて、企業に関するあらゆる質問に答えられるAIナレッジワーカー。\n• ソフトウェアを最適化し、60,000倍以上のパフォーマンス向上を達成するAIプログラマー。\n• 未見の製品の価格を正確に予測するEコマースアプリケーション。\n• 推論から学習へ移行し、フロンティアモデルとオープンソースモデルの両方をファインチューニングします。\n• 洗練されたユーザーインターフェースと高度な機能を備えたAI製品を本番環境にデプロイします。\n• AIおよびLLMエンジニアリングのスキルを向上させ、業界の最前線に立ちます。\n\n講師について\n\n\n私はEd Donnerです。AIとテクノロジーの分野で20年以上の経験を持つ起業家であり、リーダーです。自身のAIスタートアップを共同設立して売却し、2社目を立ち上げ、世界中のトップクラスの金融機関やスタートアップでチームを率いてきました。この刺激的な分野に他の人々を導き、彼らが業界の最前線で専門家になる手助けをすることに情熱を注いでいます。\n\n\nプロジェクト：\nプロジェクト1：企業のウェブサイトをインテリジェントにスクレイピングし、ナビゲートするAI搭載のパンフレット生成ツール。\nプロジェクト2：UIと関数呼び出し機能を備えた、航空会社向けのマルチモーダルなカスタマーサポートエージェント。\nプロジェクト3：オープンソースモデルとクローズドソースモデルの両方を使用して、音声から議事録とアクションアイテムを作成するツール。\nプロジェクト4：Pythonコードを最適化されたC++に変換し、パフォーマンスを60,000倍向上させるAI。\nプロジェクト5：検索拡張生成（RAG）を使用して、企業関連のあらゆる事柄に関する専門家となるAIナレッジワーカー。\nプロジェクト6：最終課題パートA – フロンティアモデルを使用して、短い説明文から製品価格を予測します。\nプロジェクト7：最終課題パートB – 価格予測でフロンティアモデルと競合するために、オープンソースモデルをファインチューニングします。\nプロジェクト8：最終課題パートC – モデルと協調してお買い得品を見つけ、特別セールを通知する自律型エージェントシステム。\n\n\nこのコースが選ばれる理由\n\n\n• 実践的な学習： 最良の学習方法は、実践することです。驚くべき結果をもたらす実用的なAIアプリケーションを構築する、実践的な演習に取り組みます。\n• 最先端の技術： 検索拡張生成（RAG）、QLoRA、エージェントなど、最新のフレームワークと技術を学び、時代を先取りします。\n• 分かりやすいコンテンツ： あらゆるレベルの学習者向けに設計されています。ステップバイステップの説明、実践的な演習、チートシート、豊富なリソースが提供されます。\n• 高度な数学は不要： このコースは実践的な応用に焦点を当てています。LLMエンジニアリングをマスターするのに、微積分や線形代数は必要ありません。\n\n\nコース構成\n\n\n第1週：基礎と最初のプロジェクト\n\n\n• トランスフォーマーの基礎を深く学びます。\n• 6つの主要なフロンティアモデルを試します。\n• ウェブをスクレイピングし、意思決定を行い、フォーマットされた営業パンフレットを作成する、最初のビジネス向け生成AI製品を構築します。\n\n\n第2週：フロンティアAPIとカスタマーサービスチャットボット\n\n\n• フロンティアAPIを探求し、3つの主要なモデルと対話します。\n• テキスト、画像、音声と対話し、ツールやエージェントを活用できる、洗練されたUIを持つカスタマーサービスチャットボットを開発します。\n\n\n第3週：オープンソースモデルの活用\n\n\n• Hugging Face (ハギングフェイス) を使用して、オープンソースモデルの世界を発見します。\n• 翻訳から画像生成まで、10の一般的な生成AIのユースケースに取り組みます。\n• 録音から議事録とアクションアイテムを生成する製品を構築します。\n\n\n第4週：LLMの選定とコード生成\n\n\n• LLM間の違いと、ビジネスのタスクに最適なモデルを選択する方法を理解します。\n• LLMを使用してコードを生成し、PythonからC++へコードを翻訳してパフォーマンスを60,000倍以上向上させる製品を構築します。\n\n\n第5週：検索拡張生成（RAG）\n\n\n• 検索拡張生成（RAG）をマスターし、ソリューションの精度を向上させます。\n• ベクトル埋め込みに習熟し、人気のオープンソースのベクトルデータストアでベクトルを探求します。\n• 今日の市場にある実際の製品と同様の、完全なビジネスソリューションを構築します。\n\n\n第6週：学習への移行\n\n\n• 推論から学習へ移行します。\n• 実際のビジネス問題を解決するために、フロンティアモデルをファインチューニングします。\n• 独自の特化モデルを構築し、AIの旅における重要なマイルストーンを達成します。\n\n\n第7週：高度な学習技術\n\n\n• QLoRAによるファインチューニングのような高度な学習技術を深く学びます。\n• 特定のタスクでフロンティアモデルを上回る性能を発揮するよう、オープンソースモデルを学習させます。\n• あなたのスキルを次のレベルに引き上げる、挑戦的なプロジェクトに取り組みます。\n\n\n第8週：デプロイと最終化\n\n\n• 洗練されたUIを備えた商用製品を本番環境にデプロイします。\n• エージェントを使用して機能を強化します。\n• 最初の本番稼働可能な、エージェント化された、ファインチューニング済みのLLMモデルを完成させます。\n• AIとLLMエンジニアリングの習得を祝い、キャリアの新たな段階に備えます。",
      "target_audience": [
        "生成AIとLLMの分野への進出を目指す、意欲的なAIエンジニアやデータサイエンティスト",
        "急速に進化するAIの状況に対応し、競争力を維持するためにスキルアップを目指すプロフェッショナル",
        "実践的なハンズオン経験を通じて、高度なAIアプリケーションの構築に関心のある開発者",
        "キャリアチェンジを目指す方や、LLMで構築されたフレームワークを通じて生産性を向上させたい方"
      ]
    },
    {
      "title": "Dominando Redes Neurais Pulsadas (SNN)",
      "url": "https://www.udemy.com/course/redes-neurais-pulsadas-snn/",
      "bio": "Domine SNNs e torne um dos poucos profissionais capacitados em uma área emergente e de alta demanda.",
      "objectives": [
        "Compreender os fundamentos das Redes Neurais Pulsadas (SNNs)",
        "Aplicar conceitos de neurociência às SNNs",
        "Aprender a implementar neurônios biológicos e artificiais em projetos de IA",
        "Conhecer as ferramentas de desenvolvimento para treinar e testar SNNs"
      ],
      "course_content": {
        "Introdução às Redes Neurais Pulsadas": [
          "Tópicos que serão abordados durante o curso",
          "Visão Geral das Redes Neurais Pulsadas I",
          "Visão Geral das Redes Neurais Pulsadas II",
          "Surgimento das Redes Neurais Pulsadas"
        ],
        "Conceitos de básicos": [
          "Neurociência aplicada as SNN",
          "O cérebro humano",
          "Funcionamento dos Neurônios I",
          "Funcionamento dos Neurônios II"
        ],
        "Ferramentas de treinamento e aplicação": [
          "Formas de treinamento de SNN",
          "Ferramentas de desenvolvimento de SNN",
          "Aplicação de SNNs em mãos robóticas para detecção de toque e escorregamento"
        ],
        "Considerações Finais": [
          "NOVOS MÓDULOS",
          "Considerações Finais"
        ]
      },
      "requirements": [
        "Curiosidade e vontade de aprender sobre redes neurais e suas aplicações"
      ],
      "description": "Se você está em busca de se destacar na área de Inteligência Artificial, chegou a sua oportunidade. O curso \"Dominando Redes Neurais Pulsadas\" é o primeiro no Brasil focado em Redes Neurais Pulsadas (SNNs), uma das áreas mais promissoras da computação neuromórfica. As SNNs representam o futuro da IA, modelando o comportamento dos neurônios biológicos de maneira precisa e eficiente. Neste curso, você aprenderá os conceitos fundamentais e as técnicas avançadas para desenvolver e aplicar SNNs em projetos reais.\nVocê será guiado por um especialista no campo, que compartilhará insights valiosos e experiências práticas para acelerar o seu aprendizado. Você descobrirá como as SNNs estão revolucionando a IA, desde as bases neurocientíficas até as ferramentas mais modernas de desenvolvimento.\nAo dominar SNNs, você estará preparado para:\n- Criar sistemas que simulam o comportamento neural humano com mais precisão.\n- Desenvolver aplicações de IA mais eficientes energeticamente.\n- Se tornar um dos poucos profissionais capacitados em uma área emergente e de alta demanda.\n\n\nO curso é dividido em três módulos detalhados, começando com uma introdução completa às redes neurais pulsadas, passando por conceitos básicos como o funcionamento dos neurônios biológicos e artificiais, até as ferramentas e ambientes de trabalho necessários para treiná-las e implementá-las em projetos. Cada aula foi cuidadosamente planejada para fornecer clareza, segurança e uma compreensão profunda do conteúdo, permitindo que você se sinta confiante ao aplicar o conhecimento adquirido.\nSe você deseja estar à frente no campo da inteligência artificial, este curso é a sua melhor escolha. Inscreva-se agora e comece a dominar as redes neurais pulsadas!",
      "target_audience": [
        "Estudantes e profissionais de IA e neurociência",
        "Pesquisadores e desenvolvedores interessados em SNNs",
        "Curiosos e entusiastas por tecnologias emergentes em IA"
      ]
    },
    {
      "title": "Taller Google Earth Engine para No programadores (NDVI-NDWI)",
      "url": "https://www.udemy.com/course/taller-google-earth-engine-para-no-programadores/",
      "bio": "Análisis Satelitales de Agua y Vegetación con Landsat 8 y Sentinel 2 + Código de automatización",
      "objectives": [
        "Configurar un proyecto en Google Earth Engine para Sentinel‑2 y Landsat 8.",
        "Reducir 200 líneas de código a solo 3 clics",
        "Filtrar colecciones por fecha, nubosidad y región de interés.",
        "Corregir las imágenes satelitales para trabajar con reflectancia en superficie",
        "Visualizar dinámicamente los mapas de NDVI y NDWI en un área de estudio",
        "Exportar resultados (GeoTIFF) a Google Drive con un solo clic.",
        "Llevar tus resultados de GEE a Qgis o Arcgis"
      ],
      "course_content": {
        "Introducción": [
          "Introducción",
          "¿Qué es Google Earth Engine?",
          "Crear cuenta de Google Earth Engine"
        ],
        "Introducción a Google Earth Engine": [
          "Creación de un proyecto en Google Earth Engine",
          "Familiarizándonos con las herramientas, variables e impresión de resultados"
        ],
        "Programación de algoritmo en Landsat 8": [
          "Demostración de lo que haremos con Landsat 8",
          "1. Creación de lista de imágenes disponibles en la zona de estudio, y su gráfica",
          "2. Calcular NDVI para todas las imágenes disponibles en la región de estudio",
          "3. Clasificación de NDVI a raster discreto para todas las imágenes disponibles",
          "4. Exportación de Composición RGB, NDVI y Clasificación"
        ],
        "Análisis general del algoritmo aplicado a Sentinel 2": [
          "Funcionamiento del algoritmo con Sentinel 2"
        ],
        "Funcionamiento de Código de Automatización de NDVI y NDWI": [
          "Funcionamiento de Código de Automatización - Final"
        ]
      },
      "requirements": [
        "Nociones básicas de programación (Pero no te preocupes, tendrás acceso a un código que automatiza todo)",
        "Conocimientos básicos en SIG y PR",
        "Conocimientos elementales de índices espectrales (NDVI y NDWI)"
      ],
      "description": "En este taller aprenderás como comenzar un proyecto en Google Earth Engine. Para la explicación paso a paso del desarrollo de algoritmo nos enfocaremos en Landsat 8 y después analizaremos rápidamente cómo funciona con Sentinel 2.\n\n\nEl algoritmo que construiremos en este taller procesa los siguientes productos:\nComposición Color Natural\nNDVI\nNDVI Clasificado\n(Los cuáles podrás exportar)\nAdemás tendrás acceso a un programa de automatización que te ahorrará de 100 a 200 líneas de código. Este programa te ayudará a procesar los siguientes resultados tanto para Landsat 8 como para Sentinel 2:\nComposición Natural\nNDVI\nNDVI Clasificado\nNDWI\nNDWI Clasificado\n(Los cuales podrás exportar)\n\n\nCon los índices calculados y con la forma en la que está construido cada algoritmo, podrás realizar estudios temporales de una manera más sencilla. Ya que podrás seleccionar una región de interés con un polígono, sus fechas de estudio y podrás visualizar una tabla con las imágenes existentes que concuerden con los filtros definidos.\n\nTambién quiero que visualices el potencial de Google Earth Engine, pues con este servicio he realizado un geoportal en el que estudié por los últimos 10 años a 11,000 cuerpos de agua en México. Concluyendo así en un total de 2,500,000 de geoprocesos. Que realizando cálculos conservadores, habrían tomado más de 50 años con un método tradicional.",
      "target_audience": [
        "Profesionales en Agronomía que buscan monitorear salud de cultivos con índices espectrales.",
        "Técnicos en SIG interesados en automatizar flujos de trabajo en Google Earth Engine.",
        "Ingenieros Forestales que necesitan evaluar vigor y cobertura vegetal tras incendios o talas.",
        "Investigadores en Ciencias Ambientales que analizan cambios del cubrimiento vegetal.",
        "Planificadores Urbanos que requieren cartografiar áreas verdes y estudiar su evolución.",
        "Profesionales en Ingeniería Ambiental que buscan aplicar teledetección en proyectos.",
        "Profesionales de ONGs de Conservación para monitorear bosques y ríos con metodologías reproducibles."
      ]
    },
    {
      "title": "[FR] Cours de Certification Ingénieur Associé en IA",
      "url": "https://www.udemy.com/course/fr-cours-de-certification-ingenieur-associe-en-ia/",
      "bio": "Maîtrisez le Machine Learning, le Deep Learning et les Fondements des Agents d’IA avec TensorFlow et PyTorch",
      "objectives": [
        "Effectuer une ingénierie avancée des caractéristiques pour les modèles d'apprentissage automatique",
        "Évaluer les performances des modèles à l’aide de la précision, du rappel, du F1 et de l’AUC",
        "Appliquer des algorithmes comme les arbres de décision, forêts aléatoires et gradient boosting",
        "Comprendre les concepts de deep learning tels que l’activation et la rétropropagation",
        "Construire des réseaux neuronaux à partir de zéro avec Python",
        "Entraîner et déployer des modèles avec TensorFlow et Keras",
        "Utiliser PyTorch pour construire, optimiser et évaluer des modèles de deep learning",
        "Comprendre les fondamentaux des agents d’IA et leurs applications concrètes"
      ],
      "course_content": {
        "Introduction au cours et à l'instructeur": [
          "Ce que vous apprendrez dans le Cours de Certification d’Ingénieur Associé en IA"
        ],
        "Ingénierie des caractéristiques et évaluation des modèles": [
          "Jour 1 : Introduction à l’ingénierie des caractéristiques",
          "Jour 2 : Mise à l’échelle et normalisation des données",
          "Jour 3 : Encodage des variables catégorielles",
          "Jour 4 : Techniques de sélection de caractéristiques",
          "Jour 5 : Création et transformation des caractéristiques",
          "Jour 6 : Techniques d’évaluation des modèles",
          "Jour 7 : Validation croisée et ajustement des hyperparamètres",
          "Ressources pour les projets"
        ],
        "Algorithmes avancés d’apprentissage automatique": [
          "Jour 1 : Introduction à l’apprentissage par ensemble (ensemble learning)",
          "Jour 2 : Bagging et forêts aléatoires",
          "Jour 3 : Boosting et gradient boosting",
          "Jour 4 : Introduction à XGBoost",
          "Jour 5 : LightGBM et CatBoost",
          "Jour 6 : Gestion des données déséquilibrées",
          "Jour 7 : Projet d’apprentissage par ensemble – Comparaison de modèles sur un jeu",
          "Ressources pour les projets"
        ],
        "Réseaux neuronaux et fondamentaux du deep learning": [
          "Jour 1 : Introduction au deep learning et aux réseaux neuronaux",
          "Jour 2 : Propagation avant et fonctions d’activation",
          "Jour 3 : Fonctions de perte et rétropropagation",
          "Jour 4 : Descente de gradient et techniques d’optimisation",
          "Jour 5 : Construction de réseaux neuronaux avec TensorFlow et Keras",
          "Jour 6 : Construction de réseaux neuronaux avec PyTorch",
          "Jour 7 : Projet de réseau neuronal – Classification d’images sur CIFAR-10",
          "Ressources pour les projets"
        ],
        "Algorithmes d’apprentissage automatique et implémentations": [
          "Introduction aux algorithmes de machine learning",
          "Implémentation de la régression linéaire en Python",
          "Implémentation de la régression Ridge et Lasso en Python",
          "Implémentation de la régression polynomiale en Python",
          "Implémentation de la régression logistique en Python",
          "Implémentation des K plus proches voisins (KNN) en Python",
          "Implémentation des machines à vecteurs de support (SVM) en Python",
          "Implémentation des arbres de décision en Python",
          "Implémentation des forêts aléatoires en Python",
          "Implémentation du gradient boosting en Python",
          "Implémentation de Naive Bayes en Python",
          "Implémentation du clustering K-Means en Python",
          "Implémentation du clustering hiérarchique en Python",
          "Implémentation de DBSCAN (clustering basé sur la densité) en Python",
          "Implémentation des modèles de mélanges gaussiens (GMM) en Python",
          "Implémentation de l’analyse en composantes principales (PCA) en Python",
          "Implémentation du t-SNE (embedding stochastique t-distribué) en Python",
          "Implémentation des autoencodeurs en Python",
          "Implémentation de l’auto-apprentissage (self-training) en Python",
          "Implémentation du Q-learning en Python",
          "Implémentation des réseaux Q profonds (DQN) en Python",
          "Implémentation des méthodes de gradient de politique en Python",
          "Implémentation du SVM à une classe en Python",
          "Implémentation des forêts d’isolement en Python",
          "Implémentation des réseaux neuronaux convolutifs (CNN) en Python",
          "Implémentation des réseaux neuronaux récurrents (RNN) en Python",
          "Implémentation de LSTM (mémoire à long et court terme) en Python",
          "Implémentation des transformeurs en Python"
        ],
        "Introduction au Machine Learning et à TensorFlow": [
          "Qu’est-ce que le Machine Learning ?",
          "Introduction à TensorFlow",
          "TensorFlow vs. autres frameworks de machine learning",
          "Installation de TensorFlow",
          "Configuration de l’environnement de développement",
          "Vérification de l’installation",
          "Introduction aux tenseurs",
          "Opérations sur les tenseurs",
          "Constantes, variables et placeholders",
          "Graphe computationnel de TensorFlow",
          "Création et exécution d’une session TensorFlow",
          "Gestion des graphes et des sessions",
          "Création d’un réseau neuronal simple feedforward",
          "Fonctions d’activation",
          "Fonctions de perte et optimiseurs",
          "Introduction à l’API Keras",
          "Création de modèles complexes avec Keras",
          "Entraînement et évaluation de modèles",
          "Introduction aux CNNs",
          "Création et entraînement de CNNs avec TensorFlow",
          "Apprentissage par transfert avec CNNs préentraînés",
          "Introduction aux RNNs",
          "Création et entraînement de RNNs avec TensorFlow",
          "Applications des RNNs : modélisation de langage, prévision de séries temporelles",
          "Sauvegarde et chargement des modèles",
          "TensorFlow Serving pour le déploiement de modèles",
          "TensorFlow Lite pour appareils mobiles et embarqués",
          "Introduction au calcul distribué avec TensorFlow",
          "Cadre d’exécution distribué de TensorFlow",
          "Mise à l’échelle avec TensorFlow Serving et Kubernetes",
          "Introduction à TFX",
          "Création de pipelines ML de bout en bout avec TFX",
          "Validation, transformation et service des modèles avec TFX",
          "Classification d’images",
          "Traitement du langage naturel",
          "Systèmes de recommandation",
          "Détection d’objets",
          "Création d’un modèle d’analyse de sentiment",
          "Création d’un système de reconnaissance d’images",
          "Développement d’un modèle de prévision de séries temporelles",
          "Implémentation d’un chatbot",
          "Réseaux antagonistes génératifs (GANs)",
          "Apprentissage par renforcement avec TensorFlow",
          "Machine Learning quantique avec TensorFlow Quantum",
          "Documentation et tutoriels TensorFlow",
          "Cours en ligne et livres",
          "Communauté et forums TensorFlow",
          "Résumé des concepts clés",
          "Prochaines étapes dans votre parcours TensorFlow"
        ],
        "Introduction à l’apprentissage avec PyTorch": [
          "Introduction à PyTorch",
          "Premiers pas avec PyTorch",
          "Manipulation de tenseurs",
          "Autograd et graphes dynamiques",
          "Création de réseaux neuronaux simples",
          "Chargement et prétraitement des données",
          "Évaluation et validation des modèles",
          "Architectures avancées de réseaux neuronaux",
          "Apprentissage par transfert et ajustement fin",
          "Gestion de données complexes",
          "Déploiement et mise en production de modèles",
          "Débogage et résolution de problèmes",
          "Entraînement distribué et optimisation des performances",
          "Couches et fonctions de perte personnalisées",
          "Techniques orientées recherche",
          "Intégration avec d’autres bibliothèques",
          "Contribution à PyTorch et participation communautaire"
        ],
        "Agents d’IA pour les débutants": [
          "1.1 : Comprendre les agents d’IA – Comment fonctionnent-ils",
          "1.2 : Introduction aux agents d’IA",
          "1.3 : Types d’agents d’IA",
          "2.1 : Technologies derrière les agents d’IA – Apprentissage automatique et agent",
          "2.2 : Traitement du langage naturel dans les agents d’IA",
          "2.3 : Agents d’IA en robotique",
          "3.1 : Frameworks et architectures pour agents d’IA",
          "3.2 : Aperçu d’AutoGPT pour les agents d’IA",
          "3.3 : Framework IBM Bee pour les agents d’IA",
          "3.4 : LangGraph pour agents d’IA à état",
          "3.5 : CrewAI pour agents d’IA collaboratifs",
          "4.1 : Applications des agents d’IA – Agents dans les opérations commerciales",
          "4.2 : Agents d’IA dans la santé",
          "4.3 : Agents d’IA dans la finance",
          "4.4 : Agents d’IA dans le divertissement",
          "4.5 : Agents d’IA dans les maisons intelligentes et l’IoT",
          "5.1 : Tendances futures et implications éthiques – L’avenir des agents d’IA",
          "5.2 : Éthique dans le développement des agents d’IA",
          "5.3 : Enjeux juridiques et réglementaires pour les agents d’IA",
          "6.1 : Impact sociétal et économique des agents d’IA",
          "6.2 : Collaboration humain–agent d’IA",
          "6.3 : Rôle des agents d’IA dans la recherche scientifique",
          "6.4 : Agents d’IA pour la sécurité publique et la défense nationale"
        ],
        "Félicitations": [
          "Quiz final à choix multiples",
          "Félicitations et bonne chance !"
        ]
      },
      "requirements": [
        "Connaissances de base en programmation Python, incluant fonctions, boucles et structures de données",
        "Avoir suivi une introduction à la science des données ou un cours débutant en IA",
        "Familiarité avec les concepts mathématiques de base tels que l’algèbre, les fonctions et les vecteurs",
        "Une certaine compréhension des probabilités et des statistiques est utile mais pas obligatoire",
        "Un ordinateur (Windows, macOS ou Linux) avec un accès Internet stable",
        "Capacité à installer et utiliser des outils comme Jupyter Notebook, TensorFlow et PyTorch (des instructions d'installation sont fournies)",
        "Curiosité et motivation pour explorer des concepts d’IA de niveau intermédiaire à avancé",
        "Volonté de s'engager dans la programmation pratique et l'apprentissage par la pratique"
      ],
      "description": "Faites passer vos compétences en IA au niveau supérieur avec le Cours de Certification d’Ingénieur Associé en IA — un programme pratique de niveau intermédiaire conçu pour vous aider à acquérir une expertise concrète en apprentissage automatique, deep learning et développement d’agents d’IA. Que vous soyez un ingénieur IA en devenir, un praticien en science des données ou un développeur souhaitant monter en compétences, ce cours vous offre une base solide en techniques avancées d’IA et en outils très demandés comme TensorFlow et PyTorch.\nNous commençons par l’Ingénierie des Caractéristiques et l’Évaluation des Modèles, où vous apprendrez à préparer les données, à extraire des caractéristiques pertinentes et à évaluer la performance des modèles à l’aide de métriques comme la précision, le rappel, le F1 score et le ROC-AUC. Ces compétences sont essentielles pour créer des modèles fiables et prêts pour la production.\nEnsuite, nous abordons les Algorithmes Avancés de Machine Learning, avec des implémentations concrètes des arbres de décision, forêts aléatoires, gradient boosting, XGBoost et apprentissage par ensemble. Vous apprendrez quand et comment utiliser chaque algorithme selon les types de données et les cas d’usage.\nNous plongeons ensuite dans les Fondamentaux des Réseaux Neuronaux et du Deep Learning, pour comprendre clairement les perceptrons, fonctions d’activation, rétropropagation et architectures de réseaux. Cette section pose les bases nécessaires pour créer vos propres modèles de deep learning.\nDans la section Algorithmes et Implémentations ML, vous programmerez différents algorithmes à partir de zéro. Vous consoliderez vos connaissances théoriques et pratiques tout en renforçant vos compétences en Python et en raisonnement mathématique.\nNous explorons ensuite le Machine Learning avec TensorFlow, où vous construirez, entraînerez et évaluerez des modèles avec l’un des frameworks de deep learning les plus utilisés. Vous apprendrez à créer des modèles Keras, manipuler les tenseurs et concevoir des boucles d’entraînement personnalisées, indispensables pour des solutions IA évolutives.\nPuis, place à l’Apprentissage avec PyTorch, où vous découvrirez comment utiliser ce framework puissant et flexible pour implémenter des modèles allant de la régression logistique aux réseaux de neurones convolutifs (CNN). Vous comprendrez le fonctionnement d’autograd, les optimisateurs et la formation de modèles dans un environnement modulaire adapté à la recherche.\nEnfin, nous introduisons Les Agents d’IA pour les Nuls, une section accessible mais puissante sur les agents autonomes et les architectures basées sur les agents. Vous découvrirez leur rôle dans la prise de décision, la planification et l’automatisation des tâches, avec des cas d’usage concrets comme les chatbots, les systèmes de recommandation et la coordination multi-agents.\nÀ la fin du cours, vous serez capable de :\nConstruire et déployer des modèles ML avancés\nComprendre les mathématiques et le code des réseaux neuronaux\nUtiliser TensorFlow et PyTorch avec assurance\nTravailler avec les concepts et applications des agents d’IA\nVous préparer à des rôles spécialisés ou à d’autres certifications en IA\nQue vous visiez un poste d’ingénieur en machine learning, de développeur IA, ou que vous souhaitiez simplement approfondir vos connaissances, ce cours vous donne tous les outils pour réussir.\nRejoignez des milliers d’apprenants et obtenez dès aujourd’hui votre Certificat d’Ingénieur Associé en IA — votre prochaine étape vers une carrière complète en intelligence artificielle !",
      "target_audience": [
        "Ingénieurs en IA en devenir souhaitant dépasser les concepts fondamentaux",
        "Développeurs logiciels cherchant à se reconvertir dans l’IA ou à renforcer leurs compétences en apprentissage automatique",
        "Analystes de données ou data scientists débutants visant à se spécialiser dans des solutions basées sur l’IA",
        "Étudiants en informatique ou disciplines connexes souhaitant acquérir une expérience pratique avec des frameworks ML concrets",
        "Professionnels de la tech souhaitant constituer un portfolio de projets IA de niveau intermédiaire",
        "Chefs de produit et responsables techniques ayant besoin d’une compréhension approfondie de l’entraînement, l’évaluation et le déploiement des modèles d’IA"
      ]
    },
    {
      "title": "Credit Risk Modelling in Python",
      "url": "https://www.udemy.com/course/credit-risk-modelling-in-python-2023/",
      "bio": "Step by step analysis needed for building credit risk model in python and machine learning",
      "objectives": [
        "Complete understanding of each and every methodology used to develop a credit risk model in Hindi language",
        "Advance feature selection techniques",
        "Complete understanding of python and library as pandas and numpy",
        "Building Advance Machine learning model",
        "Understanding of Explainable AI and model interpretability",
        "Identify the risky and good customers from credit risk scorecard",
        "Risk strategy development from credit risk model"
      ],
      "course_content": {
        "Introduction": [
          "Welcome",
          "Agenda",
          "Introduction to Credit Risk Model",
          "Introduction to bureau report"
        ],
        "Exploratory Data Analysis": [
          "Feature description",
          "Target Variable Creation",
          "Exploratory analysis for categorical features",
          "Categorical Data Processing",
          "Exploratory data analysis for numerical features"
        ],
        "Feature Selection Methods": [
          "Weight of Evidence and Information Values",
          "Characteristic Stability Index",
          "Correlation"
        ],
        "Model fitting": [
          "Logistic Regression",
          "KS",
          "PSI",
          "XGBoost"
        ],
        "Model Interpretability": [
          "Shapely Values and Partial Dependency plots"
        ],
        "ScoreCard Development": [
          "Scorecard"
        ],
        "Risk Strategy": [
          "Risk Strategy"
        ]
      },
      "requirements": [
        "No programming experience needed, everything will be expained properly"
      ],
      "description": "Are you interested in learning how to build effective credit risk models using Python and machine learning? Look no further! Our comprehensive online course is designed to take you step-by-step through the entire process, providing you with the skills and knowledge you need to develop powerful credit risk models that can be applied to real-world situations.\nThis course is conducted entirely in Hindi, making it accessible to a wider audience of learners. Our experienced instructors will guide you through the entire process, providing you with in-depth instruction on the core concepts of credit risk modeling, as well as practical tips and advice on how to apply your knowledge to real-world problems.\nYou'll learn how to use Python and machine learning algorithms to analyze credit risk data, create predictive models, and interpret the results. We'll cover everything from data pre-processing and feature engineering to model selection and performance evaluation, ensuring that you have a solid understanding of the entire credit risk modeling process.\nWhether you're an experienced data scientist or a novice just starting out, this course is designed to provide you with the skills and knowledge you need to take your career to the next level. So why wait? Sign up for our course today and start your journey towards becoming a credit risk modeling expert!",
      "target_audience": [
        "Credit risk modelling using machine learning in python",
        "Beginner data scientists to enter into fintech domain"
      ]
    },
    {
      "title": "Машинное обучение: классификация и ансамбли на Python",
      "url": "https://www.udemy.com/course/ittensive-python-machine-learning-classification/",
      "bio": "Выигрываем соревнование Kaggle с kNN, SVM, логистической регрессией, случайным лесом, XGBoost, CatBoost и LightGBM",
      "objectives": [
        "EDA: исследовательский анализ данных",
        "Точность, полнота, F1 и каппа метрики",
        "Простая классификация данных",
        "Логистическая регрессия: простая и многоуровневая",
        "Метод ближайших соседей: kNN",
        "Наивный Байес",
        "Метод опорных векторов: SVM",
        "Решающие деревья м случайный лес",
        "XGBoost и градиентный бустинг",
        "CatBoost и LightGBM",
        "Ансамбль голосования и стекинга"
      ],
      "course_content": {
        "Часть 1. Процесс машинного обучения": [
          "Задачи машинного обучения",
          "Задачи машинного обучения",
          "Модель и процесс машинного обучения",
          "Что такое ETL",
          "Процесс машинного обучения",
          "Что такое EDA",
          "Подготовка данных",
          "Подготовка данных",
          "Разбиение выборки",
          "Оптимизация гиперпараметров",
          "Недообучение и переобучение",
          "Смещение, разброс и ошибка данных",
          "Обучение модели",
          "Использование HDF"
        ],
        "Линейные метрики и модели": [
          "Метод максимального правдоподобия",
          "Метод наименьших квадратов",
          "Метод наименьших квадратов",
          "Аппроксимация пропусков в данных",
          "Аппроксимация данных",
          "Среднеквадратичная ошибка",
          "Метрики и расстояния",
          "Метрики и расстояния",
          "Линейная регрессия и L1/L2-регуляризация",
          "Линейная регрессия",
          "BIC и AIC"
        ],
        "Метрики и модели классификации": [
          "Точность и полнота",
          "F-мера",
          "ROC AUC и Gini",
          "Оценка Каппа Коэна",
          "Взвешенная квадратичная Каппа",
          "Логистическая функция потерь",
          "Метод ближайших соседей"
        ],
        "Часть 2. Практикум: Задача страхового скоринга": [
          "Страховой скоринг",
          "F1 и Каппа оценки классификации",
          "Метод ближайших соседей",
          "kNN скоринг"
        ],
        "Простые модели классификации": [
          "Наивный Байес",
          "Логистическая регрессия",
          "Дерево принятия решения",
          "Опорные векторы"
        ],
        "Практикум: Логистическая регрессия и опорные векторы": [
          "Обработка данных и оптимизация памяти",
          "Логистическая регрессия",
          "Иерархия логистической регрессии",
          "SVM: метод опорных векторов",
          "Сравнение классификации"
        ],
        "Ансамблевые модели": [
          "Ансамблевые модели",
          "Бутстрэп",
          "Бэггинг",
          "Случайный лес",
          "Out-of-Bag",
          "Сверхслучайные деревья",
          "Адаптивный бустинг",
          "LogitBoost, BrownBoost и L2Boost",
          "Градиентный спуск",
          "Градиентный бустинг и XGBoost",
          "Стохастический градиентный бустинг"
        ],
        "Практикум: Решающие деревья и ансамбли бэггинга и бустинга": [
          "Решающие деревья",
          "Случайный лес",
          "Бустинг с XGBoost",
          "Градиентный бустинг"
        ],
        "Продвинутые ансамбли": [
          "LightGBM",
          "CatBoost",
          "Ансамбль стекинга"
        ],
        "Практикум: Ансамбль стекинга и финальное решение": [
          "LightGBM",
          "CatBoost",
          "Ансамбль классификации",
          "Расчет результатов",
          "Финальное решение"
        ]
      },
      "requirements": [
        "Продвинутый Python",
        "Основы математической статистики"
      ],
      "description": "Мы разберем фундаментальные и прикладные подходы к классификации данных с помощью машинного обучения для страхового скоринга Prudential в соревновании на Kaggle вплоть до формирования конечного результата с помощью ансамбля стекинга.\nКурс разбит на 2 части. В первой части мы последовательно пройдем все этапы работы с данными: от видов задач и их постановки до работы с моделями машинного обучения для минимизации предсказательной ошибки. Дополнительно рассмотрим фундаментальные основы построения моделей машинного обучения, базовые метрики и наиболее простые модели - линейную и логистическую регрессии. А также метрики, модели и ансамбли классификации.\nВо второй части на практике разберем:\nПроведение исследовательского анализа данных для поиска зависимостей: EDA.\nМетрики классификации: точность, полнота, F1, квадратичная каппа и матрица неточностей.\nОчистка данных и оптимизация потребления памяти.\nКластеризация данных и метод ближайших соседей.\nПростая и иерархическая логистическая регрессия.\nМетод ближайших соседей и поиск оптимальной модели.\nМетод опорных векторов: SVM.\nДерево принятия решения и случайный лес (бэггинг).\nXGBosot и градиентный бустинг.\nLightGBM и CatBoost\nАнсамбль стекинга для голосования и выбора лучшего результата.\nВыгрузка результата для соревнования на Kaggle.",
      "target_audience": [
        "Аналитики Python, изучающие машинное обучение",
        "Программисты больших данных",
        "Исследователи больших данных"
      ]
    },
    {
      "title": "Data Science - Data Science et Machine Learning pour TOUS",
      "url": "https://www.udemy.com/course/data-science-data-science-et-machine-learning-pour-tous/",
      "bio": "Science des données & Apprentissage automatique- Machine Learning, régression, classification, etc. [THÉORIE UNIQUEMENT]",
      "objectives": [
        "• Relier les concepts, principes et théories de la science des données et de l'apprentissage automatique.",
        "• Comprendre la méthodologie de la science des données et de l'apprentissage automatique.",
        "• Connaître les avantages et les inconvénients des différents algorithmes de l’apprentissage automatique.",
        "• Savoir quel algorithme d’apprentissage automatique choisir en fonction du problème à résoudre."
      ],
      "course_content": {
        "Introduction et les bases": [
          "Introduction",
          "Pourquoi le Data Science",
          "Data Science et Machine Learning",
          "Variables et Machine Learning"
        ],
        "Les algorithmes de Machine Learning": [
          "La logique derrière le Machine Learning",
          "Différence entre Algorithme et Modele",
          "Apprentissage supervisé et non-supervisé",
          "Les différents algorithmes et leur utilisation",
          "Différence entre clustering et Classification",
          "Paramètre et hyper-paramètre"
        ],
        "Apprentissage supervisé": [
          "Focus sur l'apprentissage supervisé",
          "Regression Linéaire",
          "Regression lineaire multiple et polynomiale",
          "Support Vecteur Machine",
          "Arbre de décision Partie 1",
          "Arbre de decision Partie 2",
          "Arbre de decision Paramètres",
          "Arbre de decision Avantages et inconvénients",
          "Ensemble Learning Introduction",
          "Bagging et Forêt aléatoire",
          "Bagging Avantages et inconvénients",
          "Boosting",
          "Boosting avantage et inconvénients",
          "Stacking",
          "K plus proches voisins (kNN)"
        ],
        "Apprentissage non-supervisé": [
          "Clustering",
          "K-means Clustering",
          "Système de recommandation"
        ],
        "Introduction aux réseaux de neurone et au Deep Learning": [
          "Réseaux de neurones et Deep Learning les bases et les définitions",
          "La méthode du Gradient Descent"
        ],
        "Performance des modèles de Machine Learning": [
          "Conclusion"
        ]
      },
      "requirements": [
        "• Aucune connaissance préalable n'est nécessaire. Vous commencerez par les concepts de base et développerez progressivement vos connaissances sur le sujet.",
        "• Enthousiasme et volonté d'apprendre et de pratiquer."
      ],
      "description": "Science des Données et Apprentissage Automatique : Compréhension Théorique Approfondie\n\n\nLa science des données (Data Science) est un domaine vaste et fascinant, tandis que l'apprentissage automatique (Machine Learning) est une branche passionnante de la Data Science. Ce cours de deux heures offre une exploration détaillée de ces domaines pour ceux qui souhaitent comprendre leur fonctionnement.\n\n\nCe cours se distingue par son approche visuelle et simplifiée, qui démystifie les concepts et algorithmes de l'apprentissage automatique sans se perdre dans les détails mathématiques. Il se concentre sur la théorie, offrant une base solide pour quiconque souhaite exceller dans le domaine de la science des données.\n\n\nLes sections de ce cours sont interconnectées et progressives, formant un ensemble cohérent qui facilite l'apprentissage. Chaque section se construit sur les précédentes, vous permettant d'explorer des concepts de plus en plus avancés au fur et à mesure de votre progression.\n\n\nCe cours aborde les compétences les plus recherchées dans le monde réel de la science des données et de l'apprentissage automatique. Il est conçu pour être simple, facile à comprendre, et descriptif, vous permettant de progresser rapidement.\nRejoignez ce cours pour démystifier la science des données et l'apprentissage automatique. C'est une opportunité unique d'acquérir des connaissances solides dans un format accessible et inspirant !\n\n\nContenu du cours :\nAprès avoir suivi ce cours avec succès, vous serez en mesure de :\nComprendre les concepts, principes et théories de la science des données et de l'apprentissage automatique\nAppréhender la méthodologie de la science des données et de l'apprentissage automatique\nÉvaluer les avantages et les inconvénients des différents algorithmes d'apprentissage automatique\nSélectionner l'algorithme d'apprentissage automatique approprié en fonction du problème à résoudre\n\n\nCe cours s'adresse à :\nCeux qui souhaitent explorer la science des données et l'apprentissage automatique avec des données du monde réel\nLes professionnels d'autres domaines désireux de comprendre ces domaines\nLes passionnés de Machine Learning et de Data Science\nCeux qui veulent apprendre la science des données et l'apprentissage automatique, avec une perspective de mise en œuvre dans des projets concrets\nAnalystes commerciaux et autres curieux\n\n\nDécouvrez les Fondements de la Science des Données et de l'Apprentissage Automatique ! Inscrivez-vous dès maintenant pour acquérir une compréhension théorique approfondie. Rejoignez notre cours et commencez votre voyage passionnant dans le monde de la Data Science et du Machine Learning !",
      "target_audience": [
        "• Les personnes qui veulent apprendre la science des données et l'apprentissage automatique avec des ensembles de données réelles en science des données.",
        "• Les personnes issues d’autres domaines qui souhaitent comprendre la science des données (Data Science) et Machine Learning.",
        "• Les personnes qui sont passionnées par le Machine Learning et le Data Science.",
        "• Les personnes qui souhaitent apprendre la science des données et l'apprentissage automatique, ainsi que leur mise en œuvre dans des projets réalistes.",
        "• Analystes commerciaux.",
        "• Tout autre personne."
      ]
    },
    {
      "title": "AutoViMLによる分類モデル/回帰モデル作成講座:【AutoML/Python/SIGNATE】",
      "url": "https://www.udemy.com/course/autoviml-rc-learningpythonsignate/",
      "bio": "Learn how to create Supervised Regression model with AutoViML(AutoML) & Participate in SIGNATE",
      "objectives": [
        "AutoViML(AutoML)を使用した回帰モデルの作成",
        "AutoViML(AutoML)を使用した分類モデルの作成",
        "Argumentsの設定方法",
        "Signateに向けたモデルの作成"
      ],
      "course_content": {
        "はじめに": [
          "講座の概要",
          "AutoViMLとは",
          "AutoViMLのインストール"
        ],
        "回帰【Tips】": [
          "データセットの取得",
          "Argumentsの学習",
          "Auto_VIMLの実行",
          "Drill"
        ],
        "回帰【Diamonds】": [
          "Test",
          "Answer"
        ],
        "二値分類【Titanic】": [
          "データセットの取得",
          "二値分類モデルの作成"
        ],
        "マルチクラス分類【Iris】": [
          "Test",
          "Answer"
        ],
        "Challenge for SIGNATE 【山火事の消失面積予測】": [
          "コンペの参加",
          "データの取得",
          "モデルの作成と予測結果の提出"
        ],
        "Challenge for SIGNATE 【毒キノコの分類】": [
          "データの取得",
          "モデルの作成と予測結果の提出"
        ]
      },
      "requirements": [
        "Jupyter NotebookによるPythonの基本操作",
        "データセットの読込、データ前処理、モデル作成、モデルの評価の経験",
        "AutoViMLの使用が未経験な方、又は経験が浅い方"
      ],
      "description": "本講座はAutomated Machine Learning Tool（自動機械学習モデル）であるAutoViMLパッケージを使用し、公式ドキュメントを参考にしながら AutoViMLのArguments、そして回帰/分類モデルの作成、やモデルの予測精度向上のコツを学習していくコースとなっています。\nまた、講義の中では分析コンペティション（SIGANTE）にも参加し、機械学習モデル自動化ツールがどれほどの予測精度を出すか確認できます。\n手軽で実用的なツールなため、機械学習に苦手意識を持っていた方でもお勧めです。\n\n\nコース内容は以下の通りです。\nSection1:はじめに\nSection2:回帰【Tips】\nSection3:回帰【Diamonds】\nSection4:二値分類【Titanic】\nSection5:マルチクラス分類【Iris】\nSection6:Challenge for SIGNATE 【山火事の消失面積予測】\nSection7:Challenge for SIGNATE 【毒キノコの分類】",
      "target_audience": [
        "AutoViMLパッケージの使用に関心を持つ方",
        "AutoViMLを使用し回帰・分類モデルの作成を行いたい方",
        "機械学習をツールとして使いこなしたい方",
        "AutoViMLに興味があるけど、始め方が分からない方",
        "AutoMLで何らかの問題を解決したい方",
        "AIコンペの参加に関心がある方"
      ]
    },
    {
      "title": "Data Scientist(데이터 사이언티스트) 실무 과정- Part.2 데이터 분석",
      "url": "https://www.udemy.com/course/data-scientist-2022-2/",
      "bio": "[강의 교안 제공] 현직 데이터사이언티스트 전문가가 전하는 빅데이터 개발 및 분석 실무 노하우 완결편!",
      "objectives": [
        "비지니스 애널리틱스",
        "데이터 마이닝 프로세스 및 핵심 아이디어",
        "웨스트 록스베리 지역 주택 가치 예측",
        "모델 구축 : 선형 회귀 분석을 이용한 예제"
      ],
      "course_content": {
        "Part.4 데이터마이닝기반 빅데이터분석 및 시각화 1": [
          "비지니스 애널리틱스",
          "데이터 마이닝 프로세스 및 핵심 아이디어",
          "웨스트 록스베리 지역 주택 가치 예측",
          "데이터 분할의 사용과 방법",
          "모델 구축 : 선형 회귀 분석을 이용한 예제",
          "막대차트, 선 그래프, 산점도",
          "박스플롯,히스토그램",
          "히트맵, 다차원 시각화",
          "스케일 조절, 집계와 계층 구조, 확대축소, 필터링",
          "네트워크 데이터 시각화",
          "주성분 분석",
          "피벗 테이블",
          "PCA"
        ],
        "Part.4 데이터마이닝기반 빅데이터분석 및 시각화 2": [
          "예측 성능의 평가",
          "분류 모형 평가",
          "향상 차트(Lift Chart)",
          "데이터에 맞는 적합한 머신러닝 알고리즘 기법 선정",
          "분류 목적의 머신러닝 기법 적용",
          "knn",
          "나이브 베이즈(Naive Bayes) 기법",
          "분류 회귀 나무",
          "실전 데이터 분석(전처리)",
          "실전 데이터 분석(추정)",
          "실전 데이터 분석(일원배치분산분석)"
        ],
        "Part.5 인공지능기반 데이터분석 1": [
          "기계 학습 개요",
          "다차원 특징 공간, 간단한 기계학습 예제",
          "영상처리 개요",
          "머신러닝 시스템의 종류",
          "머신 러닝의 주요 도전 과제",
          "회귀 기반 머신러닝 기초",
          "회귀 기반 머신러닝 기초 데이터 이해를 위한 탐색과 시각화",
          "머신러닝을 위한 회귀 개념 및 수학 기초",
          "머신러닝을 위한 기초",
          "수치 예측 머신러닝 시각화",
          "기계학습을 위한 회귀 최종정리와 비정형 데이터마이닝 실전",
          "확률과 베이즈통계학 정리"
        ],
        "Part.5 인공지능기반 데이터분석 2": [
          "로지스틱 회귀",
          "KNN",
          "모델 진단(rmse)",
          "CI",
          "의사 결정 나무",
          "로지스틱 회귀",
          "분류모델 평가1",
          "분류모델 평가2",
          "분류모델 평가3",
          "텐서플로 설치",
          "다층퍼셉트론",
          "활성화 함수",
          "openCV를 위한 넘파이",
          "퍼센트론 실습",
          "ReLU"
        ]
      },
      "requirements": [
        "필수 선수 학습: Data Scientist(데이터 사이언티스트) 전문가 과정 (2022) - 1편"
      ],
      "description": "현직 데이터사이언티스트 전문가가 전하는 빅데이터 개발 및 분석 실무 노하우 및 체계적인 교육\n회사내에서 빅데이터 개발 및 분석 업무로 보직 변경을 하고자 하는 분을 위한 교육\n빅데이터 시스템 구축 및 분석의 A부터 Z까지 전체적인 흐름을 파악하고, 빅데이터 수집부터 적재, 처리 , 분석까지 프로젝트로 배우며 관련 노하우를 습득합니다.\n\n\n[HD]Data Scientist(데이터 사이언티스트) 전문가 과정 (2022) Part.4 데이터마이닝기반 빅데이터분석 및 시각화 1\n\n\n비지니스 애널리틱스\n데이터 마이닝 프로세스 및 핵심 아이디어\n...\n[HD]Data Scientist(데이터 사이언티스트) 전문가 과정 (2022) Part.4 데이터마이닝기반 빅데이터분석 및 시각화 2\n\n\n예측 성능의 평가\n분류 모형 평가\n...\n[HD]Data Scientist(데이터 사이언티스트) 전문가 과정 (2022) Part.5 인공지능기반 데이터분석 1\n\n\n기계 학습 개요\n다차원 특징 공간, 간단한 기계학습 예제\n...\n[HD]Data Scientist(데이터 사이언티스트) 전문가 과정 (2022) Part.5 인공지능기반 데이터분석 2\n\n\n로지스틱 회귀\nKNN\n...",
      "target_audience": [
        "데이터분석 분야 및 관련 분야 취업 및 ADP 자격 취득을 하려는 누구나"
      ]
    },
    {
      "title": "Statistics for Data Science and Business Analysis",
      "url": "https://www.udemy.com/course/statistics-for-data-science-and-business-analysis-f/",
      "bio": "Essential Office Statistics: Descriptive and Inferential Methods, Hypothesis Testing, and Regression Analysis (Arabic)",
      "objectives": [
        "Understand the fundamental concepts of descriptive and inferential statistics.",
        "Learn how to analyze and interpret data using statistical methods.",
        "Understand how to calculate and interpret confidence intervals and probabilities.",
        "Master the process of formulating and testing hypotheses in business scenarios.",
        "Explore linear and logistic regression models to predict outcomes and make data-driven decisions."
      ],
      "course_content": {
        "Introduction": [
          "Course intro",
          "Download resources"
        ],
        "A. Collecting and Summarizing Data": [
          "Types of data",
          "Measurement scales",
          "Data collection methods",
          "Data accuracy and integrity",
          "Data visualization techniques",
          "Descriptive statistics",
          "Measures of Central Tendency",
          "Measures of Variability (Spread)",
          "Shape of the Data",
          "Frequency and Cumulative Frequency Distributions",
          "The Central Limit Theorem",
          "Graphical methods for depicting distributions",
          "QUIZE"
        ],
        "B. Quantitative Concepts": [
          "Terminology",
          "Drawing Statistical Conclusions",
          "Probability terms and concepts",
          "QUIZE"
        ],
        "C. Probability Distributions": [
          "Continuous distributions",
          "General Form of Expected value and variance",
          "Common Continuous distributions",
          "Discrete distributions",
          "Approximations to probability distributions",
          "Central Limit Theorem",
          "Sampling distributions",
          "QUIZE"
        ],
        "D. Statistical Decision Making": [
          "Point Estimates",
          "Confidence Intervals",
          "Statistical Tolerance Intervals",
          "Hypothesis Testing",
          "The P-value Approach to Hypothesis Testing",
          "Significance Level, Power, Type I and Type II Errors",
          "Statistical versus Practical Significance",
          "Paired-comparison tests",
          "Goodness-of-fit Tests",
          "Analysis of variance (ANOVA)",
          "Contingency tables",
          "QUIZE"
        ],
        "E. Relationships Between Variables": [
          "Linear regression models",
          "Simple Linear Correlation",
          "Time-series analysis",
          "QUIZE"
        ],
        "Test yourself": [
          "Challenge your understanding of key concepts in data analysis",
          "Bonus"
        ]
      },
      "requirements": [
        "No prior experience needed! We'll start with the basics and gradually expand your knowledge.",
        "Everything you need is included in the course. Just bring your willingness to learn and practice."
      ],
      "description": "Unlock the power of data with our comprehensive course, \"Statistics for Data Science and Business Analysis.\" Designed for both beginners and experienced professionals, this course will equip you with the essential statistical tools needed to analyze and interpret data effectively, driving better business decisions. The statistical topics covered in this course are especially beneficial for those preparing for the Certified Six Sigma Black Belt (CSSBB) or Certified Quality Engineer (CQE) exams, providing a strong foundation in the necessary statistical techniques and methodologies.\nStarting with the basics, we’ll guide you through descriptive and inferential statistics, ensuring a solid foundation before diving into more advanced concepts. You’ll learn how to perform hypothesis testing, conduct regression analysis, and apply statistical methods to real-world business problems. Our focus is on practical application, enabling you to turn raw data into actionable insights that can boost your career or business.\nWhether you’re an aspiring data scientist, a business analyst looking to enhance your skills, or a manager seeking to make data-driven decisions, this course is for you. No prior experience in statistics is required. We’ll start from the ground up, gradually building your knowledge with clear explanations and hands-on exercises.\nBy the end of this course, you’ll have the confidence to apply statistical techniques in your work, transforming data into meaningful results that can improve performance, optimize strategies, and drive success in any industry.\nJoin us and take the first step toward mastering statistics for data science and business analysis!",
      "target_audience": [
        "Individuals who want to build a strong foundation in statistics to advance their careers in data science.",
        "Professionals looking to enhance their analytical skills to make data-driven business decisions.",
        "Those studying business, economics, data science, or related fields who want to gain practical statistical skills.",
        "Business leaders who wish to understand and leverage data insights to improve their decision-making processes.",
        "Individuals looking to use data and statistical methods to drive business growth and innovation.",
        "Those interested in using data to optimize marketing strategies and improve sales performance.",
        "Beginners who have no prior experience with statistics but want to learn how to analyze data effectively.",
        "Anyone who wants to understand how to use data to solve problems and make informed decisions, regardless of their industry or background."
      ]
    },
    {
      "title": "【한글자막】 Python 에서의 데이터 조작 : Pandas 완벽 단기 특강",
      "url": "https://www.udemy.com/course/best-data-manipulation-pandas/",
      "bio": "판다스 데이터프레임 조작의 기초부터 고급까지 깊어질 수록 어려운 판다스 라이브러리를 효과적으로 학습하고, 파이썬을 이용하여 데이터를 변환하고 정리 하는 법을 배웁니다",
      "objectives": [
        "히스토그램을 이용한 차원 축소로 데이터를 시각화할 수 있습니다",
        "여러 형식으로 데이터 프레임을 생성, 저장 및 직렬화할 수 있습니다",
        "데이터를 쉽게 정리하고 포맷할 수 있습니다",
        "결측값을 찾아 자동으로 채울 수 있습니다",
        "데이터를 그룹화, 집계 및 요약할 수 있습니다",
        "분리된 데이터를 합병할 수 있습니다",
        "전문가처럼 데이터 피벗 및 교차 테이블링이 가능해집니다",
        "시계열 데이터를 상호 분할, 요약 및 조사합니다",
        "다른 시간대의 데이터를 원활하게 작업할 수 있습니다",
        "초보자가 저지를 수 있는 흔한 실수와 함정들을 피할 방법을 배웁니다"
      ],
      "course_content": {
        "소개": [
          "소개",
          "나는 누구일까요?",
          "보너스: 학습 경로",
          "파이썬과 에디터 세팅",
          "라이브 설치",
          "준비물 챙기기"
        ],
        "데이터 집합 기초": [
          "데이터 집합 찾기",
          "주피터 노트북과 데이터 로딩",
          "판다스 vs 넘파이",
          "데이터프레임 만들기",
          "저장과 직렬화",
          "데이터프레임 검사"
        ],
        "시각적 탐색": [
          "소개 및 기초 플롯",
          "판다스 vs Matplotlib",
          "1D 분배 시각화",
          "2D 분배 시각화",
          "판다스 테이블 결과 스타일링",
          "더 높은 차원의 시각화",
          "요약"
        ],
        "데이터 조작 기초": [
          "소개, 라벨링과 정렬",
          "슬라이싱과 필터링",
          "교체와 한계값",
          "데이터 삭제와 추가",
          "지도와 벡터화 기능 추가",
          "요약"
        ],
        "그룹화(Grouping)": [
          "소개와 동기 부여",
          "기초 그룹화 구문",
          "대체법",
          "그룹별 집계",
          "요약"
        ],
        "병합": [
          "소개와 기초 구문",
          "다양한 병합 유형",
          "유용한 병합 유형",
          "요약"
        ],
        "멀티인덱스, 피보팅 그 외": [
          "소개와 기초 멀티인덱스",
          "멀티인덱스 II – 멀티인덱스의 반격",
          "스태킹과 언스태킹",
          "피보팅",
          "피봇 마진",
          "크로스탭",
          "녹이기",
          "요약"
        ],
        "타임 시리즈 데이터": [
          "소개와 데이트타임 인덱스",
          "리인덱싱",
          "리샘플링",
          "롤링 기능",
          "시간대",
          "요약"
        ],
        "결론": [
          "복습, 감사합니다",
          "추가 – 주피터 노트북 커스터마이징하기",
          "추가 – 챕터 2 데이터 훑어보기",
          "추가 – 챕터 3 시각화 훑어보기",
          "추가 – 챕터 4 기초 훑어보기",
          "추가 – 챕터 5 집단화 훑어보기",
          "추가 – 챕터 6 병합 훑어보기",
          "추가 – 챕터 7 고급 과정 훑어보기",
          "추가 – 챕터 8 타임시리즈 훑어보기"
        ]
      },
      "requirements": [
        "파이썬에 대한 기본적인 지식이 요구됩니다"
      ],
      "description": "판다스 완벽 단기 특강 with 파이썬!\n판다스 데이터프레임 조작의 기초부터 고급까지 배워보세요!\n실제 사례들을 바탕으로 한 커닝 페이퍼와 예시들 포함!\n\n\nPython 에서의 데이터 조작 : Pandas 완벽 단기 특강을 선택해야 하는 이유\n실제 데이터는 이상적이지 않기 때문에, 판다스와 같은 파이썬 라이브러리는 매우 유용합니다.\n\n\n데이터 조작에서 막혀 데이터 분석에 너무 많은 시간을 할애하고 있다면, 작업 속도를 되찾는데 이 강의가 큰 도움이 될 것입니다.\n\n\n데이터의 주인이 되세요, 데이터에 끌려다니지 마시고요!\n\n\n데이터 과학자로서 데이터 조작과 준비가 작업의 80%를 차지하고 있다면, raw 데이터를 최종 작물로 불러올 수 있는 효율적인 데이터 변형을 배우는 것은 필수적입니다.\n\n\n파이썬 라이브러리인 Pandas를 사용한 데이터 분석을 통해 데이터 왜곡을 줄여, 더욱 나은 결과를 얻고 생산성을 높이며 문제해결에 더 많은 시간을 할애할 수 있어 효과적인 전달이 가능해집니다.\n\n\n이 강의는 그런 것들을 위한 맞춤 코스죠!\n\n\nPandas 데이터프레임을 통해 고급 데이터 조작, 준비, 정렬, 혼합 및 데이터 정리 방법을 학습하여 혼란스러운 데이터를 최종 사전 분석 제품으로 만들 수 있습니다.\n이것이 바로 파이썬 라이브러리 중 판다스가 구글, 페이스북, JP 모건 등 주요 데이터 분석 회사들의 데이터 과학자들 사이에서 가장 인기 있는 이유입니다.\n\n\n만약 데이터 시각화, 통계 분석 또는 기계 학습을 준비하기 위해 판다스를 효율적으로 활용하여 데이터를 조작, 변환, 스택, 병합 및 집계하는 방법에 대해 알고 싶다면 이 과정이 안성맞춤입니다.\nSamuel Hinton 강사님에게 다음과 같은 내용을 배울 수 있습니다 :\n\n\n일반적인 고급 판다스 데이터 조작 기술을 학습하여 분석을 위해 최대한 효율적으로 raw 데이터를 최종 제품으로 가져올 수 있게 됩니다.\n데이터 작성에 시간을 낭비하지 않고 문제 해결에 더 많은 시간을 투자하여 더 나은 결과를 얻어냅니다.\n통계 분석과 기계 학습을 최대한 간단하게 하기 위한 데이터 형상화 및 조작 방법을 배웁니다.\n최신 버전의 파이썬과 업계 표준 판다스 라이브러리를 활용합니다.\n\n\n파이썬의 Pandas 라이브러리를 이용한 데이터 분석은 도움이 되지만, 분명 단점도 존재합니다.\n하지만 다음과 같은 과정들은 그 단점도 이겨낼 수 있게 도와줄 수 있죠.\n\n\n판다스는 가파른 학습 곡선을 가지고 있습니다. 판다스 라이브러리를 깊게 배울수록 학습 곡선은 더 가팔라집니다.\n이 강의는 초급 및 중급 이용자들이 판다스의 여러 측면을 순조롭게 둘러볼 수 있게 도와줍니다.\n불친절한 설명서 : 제대로 된 설명서가 없으면 새 라이브러리를 학습하기 어렵지만, 고급 기능에 관한 판다스의 설명서는 거의 도움이 되지 않습니다.\n이 강의로 고급 판다스 기술들을 쉽게 이해할 수 있으며 도움을 찾는 시간을 절약할 수 있습니다.\n\n\n강의 학습을 완료하면 다음 단계의 데이터 분석에서 유용한 결과를 도출해 낼 수 있다는 자신감과 함께 복잡한 이기종 데이터셋을 편안하게 탐구할 수 있게 됩니다.\n\n\nPython 에서의 데이터 조작 : Pandas 완벽 단기 특강의 세부 커리큘럼\n판다스 데이터프레임들을 불러오거나 만들고,\n데이터를 기본 플롯과 1차원, 2차원 그리고 다차원 시각화를 사용하여 표시할 수 있게 됩니다\n기본적인 데이터프레임 조작법 : 인덱싱, 레이블 지정, 슬라이싱 순서 지정, 필터링 등의 조작법들을 익힙니다\n고급 판다스 데이터프레임 조작법 : 다중 인덱싱, 스택, 계층 인덱싱, 피벗, 멜트 등의 조작법들을 익힙니다\n데이터프레임 그룹화 수행 : 집계, 귀책 등의 그룹화를 수행할 수 있게 됩니다\n시계열 조작 마스터하기 : 재색인화, 재샘플링, rolling 함수, 메소드 체인 그리고 필터링 등의 조작을 마스터합니다\n판다스 데이터 프레임 병합도 가능해집니다\n\n\n마지막으로, 이 강의는 실제 사례들을 바탕으로 한 수많은 커닝 페이퍼와 예시들로 구성되어 있습니다. 따라서 이론뿐만 아니라 판다스 실습도 함께 진행할 수 있게 됩니다]\n\n\n강의를 들으시고 강의와 관련하여 궁금하신 점은 무엇이든 Q&A에 남기실 수 있지만, 꼭 영어로 남겨주세요. 그래야 답변을 드릴 수 있습니다. :)\n\n\n강의에서 만나요!\n- Samuel Hinton",
      "target_audience": [
        "데이터를 전문적으로 조작하고자 하는 파이썬 학생들.",
        "역량을 늘리고자 하는 떠오르는 데이터 분석가와 과학자들.",
        "단순한 데이터 정리보다 흥미로운 문제를 해결하는 것을 좋아하는 사람.",
        "새롭게 주목받고 있는 파이썬 툴을 알고자 하는 기성 프로그래머들을 대상으로 하는 강의입니다"
      ]
    },
    {
      "title": "Machine Learning para Competições Kaggle - Curso 2",
      "url": "https://www.udemy.com/course/machine-learning-competicoes-kaggle-curso-2/",
      "bio": "Aprenda passo a passo como trabalhar com bases de dados de agrupamento e associação voltados a desafios reais no Python",
      "objectives": [
        "Como trabalhar com bases de dados reais de agrupamento e associação, aplicado em competições reais do Kaggle",
        "Desenvolva insights que permitam construir modelos de Machine Learning aplicados em problemas reais",
        "Crie vários tipos de gráficos para ajudar na compreensão e análise dos dados",
        "Use as características técnicas de cada jogador do FIFA 2019 para agrupá-los em um perfil técnico",
        "Investigue as relações entre os perfis dos jogados do FIFA 2019 com suas posições originais",
        "Utilize regras de associação para encontrar padrões em hábitos de compra dos clientes",
        "Utilize regras de associação para análise de cestas de compras (market basket analysis)"
      ],
      "course_content": {
        "Introdução": [
          "Boas-vindas e conteúdo do curso",
          "Mais sobre Inteligência Artificial",
          "Recursos para download"
        ],
        "Agrupamento: base de dados de jogadores do FIFA 2019": [
          "Introdução e importação das bibliotecas",
          "Carregamento da base de dados",
          "Valores faltantes",
          "Histograma e boxplot",
          "Tratamento da altura do jogador",
          "Tratamento do peso do jogador",
          "Preparação da base de dados para agrupamento",
          "Número de clusters com WCSS",
          "Redução de dimensionalidade com PCA",
          "Número de clusters pelo método da silhueta 1",
          "Número de clusters pelo método da silhueta 2",
          "Número de clusters pelo método da silhueta 3",
          "Número de clusters pelo método da silhueta 4",
          "Agrupamento com k-means",
          "Relacionamentos entre grupos e posições 1",
          "Relacionamentos entre grupos e posições 2",
          "Relacionamentos entre grupos e posições 3",
          "Relacionamentos entre grupos e posições 4",
          "Relacionamentos entre grupos e posições 5",
          "Seleção de atributos com Random Forest 1",
          "Seleção de atributos com Random Forest 2",
          "Classificação com Random Forest 1",
          "Classificação com Random Forest 2",
          "Classificação com Random Forest 3"
        ],
        "Regras de Associação: Instacart Market Basket Analys": [
          "Introdução e importação das bibliotecas",
          "Carregamento da base de dados",
          "Inspeção dos dados: corredores do mercado",
          "Inspeção dos dados: departamentos",
          "Inspeção dos dados: produtos",
          "Inspeção dos dados: pedidos 1",
          "Inspeção dos dados: pedidos 2",
          "Inspeção dos dados: produtos do pedido",
          "Exploração dos dados: usuários",
          "Exploração dos dados: número de pedidos",
          "Exploração dos dados: dias da semana",
          "Exploração dos dados: hora do dia",
          "Exploração dos dados: intervalo entre compras",
          "Exploração dos dados: produtos recomprados 1",
          "Exploração dos dados: produtos recomprados 2",
          "Exploração dos dados: produtos não comprados",
          "Exploração dos dados: tamanho da cesta de compras",
          "Exploração dos dados: produtos mais frequentes",
          "Regras de associação: hábitos de compras 1",
          "Regras de associação: hábitos de compras 2",
          "Regras de associação: hábitos de compras 3",
          "Regras de associação: hábitos de compras 4",
          "Associações entre produtos 1",
          "Associações entre produtos 2",
          "Associações entre produtos 3",
          "Associações entre produtos 4",
          "Associações entre produtos 5",
          "Associações entre produtos 6",
          "Associações entre produtos 7",
          "Associações entre produtos 8",
          "Associações entre produtos 9",
          "Associações entre produtos 10"
        ],
        "Anexo I: Agrupamento com k-means": [
          "Algoritmo k-means: introdução",
          "Algoritmo k-means: cálculos matemáticos",
          "Algoritmo k-means: inicialização",
          "Introdução a PCA (Principal Component Analysis)"
        ],
        "Anexo II: Regras de Associação": [
          "Algoritmo apriori - introdução",
          "Algoritmo apriori - cálculo do suporte 1",
          "Algoritmo apriori - cálculo do suporte 2",
          "Algoritmo apriori - cálculo da confiança 1",
          "Algoritmo apriori - cálculo da confiança 2",
          "Algoritmo apriori - cálculo do lift",
          "AULA BÔNUS"
        ]
      },
      "requirements": [
        "Familiaridade com os conceitos básicos e algoritmos de Machine Learning",
        "Lógica de programação, principalmente estruturas condicionais e de repetição",
        "Programação básica em Python"
      ],
      "description": "As competições de Ciência de Dados como aquelas postadas na plataforma Kaggle são uma ótima maneira de testar as habilidades adquiridas em cursos iniciais, e ainda aprender novas habilidades necessárias para resolver problemas reais. Entretanto, fazer essa transição entre um ambiente educacional e aquele que encontramos no Kaggle, que imita os desafios que devemos encontrar no mercado de trabalho, tende a ser um degrau muito grande, pois a natureza dos dados e dos problemas propostos aumenta de complexidade num nível que os cursos básicos não contemplam.\nPensando nisso, este curso tem o objetivo de preencher essa lacuna na formação dos cientistas de dados, mostrando detalhadamente como abordar os desafios, passando pelas fases de exploração e tratamento de dados, escolha de abordagem de solução, construção de um modelo, treinamento e validação. O entendimento desse processo é o primeiro passo para que os competidores possam desenvolver melhorias e começar sua escalada rumo ao topo dos rankings.\nNeste curso focaremos em duas das principais tarefas da aprendizagem de máquina não supervisionada: agrupamento e associação\nCom relação ao agrupamento, vamos trabalhar com uma base de dados do jogo FIFA Soccer 2019 e usar as características técnicas de cada jogador, juntamente com a altura e peso para agrupá-los em um perfil técnico. Investigaremos as relações entre estes perfis e as posições originais dos jogadores utilizando o algoritmo k-means e a biblioteca sklearn\nNo que se refere a associação, vamos explorar o extenso conjunto de dados Instacart Market Basket Analysis com mais de 3 milhões de transações de supermercado, compreendendo uma enorme variedade de produtos de diferentes departamentos. Faremos a geração de regras de associação com base em duas coleções de dados: hábitos de compra (dia e hora, intervalo entre pedidos) e associação de produtos (quais produtos tendem a ser vendidos juntos). Usaremos duas abordagens: na primeira vamos usar a biblioteca apyori para geração das regras, enquanto que na segunda faremos a implementação do zero do algoritmo apriori!\nVamos desenvolver todos os códigos utilizando a linguagem Python linha por linha com o Google Colab, de forma que você entenda todas as análises necessárias para participar dessas competições!",
      "target_audience": [
        "Pessoas que já estejam num nível intermediário de sua formação em Ciência de Dados, e que agora estejam procurando aprender a usar suas habilidades em desafios reais",
        "Analistas de dados que queiram aumentar seu conhecimento na área de Machine Learning",
        "Alunos de graduação que estão estudando disciplinas ligadas a área de Inteligência Artificial",
        "Qualquer pessoa interessada em Inteligência Artificial"
      ]
    },
    {
      "title": "Iniciando na Visualização de Dados com Python",
      "url": "https://www.udemy.com/course/iniciando-na-visualizacao-de-dados-com-python/",
      "bio": "O primeiro passo para você se tornar um Cientista de Dados! Curso passo-a-passo com explicações bem detalhadas.",
      "objectives": [
        "Conhecer os tipos de dados",
        "Fazer análise de conjuntos de dados",
        "Saber manipular dados com pandas",
        "Criar visualizações com matplotlib e seaborn"
      ],
      "course_content": {},
      "requirements": [
        "Familiaridade com Informática e escrita de código"
      ],
      "description": "As visualizações de dados são surpreendentemente comuns em sua vida cotidiana, mas geralmente aparecem na forma de tabelas e gráficos conhecidos.\nAs visualizações de dados podem ser usadas para descobrir fatos e tendências desconhecidos. Você pode ver visualizações na forma de gráficos de linhas para exibir as alterações ao longo do tempo. Gráficos de barras e colunas são úteis ao observar relacionamentos e fazer comparações. Gráficos de pizza são uma ótima maneira de mostrar partes-de-um-todo. E mapas são a melhor maneira de compartilhar visualmente dados geográficos.\nBoas visualizações de dados são criadas quando a comunicação, a ciência de dados e o design colidem. As visualizações de dados feitas corretamente oferecem informações importantes sobre conjuntos de dados complicados de maneiras significativas e intuitivas.\nPara criar um boas visualizações de dados, você precisa começar com dados limpos, bem fornecidos e completos. Quando seus dados estiverem prontos para visualizar, você precisará escolher o gráfico correto. Isso pode ser complicado, mas há muitos recursos disponíveis para ajudá-lo a escolher o tipo certo de gráfico para seus dados.\nHoje, mais do que nunca, as organizações estão usando visualizações de dados e ferramentas de dados para fazerem melhores perguntas e tomar melhores decisões. Novas tecnologias de computação e novos programas de software fáceis de usar facilitaram o aprendizado sobre sua empresa e tomaram melhores decisões de negócios baseadas em dados.\nPor isso, é de grande importância saber criar boas visualizações de dados, principalmente para aqueles que querem se tornar Cientistas de Dados. Recomendo este curso!",
      "target_audience": [
        "Interessados em entrar para o mundo da Ciência de Dados"
      ]
    },
    {
      "title": "Learn About Artificial Intelligence (AI) Part 1",
      "url": "https://www.udemy.com/course/belajar-mengenal-artificial-intelligence-ai-part-1/",
      "bio": "Learn More About Artificial Intelligence (AI) Part 1",
      "objectives": [
        "Konsep Dasar Kecerdasan Buatan",
        "Pentingnya Mempelajari Kecerdasan Buatan/Artificial Intelligence ( AI )",
        "Apa Itu Intelijen/Kecerdasan?",
        "Reasoning",
        "Learning",
        "Problem Solving",
        "Perception",
        "Linguistic Intelligence",
        "Apa saja yang terlibat dalam AI?",
        "Aplikasi Kecerdasan Buatan",
        "Agent & Environment"
      ],
      "course_content": {
        "Pengantar": [
          "Konsep Dasar Kecerdasan Buatan",
          "Pentingnya Mempelajari Kecerdasan Buatan/ Artificial Intelligence ( AI )",
          "Apa Itu Intelijen/Kecerdasan?",
          "Reasoning",
          "Learning",
          "Problem Solving",
          "Perception",
          "Linguistic Intelligence",
          "Apa Saja Yang Terlibat Dalam AI?",
          "Aplikasi Kecerdasan Buatan",
          "Agent & Environment",
          "Ketika Robot Melakukan Segalanya"
        ]
      },
      "requirements": [
        "Anda cukup memiliki Hak Akses Internet dengan device Laptop/PC/Smartphone saja."
      ],
      "description": "AI itu singkatan apa.\nApa itu AI. AI adalah Kecerdasan Buatan, seperti kepanjangan AI yaitu Artificial Intelligence, AI merupakan teknologi yang dirancang untuk membuat sistem komputer mampu meniru kemampuan intelektual manusia.\nApa yang dimaksud teknologi AI.\nAI dikenal sebagai teknologi yang memiliki potensi besar untuk mengubah kehidupan manusia di masa depan. Secara umum, AI merujuk pada program komputer yang dirancang untuk meniru kecerdasan manusia, termasuk kemampuan pengambilan keputusan, logika, dan karakteristik kecerdasan lainnya.\nApa itu AI dan manfaatnya.\nArtificial Intelligence  atau AI merujuk pada kemampuan mesin untuk meniru atau meniru kecerdasan manusia. Ini mencakup berbagai teknik dan metode yang memungkinkan komputer untuk memahami, belajar, dan mengambil keputusan berdasarkan data yang diberikan.\nApa tujuan dari AI.\nTujuan dari AI adalah menciptakan mesin yang mampu berpikir, belajar, merencanakan, dan menyelesaikan tugas secara mandiri.\nAplikasi AI apa aja.\nBerikut eduparx dot id merangkum beberapa aplikasi AI gratis yang bisa digunakan untuk memudahkan pekerjaan di berbagai bidang:\nGrammarly. Grammarly adalah alat pengecek tata bahasa dan ejaan yang didukung oleh AI.\nGoogle Docs.\nTrello.\nSlack.\nZoom.\nApa saja bahaya AI.\nDaftar Bahaya AI\nKecerdasan Buatan yang Mandiri  atau Autonomous AI Kecerdasan buatan yang mandiri adalah jenis AI yang dapat mengambil keputusan tanpa campur tangan manusia.\nPengangguran Massal dan Disparitas Sosial.\nPelanggaran Privasi.\nBias dan Diskriminasi.\nKeamanan Cyber dan Ancaman Militan.\nAI diciptakan oleh siapa.\nJohn McCarthy adalah penemu dan pelopor dari Kecerdasan Buatan atau Artificial Intelligence\nKenapa AI diciptakan.\nSelain itu, Artificial Intelligence  atau AI diciptakan dengan tujuan untuk berpikir, dimana agen cerdas harus dapat berpikir secara logis dan membuat keputusan yang baik. Belajar, agen cerdas harus dapat belajar dari pengalaman dan meningkatkan kinerjanya seiring waktu.\nApakah AI sama dengan robot.\nRobot adalah peralatan mekanis dan elektronis  atau mekatronika, AI adalah sebuah program berbasis komputer. Robot berfokus pada dunia fisik, AI berfokus pada analisis informasi dan melakukan komputasi.\nUntuk lebih detailnya, Anda dapat membeli dan mempelajari Kursus Online yang telah kami sediakan yaitu Belajar Mengenal Artificial Intelligence atau AI Part 1, terima kasih.\n\n\nWhat is the abbreviation for AI?\n\n\nWhat is AI? AI is Artificial Intelligence, like the abbreviation AI, namely Artificial Intelligence, AI is a technology designed to make computer systems able to imitate human intellectual abilities.\n\n\nWhat is AI technology.\n\n\nAI is known as a technology that has great potential to change human life in the future. In general, AI refers to computer programs designed to imitate human intelligence, including decision-making abilities, logic, and other intelligence characteristics.\n\n\nWhat is AI and its benefits.\n\n\nArtificial Intelligence or AI refers to the ability of machines to imitate or mimic human intelligence. It includes various techniques and methods that allow computers to understand, learn, and make decisions based on the data provided.\n\n\nWhat is the purpose of AI.\n\n\nThe goal of AI is to create machines that are able to think, learn, plan, and complete tasks independently.\n\n\nWhat are the AI applications.\n\n\nHere eduparx dot id summarizes some free AI applications that can be used to facilitate work in various fields:\n\n\nGrammarly. Grammarly is a grammar and spelling checker powered by AI.\n\n\nGoogle Docs.\n\n\nTrello.\n\n\nSlack.\n\n\nZoom.\n\n\nWhat are the dangers of AI.\n\n\nList of AI Dangers\n\n\nAutonomous Artificial Intelligence or Autonomous AI Autonomous artificial intelligence is a type of AI that can make decisions without human intervention.\n\n\nMass Unemployment and Social Disparity.\n\n\nPrivacy Violation.\n\n\nBias and Discrimination.\n\n\nCyber Security and Militant Threats.\n\n\nAI was created by whom.\n\n\nJohn McCarthy is the inventor and pioneer of Artificial Intelligence or Artificial Intelligence\n\n\nWhy was AI created.\n\n\nIn addition, Artificial Intelligence or AI was created with the aim of thinking, where intelligent agents must be able to think logically and make good decisions. Learning, intelligent agents must be able to learn from experience and improve their performance over time.\n\n\nIs AI the same as robots.\n\n\nA robot is a mechanical and electronic or mechatronic device, AI is a computer-based program. Robots focus on the physical world, AI focuses on analyzing information and performing computations.\n\n\nFor more details, you can purchase and study the Online Course that we have provided, namely Learning to Get to Know Artificial Intelligence or AI Part 1, thank you.",
      "target_audience": [
        "Kursus Online ini Terbuka Untuk Seluruh Kalangan Masyarakat."
      ]
    },
    {
      "title": "Spark & PySpark",
      "url": "https://www.udemy.com/course/spark-pyspark/",
      "bio": "Twój klucz do Big Data",
      "objectives": [
        "Co to jest Spark i dlaczego jest wykorzystywany w pracy z Big Data",
        "Konfiguracja środowiska do pracy ze Sparkiem",
        "Data Frame i definiowanie struktury danych",
        "Budowanie wyrażeń PySpark",
        "Transformacja danych",
        "Wartości NULL",
        "Funkcje wykorzystywane w analizie: grupowanie, top 10, funkcje okienkowe,",
        "Łączenie danych i rodzaje joinów",
        "Partycjonowanie, odczyt i zapis sanych"
      ],
      "course_content": {
        "Wprowadzenie": [
          "Wprowadzenie",
          "O Sparku",
          "Materiały kursowe"
        ],
        "Środowisko pracy": [
          "Wprowadzenie",
          "Gdzie uruchomić Sparka – opcja 1 – Google Colab",
          "Gdzie uruchomić Sparka – opcja 2 - Databricks",
          "Spark na Twoim komputerze – instalacja wymaganych programów (opcjonalnie)",
          "Spark na Twoim komputerze – konfiguracja i uruchomienie (opcjonalnie)",
          "Spark na Twoim komputerze - uruchomienie notebooka (opcjonalnie)"
        ],
        "Sesja Spark i Spark Data Frame": [
          "Wprowadzenie",
          "Jak pracować z sesją Spark?",
          "Jak jest zbudowany DataFrame?",
          "Kolumny i wyrażenia"
        ],
        "Transformacje": [
          "Wprowadzenie",
          "Filtrowanie danych",
          "Filtrowanie danych zagnieżdżonych",
          "Dodawanie, usuwanie, zmiana nazwy kolumny i niuanse...",
          "Kolumny i funkcje tekstowe",
          "Funkcje daty i czasu",
          "Projekt - Oczyszczanie danych"
        ],
        "Praca z danymi": [
          "Wprowadzenie",
          "Null albo nie null - oto jest pytanie - eliminacja null",
          "Łączenie data frame - union",
          "OrderBy, GroupBy i agregacje",
          "Distinct i top - zastosowanie",
          "Window functions"
        ],
        "Łączenie danych": [
          "Wprowadzenie",
          "Wprowadzenie do join - przygotowanie danych",
          "Wprowadzenie do join - partycjonowanie tabel",
          "Inner Join",
          "Left/Right Join",
          "Porównanie joinów"
        ],
        "Odczyt i zapis danych": [
          "Wprowadzenie",
          "Opcje odczytu danych (format CSV i pokrewne)",
          "Obsługa błedów podczas odczytu danych (parametr mode)",
          "Json - wprowadzenie",
          "Pliki Json",
          "Architektura Sparka: driver, worker, executor",
          "Zapis danych i format Parquet"
        ],
        "Bonus lecture": [
          "Bonus lecture"
        ]
      },
      "requirements": [
        "Znajomość Pythona",
        "Rozumienie podstawowych pojęć z zakresu pracy z danymi: tabela, rekord, kolumna"
      ],
      "description": "Spark to narzędzie, którego możemy użyć do przetwarzania ogromnych ilości danych – Big Data - i to zarówno na etapie ich oczyszczania, ale też później podczas budowania modeli uczenia maszynowego. Ta moc Sparka bierze się z tego, że jedno niewinne polecenie jest w tle rozsyłane przez Sparka do wielu maszyn zwanych workerami, które te dane przetwarzają i odsyłają gotowe wyniki, ale… bez obaw – wszystko dzieje się w tle, a developer po prostu skupia się na tym co lubi najbardziej, czyli pisaniu działającego kodu. I o pisaniu takiego kodu jest ten kurs.\n\n\nJakkolwiek by to brzmiało – Spark nie jest trudny. Dane trzeba wczytać, tyle tylko że wczytujemy je najczęściej z luźnych plików. Trzeba je odfiltrować, dodać kolumnę, usunąć kolumnę, w oparciu o istniejące dane wyznaczyć nowe. Znaleźć braki i je czymś uzupełnić, a wyeliminować wartości niepotrzebne. Czasami dane są rozrzucone między wiele tabel. W takim przypadku trzeba je ze sobą połączyć. Do każdej z tych operacji mamy odpowiednie polecenie i na tym kursie możesz je poznać.\n\n\nSpark to platforma, która pozwala na pisanie swoich programów w Pythonie, SQL, Scali czy języku R. W tym kursie zajmujemy się pythonową wersją API Sparka, zwaną PySpark. Dlatego znajomość podstaw pracy z Pythonem jest tutaj niezbędna.\n\n\nNa kursie zaczynamy od kilku propozycji środowiska w jakim można pracować ze Sparkiem. Następnie przyglądamy się poszczególnym obszarom pracy z danymi i z lekcji na lekcję powiększamy zbiór znanych funkcji.\n\n\nDo każdej lekcji dostajesz do dyspozycji\nmateriał video\nzadania wraz z propozycjami rozwiązania tych zadań na GitHub.\nNa zakończenie kursu możesz podjąć się zbudowania małego projektu.\nDo kursu jest też dołączony podręcznik PDF z krótką notatką z lekcji i treścią zadań.\n\n\nWarto znać Sparka, bo w Data Science, Machine Learning czy AI, od danych się nie ucieknie. Spark pracuje w wielu innych produktach, jak np. Databricks, Synapse czy Microsoft Fabric. A tych danych jest coraz to więcej i ktoś musi je zrozumieć i przygotować.\n\n\nDlatego zapraszam na kurs „Spark i PySpark. Obejrzyj lekcje próbne, dodaj kurs do koszyka i poznaj potężne narzędzie do obróbki i analizy danych – Spark – Twój klucz do analizy Big Data.\n\n\nTwój trener, Rafał",
      "target_audience": [
        "Analitycy danych",
        "Data Scientists",
        "Data Engineers",
        "Programiści Pythona poszerzający wiedzę o Sparka"
      ]
    },
    {
      "title": "파이썬을 이용한 데이터 분석",
      "url": "https://www.udemy.com/course/data-analysis-python/",
      "bio": "미국 대형은행 애널리틱스 최고 책임자가 참여한 파이썬 데이터 분석",
      "objectives": [
        "인터넷에 있는 여러 분야의 데이터를 손쉽게 수집하고 정리할 수 있습니다.",
        "엑셀에서는 다룰 수 없는 방대한 크기의 데이터를 쉽게 다룰 수 있게 됩니다.",
        "데이터 분석을 위한 파이썬, 파이썬 패키지 등을 익힐 수 있습니다.",
        "의료, 마케팅, 금융, 행정 등 거의 모든 분야의 데이터 분석에 응용될 수 있는 기본적인 툴을 이해하고 파이썬을 이용해 간편하게 할 수 있습니다.",
        "실무에 필요한 데이터 분석 역량을 키울 수 있습니다."
      ],
      "course_content": {
        "[챕터1] 데이터 사이언티스트가 되는 첫걸음": [
          "데이터 사이언스의 열린 공간, 깃과 깃허브 활용하기",
          "파이썬을 더 유용하게 만드는 모듈과 나만의 함수 만들기",
          "파일을 읽고 쓰는 다양한 방법",
          "판다스 라이브러리 설치와 기본 사용법",
          "데이터 분석 워밍업 : 판다스를 이용해 배운 내용 복습하기"
        ],
        "[챕터2] 판다스를 이용한 Descriptive analysis": [
          "판다스, NumPy를 이용해 데이터 구조 다루기",
          "판다스와 Numpy 를 이용한 기본 데이터 분석",
          "데이터 클리닝, 분석 전 여러 데이터를 붙이거나 제거해 준비하기",
          "서울 지하철역 데이터로 기본 분석하기",
          "파이썬 프로그래밍 언어의 구조: 객체, 클래스, 메소드"
        ],
        "[챕터3] 판다스를 이용한 심화된 Descriptive analysis": [
          "판다스의 심화 버전 배우기",
          "데이터를 더 정교하게 만드는 멀티 인덱스 이용하기",
          "Groupby를 다른 데이터 구조에 사용하기",
          "여러 통계치를 보여주는 피벗 테이블 만들기",
          "데이터를 원하는 대로 수정하기"
        ],
        "[챕터4] Matplotlib을 이용한 데이터 비주얼화하기": [
          "파이썬을 활용한 데이터 시각화 - Matplotlib 사용하기",
          "원하는 대로 정교한 그래프 그리기",
          "막대 그래프와 히스토그램, 산포도 그리기",
          "실전편: Kaggle에서 Youtube trend 데이터를 이용해 분석하기"
        ]
      },
      "requirements": [
        "데이터분석을 잘 몰라도, 파이썬은 커녕 코딩을 한번도 해보지 않았어도, 파이썬 프로그래밍을 활용하여 데이터분석을 하실 수 있습니다."
      ],
      "description": "파이썬은 데이터 사이언스와 현대 금융의 공용어로, 다수의 국제은행과 회사에서 빠르게 채택되고 있는 추세입니다.\n본 강의는 Numpy와 Matplotlib를 포함한 많은 주요 파이썬 개념을 차근차근 설명합니다.\n\n\n1. 파이썬 분석 대표 라이브러리(Pandas, Numpy, Matplotlib)로 데이터 다루기!\n- 서울시 버스/지하철 이용데이터, Youtube의 각종 이용데이터 등 실생활에 밀접한 데이터 활용\n- 데이터 분류 등 전처리, 시각화 등 분석작업에 가장 많이 ㅆ는 라이브러리 활용 방법 습득\n2. 데이터 수집/분석을 위한 나만의 자동화 tool 개발 가능\n엑셀보다 훨씬 간편하고 효과적인 분석 tool 구현 & 데이터 분석에 쓰이는 파이썬\n- 엑셀보다 어렵지 않도록 파이썬을 쉽게 가르쳐 드립니다.\n- 엑셀보다 대용량 데이터 처리 및 데이터 시각화 등 엑셀의 한계를 넘어 쉽고 빠르게 분석 가능한 파이썬의 필수 분석 코드 중심으로 집중 학습 가능\n\n\n파이썬 데이터 분석 강좌 (3단계 핵심 커리큘럼 구성)\n1단계 데이터 사이언티스트가 되는 첫걸음\n- 데이터 공간 깃과 깃허브 활용하기\n- 판다스 라이브러리 설치와 기본 사용법 익히기\n2단계 기술적 분석(Descriptive Analysis)\n- 판다스, Numpy를 이용한 데이터 구조 다루기와 분석하기\n- 판다스 심화 버전 및 멀티 인덱스 이용하기\n3단계 Matplotlib을 이용한 데이터 비주얼화\n- 파이썬을 활용한 데이터 시각화, 정교한 그래프 그리기\n- Kaggle에서의 데이터 분석하기",
      "target_audience": [
        "파이썬을 통해 데이터 분석을 하고 싶은 분",
        "파이썬 기본 과정을 학습한 후, 이를 실생활에 적용하고 싶은 분",
        "파이썬을 통한 프로그래밍 직무 스킬을 더 향상하고 싶은 분",
        "데이터 분석 분야에 취업/이직을 하고 싶으신 분",
        "누구나 쉽게 따라 할 수 있는 파이썬 프로그래밍을 학습한 분"
      ]
    },
    {
      "title": "สร้างระบบ AI, Object Detection ภายใน 1 ชั่วโมงด้วย Roboflow",
      "url": "https://www.udemy.com/course/ai-object-detection-1-roboflow-q/",
      "bio": "เรียนรู้กระบวนการภาพรวมของระบบ Object Detection ตั้งแต่ 0 จนสามารถนำไปใช้งานได้จริง",
      "objectives": [
        "ผู้เรียนจะได้เรียนรู้การนำเข้าข้อมูลเพื่อให้ง่ายต่อการใช้ในการเทรนโมเดล.",
        "ผู้เรียนจะได้รับความเข้าใจอย่างถี่ถ้วนเกี่ยวกับเนื้อหาของการสร้างระบบ object detection ตั้งแต่เริ่มต้นจนจบกระบวนการพัฒนา",
        "ผู้เรียนจะได้เรียนรู้เครื่องมือสำหรับการปรับแต่งข้อมูล (data augmentation) เพื่อเพิ่มประสิทธิภาพในการเทรนแบบจำลอง.",
        "ผู้เรียนจะได้เรียนรู้เครื่องมือช่วยในการทำ Label ข้อมูล, ทำให้ง่ายต่อการสร้างข้อมูลที่เป็นไปได้สำหรับการฝึกโมเดล Object Detection.",
        "ผู้เรียนจะได้เรียนรู้และบริการการเทรนโมเดล Machine Learning โดยใช้ข้อมูลที่เตรียมไว้ใน Roboflow.",
        "ผู้เรียนจะได้เรียนรู้ในการส่งออกโมเดลที่เทรนเสร็จสิ้นไปยังแพลตฟอร์มอื่น ๆ หรือประยุกต์ใช้ในงานอื่น ๆ",
        "ผู้เรียนจะได้ทดลองและประเมินประสิทธิภาพของโมเดลได้",
        "ผู้เรียนสามารถนำความรู้ที่ได้ไปประยุกต์ใช้ในงานต่าง ๆ ในการเรียนและการทำงานจริงได้",
        "ผู้เรียนจะเข้าใจการทำงานของ object detection ได้ดี"
      ],
      "course_content": {
        "บทนำ": [
          "เรียนรู้กระบวนการภาพรวมของระบบ Object Detection",
          "สมัครใช้งานโปรแกรม Roboflow และเรียนรู้เครื่องมือในเบื้องต้น"
        ],
        "เตรียมข้อมูล (Data Preparation):": [
          "รวบรวมข้อมูล: รวบรวมภาพหรือวิดีโอที่มีวัตถุที่ต้องการตรวจจับ เพื่อฝึกโมเดล.",
          "ติดป้าย (Labeling): ติดป้ายบนภาพเพื่อระบุตำแหน่งของวัตถุที่ต้องการตรวจจับ."
        ],
        "การสร้างและฝึกโมเดล (Model Creation and Training):": [
          "สร้างโมเดลที่เหมาะสมสำหรับงาน Object Detection โดยใช้ Roboflow"
        ],
        "การปรับแต่งและการพัฒนา (Fine-tuning and Improvement):": [
          "ปรับแต่งโมเดลหากต้องการประสิทธิภาพที่ดีขึ้นโดยจัดระเบียบและนำเข้าชุดข้อมูลใหม่"
        ],
        "การนำไปใช้ (Deployment):": [
          "นำโมเดลที่ฝึกมาแล้วไปใช้งานในสภาพแวดล้อมจริงแบบ Real Time"
        ],
        "แบบทดสอบหลังเรียนรู้": [
          "การให้แบบทดสอบหลังเรียนรู้ในด้าน object detection"
        ]
      },
      "requirements": [
        "ผู้เรียนไม่จำเป็นต้องมีความรู้ด้านวิศวกรรมปัญญาประดิษฐ์มาก่อน คุณจะได้เรียนรู้ทุกสิ่งที่คุณอยากเรียน และสามารถทำตามไปพร้อมกันได้เลยครับ"
      ],
      "description": "การเรียนรู้กระบวนการภาพรวมของระบบ Object Detection นั้นเป็นกระบวนการที่พึ่งพาการเรียนรู้เชิงลึก (Deep Learning) โดยที่เน้นไปที่การตรวจจับวัตถุในภาพหรือวิดีโอ โดยมีขั้นตอนหลัก ๆ ดังนี้:\nเตรียมข้อมูล (Data Preparation):\nรวบรวมข้อมูล: รวบรวมภาพหรือวิดีโอที่มีวัตถุที่ต้องการตรวจจับ เพื่อใช้เป็นข้อมูลสำหรับการฝึกโมเดล.\nติดป้าย (Labeling): ติดป้ายบนภาพเพื่อระบุตำแหน่งของวัตถุที่ต้องการตรวจจับ.\nการสร้างและฝึกโมเดล (Model Creation and Training):\nเลือกโมเดล: เลือกโมเดลที่เหมาะสมสำหรับงาน Object Detection ในตัวอย่างนี้เราจะทดลองใช้โฒเดลของทาง Roboflow โดยตรงเพื่อให้ง่ายต่อการใช้งานและทำความเข้าใจ.\nฝึกโมเดล: ใช้ข้อมูลที่เตรียมไว้เพื่อฝึกโมเดล โดยปรับพารามิเตอร์เพื่อให้โมเดลสามารถตรวจจับวัตถุได้อย่างแม่นยำ.\nการทดสอบและประเมิน (Testing and Evaluation):\nทดสอบโมเดล: ใช้ชุดข้อมูลทดสอบที่ไม่ได้ใช้ในขั้นตอนการฝึกโมเดลเพื่อประเมินประสิทธิภาพของโมเดล.\nประเมินโมเดล: ใช้ตัววัดเชิงคุณภาพและเชิงปริมาณ เช่น Precision, Recall, F1 Score เพื่อประเมินประสิทธิภาพของระบบ.\nการปรับแต่งและการพัฒนา (Fine-tuning and Improvement):\nปรับแต่ง (Fine-tuning): ปรับแต่งโมเดลหากต้องการประสิทธิภาพที่ดีขึ้นบนชุดข้อมูลที่ต่างจากชุดข้อมูลที่ใช้ฝึกเดิม.\nพัฒนา: พัฒนาโมเดลหรือเทคนิคใหม่เพื่อปรับปรุงประสิทธิภาพ.\nการนำไปใช้ (Deployment):\nการนำโมเดลไปใช้งาน: นำโมเดลที่ฝึกมาแล้วไปใช้งานในสภาพแวดล้อมจริง.\nการปรับใช้: ปรับแต่งโมเดลหรือระบบตรวจจับเพื่อให้สามารถทำงานได้อย่างเหมาะสมในสถานการณ์ที่ใน.\nการเรียนรู้กระบวนการภาพรวมของระบบ Object Detection เน้นไปที่การค้นพบวัตถุในภาพหรือวิดีโอ และเป็นหนึ่งในแนวทางการประยุกต์ใช้ปัญญาประดิษฐ์ในงานต่าง ๆ เช่น สาธารณูปโภค, การวิเคราะห์ทางการแพทย์, หรือการตรวจสอบความปลอดภัย.",
      "target_audience": [
        "หลักสูตรเรียน ปูพื้นฐานสร้างระบบ AI, Object Detection อย่างมืออาชีพด้วย Roboflow",
        "สามารถเรียนได้ทุกคน ไม่จำเป็นต้องมีความรู้มาก่อน"
      ]
    },
    {
      "title": "Biblioteka OpenCV.",
      "url": "https://www.udemy.com/course/biblioteka-opencv/",
      "bio": "Przetwarzaj obrazy w Pythonie!",
      "objectives": [
        "Podstawy Pythona, Jupyter Notebooka, Google Colabolatory",
        "Biblioteka OpenCV",
        "Obsługa obrazów (ładowanie, wyświetlanie, zapisywanie)",
        "Obsługa video i streamów",
        "Rysowanie elementów na obrazie (linie, tekst, polilinie)",
        "Transformacje obrazu (skalowanie, rotacja, translacja)",
        "Transformacje afiniczna i perspektywiczna",
        "Progowanie - różne rodzaje",
        "Wyrównanie histogramu, CLAHE",
        "Rozmycie i wykrywanie krawędzi",
        "Wykrywanie konturów",
        "Transformata Hougha",
        "Operacje morfologiczne",
        "Użycie klasyfikatora Haara i klasyfikatora HOG",
        "Trenowanie własnego klasyfikatora HOG",
        "Nakładanie obrazów",
        "OCR - optyczne rozpoznawanie znaków",
        "Przetwarzanie wzorców",
        "Pisanie i zastosowanie sieci neuronowej i konwolucyjnej",
        "Zastosowanie gotowych wag i konfiguracji sieci neuronowych",
        "Algorytm YOLO",
        "Nauczysz się trenować własny klasyfikator do detekcji marek samochodów",
        "W projekcie praktycznym dowiesz się, jak wyodrębnić tekst z obrazów - przed Tobą analiza i wykrywanie cyfr z karty kredytowej!"
      ],
      "course_content": {
        "Wstęp do analityki obrazowej": [
          "Wstęp. Instalacja środowiska. Biblioteka OpenCV",
          "Czym jest obraz? Podstawowe operacje na obrazach",
          "Czym jest wideo? Podstawowe operacje na wideo"
        ],
        "Rysowanie podstawowych kształtów": [
          "Linie, prostokąty, wielokąty, tekst"
        ],
        "Transformacje obrazów": [
          "Skalowanie, translacja, rotacja, transformacje: afiniczna i perspektywiczna"
        ],
        "Progowanie": [
          "Progowanie proste, adaptacyjne, Otsu"
        ],
        "Filtrowanie": [
          "Wyrównanie histogramu, CLAHE, wygładzanie i rozmycie obrazów"
        ],
        "Wykrywanie krawędzi": [
          "Operator Sobela i operator Canny'ego"
        ],
        "Wykrywanie konturów": [
          "Transformacja Hougha do wykrywania linii i okręgów"
        ],
        "Operacje morfologiczne na obrazach": [
          "Erozja, dylatacja, otwarcie, domknięcie"
        ],
        "Kaskady Haara": [
          "Detekcja twarzy za pomocą kaskady Haara"
        ],
        "Klasyfikator według metody Histogram of Oriented Gradients (HOG)": [
          "Wykorzystanie HOGa do detekcji osób",
          "Trenowanie własnego klasyfikatora do detekcji marek samochodów"
        ]
      },
      "requirements": [
        "Chęć poznania OpenCV!"
      ],
      "description": "Ludzkie oko jest doskonałe. Przez narząd wzroku codziennie odbieramy multum bodźców z otoczenia. Nasz mózg ma nie lada zadanie – zinterpretować to, co widzimy. To właśnie rozpoznanie i klasyfikacja obrazu mają kluczowy wpływ na rozumienie czerpanych ze świata informacji. A jak widzi komputer? Naukowcy zwykli podglądać i naśladować mechanizmy zachodzące w przyrodzie. Tak oto nasza percepcja wzrokowa stała się pierwowzorem dla wizji komputerowej (computer vision). Ta rewolucyjna dziedzina informatyki jest poświęcona rozumieniu informacji wizualnych przez maszynę.\nPomimo że proste algorytmy przetwarzania obrazów towarzyszą ludzkości od 60 lat, rozwój widzenia komputerowego jest wykładniczy. Dotyczy w szczególności ostatniej dekady. Przełom ten jest związany z coraz większą mocą obliczeniową współczesnych komputerów i kart graficznych. Dzięki zrównoleglaniu złożonych obliczeń postęp technologiczny zrewolucjonizował także uczenie maszynowe i uczenie głębokie. Niegdyś niemożliwe trenowanie sieci neuronowych na podstawie dużej ilości danych wejściowych jest dziś typowym zadaniem. Kamery rejestrujące ruch, autonomiczne samochody, biometria i rozpoznawanie twarzy to już nie science fiction – to część naszej rzeczywistości!\nA więc dowiedz się, jak... widzą maszyny, i nie daj się prześcignąć współczesności!\nPoznaj OpenCV - największą wieloplatformową bibliotekę do przetwarzania obrazów i video. Znajdziesz tu wiele wbudowanych funkcji i algorytmów do analizy ruchu, detekcji obiektów i rozpoznawania gestów. OpenCV przystosowane jest do analizy w czasie rzeczywistym. Z powodzeniem obsługuje głębokie sieci neuronowe, a nawet generatywne sieci współzawodniczące GAN.\nBy nauka była jeszcze przyjemniejsza, wszystkie przykłady zaimplementowano w Pythonie. Według rankingów popularności to najpowszechniej używany język programowania. Mało tego - jest także technologią numer jeden stosowaną przez inżynierów machine learning i data science.\nCo Cię czeka w trakcie naszego profesjonalnego szkolenia?\nPodczas pracy z tym kursem video poznasz takie zagadnienia jak:\nPodstawy Pythona, Jupyter Notebooka, Google Colabolatory\nBiblioteka OpenCV\nObsługa obrazów (ładowanie, wyświetlanie, zapisywanie)\nObsługa video i streamów\nRysowanie elementów na obrazie (linie, tekst, polilinie)\nTransformacje obrazu (skalowanie, rotacja, translacja)\nTransformacje afiniczna i perspektywiczna\nProgowanie - różne rodzaje\nWyrównanie histogramu, CLAHE\nRozmycie i wykrywanie krawędzi\nOperator Sobela i Canny’ego\nWykrywanie konturów\nTransformata Hougha\nOperacje morfologiczne\nUżycie klasyfikatora Haara i klasyfikatora HOG\nTrenowanie własnego klasyfikatora HOG\nNakładanie obrazów\nOCR - optyczne rozpoznawanie znaków\nPrzetwarzanie wzorców\nPisanie i zastosowanie sieci neuronowej i konwolucyjnej\nZastosowanie gotowych wag i konfiguracji sieci neuronowych\nAlgorytm YOLO\nCo więcej...\nNauczysz się trenować własny klasyfikator do detekcji marek samochodów\nW projekcie praktycznym dowiesz się, jak wyodrębnić tekst z obrazów - przed Tobą analiza i wykrywanie cyfr z karty kredytowej!\nBiblioteka OpenCV. Przetwarzaj obrazy w Pythonie! wprowadzi Cię w praktyczne zagadnienia wizji komputerowej. Od podstaw poznasz bibliotekę OpenCV - od najprostszych instrukcji aż po zakres średnio zaawansowany.\nNauczysz się rysować obiekty, wykrywać krawędzie i kontury, a także wykonywać transformacje obrazów (skalowanie, rotację, translację). Za pomocą klasyfikatora Haara przeprowadzisz detekcję twarzy. Następnie płynnie przejdziesz do trenowania własnych klasyfikatorów i pisania sieci konwolucyjnej. Dowiesz się, jak przetwarzać obraz i video z użyciem algorytmów uczenia maszynowego i sztucznej inteligencji. Po ukończeniu szkolenia będziesz w stanie samodzielnie dodać moduł logowania z rozpoznawaniem twarzy na swoją stronę internetową czy wdrożyć algorytm zliczający liczbę aut, które przejeżdżają po drodze. Biblioteka OpenCV. Przetwarzaj obrazy w Pythonie! jest właściwym przewodnikiem zarówno dla kogoś, kto jeszcze nie pracował z obrazami, jak i dla osoby średnio zaawansowanej - pozwoli uporządkować wiedzę i dokonać podsumowania podstawowych możliwości.\nMachine learning engineer\nSpecyfika pracy machine learning engineera z nastawieniem na obraz polega na dużym zróżnicowaniu. Od inżyniera wizji komputerowej wymaga się zdolności analitycznego myślenia, rozwiązywania problemów matematycznych i znajomości bibliotek używanych do obróbki obrazu. Podstawowym zadaniem na tym stanowisku jest tworzenie algorytmów, które będą przetwarzać duże zbiory danych wizualnych. Przydatne są także umiejętności interpersonalne, gdyż często potrzebna jest konsultacja ze specjalistami innych dziedzin.\nZnane koncerny, jak również startupy chętnie wdrażają systemy wizyjne, by zautomatyzować złożone procesy. Ofert pracy stale przybywa, a wizja komputerowa to silnie rozwijająca się branża. Dziedziny, w których jej stosowanie stwarza nowe możliwości, to robotyka, medycyna, astronomia, radiologia, metrologia, sejsmologia, metalurgia i wiele innych.\nSystematyczność i ciężka praca to droga do sukcesu. Nikt nie urodził się wszechwiedzący. Jeśli będziesz poświęcać godzinę dziennie na naukę i samorozwój, prędzej czy później osiągniesz zamierzony cel.",
      "target_audience": [
        "Dla osób chcących poznać OpenCV"
      ]
    },
    {
      "title": "찍어먹도록! 쉽게 이해하는 ChatGPT(AI)",
      "url": "https://www.udemy.com/course/withchatgpt/",
      "bio": "ChatGPT와 함께",
      "objectives": [
        "ChatGPT를 업무에 활용하는 방법을 이해하게 됩니다.",
        "ChatGPT와 스프레드시트로 만드는 업무 자동화 맛보기",
        "ChatGPT와 비즈니스의 미래에 대해 엿볼 수 있습니다.",
        "ChatGPT(AI)의 작동방식을 알게 됩니다."
      ],
      "course_content": {
        "1. ChatGPT(AI)의 이해": [
          "1-1 ChatGPT와 함께 불가능에 맞설 수 있습니다",
          "1-2 ChatGPT에게 질문하는 기술",
          "1-3 AI란 도대체 무엇일까"
        ],
        "2. ChatGPT 활용 1": [
          "2-1. 교안을 만드는 ChatGPT",
          "2-2. 줄거리 만들기(4P 동화)",
          "2-3. 법정에서 쓰이는 서류까지"
        ],
        "3. ChatGPT의 활용 2": [
          "3-1 스프레드시트 이용",
          "3-2 이미지를 만드는 AI - 'DALL-E2' 'Midjourney'",
          "3-3 ChatGPT(AI)를 대하는 태도"
        ]
      },
      "requirements": [
        "구글아이디",
        "ChatGPT를 사용할 수 있는 인터넷 환경",
        "생존하겠다는 의지"
      ],
      "description": "ChatGPT와 함께 살아가야 하는 분들을 위해, 두려워 하지 말고 함께 활용해보자는 의도로 만들었습니다. ChatGPT의 개략적인 이해, ChatGPT의 활용법(질문하는 기술, 스프레드시트와 API, 이미지를 생성하는 AI인 scribblediffusion, DALL-E2, Midjourney) 등을 다루어 봅니다. 부디 업무에서 ChatGPT에 대한 자신감을 갖게 되시길 바라고 도 바랍니다.",
      "target_audience": [
        "ChatGPT, AI시대가 두려운 직장인"
      ]
    },
    {
      "title": "Modelagem de regressão generalizada com R",
      "url": "https://www.udemy.com/course/regressao-generalizada-r/",
      "bio": "Independência para fazer previsões estatísticas!",
      "objectives": [
        "Análises e previsões das relações entre diferentes variáveis.",
        "Construção e execução de modelos lineares e não lineares.",
        "Ajuste e avaliação de performance preditiva.",
        "Adquirir autonomia para modelagem estatística preditiva.",
        "Aplicações dos modelos em diferentes contextos."
      ],
      "course_content": {
        "Apresentação e materiais do curso": [
          "Apresentação do curso e instruções gerais",
          "Instruções de uso dos materiais",
          "Material do curso"
        ],
        "Modelos Lineares": [
          "Conceitos e pressupostos",
          "Pressupostos: testes exploratórios e diagnósticos",
          "Estimativa e interpretação de parâmetros do modelo linear (LM)",
          "Ajuste do modelo LM e interpretação dos resíduos",
          "Distribuições de erros e modelos lineares generalizados(GLM)",
          "GLM Poisson e Quasipoisson: estimativa e interpretação de parâmetros",
          "GLM Binomial Negativo: estimativa e interpretação de parâmetros",
          "GLM Binomial e Quasibinomial: estimativa e interpretação de parâmetros",
          "GLM logístico: estimativa e interpretação de parâmetros",
          "GLMs Gaussian e Gama: estimativa e interpretação de parâmetros",
          "Estimativa de (Pseudo) R² para GLMs",
          "Modelos mistos: efeitos fixos e aleatórios",
          "Modelos mistos: autocorrelação espacial e temporal",
          "Diagnóstico de autocorrelação espacial",
          "Modelos lineares mistos (LME) gaussianos",
          "Modelos mistos generalizados (GLMM): estimativa e interpretação de parâmetros",
          "GLMMs Penalizados: estimativas e interpretações",
          "GLMMs Penalizados com estruturas correlação espacial e temporal",
          "Testes a posteriori (PostHoct): comparação múltipla de médias"
        ],
        "Modelos não lineares": [
          "Modelos não lineares generalizados (GNM)",
          "Modelos generalizados aditivos (GAM): conceitos, parâmetros e diagnósticos",
          "GAM: concurvidade (correlações não lineares)",
          "GAM: estimativa e interpretação dos parâmetros",
          "Modelos aditivos mistos (GAMM) - estimativa e interpretação dos parâmetros"
        ],
        "Seleção de modelos": [
          "Seleção com critério Akaike para modelos lineares",
          "Média dos melhores modelos selecionados com Akaike",
          "Seleção Akaike para modelos não lineares"
        ],
        "Visualizações gráficas de relações lineares e não lineares": [
          "Produção de gráficos para modelos de regressão"
        ]
      },
      "requirements": [
        "É desejável conhecimento básico de R (ex: instalação, uso de pacotes). Porém, o conteúdo é acessível a todos os públicos."
      ],
      "description": "As aplicações de estatística são essenciais para modelar e prever processos, eventos e fenômenos nos mais diversos contextos. Com essas previsões é possível antecipar padrões futuros através do entendimento das relações entre diferentes variáveis, permitindo assim a elaboração de estratégias e planos para tomadas de decisão.\nEsse curso é focado numa abordagem objetiva e direta para execução prática de modelos de regressão (lineares e não lineares) no ambiente de programação do R e interpretações dos seus parâmetros. Através dessa abordagem você irá adquirir autonomia para escolher, ajustar e interpretar modelos de regressão e aplicá-los às suas necessidades analíticas. Dessa forma, você traçará o caminho da independência analítica no emprego desses modelos para ajudar a prever, explicar e solucionar problemas na sua área profissional, seja no setor empresarial ou acadêmico.\nO curso é embasado em sólida literatura de modelagem estatística,  incluindo livros publicados pelo ministrante. Tal conhecimento é transposto e explicado com execução dos modelos no R, agregando assim fundamentação teórica demonstrada na prática. Além disso, são utilizados bancos de dados de amostragens reais, visando tornar o curso ainda mais prático e didático.\nPortanto, esse curso aumentará o seu diferencial competitivo, ampliando suas chances de ocupar bons cargos na sua jornada profissional.",
      "target_audience": [
        "Pesquisadores, professores, pós-graduandos, analistas de dados, analistas de negócio, estatísticos. Empresas e profissionais em geral que desejam enriquecer o currículo."
      ]
    },
    {
      "title": "Beginner's Course to Start Coding with Python (Urdu / Hindi)",
      "url": "https://www.udemy.com/course/beginners-course-to-start-coding-with-python-urdu-hindi/",
      "bio": "Python Seekhiye: Asaan Tareeqay Se Programming Karen",
      "objectives": [
        "Be-Able to understand Business Problems and Pick Programming Concepts Wisely.",
        "Develop fundamental understanding of Python syntax and data structures.",
        "Understand Problem Solving Techniques to implement solutions.",
        "Gain confidence in writing and executing Python code independently."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Installing Python and Anaconda Environment",
          "Setting up Jupyter Notebook"
        ],
        "Basic Python Concepts": [
          "Writing basic code in Python",
          "Working with various Data types in Python",
          "Assigning Values to Variables and Operations in Python"
        ],
        "Intermediate Operations and Structures (Multiple Items)": [
          "Manipulating Strings in Python",
          "Storing Multiple Items in Python with Lists",
          "Remembering Multiple Attributes of an Object in Python",
          "Intermediate Python Features(Test)"
        ],
        "Implementing Advance Logics and Flows": [
          "Working with Conditional Logic Flow in Python",
          "Repeating Tasks in Python with Loops",
          "Creating Mini Programs with Functions and Writing Larger Code Blocks in Python",
          "Advance Logics & Flows (Test)"
        ]
      },
      "requirements": [
        "No programming experience needed. You will learn everything as a Non-CS Student / Professional looking to get Started into Code"
      ],
      "description": "English\n\nEmbark on your journey into the world of programming with Python through this comprehensive beginner's course. Designed for Urdu/Hindi speakers, you'll learn essential Python concepts, from writing basic code to working with various data types and structures. Dive into manipulating strings, storing data with lists, and implementing conditional logic flows. By the end, you'll be equipped to create mini programs, write larger code blocks, and confidently take your first steps in Python programming.\n\n\nSection 1: Introduction\nLecture 1: Introduction\nSection 2: Basic Python Concepts\nLecture 2: Writing basic code in Python\nLecture 3: Working with various Data types in Python\nLecture 4: Assigning Values to Variables and Operations in Python\nSection 3: Intermediate Operations and Structures\nLecture 5: Manipulating Strings in Python\nLecture 6: Storing Multiple Items in Python with Lists\nLecture 7: Remembering Multiple Attributes of an Object in Python\nSection 4: Implementing Advance Logics and Flows\nLecture 8: Working with Conditional Logic Flow in Python\nLecture 9: Repeating Tasks in Python with Loops\nLecture 10: Creating Mini Programs with Functions and Writing Larger Code Blocks in Python\n\n\nEnroll to learn Programming fundamentals and Coding techniques to solve business Problems.\n\n\nRegister Now and Change your Life with Programming. Thanks",
      "target_audience": [
        "Beginners in Python, Interested to develop rigor in coding, and getting to start with Data Science Track"
      ]
    },
    {
      "title": "Data Wrangling mit dem Tidyverse",
      "url": "https://www.udemy.com/course/data-wrangling-mit-dem-tidyverse/",
      "bio": "Lerne die nötigen Skills um deine Datensätze zu bändigen",
      "objectives": [
        "Data Wrangling mit den R Packeten aus dem Tidyverse"
      ],
      "course_content": {
        "Einleitung": [
          "Willkommen!"
        ],
        "Quick Refresher zu R Studio und R": [
          "Nutzung von R Studio",
          "Vektoren",
          "Listen",
          "Data Frames"
        ],
        "Basic Tidyverse (dplyr)": [
          "Basic dplyr operationen mit einem Datensatz",
          "Der split-apply-combine Workflow",
          "Mergen von Daten"
        ],
        "Restrukturieren von Datensätzen mit tidyr": [
          "Tidyr: Long und Wide Formate von Daten"
        ],
        "Das Stringr package": [
          "Exkurs zu einigen nützlichen stringr Funktionen"
        ],
        "Intermediate Tidyverse": [
          "Filtern von mehreren Spalten",
          "Fortgeschrittenes Selektieren und Berechnen von Spalten",
          "Zusammenfassen mehrerer Spalten"
        ],
        "Functional Programming - Einführung in das purrr Paket": [
          "Purrr Basics",
          "Varianten von map",
          "Abfangen von Fehlern",
          "Reduce und Accumulate",
          "Projekt 3 - Arbeiten mit Sales Daten von Geschäften Teil 1",
          "Projekt 3 - Arbeiten mit Sales Daten von Geschäften Teil 2"
        ],
        "Advanced Data Wrangling mit dem tidyverse": [
          "Non Standard Evaluation und Funktionen",
          "Eigene Funktionen mit dplyr"
        ]
      },
      "requirements": [
        "Grundkenntnisse in R"
      ],
      "description": "Willkommen zu diesem R-Kurs, in dem du eine umfassende Einführung in die mächtige Programmiersprache R und das Tidyverse erhältst. R ist eine kostenlose und vielseitige Sprache für Statistik und Data Science, die in den 1990er Jahren entwickelt wurde und mittlerweile weit verbreitet in der akademischen Welt und der Industrie ist. Sie ist besonders gut geeignet für die Analyse von großen und komplexen Datenmengen und bietet zahlreiche Funktionen und Bibliotheken für die Datenvisualisierung.\nDas Tidyverse ist eine Sammlung von Libraries, die das Data Wrangling mit R vereinfachen und das Arbeiten mit Daten deutlich angenehmer gestalten. Du wirst lernen, wie du die wesentlichen Funktionen dieser Libraries anwendest und dich Schritt für Schritt damit vertraut machen. Am Ende des Kursed wendest du dein neues Wissen in einem Projekt an und erhältst zusätzlich eine Einführung in das Schreiben von eigenen Funktionen mit dplyr und dem Konzept von \"Non Standard Evaluation\".\nMit dem erworbenen Wissen wirst du in der Lage sein, große und komplexe Datensätze problemlos in R zu verarbeiten und dich mit dem Tidyverse vertraut zu machen. Du wirst die Fähigkeiten erwerben, die du benötigst, um erfolgreich in der Welt der Data Science und Statistik zu bestehen.\nIch wünsche dir viel Vergnügen beim Einstieg in die Welt von Data Science!",
      "target_audience": [
        "R Nutzer, die bereits Grundkenntnisse in R haben und sich mit dem Tidyverse vertraut machen wollen"
      ]
    },
    {
      "title": "Machine Learning con Python",
      "url": "https://www.udemy.com/course/machine-learning-python/",
      "bio": "Impara come usare NumPy, Pandas, Matplotlib, Plotly, Scikit-Learn per creare algoritmi di Machine Learning in Python",
      "objectives": [
        "Machine Learning",
        "Python",
        "Programmazione"
      ],
      "course_content": {
        "Introduzione al corso": [
          "Introduzione al corso"
        ],
        "Download e installazione software": [
          "Download Anaconda",
          "Installazione Anaconda",
          "Panoramica Anconda Navigator"
        ],
        "Primo programma con Python": [
          "Primo programma con Python"
        ],
        "Operatori matematici": [
          "Operatore somma",
          "Operatore differenza",
          "Operatore moltiplicazione",
          "Operatore divisione"
        ],
        "Controlli condizionali": [
          "IF ELSE",
          "IF ELIF ELSE"
        ],
        "Dati in input": [
          "Dati in input (Parte I)",
          "Dati in input (Parte II)"
        ],
        "Costrutti iterativi": [
          "Ciclo FOR (Parte I)",
          "Ciclo FOR (Parte II)"
        ],
        "Funzioni": [
          "Introduzione alle funzioni",
          "Primo programma con funzioni",
          "Secondo programma con funzioni"
        ],
        "Jupyter Notebook": [
          "Introduzione Jupyter Notebook",
          "Creazione primo Notebook in Jupyter",
          "Comandi di Markdown (Parte I)",
          "Comandi di Markdown (Parte II)",
          "Comandi di Markdown (Parte III)"
        ],
        "Libreria Matplotlib": [
          "Introduzione Matplotlib",
          "Sottomodulo PyPlot",
          "Metodo Scatter",
          "Introduzione libreria Numpy",
          "Comando Marker",
          "Comando Linestyle",
          "Aggiunta di Label",
          "Subplot di grafici",
          "Grafici a barre",
          "Grafici a torta"
        ]
      },
      "requirements": [
        "Voglia di imparare"
      ],
      "description": "Benvenuti nel corso di Machine Learning con Python.\nQuesto corso è dedicato a chiunque voglia imparare da subito le principali tecniche ed algoritmi del Machine Learning.\nInizieremo il corso installando insieme l’ambiente di lavoro.\nSubito dopo cominceremo a sporcarci le mani ed impratichirci con il linguaggio di programmazione Python in modo che anche ci non abbia mai programmato con tale tecnologia sarà in grado di costruire algoritmo di apprendimento automatico.\nSuccessivamente vedremo insieme le principali librerie utilizzate per il Machine Learning.\nDurante il corso analizzeremo in maniera teorica e pratica i due principali tipi di apprendimento:\nApprendimento supervisionato.\nApprendimento non supervisionato\nPrima di affrontare tali apprendimenti, analizzeremo una sezione dedicata ai dataset e alla loro manipolazione.\nPer quanto riguarda l’apprendimento supervisionato analizzeremo:\nRegressione Lineare;\nRegressione Logistica;\nSupport Vector Machine (SVM).\nNella parte finale del corso passeremo alla seconda categoria di apprendimento: l'apprendimento non supervisionato in cui affronteremo il problema del clustering, ovvero come creare automaticamente dei gruppi di dati riconoscendo delle features condivise all'interno del dataset; a questo scopo studieremo l'algoritmo K-Means (algoritmo di clustering più diffuso.\nSe stai cercando un corso di Machine Learning con il linguaggio di programmazione Python questo è il corso che fa per te.\nTi aspetto per affrontare insieme il fantastico mondo dell'apprendimento automatico.",
      "target_audience": [
        "Docenti",
        "Studenti",
        "Lavoratori",
        "Sviluppatori curiosi di approfondire la data science",
        "Sviluppatori curiosi di approfondire il Machine Learning"
      ]
    },
    {
      "title": "【初心者向け】ローカルLLMを使ってRAGを導入したチャットアプリを作ろう！いくら使っても無料のAI環境を手に入れよう！",
      "url": "https://www.udemy.com/course/local-llm/",
      "bio": "ローカルLLMを使ってRAGが導入された本格的なチャットボットを作ろう！ローカルLLM実行ツールであるOllamaとPythonを使ってローカル環境にいくら実行しても無料のAI環境を作ろう！",
      "objectives": [
        "ローカルLLMの使い方",
        "ローカルLLM実行ツール「Ollama」の使い方",
        "Pythonの基礎",
        "Streamlitを使ったWebアプリの作り方",
        "ローカルLLMを用いたRAGの実装方法"
      ],
      "course_content": {
        "紹介": [
          "イントロダクション"
        ],
        "ローカルLLMとは？": [
          "ローカルLLMとは？",
          "ローカルLLMのモデルの種類",
          "ローカルLLMをインストールして使ってみよう！"
        ],
        "Pythonの使い方": [
          "Pythonを学ぼう！",
          "Pythonの実行環境",
          "Google Colabの使い方",
          "演算子の種類を学ぼう！",
          "Pythonの変数と型を学ぼう！",
          "list型(リスト)を学ぼう！",
          "dict型(辞書)を学ぼう！",
          "print関数を学ぼう！",
          "文字列の操作方法を学ぼう！",
          "条件分岐のif文を学ぼう！",
          "繰り返し処理のfor文を学ぼう！",
          "関数の作り方と使い方を学ぼう！",
          "Python構文の復習"
        ],
        "Ollamaのサーバーを立ち上げてローカルLLMをPythonから呼び出して動かしてみよう！": [
          "Ollamaのサーバーを立ち上げてみよう！",
          "VScodeの準備とPythonの準備",
          "PythonプログラムからローカルLLMを動かそう！"
        ],
        "ChatUIを作ってアプリケーション上からローカルLLMとやり取りをしてみよう！": [
          "サイドバーでモデルやパラメータを指定できるようなUIを作っていこう！",
          "ユーザーのプロンプトに対して回答を表示できるようにしていこう！",
          "session_stateを使って会話履歴を保存して表示しよう！",
          "LLMが会話の履歴から回答してくれるようにしよう！",
          "Stream出力について学ぼう！",
          "逐次的にStream出力で返答を返してくれるUIを実装していこう！"
        ],
        "RAGをローカルLLMに適用してみよう！": [
          "ベクトル化するためのモデルをインストールしよう！",
          "RAGとは？",
          "ChromaDBのセットアップ",
          "ベクトル化関数とドキュメント読み込み関数を作っていこう！",
          "テキストをチャンクに分ける処理をWordファイルアップロード処理を書いていこう！",
          "ファイルをベクトル化してDBに保存する処理を書いていこう！",
          "RAGのメインの処理を書いていこう！",
          "プログラムを完成させてRAGを実行してみよう！",
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "Pythonの基礎から学びますのでプログラミングの知識は特に必要ありません"
      ],
      "description": "このコースでは、初心者でもゼロから始められるローカル大規模言語モデル（LLM）を活用したチャットアプリ開発を学びます。\n\nクラウドに依存せず、自分のPC上で動く安全・高速なAIチャットを構築し、最終的にはRAG（Retrieval-Augmented Generation）を導入して本格チャットボットを完成させます。\n\n\n具体的には以下の流れで学んでいきます。\n\n\n1. 環境構築：Ollamaをインストールし、Llamaモデルをローカル実行\n2. Python実装：ターミナルからPythonコードへの移行\n3. UI開発：StreamlitでチャットボットWebアプリを作成\n4. RAG導入：Embeddingsモデルでドキュメント検索を組み込み、回答精度を強化\n\n\nローカルLLMと最新AI技術を組み合わせて、あなただけのオリジナルチャットボットを作ってみましょう！",
      "target_audience": [
        "ローカルLLMの使い方を知りたい方",
        "ChatGPTのようなAIチャットを自分のPC上で動かしてみたい方",
        "RAGを使ったチャットボットの実装方法を基礎から学びたい方",
        "API利用料や通信制限に縛られないローカルAI環境を手に入れたい方"
      ]
    },
    {
      "title": "Python ile Piyasa Analiz Eğitimi ve Algo Trade Robot Projesi",
      "url": "https://www.udemy.com/course/python-ile-kripto-ve-hisse-analizi-baslangctan-uzmanlga/",
      "bio": "Python ile Finansal Veri Analizi ve Ticaret Stratejileri",
      "objectives": [
        "Python ile finansal veri analizinin temellerini öğrenerek, hisse ve kripto verilerini etkili bir şekilde analiz edebileceksiniz.",
        "Gerçek veriler üzerinde veri temizleme ve düzenleme tekniklerini uygulayarak veri hazırlama becerilerinizi geliştireceksiniz.",
        "Teknik analiz göstergeleri olan RSI, MACD ve Bollinger Bantları gibi araçları hesaplayarak trend yorumlama yeteneğinizi artıracaksınız.",
        "Algoritmik ticaret stratejilerini sıfırdan oluşturmayı ve bu stratejilerin performansını analiz etmeyi öğrenerek pratik yapacaksınız.",
        "PyPortfolioOpt gibi popüler Python kütüphaneleriyle portföy optimizasyonu yapmayı ve yatırım stratejilerinizi daha verimli hale getirmeyi başaracaksınız.",
        "Pandas ve matplotlib kullanarak finansal verileri görselleştirip, raporlar oluşturma ve sunum yapma becerilerinizi güçlendireceksiniz.",
        "Zaman serisi analizi ile verilerdeki trend ve otokorelasyonları keşfederek geleceğe yönelik analizler yapabileceksiniz.",
        "Finansal verilerle çalışmanın zorluklarını ve bunları aşmanın yöntemlerini öğrenerek, veri odaklı projelerde uzmanlaşacaksınız."
      ],
      "course_content": {
        "Giriş": [
          "Giriş",
          "Python Diline Giriş, Veri Türleri, Değişkenler Veri Tipleri, Matematik İşlemler"
        ],
        "Kontrol Yapıları ve Veri Yapıları": [
          "Koşullu ifadeler: if, else, elif,Döngüler: for ve while,Liste ve tuple kullanımı"
        ],
        "Fonksiyonlar": [
          "Fonksiyon tanımlama ve çağırma,Parametreler ve dönüş değerleri,Lambda fonksiyon",
          "Alıştırmalar ve Ödevler"
        ],
        "Python Modülleri ve Paketleri": [
          "Import ile modüllerin kullanımı ve örnekler",
          "Pandas ve Dataframe mantığı ve Excel dosyaları analizleri",
          "Uygulama ve Ödev"
        ],
        "Matplotlib ve Seaborn ile Grafikler Hazırlama": [
          "Matplotlib ile Hisse ve Kripto Ürünlerin Grafiklerinin Hazırlanması",
          "Çubuk Grafiği (Bar Plot): Kripto Para Günlük İşlem Hacmi",
          "Dağılım Grafiği (Scatter Plot): Kripto Para Getiri ve Volatilite İlişkisi",
          "Özelleştirilmiş Line Plot: Hisse Senedi Fiyatları Karşılaştırması",
          "Seaborn ile İleri Görselleştirme: Kripto Para İstatistikleri"
        ],
        "Finansal Verileri Pandas ile İşleme: Zaman Serisi, Filtreleme Hareketli Ortalama": [
          "Finansal Verileri Pandas ile İşleme: Zaman Serisi, Filtreleme Hareketli Ortalama"
        ],
        "Yfinance,Binance Üzerinden Hisse Senedi ve Kripto Verilerini Çekerek Analizler": [
          "Yfinance ve ccxt modüllerine giriş",
          "Hisse senedi ve kripto para verilerini çekme, Veri Analizi,Veri Ayıklama"
        ],
        "İleri Zaman Serisi Analizi Tahmin ve Trend Analizleri": [
          "AutoRegressive Integrated Moving Averag Metodu ile Zaman Serisi Modelleme",
          "Otokorelasyon ve trend analizi ve basit regresyon uygulamaları"
        ],
        "Teknik Analiz Göstergeleri Anlamları ve Kodlama": [
          "RSI (Relative Strength Index) İndikatörü ve RSI Hesaplama Adımları Kodlaması",
          "MACD (Moving Average Convergence Divergence) Yorumlama Sinyal Mantığı Kodlama",
          "Bollinger Bantları Mantığı Analiz Çıktıları Kodlaması"
        ],
        "Portföy Optimizasyonu Projesi": [
          "PyPortfolioOpt kütüphanesini kullanarak portföy optimizasyonu uygulaması kodlama"
        ]
      },
      "requirements": [
        "Kursa katılmak için herhangi bir programlama bilgisine gerek yoktur. Temel bilgisayar kullanım bilgisi yeterlidir. Python ve ilgili araçları öğrenmeye açık olmanız yeterli."
      ],
      "description": "Öğrenciler, veri analizinin yanı sıra finansal verilerin tarihsel performansını analiz ederek yatırım kararlarını nasıl daha bilinçli bir şekilde alabileceklerini öğrenecekler. Gerçek hayat senaryolarına dayalı örneklerle, piyasa trendlerini nasıl tespit edeceklerini ve bu trendlerden nasıl yararlanabileceklerini keşfedecekler. Ayrıca, zaman serisi analizleri ve trend analizleri gibi ileri düzey konularla, piyasaların geçmiş verilerini kullanarak gelecekteki hareketlerini tahmin etmeye yönelik teknikler hakkında bilgi sahibi olacaklar.Kurs sırasında, öğrenciler veri temizleme ve düzenleme tekniklerini kullanarak verilerin analiz için nasıl optimize edileceğini öğrenirler. Bu sayede, yatırım stratejilerinin doğruluğunu ve etkinliğini artırmak için verilerin en iyi şekilde hazırlanması sağlanır. Ayrıca, kurs, finansal piyasaların değişkenliğini ve riskini ölçen göstergeleri hesaplama ve bu göstergelerin yorumlanmasını içerir. Öğrenciler, gerçek hayat verileriyle çalışarak, öğrendiklerini pratikte test etme ve geliştirme imkanı bulurlar. Kursun sonunda, yatırımcıların bilinçli ve veriye dayalı kararlar almasını destekleyen güçlü analiz yetenekleri kazanmış olacaklar\nKurs boyunca, finansal verilerin güvenli ve verimli bir şekilde nasıl saklanacağını ve analiz edileceğini gösteren pratik bilgiler de paylaşılacak. Öğrenciler, Python'da algoritmik ticaret sistemleri geliştirmeye yönelik en iyi uygulamaları öğrenecek ve bu sistemleri gerçek piyasa koşullarına nasıl uyarlayacaklarını anlayacaklar. Eğitim, portföy optimizasyonu ve risk yönetimi ile desteklenerek, katılımcıların sağlam ve sürdürülebilir yatırım stratejileri oluşturmasına yardımcı olacak.Ayrıca alacağınız eğitim ile finansal analiz becerilerinizi geliştirerek bilinçli yatırım kararları almanızı sağlayacaktır.",
      "target_audience": [
        "Finansal analiz, veri bilimi ve yatırım stratejilerine ilgi duyanlar.",
        "Algoritmik ticaret ve veri analizi konularına başlangıç seviyesinde veya daha ileri düzeyde meraklı bireyler.",
        "Kendi yatırım stratejilerini geliştirmek isteyen Python öğrenicileri ve finansal teknolojiye ilgi duyanlar."
      ]
    },
    {
      "title": "ปลดล็อคความเข้าใจการวิเคราะห์ Big Data",
      "url": "https://www.udemy.com/course/big-data-g/",
      "bio": "ทุกอย่างที่จำเป็นต้องรู้เกี่ยวกับ Big Data Analytics และ Technologies เพื่อการประมวลผลข้อมูลจำนวนมหาศาล",
      "objectives": [
        "เข้าใจที่มาและหลักการของ \"Big Data\" และการปฏิวัติอุตสาหกรรม 4.0 เกี่ยวข้องกับ Big Data อย่างไร",
        "เข้าใจว่าเทคโนโลยี IoTs เข้ามามีบทบาทอย่างไรกับการเกิดขึ้นของ Big Data",
        "เรียนรู้ว่าทำไมการวิเคราะห์ข้อมูลจำนวนมหาศาล (Big Data Analytics - BDA) จึงต้องใช้ Machine Learning (ML) และ Cloud Computing (CC) เป็นส่วนประกอบสำคัญ",
        "ทบทวนการจัดการข้อมูลในองค์กร เราทำกันมาอย่างไร",
        "เติมเต็มหลักพื้นฐานของแนวคิดในการประมวลผลและวิเคราะห์ข้อมูล เบื้องหลังระบบนิเวศน์ของ Big Data Architecture และฐานข้อมูลสำหรับ Big Data",
        "เห็นภาพกว้างด้านนวัตกรรมทคโนโลยีการประมวลผลข้อมูลจำนวนมหาศาล เช่น DFS, Hadoop, MapReduce, YARN และ Ecosystem ตัวหลักอื่นๆ",
        "เรียนรู้การประมวลผลข้อมูลแบบ Real-Time Stream Processing รวมถึงแพลตฟอร์ม Stream Analytics ในเชิงพานิชย์และระบบประมวลผลข้อมูลสตรีม",
        "เข้าใจการจัดการ Big Data ในองค์กรขนาดใหญ่ด้วย Enterprise Data Lake และหลักการสถาปัตยกรรมแลมบ์ดา (Lambda Architecture - LA)",
        "เพิ่มความเข้าใจเกี่ยวกับงานด้านวิทยาศาสตร์ข้อมูล (Data Science)",
        "เรียนรู้ Case Study #1 หลักการค้นหาความหมายจากข้อมูลความคิดเห็นของประชาชน หรือ Text Mining",
        "เรียนรู้ Case Study #2 แอพพลิเคชั่น BDA ที่เกี่ยวกับ IoT",
        "เรียนรู้ Case Study #3 วิเคราะห์การใช้เว็ปแอพพลิเคชั่นจากข้อมูล Logs ขององค์กรขนาดใหญ่",
        "เรียนรู้หลากหลายตัวอย่างแนวทางการประยุกต์ใช้งาน BDA ที่แทรกให้ระหว่างบท",
        "และยูนิตพิเศษปิดท้าย เพิ่มความเข้าใจเกี่ยวกับ งาน ทักษะ และการศึกษา ในโลกที่กำลังเปลี่ยนไปเป็นยุคอัตโนมัติ (Skills and Education in a World of Automation)"
      ],
      "course_content": {},
      "requirements": [
        "หลักสูตรทั้งหมดเป็นการบรรยายเพื่อปูพื้นฐานความรู้ความเข้าใจเกี่ยวกับโลกการทำงานกับข้อมูลจำนวนมหาศาล ไม่จำเป็นต้องจบการศึกษาด้านไอทีหรือมีประสบการณ์การเขียนโปรแกรมมและไม่ต้องมีประสบการณ์ในการวิเคราะห์ข้อมูล หรือมีความรู้ด้านสถิติศาสตร์มาก่อน สิ่งที่ต้องการคือความรู้ด้านคอมพิวเตอร์ขั้นพื้นฐาน"
      ],
      "description": "ปลดล็อคความเข้าใจในเรื่องการนำข้อมูลจำนวนมหาศาลมาทำการวิเคราะห์ หรือ Big Data Analytics (BDA) เพิ่มเติมความรู้ของคุณเพื่อสร้างความพร้อมสำหรับการทำงานในยุคของข้อมูลจำนวนมหาศาล ที่มีบทบาทสำคัญกับทุกองค์กรและมีผลอย่างสูงต่อความอยู่รอดของธุรกิจทุกวันนี้ หลักสูตรนี้แบ่งออกเป็น 6 Units\nUnit 1 – Introduction to Big Data: ข้อมูลดิจิตอลมีมานานสักพักแล้วพร้อมๆกับการเกิดขึ้นของระบบคอมพิวเตอร์ แล้วอะไรที่แบ่งแยกว่าข้อมูล Big และต้องใหญ่ขนาดไหนจึงจะเรียกว่า Big ข้อมูลพวกนี้เกิดขึ้นได้อย่างไร สำคัญอย่างไร และมีข้อมูลกี่ชนิดในยุคดิจิตอลนี้ เราจะกล่าวถึงหลักการของ \"Big Vs\"และคุยกันเกี่ยวกับ IoTs มามีบทบาทอย่างไรในการเกิดขึ้นของ Big Data\nUnit 2 – การวิเคราะห์ข้อมูลจำนวนมหาศาล (Big Data Analytics = Machine Learning + Cloud Computing): กล่าวถึงพื้นฐานระบบไอทีและกระบวนการที่ต้องมีเพื่อการพัฒนา BDA เช่น Machine Learning และ Cloud Computing เพื่อที่เราจะได้เข้าใจหลักพื้นฐานของแนวคิดในการประมวลผลและวิเคราะห์ รวมถึง Core Concepts เบื้องหลังระบบนิเวศน์ เช่น Big Data Architecture และ เทคนิคฐานข้อมูลสำหรับ Big Data\nUnit 3 – เทคโนโลยีสำหรับบิ๊กดาต้า (Big Data Technologies): การที่จะทำงานกับ Big Data ได้อย่างมีประสิทธิภาพ จำเป็นต้องมีเทคโนโลยีที่มาประสานการทำงานกันในการจัดการข้อมูล Big Data โดยจะเริ่มจาก Google file system (GFS) เจ้าของนวัตกรรมทคโนโลยีอย่าง Distributed File System (DFS) จากนั้นจะพูดถึง Hadoop, MapReduce และ Ecosystem ตัวหลักอื่นๆของ Hadoop\nUnit 4 – การประมวลผลข้อมูลแบบเร็วเท่าเวลาจริง (Real-Time Stream Processing): เรื่องของไอที (Information Technology – IT) เป็นเรื่องที่เปลี่ยนแปลงตลอดเวลาไม่หยุดนิ่ง หลายปีก่อนเราไม่เคยพูดถึงการจัดการฐานข้อมูล หรือการพัฒนาระบบคอมพิวเตอร์ที่ผลิตข้อมูลเร็วแบบเท่าเวลาจริง หรือ real-time แต่วันนี้เราพูดถึงระบบคอมพิวเตอร์และการวิเคราะห์ข้อมูลแบบ real-time กันแล้ว หน่วยนี้จึงว่าด้วยเรื่อง การประมวลผลข้อมูลแบบเร็วเท่าเวลาจริง (Real-Time Stream Processing)\nUnit 5 - การจัดการบิ๊กดาต้าในองค์กรขนาดใหญ่ (Enterprise Data Lake): ข้อมูลคือสิ่งที่สำคัญมากสำหรับทุกหน่วยงาน รวมทั้งบริษัทขนาดกลางถึงใหญ่ (enterprise) บริษัท enterprise รวบรวมข้อมูลเอาไว้ได้มากมายจริงๆ เราจะกล่าวถึงหลักการพื้นฐานในการจัดการข้อมูล Big Data ขององค์กรโดยการสร้าง Enterprise Data Lake และใช้แนวคิดสถาปัตยกรรมแลมบ์ดา (Lambda Architecture - LA)\nUnit 6 – จากบิ๊กดาต้าสู่ข้อมูลอันชาญฉลาด (Big Data to Smart Data): คอร์สสุดท้ายเป็นหัวข้อที่สำคัญที่สุดเลยก็ว่าได้ ถ้าไม่มีขั้นตอนนี้ก็จะไม่ครบกระบวนการของเรื่อง Big Data Analytics (BDA) ทำความเข้าใจงานด้านวิทยาศาสตร์ข้อมูล (Data Science) และกรณีศึกษา Big Data และแนวทางการประยุกต์ใช้งาน นอกจากนี้ยังมียูนิตพิเศษปิดท้าย เพิ่มความเข้าใจให้คุณเกี่ยวกับ งาน ทักษะ และการศึกษา ในโลกที่กำลังเปลี่ยนไปเป็นยุคอัตโนมัติ (Skills and Education in a World of Automation)",
      "target_audience": [
        "เหมาะสำหรับผู้เริ่มต้นและบุคคลทั่วไป และหลักสูตรนี้ยังเหมาะกับผู้บริหาร นักกลยุทธ์ นักวิเคราะห์ ฝ่ายนโยบาย/วางแผน และผู้ปฏิบัติงานทุกระดับที่สนใจหาความรู้ในการนำเทคโนโลยีด้านข้อมูลสมัยใหม่มาเพิ่มประสิทธิภาพการวางแผนงาน และช่วยในการตัดสินใจให้มีประสิทธิภาพมากขึ้น"
      ]
    },
    {
      "title": "Generative AI for Python Backend Developers",
      "url": "https://www.udemy.com/course/generative-ai-for-python-backend-developers/",
      "bio": "Learn how to integrate Generative ai in your backend projects and tasks easily",
      "objectives": [
        "A Practical Understanding of Generative AI: Learn the core concepts behind large language models (LLMs), vector databases, agents, and how these tools can be us",
        "Hands-On Backend Integration Skills: Master the process of connecting AI models and tools to web backends using frameworks like Flask and",
        "Real-World Project Experience: Build and deploy actual backend features such as chatbots, permission-based data retrieval, and automated document data extractio",
        "Modern Backend Tooling: Work with Docker, Celery, and WebSockets to create scalable AI services",
        "Security and Permissions: Implement robust authentication, authorization, and data privacy measures in AI-enabled systems.",
        "End-to-End Development Workflow: From Python basics to advanced backend operations, students will learn how to design and build AI-powered backend solu"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Course Notes",
          "Course Resources"
        ],
        "Classical Machine Learning & Deep Learning Crash Course": [
          "Classical Machine Learning",
          "Q&A",
          "Q&A 2",
          "Deep Learning",
          "Q&A"
        ],
        "Generative AI Intro": [
          "Generative Intro",
          "Mlops Intro",
          "Data Roles",
          "Math in Ml Field",
          "Transformers & LLM",
          "Choose LLM",
          "Ollama Basics"
        ],
        "Streamlit Basics": [
          "Streamlit part 1",
          "Streamlit Part 2",
          "Q&A"
        ],
        "ORMs , Databases & Vector Databases": [
          "Python ORM",
          "SQl Alchemy Part 1",
          "SQL Alchemy Part 2",
          "SQL Alchemy Part 3",
          "Vector DB",
          "Q&A",
          "New Updates"
        ],
        "Projects": [
          "Streamlit Project 1",
          "Streamlit Project 2",
          "Flask Langchain Chat",
          "Custom Stream Component Project",
          "Q&A"
        ],
        "Flask Basics": [
          "Flask Intro",
          "Flask Books App Part1",
          "Flask Books Search",
          "Flask Celery & Redis",
          "Flask Web Sockets & Notification Project",
          "Advice about Work",
          "Flask Web Socket Chat App Project",
          "Using AI as Developer",
          "Flask API"
        ],
        "FastAPI Basics": [
          "FastAPI Basics",
          "FastAPI Books Store Part 1",
          "FastAPI Part 2",
          "Q&A",
          "FastAPI Part 3",
          "FastAPI Web Sockets Projects"
        ],
        "Langchain Basics": [
          "Pydantic Basics",
          "Langchain Intro",
          "Langchain with Ollama & Langsmith",
          "Langchain Part 3",
          "Document Loaders",
          "Memory",
          "Q&A",
          "Callbacks"
        ]
      },
      "requirements": [
        "Basic Python knowledge (data types, loops, functions).",
        "No previous experience with AI, Flask, or FastAPI required—everything is taught from scratch"
      ],
      "description": "Unlock the power of Generative AI in real-world backend projects with this comprehensive, hands-on course designed for backend developers seeking to bridge the gap between AI and backend systems. Whether you’re a Python developer, backend engineer, or aspiring full-stack data scientist, this course will equip you with the practical skills and deep understanding needed to integrate Generative AI tools and concepts into production-ready backend solutions.\nWhat You’ll Learn\nCore Concepts of Generative AI: Understand the fundamentals of large language models (LLMs), vector databases, agents, and data handling, with a focus on practical backend integration.\nAI-Backend Integration: Learn how to connect AI models and tools (like Hugging Face) to web backends using frameworks such as Flask and FastAPI.\nReal-World Use Cases: Tackle real backend challenges, including user management, permissions, and secure data access in AI-powered systems.\nModern Backend Stack: Master Docker, Celery, and WebSockets for scalable, production-grade AI services.\nCustom AI Solutions: Build and extend chatbots, automate data extraction, and implement advanced features like streaming responses and permission-based data retrieval.\nProject-Based Learning: Apply your knowledge through real-life projects and case studies inspired by actual industry needs, not just toy examples.\nCourse Features\n40+ Hours of Interactive, Live Instruction: Delivered over Zoom with recorded sessions available for review, ensuring you never miss a detail.\nPractical Workshops & Open Discussions: Regular live sessions dedicated to solving real-world cases, brainstorming solutions, and collaborative coding.\nComprehensive Notebooks & Resources: All theoretical and practical content is provided as interactive notebooks, hosted on GitHub.\nAccessible for All Levels: Starts from Python basics, with step-by-step introductions to Flask, FastAPI, and all AI tools—no prior AI or advanced math required.\nEnd-to-End Backend Coverage: Covers everything from foundational backend operations to advanced AI integration, including API design, database management (SQL and vector DBs), and deployment with Docker.\nCutting-Edge Tools: Work with open-source LLMs, LangChain, Streamlit, and more, focusing on cost-effective, scalable solutions.\nReal-World Security & Permissions: Learn how to implement authentication, authorization, and data privacy in AI-enabled backend systems.",
      "target_audience": [
        "Backend Developers who want to integrate Generative AI into their projects and build real-world, production-ready AI features.",
        "Python Developers aiming to expand their skillset into AI-powered backend systems.",
        "Full-Stack Developers interested in mastering the backend aspects of AI applications, including APIs, databases, and security.",
        "Technical Enthusiasts and Career Changers who want to gain practical, job-ready experience with Generative AI and backend integration, regardless of prior AI experience."
      ]
    },
    {
      "title": "Grafana - Wizualizacja danych w czasie rzeczywistym",
      "url": "https://www.udemy.com/course/grafana-wizualizacja-danych-w-czasie-rzeczywistym/",
      "bio": "Wizualizacja, Monitoring, Dane czasowe, Integracja",
      "objectives": [
        "Zapoznasz się z całym ekosystemem związanym z monitoringiem (Grafana, Docker Compose, Prometheus)",
        "Poznasz wszystkie elementy interfejsu użytkownika Grafany",
        "Opanujesz techniki pracy z dashboardami, w tym panel rows (grupowanie wykresów)",
        "Nauczysz się zarządzać wykresami poprzez takie funkcje jak przegląd, edycja, udostępnianie i eksploracja",
        "Dowiesz się, czym jest i jak stosować wersjonowanie dashboardów",
        "Przyjrzysz się formatowaniu wykresów, w tym nadpisywaniu domyślnych wartości i dodawaniu transformacji do wykresów",
        "Poznasz typy wizualizacji, takie jak Stat Panel, Gauge, Bar Gauge, Tabel Panel, Heatmap, Histogram i Geomap",
        "Skonfigurujesz źródła danych na przykładzie bazy MySQL",
        "Zaznajomisz się z zaawansowanymi technikami zarządzania danymi",
        "Zrozumiesz, czym w praktyce jest alertowanie, w tym protokół SMTP i alertowanie na e-mail",
        "Zaimportujesz dashboardy, zarówno te dostarczane przez społeczność Grafany, jak i z Prometheusa",
        "Zmonitorujesz metryki z MongoDB",
        "Przybliżysz sobie temat adnotacji w Grafanie, nauczysz się je dodawać i filtrować"
      ],
      "course_content": {
        "Wstęp": [
          "Wstęp"
        ],
        "Wprowadzenie do Grafany i jej instalacja": [
          "Wprowadzenie",
          "Monitoring w pigułce: zbieranie, zapisywanie i wizualizacja danych",
          "Instalacja Docker Compose",
          "Interfejs użytkownika"
        ],
        "Praca z dashboardami": [
          "Panel rows - grupowanie wykresów",
          "Zarządzanie wykresami - przegląd, edycja, udostępnianie, eksploracja itp.",
          "Wersjonowanie dashboardów"
        ],
        "Formatowanie wykresów": [
          "Edytowanie wykresów i wizualizacja",
          "Nadpisywanie domyślnych wartości dla elementów",
          "Dodawanie transformacji do wykresów"
        ],
        "Typy wizualizacji": [
          "Stat Panel",
          "Gauge i Bar Gauge",
          "Tabel Panel",
          "Heatmap",
          "Histogram",
          "Geomap"
        ],
        "Relacyjna baza danych jako źródło": [
          "MySQL - konfiguracja w Docker Compose",
          "MySQL - tworzenie tabeli",
          "MySQL jako źródło",
          "Kreowanie szeregu czasowego z użyciem now()"
        ],
        "Zmienne w dashboardach": [
          "Rodzaje zmiennych",
          "Zmienne w dashboardach",
          "Pozostałe typy zmiennych"
        ],
        "Zarządzanie": [
          "Zarządzanie rolami",
          "Zarzadzanie dostępami do dashboardów",
          "Zarządzanie organizacjami"
        ],
        "Alertowanie": [
          "Protokół SMTP",
          "Alertowanie na maila"
        ],
        "Importowanie dashboardów": [
          "Dashboardy dostarczane przez społeczność",
          "Prometheus",
          "Go Processes"
        ]
      },
      "requirements": [
        "Nie musisz mieć doświadczenia, wszystkiego nauczysz się z tego kursu"
      ],
      "description": "Obierz kurs na... analizę danych w czasie rzeczywistym\nJeśli w swojej pracy masz lub miewasz do czynienia z danymi, z pewnością orientujesz się, że do tego celu stworzono dotąd całkiem sporo narzędzi. Nic dziwnego – przy tej liczbie danych, z jaką spotykamy się w dzisiejszym cyfrowym świecie, zdolność do ich sprawnego analizowania i wyciągania z nich konstruktywnych wniosków daje przewagę konkurencyjną. I tu na scenę wkracza Grafana: wieloplatformowa aplikacja internetowa typu open source, służąca dokładnie temu: analizie i wizualizowaniu danych w czasie rzeczywistym. W tym zakresie Grafana – dzięki elastyczności, możliwości integracji z rozmaitymi źródłami danych, a także łatwości w tworzeniu czytelnych i interaktywnych dashboardów – pozostaje bezkonkurencyjna.\n\nChcesz pogłębić swoje kompetencje w zakresie analizy danych, monitorowania wskaźników wydajności aplikacji, infrastruktury IT czy nawet urządzeń IoT? Naucz się korzystać z Grafany!\nGrafana wspiera integrację z ponad 30 źródłami danych, w tym popularnymi bazami danych i platformami monitorowania. Wśród funkcjonalności, jakie oferuje, jest tworzenie wykresów i diagramów pozwalających w jasny, czytelny sposób prezentować dane, a także ustawianie alertów przydatnych do ich monitorowania. Narzędzie, powstałe w 2014 roku, stale się rozwija, o co dba stojąca za nim firma Grafana Labs. Regularnie organizuje ona konferencje i webinary i w ten sposób zapewnia społeczności skupionej wokół Grafany platformę wymiany wiedzy i doświadczeń. Sama społeczność nie pozostaje bierna – aktywnie rozwija i udostępnia użytkownikom nowe pluginy, poszerzające możliwości aplikacji.\nCo Cię czeka podczas naszego profesjonalnego szkolenia\nW trakcie kursu między innymi:\nZapoznasz się z całym ekosystemem związanym z monitoringiem (Grafana, Docker Compose, Prometheus)\nPoznasz wszystkie elementy interfejsu użytkownika Grafany\nOpanujesz techniki pracy z dashboardami, w tym panel rows (grupowanie wykresów)\nNauczysz się zarządzać wykresami poprzez takie funkcje jak przegląd, edycja, udostępnianie i eksploracja\nDowiesz się, czym jest i jak stosować wersjonowanie dashboardów\nPrzyjrzysz się formatowaniu wykresów, w tym nadpisywaniu domyślnych wartości i dodawaniu transformacji do wykresów\nPoznasz typy wizualizacji, takie jak Stat Panel, Gauge, Bar Gauge, Tabel Panel, Heatmap, Histogram i Geomap\nSkonfigurujesz źródła danych na przykładzie bazy MySQL\nZaznajomisz się z zaawansowanymi technikami zarządzania danymi\nZrozumiesz, czym w praktyce jest alertowanie, w tym protokół SMTP i alertowanie na e-mail\nZaimportujesz dashboardy, zarówno te dostarczane przez społeczność Grafany, jak i z Prometheusa\nZmonitorujesz metryki z MongoDB\nPrzybliżysz sobie temat adnotacji w Grafanie, nauczysz się je dodawać i filtrować\nGrafana. Kurs video. Monitorowanie, analiza i wizualizacja danych w czasie rzeczywistym kończy się na poziomie średnio zaawansowanym. Zapewnia zdobycie solidnych podstaw, które umożliwiają dalsze samodzielne tworzenie zaawansowanych dashboardów, efektywne zarządzanie danymi i używanie Grafany do monitorowania danych w czasie rzeczywistym. Szkolenie zawiera swojego rodzaju przekrój informacji, od podstawowych funkcji po zaawansowane techniki, i przygotowuje uczestnika do rozwiązywania realnych problemów biznesowych z wykorzystaniem Grafany.",
      "target_audience": [
        "Kurs skierowany jest do osób początkujących"
      ]
    },
    {
      "title": "Pythonで体系的に学ぶデータサイエンスとAIの初歩 Vol.1 Python基礎・暗号・微分【文科大臣賞博士が指導】",
      "url": "https://www.udemy.com/course/pythonai-vol1/",
      "bio": "Pythonの基礎、数と暗号（複素数、マンデルブロ集合、素数、公開鍵暗号RSA）、関数と微分（matplotlib、3次元のグラフ、陰関数、極限）",
      "objectives": [
        "Pythonの基礎知識を得て、簡単なプログラムを作成できるようになります。",
        "データサイエンスやAIの学習に必要な数学・統計などの初歩的基礎知識を体系的に得ることができます。",
        "データサイエンスとAIを本格的に学ぶにあたって必要な初歩的事項を身に付けることができます。",
        "本Vol.1では特に「Pythonの基礎」、「数と暗号」、「関数と微分」についての基礎知識を得ることができます。"
      ],
      "course_content": {
        "コースの紹介": [
          "本コースの特長",
          "コースの紹介"
        ],
        "演習　理解度チェッククイズ": [
          "演算子",
          "演算子に関するクイズの解説"
        ],
        "Anacondaのインストールと起動方法、付属のテキストとテンプレートプログラムについて": [
          "言語のインストールと付属資料について"
        ],
        "Jupyter Notebook バージョン6とバージョン7の違い": [
          "Jupyter Notebook バージョン7のUIなどについて",
          "NBバージョン7では図の描画にplt.show()が必要",
          "NBメニューの日本語化の方法"
        ],
        "電卓のような使い方、データ型、変数": [
          "電卓のような使い方",
          "Anacondaの起動",
          "新規のnotebookを作成する",
          "Markdownでコメントを入れる",
          "notebookファイルの開き方",
          "加減乗除",
          "データ型",
          "変数とは",
          "2つの変数間で値を入れ替える"
        ],
        "参照の割り当て": [
          "参照の割り当て　説明",
          "参照の割り当て　プログラムで確認",
          "参照の割り当て　図による説明"
        ],
        "演算子": [
          "算術演算子",
          "代入演算子",
          "ブーリアン型と論理演算子",
          "ド・モルガンの法則",
          "NANDとXOR"
        ]
      },
      "requirements": [
        "簡単なプログラミングの経験があった方が望ましいです。",
        "中学生レベル、できれば高校生レベルの数学的素養があると理解が早いです。"
      ],
      "description": "■「Pythonで体系的に学ぶデータサイエンスとAIの初歩」シリーズ全4巻ではデータサイエンスとAIの初歩を学ぶのに必要な数学、統計学などの初歩的基礎知識を体系的に学んでいきます。高校の新学習指導要領の「情報I」および「情報II」の内容も多く含んでいます。\n■高校生程度の数学の知識があれば理解できるように説明しているので、いきなりデータサイエンスやAIのコースは荷が重いという方にお勧めです。実際に中高生向けのプログラミング教室での10年の実績があります。\n■実際のプログラミング教室での授業から、最新の内容やテキストにはないが役立つ内容を補足事項として随時アップデートしています。\n■Anacondaをインストールして、Jupyter Notebookでプログラミングを行なっていきます。\n■学習内容\nVol.1ではPythonの基礎、数と暗号、関数と微分\nVol.2では確率と統計、ベイズ統計、データサイエンスとAIの初歩\nVol.3では線形代数、主成分分析、自然言語処理\nVol.4ではネットワークの基礎知識、ブロックチェーン\nについて学んでいきます。\n■4コースともPDF形式のテキスト（約100ページ）が付属しています。\nまた、タイピングが大変だったり時間がないという方のためにある程度コードを記載したテンプレートプログラムをつけてあります。完成したサンプルプログラムもついています。\n■Vol.1の内容は以下の通りです。\nPythonの基礎\n演算子\n構造化プログラミング\n誤差\n関数\nクラス\n標準ライブラリと外部ライブラリ\n2次元画像\nサブプロット\n数と暗号\n数の体系\n複素数\n充填ジュリア集合とマンデルブロ集合\n素数判定アルゴリズム\n因数分解\n素因数分解\nシーザー暗号とRSA暗号\n関数と微分\n関数のグラフ\n1次関数の応用\n3次元のグラフ\n陰関数のグラフ\n2次関数の応用\n極限\n微分",
      "target_audience": [
        "データサイエンスやAIに関心を持つが、いきなり本格的なAIプログラミングは困難なPythonプログラミング初心者。",
        "高校の新学習指導要領「情報I」、「情報II」レベルのプログラミングを学びたい方。",
        "高校の新学習指導要領「情報I」、「情報II」レベルのプログラミングを教えたい教師。",
        "データサイエンスに興味を持つ大学生。",
        "データサイエンスの基礎知識を身につけたい社会人やビジネスマン。",
        "リスキリングでデータサイエンスの基礎を習得したいビジネスマン。"
      ]
    },
    {
      "title": "【初心者向け】PythonでGoogleスプレッドシートを操作して煩雑な日々の業務を効率化・自動化する方法を学ぼう！",
      "url": "https://www.udemy.com/course/python-spreadsheet/",
      "bio": "Googleスプレッドシートの各シートの売上データをPythonを使って集約して可視化したり回帰モデル構築したりする方法を学んでいこう！これでGoogleスプレッドシートを使った日々の業務も効率化できること間違いなし！",
      "objectives": [
        "Pythonの基礎",
        "Pythonを使ってGoogleスプレッドシートを操作する方法",
        "Googleスプレッドシートのシート別売上データをPythonで集計可視化する方法",
        "Googleスプレッドシートのシート別売上データをPythonで集約し回帰モデルを作る方法"
      ],
      "course_content": {
        "イントロダクション": [
          "はじめに"
        ],
        "Pythonの使い方": [
          "Pythonを学ぼう！",
          "Pythonの実行環境",
          "Google Colabの使い方",
          "演算子の種類を学ぼう！",
          "Pythonの変数と型を学ぼう！",
          "list型(リスト)を学ぼう！",
          "dict型(辞書)を学ぼう！",
          "print関数を学ぼう！",
          "文字列の操作方法を学ぼう！",
          "条件分岐のif文を学ぼう！",
          "繰り返し処理のfor文を学ぼう！",
          "関数の作り方と使い方を学ぼう！",
          "Pandasの使い方を学ぼう！",
          "Numpyについて学ぼう！",
          "Matplotlibの使い方を学ぼう！",
          "Seabornについて学ぼう！",
          "Python構文の復習"
        ],
        "【基礎編】PythonでGoogleスプレッドシートを操作してみよう！": [
          "Google Cloudのアカウントを作成",
          "GoogleスプレッドシートのAPIキーを取得して設定",
          "Googleスプレッドシートを指定して操作する方法",
          "Googleスプレッドシートへのデータの追加方法と取得方法",
          "Googleスプレッドシートへのデータの更新方法、検索方法、削除方法",
          "フォーマットの変更や他人への共有方法"
        ],
        "【応用編①】売上明細データの集計可視化": [
          "応用編：売上明細データの集計可視化でやりたいこと",
          "売上明細データ生成の関数作成",
          "売上明細データ生成",
          "売上明細データを集計",
          "Summaryシートに出力",
          "月別のデータをまとめて積み上げ棒グラフで表示①",
          "月別のデータをまとめて積み上げ棒グラフで表示②"
        ],
        "応用編②：広告量データから売上を予測する回帰モデルの構築": [
          "データ生成①：日付と広告量から売上データを生成する関数",
          "データ生成②：各データを生成する関数を作成し1月~12月のデータを生成",
          "次のレクチャーで使う「線形回帰分析」についての説明",
          "スプレッドシートのデータを集約して特徴量を生成",
          "線形回帰モデルを構築し予測値を出力",
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "Pythonの基礎から学んでいくのでプログラミングの経験は不要です"
      ],
      "description": "このコースでは、Pythonを使ってGoogleスプレッドシートを操作する方法について解説していきます！\n\n\n日々の業務で多くの人が使っているGoogleスプレッドシート。しかし時にはスプレッドシートの作業は非常に退屈で煩雑で苦労します。\n\n\nそんな業務をPythonを使うことでほぼ自動化して効率化することができるかもしれません。\n\n\n基礎編で一通りPython×Googleスプレッドシートの操作方法を学んだ後は、実践編で実務で発生しそうな具体的なユースケースを元に集計や可視化、モデル構築をおこなっていきます！\n\n\n日々の非効率なスプレッドシート業務から解放されましょう！",
      "target_audience": [
        "Pythonを使ってGoogleスプレッドシートを操作する方法に興味のある方",
        "Googleスプレッドシートを使った日々の業務を効率化したい方"
      ]
    },
    {
      "title": "Fundamentals of artificial intelligence (in Arabic)",
      "url": "https://www.udemy.com/course/fundamentals-of-artificial-intelligence-in-arabic/",
      "bio": "أساسيات الذكاء الاصطناعي وخوارزميات البحث الذكية",
      "objectives": [
        "تعلم أساسيات ومفاهيم الذكاء الاصطناعي",
        "تعلم وفهم طرق عمل الوكلاء الأذكياء",
        "تعلم كيفية حل ونمذجة المشاكل المعقدة",
        "تعلم طرق البحث الذكية",
        "تعلم آلية عمل الخوارزمية الجينية"
      ],
      "course_content": {
        "An Introduction to Artificial Intelligence": [
          "Some foundations of artificial intelligence",
          "What is Intelligence?",
          "What is Artificial Intelligence?",
          "Systems that Act Like Humans",
          "Turing Test; the Imitation Game",
          "The Chinese Room Argument / Strong Vs. Weak AI",
          "Systems that Think Like Humans",
          "Systems that Think Rationaly"
        ],
        "Intelligent Agents / AI Related Disciplines": [
          "Intelligent Agents in the World",
          "Specifying the Task Environment [ PEAS ]",
          "Goal-based Agents versus Cost-based Agents",
          "Environment Types",
          "Examples of the Environment Types",
          "Learning Agents"
        ],
        "Problem Solving as Search _ State Space Search": [
          "State Space Search",
          "Exhaustive vs heuristics search",
          "Search Problem Components",
          "State apace & Graph theory",
          "Complexity metrics",
          "State Space Search Strategies",
          "Search: Basic idea",
          "Tree Search Algorithm Outline",
          "Handling Repeated States",
          "Backtracking Search"
        ],
        "Problem Solving as Search - Blind vs. Heuristic Strategies": [
          "Uninformed Search (Blind/ Exhaustive/ Brute-Force Search)",
          "Depth-First Strategy",
          "Breadth-First Strategy",
          "Depth-Limited Strategy",
          "Depth-First Strategy versus Breadth-First Strategy",
          "Bidirectional Search",
          "Uniform-Cost Strategy"
        ],
        "Problem Solving as Search - Adversarial Search": [
          "Heuristic Function",
          "Heuristic & Cost Function",
          "Symmetry / Heuristic Reduction",
          "Adversarial Search ( Heuristics in Two-Person Games )",
          "Minimax procedure",
          "Alpha-beta procedure"
        ],
        "Evolutionary Computation l Genetic Algorithms": [
          "Genetic Algorithms (GA) ASPIRATION",
          "Genetic Algorithms (GA) OVERVIEW",
          "General Scheme of Genetic Algorithms",
          "Genetic Algorithms (GA) OVERVIEW The Metaphor",
          "A Basic Example -The 8 Queens Problem",
          "Simple Genetic Algorithm",
          "GA Convergence"
        ],
        "Genetic Algorithms Components": [
          "GA Terminology",
          "Example .. The MAXONE problem",
          "Components of GA",
          "Components of GA .. Representation (Encoding)",
          "Components of GA .. Selection",
          "Components of GA .. CrossOver",
          "Components of GA .. Mutation, Elitism, and Fitness Function",
          "travelling salesman problem using GA"
        ]
      },
      "requirements": [
        "لا يوجد"
      ],
      "description": "تهتم الدورة التدريبية Fundamentals of artificial intelligence (in Arabic)\nبشرح كورس  Artificial Intelligence والذي يحمل الكود رقم CS361\nوالشرح بلغة عربية بسيطة وواضحة لمعظم نقاط الكورس رقم CS361\n\n\n- تتطرق الدورة التدريبية بشرح المفاهيم والتعاريف الأساسية للذكاء الاصطناعي، كما تتعرض الدورة لتاريخ ونشأة الذكاء الاصطناعي والعلوم والمعارف المختلفة التى يعتمد عليها.\n\n\n- تشرح الدوة أنواع وتصنيفات الذكاء الاصطناعي الأربعة والتىتندرج تحتها جميع تطبيقات الذكاء الاصطناعي، من بناء أنظمة خبيرة تتصرف بشكل يحاكي التصرف البشري، أو تتصرف بشكل منطقي، أو تحاكي طريقة تفكير البشر في حل المشاكل، أوتفكربناء علي قواعد منطقية.\n\n\n- كما تتعرض الدورة للتفريق بين الذكاء الاصطناعي من خلال الوكلاء الأذكياء وطرق البحث المختلفة، وبين والتعلم الآلي وعلوم البيانات.\n\n\n- فهم وتعريف الوكلاء الأذكياء وأنواعهم وأنواع البيئات المختلفة، وكيفية تطويرهم لحل مشاكل البحث المختلفة مع الأمثلة.\n\n\n- مشاكل الوكيل الواحد ومشاكل الوكلاء المتعددة في البيئات المختلفة.\n\n\n- شرح خوارزميات البحث العمياء والتى تعمل مسح لجميع مناطق الحل مع الأمثلة.\nDepth-First Strategy\nBreadth-First Strategy\nDepth-Limited Strategy\n\n\n- شرح خوارزميات البحث الذكية والدوال الحدسية مع الأمثلة.\nAdversarial Search ( Heuristics in Two-Person Games )\nSymmetry / Heuristic Reduction\nMinimax procedure\nAlpha-beta procedure\n\n\n- شرح آلية التعامل مع الخوارزمية الجينية كتطور لخوارزميات البحث التقليدية، ومكوناتها وأمثلة عليها.\nGeneral Scheme of Genetic Algorithms\nGA Convergence\nComponents of GA\nA Basic Example -The 8 Queens Problem\ntraveling salesman problem using GA\n\n\nFundamentals of artificial intelligence",
      "target_audience": [
        "لكل طلاب كليات الهندسة وعلوم الحاسب وكليات الذكاء الاصطناعي",
        "الباحثين والدارسين في مجال الذكاء الاصطناعي وعلوم البيانات",
        "كل من يريد تعلم أساسيات ومفاهيم وتطبيقات الذكاء الاصطناعي"
      ]
    },
    {
      "title": "Machine Learning : Le guide complet version 2025",
      "url": "https://www.udemy.com/course/machine-learning-le-guide-complet/",
      "bio": "Big Data et Machine Learning : Les concepts et les outils de la data science",
      "objectives": [
        "Comprendre le rôle stratégique de la gestion des données pour l’entreprise",
        "Identifier ce qu’est la donnée, et en quoi consiste le fait d’assurer la qualité de données",
        "Synthétiser le cycle de vie de la donnée",
        "Assurer l’alignement des usages métiers avec le cycle de vie de la donnée",
        "Découvrir les bonnes pratiques en matière de contrôle de qualité des données",
        "Familiarisez- vous avec les bibliothèques d'apprentissage automatique de Python, notamment scikit -learn, ...",
        "Assurer la mise en oeuvre de la gouvernance de la donnée",
        "Maitriser les bases de l’analyse business",
        "Choisir des indicateurs et comprendre les données associées"
      ],
      "course_content": {
        "PART 1 - Introduction au big data": [
          "Bienvenue !!",
          "Dr. Firas Partenaire formateur Udemy",
          "Présentation rapide : \"Qui suis-je ?\"",
          "Les méthodes et outils pédagogiques de la formation",
          "FAQ Udemy",
          "Bienvenue sur Udemy ! Présentez-vous !",
          "Le big data et le modèle 5V",
          "Big data et informatique décisionnelle",
          "Usage du Big Data"
        ],
        "PART 2 - Exploitation des Centres de données et Cloud computing": [
          "Data center et cloud computing",
          "Exploitation des Data center et cloud",
          "Map reduce",
          "Hadoop"
        ],
        "PART 3 - Base de données et analyse des mégadonnées": [
          "Data bases traditionelles",
          "Analyse des données et Machine learning"
        ],
        "PART 4 - Intelligence artificielle": [
          "Les bases de l'intelligence artificielle",
          "L'intelligence artificielle et le machine learning",
          "L'impact de l'intelligence artificielle sur les réseaux sociaux"
        ],
        "PART 5 - Faites vos premiers pas en Python": [
          "Support de cours",
          "Introduction Python",
          "Installation Python 3.8.3",
          "Exécuter le programme Python",
          "Les fonctions",
          "Les opérations de base",
          "Les opérations spécifiques à Python",
          "Ordre et priorité",
          "Les types de nombre part-1",
          "Les types de nombre part-2",
          "Fonction INPUT",
          "Manipulation de chaine de caractère",
          "Manipulation de chaine de caractère part-2",
          "Changer les types avec les fonctions prédéfini avec Python",
          "Changer les types dans le input",
          "Les variables",
          "Changer les types des variables",
          "Les règles pour la création des variables",
          "Opérations sur place",
          "Type boolean",
          "Lancer l'éditeur de code Atom",
          "Les structures de contrôle",
          "Plusieurs conditions de contrôle",
          "Plusieurs conditions de contrôle Part-2",
          "Plusieurs conditions de contrôle Part-3",
          "Les conditions logiques",
          "Exercice avec les conditions",
          "Correction Exercice avec les conditions",
          "Structure de contrôle avec boucle While",
          "Structure de contrôle avec boucle While Part-2",
          "Structure de contrôle avec boucle While Part-3",
          "Création des listes",
          "Manipulation de la liste Part-1",
          "Manipulation de la liste Part-2",
          "Manipulation de la liste Part-3",
          "Manipulation de la liste Part-4",
          "Les recherches dans une liste",
          "Application des méthodes sur les listes",
          "EXERCICE : manipulation des listes",
          "Correction Exercice avec les conditions",
          "Structure de contrôle avec boucle While",
          "Structure de contrôle avec boucle While Part-2",
          "Structure de contrôle avec boucle While Part-3",
          "Création des listes",
          "Manipulation de la liste Part-1",
          "Manipulation de la liste Part-2",
          "Manipulation de la liste Part-3",
          "Manipulation de la liste Part-4",
          "Les recherches dans une liste",
          "Les fonctions dans les listes",
          "Application des méthodes sur les listes",
          "EXERCICE : manipulation des listes",
          "Exercice avec les listes",
          "Manipulation des listes avec les boucles",
          "La boucle FOR Part-1",
          "La boucle FOR Part-2",
          "La boucle FOR Part-3",
          "Création des dictionnaires",
          "Chercher la valeur d'une clé",
          "Mettre une liste dans un dictionnaire",
          "Ajouter un nouveau clé à un dictionnaire",
          "Recherche d'une clé",
          "Méthode Get",
          "Méthode KEYS",
          "Exercice avec la manipulation des dictionnaires",
          "Exercice avec les dictionnaires",
          "Définition d'une fonction",
          "Création d'une fonction",
          "Utilisation de RETURN dans une fonction",
          "Exercice: Fonction",
          "Exercice avec une fonction",
          "Les types d'erreurs dans Python"
        ],
        "Obtenir le certificat": [
          "Obtenir le certificat"
        ],
        "Bonus": [
          "Bonus"
        ]
      },
      "requirements": [
        "Aucune connaissance technique particulière n’est nécessaire"
      ],
      "description": "Ce cours présente une introduction aux concepts qui détermine les bases du machine learning, et en propose une vision pour comprendre les enjeux des projets d’analyse de données, pour appréhender les concepts sous-jacents.\nObjectifs pédagogiques :\nLe but de ce cours est de vous accompagner dans votre découverte du machine learning et de vous fournir les outils nécessaires à :\nIdentitéﬁer les problèmes qui peuvent être résolus par des approches de machine learning ;\nFormaliser ces problèmes en termes de machine learning ;\nIdentitéﬁer les algorithmes classiques les plus appropriés pour ces problèmes et les mettre en œuvre ;\nImplémenter ces algorithmes par vous-même aﬁn d’en comprendre les tenants et aboutissants ;\nEvaluer et comparer de la manière la plus objective possible les performances de plusieurs algorithmes de machine learning pour une application particulière\nNotez bien :\nLa formation se déroulera sous forme de cours théorique & pratique, d'exemples concrets et d'ateliers pour permettre aux participants de mettre en pratique les concepts appris. Des exercices, des mises en situation et des études de cas seront utilisés pour renforcer les connaissances.\n\n\nRessources d’apprentissage complémentaires :\nAtelier en ligne\nDocumentation\nConsultez des exemples de tableaux de bord, de rapports et de fichiers de bureau.\nEnfin, je m'engage à vous fournir la formation la plus complète possible sur Udemy pour vous permettre de réussir dans votre apprentissage.\nJe m'engage à répondre rapidement à vos questions pour vous aider à comprendre les concepts de la formation.\nJe vais ajouter des cas pratiques sur demande pour vous donner des exemples concrets de ce que vous apprenez.\nJe vais vous accompagner avec des cas pratiques et d'autres ressources utiles pour vous aider à mettre en pratique ce que vous apprenez.\nCes ajouts de vidéos seront, bien entendu, gratuits si vous avez acquis la formation.\nComment me contacter ? Je reste disponible dans la rubrique Question/Réponses d'Udemy pour répondre à vos questions.\nÀ la fin de ce cours, si vous le suivez en entier et réussissez l'ensemble des quizz : Obtenez votre certification électronique à insérer dans votre CV et profil LinkedIn.\n\nDr. Firas",
      "target_audience": [
        "aux décideurs informatiques qui souhaitent aller au-delà des discours marketing et mieux comprendre les mécanismes de fonctionnement et les outils du Big Data ;"
      ]
    },
    {
      "title": "Introducción a la Inteligencia Artificial",
      "url": "https://www.udemy.com/course/introduccion-a-la-inteligencia-artificial-s/",
      "bio": "Descubriendo el Futuro: Curso de Introducción a la Inteligencia Artificial",
      "objectives": [
        "Comprender los Conceptos Fundamentales de la Inteligencia Artificial (IA): Los estudiantes deberían poder explicar los principios básicos de la IA",
        "Familiarización con las Técnicas de Aprendizaje Automático: Adquirir conocimientos básicos sobre algoritmos de aprendizaje automático.",
        "Desarrollar Habilidades en el Manejo de Herramientas y Plataformas de IA: Aprender a utilizar herramientas y plataformas comunes utilizadas.",
        "Conciencia Ética y Social sobre la IA: Desarrollar una comprensión de los aspectos éticos y sociales relacionados con la implementación de la IA."
      ],
      "course_content": {
        "Introducción al Curso": [
          "Presentación del Curso",
          "Introducción",
          "Introducción a la Inteligencia Artificial y Machine Learning"
        ],
        "¿Qué es la Inteligencia Artificial?": [
          "Video Clase 1. Definiciones, Historia IA Débil y Fuerte",
          "IA Qué Es?",
          "Historia y evolución de la IA.",
          "Tipos de IA: IA débil vs. IA fuerte.",
          "Video clase Ética",
          "Ética en la Inteligencia Artificial.",
          "Video clase. Introducción al Aprendizaje Automático.",
          "Introducción al Aprendizaje Automático.",
          "Video clase Algoritmos de Machine Learning.",
          "Algoritmos de Machine Learning.",
          "Video clase. Conjuntos de datos y preprocesamiento.",
          "Conjuntos de datos y preprocesamiento.",
          "Evaluación de modelos de ML.",
          "Video clase. Conceptos básicos de las redes neuronales.",
          "Conceptos básicos de las redes neuronales.",
          "Audio clase Entrenamiento de redes neuronales.",
          "Entrenamiento de redes neuronales.",
          "Video clase Redes neuronales convolucionales.",
          "Redes neuronales convolucionales.",
          "Video clase Redes Neuronales Recurrentes",
          "Redes neuronales recurrentes.",
          "Video clase Visión por computadora y procesamiento de imágenes.",
          "Visión por computadora y procesamiento de imágenes.",
          "Video clase Procesamiento de Lenguaje Natural",
          "Procesamiento de lenguaje natural (NLP).",
          "Video clase Robótica y Automatización.",
          "Robótica y Automatización.",
          "Casos de estudio y proyectos prácticos.",
          "Selección de proyectos y definición de objetivos.",
          "Recopilación y etiquetado de datos.",
          "Diseño y entrenamiento de modelos de IA.",
          "Evaluación y optimización de proyectos.",
          "Video clase. Tendencias y Avances en IA",
          "Tendencias y avances en IA.",
          "Ética y responsabilidad en la IA.",
          "Oportunidades profesionales en IA.",
          "Chat GPT",
          "Recomendaciones Bibliográficas"
        ],
        "Libro y otros textos complementarios": [
          "Texto del curso",
          "Marco Ético para el Desarrollo y la Implementación de la Inteligencia Artificial",
          "Propuesta de Valor: La Inteligencia Artificial como Motor de su Próxima Ventaja",
          "Explorando el Mundo del Aprendizaje Automático",
          "Un Viaje a Través de la Historia de la Inteligencia Artificial"
        ],
        "Evaluación": [
          "Evaluación. Responde las preguntas."
        ]
      },
      "requirements": [
        "No es necesario contar con conocimientos previos."
      ],
      "description": "**Presentación del Curso: Introducción a la Inteligencia Artificial**\n\n\nBienvenido al curso \"Introducción a la Inteligencia Artificial\". Este curso te proporcionará una introducción completa al apasionante mundo de la inteligencia artificial (IA), desde los conceptos fundamentales hasta las aplicaciones avanzadas. Durante este viaje de aprendizaje, adquirirás los conocimientos teóricos y prácticos necesarios para comprender y trabajar en uno de los campos más emocionantes y transformadores de la tecnología moderna.\n\n\n**¿Qué es la Inteligencia Artificial?**\n\n\nComenzaremos explorando los conceptos básicos de la Inteligencia Artificial, desentrañando su definición y comprendiendo su alcance en la actualidad. Descubrirás cómo la IA ha evolucionado a lo largo de la historia y cómo ha llegado a desempeñar un papel crucial en una amplia gama de aplicaciones en nuestro mundo digital.\n\n\n**Historia y Evolución de la IA**\n\n\nSumergiremos en la historia y evolución de la IA, desde sus inicios hasta los avances más recientes. Entenderás cómo la IA ha pasado de ser una idea en la ciencia ficción a una realidad en la vida cotidiana.\n\n\n**Tipos de IA: IA Débil vs. IA Fuerte**\n\n\nExploraremos la distinción entre la IA débil y la IA fuerte, analizando sus diferencias fundamentales y sus aplicaciones en la vida real. Aprenderás cómo estas dos categorías de IA impactan en diferentes aspectos de nuestra sociedad.\n\n\n**Ética en la Inteligencia Artificial**\n\n\nLa ética en la IA es un tema crucial. Discutiremos cuestiones éticas y responsabilidades asociadas con el desarrollo y uso de la IA, asegurando que estés preparado para abordar estos desafíos en tu carrera.\n\n\n**Introducción al Aprendizaje Automático**\n\n\nEl Aprendizaje Automático (Machine Learning) es un pilar fundamental de la IA. Te introduciremos a este campo emocionante, explicando cómo las máquinas pueden aprender a partir de datos y mejorar su rendimiento con el tiempo.\n\n\n**Algoritmos de Machine Learning**\n\n\nProfundizaremos en los algoritmos de Machine Learning, desde el aprendizaje supervisado hasta el no supervisado. Aprenderás cómo se aplican estos algoritmos para resolver problemas del mundo real.\n\n\n**Redes Neuronales y Procesamiento de Datos**\n\n\nExploraremos conceptos clave de las redes neuronales, incluyendo su entrenamiento y aplicaciones, como las redes neuronales convolucionales para el procesamiento de imágenes y las redes neuronales recurrentes para el procesamiento de secuencias y lenguaje natural.\n\n\n**Aplicaciones Prácticas y Casos de Estudio**\n\n\nNo solo te sumergirás en la teoría, sino que también aplicarás tus conocimientos a través de casos de estudio y proyectos prácticos. Aprenderás a seleccionar proyectos, recopilar datos, diseñar modelos y evaluar tus resultados.\n\n\n**Tendencias y Futuro de la IA**\n\n\nFinalmente, exploraremos las tendencias actuales y futuras en el campo de la IA, y te prepararemos para aprovechar las emocionantes oportunidades profesionales que ofrece este campo en constante evolución.\n\n\nEste curso es adecuado tanto para principiantes como para aquellos con experiencia previa en tecnología. No importa si eres un estudiante ansioso por explorar un nuevo campo o un profesional en busca de nuevas habilidades, este curso te brindará las herramientas y el conocimiento necesarios para avanzar en la Inteligencia Artificial.\n\n\nÚnete a nosotros en este apasionante viaje de aprendizaje y descubre el mundo de la Inteligencia Artificial. ¡Empecemos!",
      "target_audience": [
        "Principiantes en Inteligencia Artificial: Ideal para aquellos que están comenzando a explorar el campo de la IA y desean obtener una comprensión básica de sus conceptos y aplicaciones. Estudiantes y Profesionales de Disciplinas Relacionadas: Puede ser beneficioso para estudiantes o profesionales de campos como la informática, la ingeniería, las matemáticas o la estadística que deseen expandir sus conocimientos en IA. Entusiastas de la Tecnología: Individuos con interés en la tecnología y cómo la IA está transformando diversos sectores. Emprendedores y Gerentes de Negocios: Personas interesadas en comprender cómo la IA puede ser aplicada para mejorar procesos y tomar decisiones basadas en datos en sus negocios o industrias."
      ]
    },
    {
      "title": "Primeros Pasos en la Gestión de Seguridad de la Información",
      "url": "https://www.udemy.com/course/iso27001-fundamentos-gestion-de-seguridad-de-la-informacion/",
      "bio": "Fundamentos ISO/IEC 27001:2022 - Certificate",
      "objectives": [
        "Conceptos Claves",
        "Estructura ISO 27001",
        "Liderazgo",
        "Planificación"
      ],
      "course_content": {
        "Introducción": [
          "Bienvenidos",
          "Antes de iniciar",
          "Objetivo de curso",
          "Acerca del instructor"
        ],
        "Introducción y Antecedentes": [
          "Introducción",
          "Historia de la Norma",
          "ISO/IEC 27001:2022 Estructura",
          "ISO 27000 Familia de Normas"
        ],
        "Conceptos Claves": [
          "¿Qué es un SGSI? - Información y Principios Generales",
          "La Seguridad de la Información",
          "El Sistema de Gestión",
          "Factores Críticos de Éxito de una SGSI",
          "Beneficios de la Familia de Normas en el SGSI"
        ],
        "Estructura ISO 27001": [
          "Estructura de la Norma - ISO 27001",
          "Ciclo Deming PHVA Y SGSI"
        ],
        "Contexto de la Organización": [
          "Comprensión de la Organización y de su Contexto",
          "Desarrollar el contexto de la organización.",
          "Comprensión de las Necesidades y Expectativas de las Partes Interesadas",
          "Determinación del Alcance del SGSI",
          "Sistema de Gestión de la Seguridad de la Información",
          "Definir el alcance del SGSI"
        ],
        "Liderazgo": [
          "Liderazgo y Compromiso",
          "Política",
          "Roles, Responsabilidades y Autoridades en la Organización"
        ],
        "Planificación": [
          "Acciones para Tratar los Riesgos y Oportunidades",
          "Estructura de la Norma ISO 31000 Gestión de Riesgos – Directrices",
          "Definir Declaración de Aplicabilidad para 5 Controles del Anexo A.",
          "Objetivos de Seguridad de la Información y Planificación para su Consecución",
          "Planificación de cambios",
          "Definir los Objetivos de Seguridad de la Información"
        ],
        "Soporte": [
          "Recursos",
          "Competencia",
          "Concienciación",
          "Comunicación",
          "Información Documentada"
        ],
        "Operación": [
          "Planificación y Control Operacional",
          "Apreciación de los Riesgos de Seguridad de la Información",
          "Tratamiento de los Riesgos de Seguridad de la Información"
        ],
        "Evaluación del Desempeño": [
          "Seguimiento, Medición, Análisis y Evaluación",
          "Auditoría Interna",
          "Revisión por la Dirección"
        ]
      },
      "requirements": [
        "Nivel Basico",
        "Conocimeintos básicos de técnologia",
        "Conocimientos Básicos de Seguridad de la Información"
      ],
      "description": "El curso esta enfocado para cualquier persona que esté interesada en ampliar sus conocimientos en la Norma ISO/IEC 27001. Además de poder realizar el examen de Certificación ISO 27001:2022 Foundation - I27001F de Certiprof\n\n\nObjetivos\n\n\nComprender y analizar los fundamentos de la norma ISO 27001:2022.\nFamiliarizarse con los principios, conceptos y requisitos de la norma ISO/IEC 27001:2022.\nEntender cómo desarrollar un SGSI.\nRevisar y comprender el Anexo A de la norma ISO 27001.\n\n\nLa certificación ISO 27001:2022 Foundation - I27001F tiene como objetivo describir los requisitos para establecer, implementar, mantener y mejorar continuamente un sistema de seguridad de la información, ciberseguridad y protección de la privacidad en el contexto de la organización. La ISO 27001 es una norma internacional publicada por la Organización Internacional de Normalización (ISO).\n\n\nLa seguridad de la información y la protección de la privacidad son hoy en día activos de incalculable valor, ya que la pérdida de los mismos por diferentes motivos puede tener un gran impacto en las organizaciones y afectar negativamente su reputación. La norma ISO 27001:2022 puede implantarse en cualquier tipo de organización, con o sin ánimo de lucro, privada o estatal, pequeña o grande.\n\n\nLa formación consistirá en presentaciones de temas con el uso del material y ejemplos. Se espera que durante la formación los alumnos aprendan las prácticas fundamentales para la implantación y gestión de un SGSI. Es muy recomendable trabajar con la traducción oficial de la norma de cada país.\n\n\nAgenda General:\n\n\nFundamentos de la Norma ISO 27001\nIntroducción a la Norma.\nTérminos y definiciones.\nEntendimiento de numerales de la Norma.\nIdentificación de requisitos.\nQué son los objetivos de control.\nConclusiones y preguntas de apoyo.\n\n\nObjetivos de Aprendizaje:\n\n\nComprender y analizar los fundamentos de la norma ISO 27001:2022.\nFamiliarizarse con los principios, conceptos y requisitos de la norma ISO/IEC 27001:2022.\nEntender cómo desarrollar un SGSI.\nRevisar y comprender el Anexo A de la norma ISO 27001.\n\n\nRequisitos para la certificación:\n\n\nNo hay ningún requisito formal para esta certificación.\n\n\nExamen de Certificación:\n\n\nAl finalizar este curso, los participantes estarán equipados con el conocimiento y las habilidades necesarias para cumplir con temas de Fundamentos de ISO/IEC 27001, para presentar el examen de certificación de certiprof\n\n\nFormato: Opción múltiple.\nPreguntas: 40.\nIdioma: Español.\nPuntaje de aprobación: 32/40 u 80 %.\nDuración: 60 minutos máximo.\nLibro abierto: No.\nEntrega: Este examen está disponible en línea.\nSupervisado: A discreción del Socio\n\n\nÚnete para explorar y tener los conocimientos de ISO/IEC 27001 Fundamentos.",
      "target_audience": [
        "Analistas de ciberseguridad",
        "Oficiales de Seguridad de la Información",
        "Oficiales de protección de datos",
        "Entusiastas de la seguridad de la información"
      ]
    },
    {
      "title": "Análises ambientais com QGIS",
      "url": "https://www.udemy.com/course/analises-ambientais-qgis/",
      "bio": "Aplicações práticas e eficientes para gestão ambiental",
      "objectives": [
        "Análises digitais de terreno e hidrografias",
        "Condições, uso e manejo do solo",
        "Modelagem espacial de dados georreferenciados",
        "Delimitação de áreas de APPs"
      ],
      "course_content": {
        "Apresentação, instruções e materiais do curso": [
          "Apresentação: objetivo e benefícios do curso",
          "Instruções e materiais",
          "Referências Bibliográficas usadas"
        ],
        "Bacias Hidrográficas, análise digital de terreno e modelagem espacia": [
          "Introdução parte 1: Bacias Hidrográficas",
          "Introdução parte 2: análise digital de terreno e modelagem espacial"
        ],
        "Obtenção e correção de imagens de modelo digital de elevação (MDE)": [
          "Obtenção de imagem do modelo digital de elevação (MDE)",
          "Ativação e configuração do complemento GRASS",
          "Correções de Imagem do MDE"
        ],
        "Cálculos de superfície e análises de drenagem": [
          "Cálculos de superfície a partir do MDE: declividade, aspecto e sombreamento",
          "Cálculos de superfície a partir do MDE: declividade, aspecto e sombreamento",
          "Análise de drenagem: Exportação de camadas"
        ],
        "Modelagem USLE (Equação universal de perda do solo)": [
          "Obtenção e correção de dados pedológicos e de uso do solo",
          "Erodibilidade do solo (K)",
          "Erovisidade da Chuva (R)",
          "Gradiente - declividade da rampa (S)",
          "Comprimento da rampa (L)",
          "Uso e manejo do solo (C)",
          "Práticas conservacionistas (P) e resultados finais"
        ],
        "Análise ambiental": [
          "Dados, parâmetros e delimitação de APPs",
          "Execução da delimitação de APPs",
          "Confrontamento do uso e cobertura da terra",
          "SAGA: Configurações e usos parte 1",
          "SAGA: Configurações e usos parte 2"
        ]
      },
      "requirements": [
        "É desejável domínio básico do QGIS. Entretanto, o conteúdo do curso é acessível a todos os níveis de experiênci"
      ],
      "description": "A necessidade de monitorar e mapear o ambiente natural, especialmente em face às intensas mudanças globais da presente era, se torna crescentemente necessária. Para atender e cumprir com essa significativa demanda pessoas físicas, empresas e instituições de pesquisas requerem o uso e aplicação de ferramentas robustas, eficientes e de baixo custo.\nDentre as diversas ferramentas atualmente ofertadas para obter mapeamentos ambientais, o software QGIS é uma excelente e robusta opção. Baseado em linguagem computacional Python, o QGIS possui implementações diversas contidas em plugins para essa finalidade. Ainda melhor, o QGIS é um software livre, ou seja, não é preciso pagar para usá-lo.\n\n\nCom esse curso você aprenderá a usar diversas ferramentas SIG (Sistemas de Informação Geográfica) para análises ambientais em seus diversos componentes: análises de terreno (ex: declividade), hidrografia (ex: fluxos), solos (ex: erosividade, uso e manejo).  Assim, você terá uma visão do emprego das ferramentas principais do QGIS para usá-las na gestão e conservação de recursos hídricos e dos solos. Adicionalmente, você aprenderá como avaliar confrontamento do uso e cobertura da terra e delimitar áreas de APPs (Áreas de preservação permanente).\n\n\nPortanto, esse curso visa te fornecer a capacitação necessária para que adquira sua autonomia em criar planos de prevenção de incidentes ambientais e consequentemente prejuízos de recursos hídricos e dos solos.",
      "target_audience": [
        "Estudantes (graduandos, pós-graduandos), pesquisadores, consultores ambientais, empresas e profissionais que desejam incrementar suas habilidades e registros curriculares"
      ]
    },
    {
      "title": "Flink体系课：从入门到开发实战",
      "url": "https://www.udemy.com/course/flink-tr/",
      "bio": "掌握Flink实时框架的基本操作使用，案例实战开发",
      "objectives": [
        "帮助学员掌握数据实时计算能力",
        "帮助 Flink 开发工程师实现进阶， Flink 核心组件和生态系统搭建",
        "通过系统培训，结合业务场景，提升大数据系统实时业务处理能力。",
        "让开发者具备运用Flink框架进行大数据开发的能力。"
      ],
      "course_content": {
        "课程简介": [
          "课程介绍",
          "讲师介绍",
          "课程大纲"
        ],
        "一、Flink原理及应用场景入门": [
          "1.1. Flink基本原理及应用场景分析",
          "1.2. Flink vs Storm vs SparkStreaming",
          "2.1.Flink local集群安装以及集群代码提交执行",
          "2.2.Flink standalone集群安装部署",
          "2.3.Flink on yarn的两种方式",
          "2.4.Flink on yarn内部实现",
          "3.1.Flink standalone集群HA配置",
          "3.2.如何解决集群启动失败的问题",
          "3.3.Flink on yarn集群HA配置",
          "3.4. Flink scala shell代码调试",
          "4.1.课程内容介绍",
          "4.2.DataStream之source讲解",
          "4.3.DataStream之自定义source-1-(java代码)",
          "4.4.DataStream之自定义source-2-(java代码)",
          "5.1.TableApi简介+Flink支持的dataType和序列化",
          "5.2.Flink Broadcast广播变量-(java代码)",
          "5.3.Flink Broadcast广播变量-(scala代码)",
          "5.4.Flink Accumulators-Counters-(java代码)",
          "5.5.Flink Accumulators-Counters-(scala代码)",
          "5.6.Flink Distributed Cache-(java+scala代码)",
          "6.1.Flink state之keyedState分析",
          "6.2.Flink state之operatorState分析",
          "6.3.Flink checkPoint分析",
          "6.4.Flink state backend详细分析",
          "7.1.Flink state backend实战演示",
          "7.2.Flink Restart Strategies(重启策略) 分析",
          "7.3.Flink 从checkpoint恢复数据",
          "7.4.Flink savePoint的使用详解",
          "8.1.Flink watermark介绍",
          "8.2.Flink watermark解决乱序数据-1",
          "8.3.Flink watermark解决乱序数据-2",
          "9.1.Flink parallelism并行度分析",
          "9.2.Flink UI界面介绍",
          "9.3.Flink kafka-connector分析",
          "9.4.kafka-connector代码操作-(java代码)",
          "9.5.kafka-connector代码操作-(scala代码)",
          "9.6.Flink 生产环境配置介绍",
          "10.1.DataStreamAPI之transformation-(java代码)",
          "10.2.DataStreamAPI之partition-(java代码)",
          "10.3.DataStreamAPI之sink-(java代码)",
          "11.1.DataStreamAPI之source-(scala代码)",
          "11.2.DataStreamAPI之transformation-(scala代码)",
          "11.3.DataStreamAPI之partition-(scala代码)",
          "11.4.DataStreamAPI之sink-(scala代码)",
          "12.1.DataSetAPI之transformation-1-(java代码)",
          "12.2.DataSetAPI之transformation-2-(java代码)",
          "12.3.DataSetAPI之partition-(java代码)",
          "12.4.DataSetAPI之transformation-1-(scala代码)",
          "12.5.DataSetAPI之transformation-2-(scala代码)",
          "13.1.Flink Window详解",
          "13.2.Flink Time介绍"
        ],
        "二、Flink案例开发": [
          "1.1.Flink案例开发需求分析",
          "1.2.滑动窗口单词计数-java代码实现",
          "1.3.滑动窗口单词计数-scala代码实现",
          "1.4.batch批处理-java代码实现",
          "1.5.batch批处理-scala代码实现",
          "1.6.Flink streaming和Batch代码层面的使用区别",
          "2.1.实战需求分析(数据清洗[实时ETL])",
          "2.2.数据清洗[实时ETL]-java代码实现-1",
          "2.3.数据清洗[实时ETL]-java代码实现-2",
          "2.4.数据清洗[实时ETL]-java代码提交集群运行",
          "2.5.数据清洗[实时ETL]-把任务提交命令封装成脚本",
          "2.6.数据清洗[实时ETL]-scala代码实现",
          "3.1.实战需求分析(数据报表)",
          "3.2.数据报表-java代码实现-1",
          "3.3.数据报表-java代码实现-2",
          "4.1.数据报表-es和kibana的安装",
          "4.2.数据报表-运行任务",
          "4.3.数据报表-执行脚本封装",
          "4.4.数据报表-scala代码实现",
          "4.5.项目代码地址"
        ],
        "课后寄语": [
          "课后寄语"
        ]
      },
      "requirements": [
        "0基础学员"
      ],
      "description": "本课程以 Flink 原理及应用场景入门，涵盖了 Flink 实时案例开发与集群安装部署，在学员掌握到一定程度后，再通过调用 Flink 中的 DataStream、DataSet 等接口，带领学员完成 Flink 项目实践，系统学习掌握这门技术。\n讲师徐葳，著有《Flink入门与实战》畅销书。 专注于大数据技术研究与开发，拥有多年一线互联网大厂软件研发经验。 曾主导开发海外舆情监控系统、数据采集平台、OLAP数据分析平台、数据仓库、PB级数据检索系统等。 针对大数据生态圈中的数据采集、离线数据计算、实时数据计算、海量数据查询等技术领域有一定见解。 曾为移动研究院、中移在线、中国联通、中信银行、平安银行等企业多次进行大数据技术企培。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "大数据开发工程师",
        "Flink开发工程师",
        "测试工程师（大数据方向）",
        "初级软件开发工程师"
      ]
    },
    {
      "title": "STATA de A à Z : De Débutant à Expert en Analyse Statistique",
      "url": "https://www.udemy.com/course/statistiques_avec_stata/",
      "bio": "Un Guide Pratique pour Explorer, Manipuler et Analyser Vos Données avec le logiciel STATA.",
      "objectives": [
        "Maîtriser les Commandes et les Fonctionnalités Essentielles de STATA",
        "Réaliser des Analyses Statistiques Complètes et Avancées",
        "Appliquer des Techniques Avancées de Modélisation et d'Analyse de Données",
        "Développer des Compétences en Visualisation de Données et en Reporting"
      ],
      "course_content": {
        "Introduction": [
          "Bienvenue au Cours",
          "Installation et Configuration de STATA",
          "Découvrir l'interface de Stata"
        ],
        "Comprendre et Manipuler les DataFrames et les Variables dans Stata": [
          "Qu'est-ce qu'un DataFrame ?",
          "Types de variables dans Stata",
          "Propriétés des variables",
          "Créer et manipuler des variables"
        ],
        "Concepts Statistiques": [
          "Introduction aux statistiques et leur importance",
          "Les concepts fondamentaux des statistiques"
        ],
        "Créer des Graphiques et Visualiser les Données": [
          "Introduction aux graphiques dans Stata",
          "Créer des graphiques personnalisés"
        ],
        "Analyse Statistique Avancée": [
          "Régression Logistique",
          "Analyse Descriptive Bivariée",
          "ANOVA (Analyse de la Variance)"
        ],
        "Résolution de problématiques à l’aide de Stata": [
          "Guide méthodologique pour l’utilisation de Stata dans la recherche",
          "Comment utiliser ChatGPT pour résoudre une problématique"
        ],
        "Conclusion": [
          "Révisions et meilleures pratiques"
        ],
        "Ressources et Astuces": [
          "Guide Essentiel des Commandes Stata",
          "Trucs et Astuces pour STATA",
          "Les Meilleures Ressources Open Data",
          "Impact des Variables SocioDémographiques",
          "Exemples de dataset pour les exercices"
        ]
      },
      "requirements": [
        "Notions de Base en Utilisation d’un Ordinateur",
        "Accès à Internet",
        "Ce cours commence par les bases et vous guide pas à pas jusqu'à des analyses avancées"
      ],
      "description": "Apprenez à maîtriser STATA, l’un des logiciels d’analyse de données les plus puissants et les plus utilisés dans les domaines de la recherche, de l’économie, des sciences sociales, de la santé publique et bien plus encore. Ce cours complet est conçu pour vous guider pas à pas, des bases de l’utilisation de STATA aux techniques d’analyse statistique avancées, en passant par la programmation et l’automatisation.\nCe que vous allez apprendre :\nDécouvrir et Maîtriser les Commandes de Base de STATA : Apprenez à importer, manipuler, nettoyer et gérer vos données de manière efficace avec STATA.\nRéaliser des Analyses Statistiques et Économétriques Complètes : Des statistiques descriptives aux régressions multiples, logistiques et analyses de survie, maîtrisez les techniques essentielles pour vos projets de recherche et analyses professionnelles.\nCréer des Visualisations de Données Impactantes : Explorez les outils de visualisation de STATA pour créer des graphiques clairs et attrayants qui communiquent efficacement vos résultats.\nAutomatiser Vos Analyses avec la Programmation STATA : Découvrez comment automatiser vos processus d’analyse avec des scripts et des programmes personnalisés pour gagner du temps et réduire les erreurs.\nPourquoi ce cours est-il fait pour vous ?\nQue vous soyez étudiant, chercheur, analyste de données, économiste ou simplement curieux d’apprendre à utiliser STATA, ce cours est conçu pour répondre à vos besoins. Il est accessible aux débutants tout en offrant une profondeur d’apprentissage suffisante pour les utilisateurs plus avancés. Vous suivrez des vidéos explicatives, participerez à des exercices pratiques, et travaillerez sur des études de cas réelles pour renforcer votre compréhension et vos compétences.\nAvantages de ce cours :\nSupport pédagogique varié : vidéos, quiz, projets pratiques et exercices interactifs.\nAccès à vie au contenu : Reprenez le cours à votre rythme, autant de fois que vous le souhaitez.\nCertificat de fin de cours : Obtenez un certificat de réussite que vous pourrez ajouter à votre CV ou à votre profil LinkedIn.\nRejoignez ce cours maintenant et devenez un expert de l'analyse de données avec STATA, prêt à appliquer vos nouvelles compétences dans des contextes académiques ou professionnels !",
      "target_audience": [
        "Ce cours est conçu pour les étudiants, doctorants, et chercheurs qui souhaitent apprendre à utiliser STATA pour analyser des données dans le cadre de leurs études ou de leurs projets de recherche en économie, sociologie, sciences politiques, psychologie, santé publique, ou toute autre discipline où l’analyse de données est essentielle.",
        "Les analystes de données, statisticiens, économistes, et consultants qui utilisent ou envisagent d’utiliser STATA dans leurs activités professionnelles bénéficieront d'une formation complète pour maîtriser ce logiciel puissant et améliorer leurs compétences en analyse statistique et économétrique.",
        "Ceux qui n’ont aucune expérience préalable avec STATA mais qui souhaitent se lancer dans l’analyse de données trouveront ce cours accessible et structuré, avec des explications claires, des démonstrations pratiques, et des exercices interactifs qui les guideront pas à pas du niveau débutant à avancé."
      ]
    },
    {
      "title": "Análise de Sentimentos em Dados Reais utilizando Python",
      "url": "https://www.udemy.com/course/analise-de-sentimentos-em-dados-reais-utilizando-python/",
      "bio": "Aprenda a utiliza técnicas de inteligência artificial para processamento de textos passo a passo",
      "objectives": [
        "Introdução à análise de sentimentos",
        "Técnicas de inteligência artifical",
        "Configuração do ambiente de análise",
        "Análise exploratória de dados textuais",
        "Pré-processamento de texto eficiente",
        "Visualização de palavras-chave com WordCloud",
        "Tokenização",
        "StopWords",
        "Representação de dados numericamente",
        "Classificação dos dados utilizando técnicas de aprendizado de máquina"
      ],
      "course_content": {},
      "requirements": [
        "Conhecimento básico em linguagem python"
      ],
      "description": "Bem-vindo ao  curso \"Análise de Sentimentos em Dados Reais Utilizando Python\" na Udemy! Prepare-se para uma jornada emocionante pelo mundo dos dados textuais dos aplicativos de entrega de alimentos, como iFood, AiQFome, Habibs e diversos outros aplicativos. Neste curso abrangente, vamos explorar minuciosamente o processo fascinante de análise de sentimentos em dados do mundo real.\nComeçaremos pelos fundamentos da análise de dados, mergulhando profundamente na mineração e preparação dos dados brutos desses aplicativos populares. Vamos guiá-lo através de técnicas avançadas de pré-processamento de texto, onde mensagens de clientes se transformam em informações estruturadas e compreensíveis.\nEste curso é uma jornada pelas técnicas de inteligência artificial e processamento de texto. Você não apenas aprenderá a ciência por trás da conversão da linguagem humana em dados numéricos compreensíveis, mas também explorará uma variedade de estratégias para representar o texto de forma eficaz e significativa.\nNo cerne do curso, você se aprofundará na construção de modelos de aprendizado de máquina especializados para análise de sentimentos. Guiaremos você na aplicação de algoritmos poderosos, capacitando-o a classificar as mensagens dos clientes em categorias relevantes, revelando os sentimentos por trás de cada revisão. Após a construção meticulosa do modelo, você será treinado para avaliá-lo cuidadosamente, assegurando que suas análises sejam não apenas precisas, mas também confiáveis.\nEste curso é para qualquer pessoa apaixonada por dados, desde entusiastas até cientistas de dados em ascensão e desenvolvedores curiosos. Se você está ansioso para compreender profundamente as opiniões dos clientes e deseja utilizar técnicas de inteligência artificial de ponta para extrair insights valiosos, esta jornada é para você.\nPrepare-se para uma experiência educacional envolvente, onde você ganhará habilidades práticas e conhecimento aprofundado em análise de sentimentos em dados reais. Junte-se a nós agora e inicie sua jornada para se tornar um especialista em processamento de texto e análise de sentimentos com Python!",
      "target_audience": [
        "Pessoas que querem dar seu primeiro passo em análise de sentimentos",
        "Estudantes de graduação e pós-graduação que estejam cursando disciplinas sobre este assunto",
        "Qualquer pessoa interessada em inteligência artificial, ciência de dados e machine learning",
        "Pessoas que estão envolvidos na área de ciência de dados e almejam progredir em suas trajetórias profissionais, bem como expandir seu portfólio.",
        "Pessoas que desejam aplicar inteligência artificial em seus projetos, seja pessoal ou empresarial"
      ]
    },
    {
      "title": "Machine Learning pour la classification des données LiDAR 3D",
      "url": "https://www.udemy.com/course/machine-learning-3d/",
      "bio": "Maîtriser la classification des nuages de points 3D avec les forêts aléatoires : Un guide complet du débutant à l'expert",
      "objectives": [
        "Comment appliquer la classification des nuages de points 3D à des problèmes concrets ?",
        "Comment représenter les nuages de points 3D d'une manière adaptée à l'apprentissage automatique ?",
        "Comment utiliser les forêts aléatoires pour classer les nuages de points 3D",
        "Comment évaluer la performance d'un classificateur de nuages de points 3D ?"
      ],
      "course_content": {
        "Introduction aux Nuages de Points 3D": [
          "Introduction aux Nuages de Points 3D",
          "Workflow 3D Complet",
          "Ressources et Terminologie"
        ],
        "Introduction au Machine Learning": [
          "Introduction au Machine Learning",
          "Apprentissage Supervisé VS Non-supervisé",
          "Apprentissage Ensembliste"
        ],
        "Partie pratique avec Python": [
          "Pratique"
        ]
      },
      "requirements": [
        "Connaissance de base de l'apprentissage automatique",
        "Une certaine expérience de la programmation Python",
        "Aucune connaissance préalable des nuages de points 3D n'est requise."
      ],
      "description": "Dans ce cours, vous apprendrez à utiliser l'apprentissage automatique pour classer les nuages de points 3D. Les nuages de points 3D sont un type de données qui représentent la forme en 3D d'objets et de surfaces. Ils sont largement utilisés dans une variété d'applications, y compris la robotique, la conduite autonome et la vision par ordinateur.\n\n\nDans la première partie du cours, vous apprendrez les bases de la classification des nuages de points 3D et comment représenter les nuages de points 3D d'une manière adaptée à l'apprentissage automatique. Vous découvrirez également les forêts aléatoires, un type d'algorithme d'apprentissage automatique bien adapté à la classification des nuages de points 3D.\n\n\nDans la deuxième partie du cours, vous apprendrez à former et à évaluer un classificateur de nuages de points 3D à l'aide de forêts aléatoires. Vous apprendrez également à appliquer la classification de nuages de points 3D à des problèmes réels, tels que la détection d'objets et la segmentation de scènes.\n\n\nA la fin de ce cours, vous serez capable de :\n\n\nComprendre les bases de la classification des nuages de points 3D\n\n\nReprésenter les nuages de points 3D d'une manière adaptée à l'apprentissage automatique\n\n\nUtiliser des forêts aléatoires pour former et évaluer des classificateurs de nuages de points 3D\n\n\nAppliquer la classification de nuages de points 3D à des problèmes réels.\n\n\n--> Partagez !",
      "target_audience": [
        "Ce cours s'adresse à tous ceux qui souhaitent apprendre à utiliser l'apprentissage automatique pour classer des nuages de points en 3D. Cela inclut les étudiants, les chercheurs et les professionnels dans une variété de domaines, tels que la vision par ordinateur, la robotique et la conduite autonome."
      ]
    },
    {
      "title": "Curso Básico: Fundamentos de Inteligência Artificial",
      "url": "https://www.udemy.com/course/curso-basico-fundamentos-de-inteligencia-artificial/",
      "bio": "Aprenda sobre Inteligência Artificial sem complicação (Learn about Artificial Intelligence without complication)!",
      "objectives": [
        "História Básica da Inteligência Artificial (Basic history of Artificial Intelligence)",
        "Grafos, Matrizes de Adjacência e Árvores de Peso Mínimo (Graphs, Adjacence's Matrices & Minimun Weight Tree)",
        "Busca por Largura e Busca em Profundidade (Breadth-First Search & Depth-First Search)",
        "Busca Heurística com Best-First e A* (Heuristic Search Using Best-First and A* Algorithms)",
        "Algoritmos Kruskal, Djikstra e Prim (Kruskal, Djikstra e Prim Algorithms)",
        "Fundamentos de PEAS e Sistemas Multiagentes (PEAS & Multi-Agent Systems Fundamentals)",
        "Busca Competitiva MINIMAX (Competitive Search MINIMAX)",
        "Lógica Proposicional e Lógica Fuzzy (Propositional and Fuzzy Logics)",
        "Resolução de Pathfinding (Pathfinding Resolution)",
        "Redes Neurais Perceptron e BackPropagator (Perceptron and BackPropagator's Neural Artificial Networks)"
      ],
      "course_content": {
        "Introdução (Introduction)": [
          "Breve História da Inteligência Artificial e suas Definições (Creation of IA)",
          "Teste de Turing (The Turing Test)",
          "Tipos de Problemas (Types of Problems)",
          "Agentes Racionais e Quadro PEAS (Rational Agents & PEAS)"
        ],
        "Grafos (Graphs)": [
          "Tipos de Grafos (Types of Graphs)",
          "Matriz de Adjacência (Adjacence's Matrix)",
          "Busca em Largura (Breadth-First Search)",
          "Busca em Profundidade (Depth-First Search)"
        ],
        "Busca Heurística (Heuristic Search)": [
          "Definição de Heurística (Heuristic's Definition)",
          "Algoritmo Best-First (Best-First's Algorithm)",
          "Algoritmo A* (A*'s Algorithm)",
          "Hill Climbing"
        ],
        "Busca em Árvores Mínimas (Minimun Tree Searching)": [
          "Algoritmo Kruskal (Kruskal's Algorithm)",
          "Algoritmo Djikstra (Djikstra's Algorithm)",
          "Algoritmo Prim (Prim's Algorithm)"
        ],
        "Sistemas Multiagentes (Multi-Agent Systems)": [
          "Características dos Sistemas Multiagentes (Main Characteristics)"
        ],
        "Exemplo Prático - Mundo de Wumpus (Practical Example - Wumpus's World)": [
          "Mundo de Wumpus (Wumpus's World)"
        ],
        "Busca Competitiva MINIMAX (Competitive Search MINIMAX)": [
          "MINIMAX com 2 jogadores (MINIMAX with 2 Players)",
          "MINIMAX com mais de 2 jogadores (MINIMAX with More than 2 Players)",
          "MINIMAX com Poda Alfa-Beta (MINIMAX Alpha-Beta Pruning)"
        ],
        "Lógica Proposicional (Propositional Logic)": [
          "Conceitos Básicos (Basic Concepts)",
          "Sintaxe e Argumentos (Sintax & Arguments)",
          "Tabela Verdade (Truth-Table)",
          "Inferência (Applying Inference)"
        ],
        "Lógica Fuzzy (Fuzzy Logic)": [
          "Introdução à Lógica Fuzzy (Introduction)",
          "Fuzzificação (Fuzzification)",
          "Modificadores Linguísticos e Fórmulas (Linguistic Modifiers & Formulas)",
          "Defuzzificação (Defuzzification)"
        ],
        "Pathfinding": [
          "Pathfinding sem o Uso de Diagonais (Pathfinding without Diagonals)",
          "Pathfinding Utilizando Diagonais (Pathfinding with Diagonals)"
        ]
      },
      "requirements": [
        "Conhecimentos Básicos de Matemática e Lógica (Basic Knowledge of Math and Logic)"
      ],
      "description": "Curso de Fundamentos de Inteligência Artificial! Aprenda sobre esse assunto tão requerido em TI sem muita complicação seguindo vídeo aulas breves e explicativas!\nNesse curso online serão abordados diversos temas dentro desse fascinante assunto, passando por uma breve história sobre sua concepção e características, apresentação de diferentes algoritmos de resolução de problemas, lógicas para transformar valores e conceitos do mundo real para que uma máquina consiga entender, e uma breve pincelada no campo de redes neurais artificiais onde é possível ensinar uma máquina a aprender padrões (tudo com exemplos resolvidos durante as aulas e exercícios para reforçar todo o aprendizado).\nAs aulas são divididas em tópicos que abrangem cada etapa do aprendizado, seguidas por exercícios dependendo do assunto (a resolução dos exercícios também é fornecida, sempre na aula seguinte).\nAo final do curso, você conseguirá resolver diversos problemas que utilizem algum dos algoritmos ou técnicas explicadas durante os vídeos, podendo aplicar esse conhecimento tanto para a área de Jogos Digitais quanto em qualquer outra dentro do ramo de Tecnologia da Informação.\nDesejo muito sucesso para todos que adquiriram o curso, e que todos aproveitem e aprendam bastante no decorrer das aulas!\n\n\n(Basic Course of Artificial Intelligence! Learn about this much-needed subject in IT without much complication following brief and explanatory video lessons! With videos subtitled in English!\nIn this online course will be discussed various topics within this fascinating subject, going through a brief history about its creation and characteristics, presentation of different problem solving algorithms, logic to transform real world values and concepts so that a machine can understand, and a brief lessons in the field of artificial neural networks where it's possible to teach a machine how to learn patterns (all with examples solved during classes and exercises to reinforce all learning).\nClasses are divided into topics that cover each stage of learning, followed by exercises depending on the subject (exercise resolution is also provided, always in the next class).\nAt the end of the course, you'll be able to solve several problems that use some of the algorithms or techniques explained during the videos, being able to apply this knowledge both for the Digital Games area and any other within the branch of Information Technology.\nI wish much success for all who have acquired the course, and that everyone enjoys and learn a lot with the classes!)",
      "target_audience": [
        "Pessoas que desejam conhecer o básico sobre algoritmos utilizados em Inteligência Artificial ou que tenham muita vontade de aprender sobre como funcionam os diferentes métodos aplicados nesse ramo de Tecnologia da Informação.",
        "People who want to know the basics about algorithms used in Artificial Intelligence or who are very keen to learn about how the different methods applied in this branch of Information Technology work."
      ]
    },
    {
      "title": "データサイエンス実戦講座［第５回］ベイズファクターとオルタナティブな仮説検定",
      "url": "https://www.udemy.com/course/5-vhcpgp/",
      "bio": "ベイズ統計学の仮説検定手法を徹底解説―頻度論的統計学のp値による有意性検定からベイズファクターによる帰無仮説と対立仮説の相対評価へ―",
      "objectives": [
        "自然現象や社会現象のメカニズムを分析するデータサイエンスの様々な手法について、シリーズのコースに分けて1つずつ習得していきます。古典的な頻度論の統計学やベイズ統計学の基礎から、機械学習や多変量解析、そして最新のディープラーニングまで、原理の理解と実務への応用を目指します。",
        "ベイズ統計学の仮説検定を学習します。ベイズでは帰無仮説および対立仮説それぞれのもとでデータが得られる確からしさを求め、両者の比であるベイズファクターを使って比較します。頻度論のp値による検定では帰無仮説の棄却の可否を判定するだけでしたが、ベイズでは帰無仮説と対立仮説の確からしさを連続的に相対評価できる利点があります。",
        "ベイズファクターは帰無仮説の確からしさと対立仮説の確からしさの比で定義されます。どちらの仮説が他よりどれくらい確からしいかが分かるので、検定の結果に基づく意思決定と行動選択を、より柔軟に、よりきめ細かく設定することが可能になります。",
        "ベイズファクターを求めるプロセスはベイズ推定そのものですので、母集団の分布特性に応じて共役分布によって解析的に算出できるケースと、数値近似するケースがあります。大きな利点を持つベイズファクターですが、計算が複雑なので適用結果を正しく理解することが求められます。",
        "頻度論で学んだ多数の仮説検定手法をベイズの仮説検定手法に置き換えます。第２回と第３回で使用した演習問題にベイズの検定手法を適用し、p値とベイズファクターによる判定結果を比較しながら代替可能な検定手法を判定します。第１回のタイトル、「統計的有意性の終焉」に対する現時点での解答を提示します。"
      ],
      "course_content": {
        "はじめに": [
          "ベイズ統計の仮説検定の３つのポイント"
        ],
        "1. ベイズ統計の仮説検定": [
          "1-1 頻度論の検定の復習",
          "1-2 ベイズ検定の原理",
          "1-3 検定結果の解釈"
        ],
        "2. １サンプルの平均値の検定": [
          "2-1 １サンプルの平均値の検定問題",
          "2-2 １サンプルｔ検定の演習問題と解法(その1)",
          "2-3 １サンプルｔ検定の演習問題と解法(その2)"
        ],
        "3. ２サンプルの平均値の検定": [
          "3-1 対応のある２サンプルの平均値の検定問題",
          "3-2 対応のある２サンプルｔ検定の演習問題と解法",
          "3-3 独立した２サンプルの平均値の検定問題",
          "3-4 独立した２サンプルｔ検定の演習問題と解法"
        ],
        "4. ２サンプル以上の平均値の検定": [
          "4-1 ２サンプル以上の平均値の検定問題",
          "4-2 分散分析の演習問題と解法"
        ],
        "5. 適合性と独立性の検定": [
          "5-1 適合性の検定問題",
          "5-2 適合性検定の演習問題と解法",
          "5-3 独立性の検定問題",
          "5-4 独立性検定の演習問題と解法"
        ],
        "6. まとめ": [
          "ベイズの仮説検定のまとめ"
        ],
        "ボーナスレクチャー": [
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "高校レベルの数学力があれば十分です。なくても丁寧に説明しますので心配ありません。",
        "正確を期すために数理モデルは示しますが、直感的な理解と解釈を重視してグラフや図による説明を主体にします。"
      ],
      "description": "ベイズ統計学の仮説検定手法の3つのポイントについて解説します。\n１.　オルタナティブな仮説検定：アメリカ統計協会は従来の統計的有意性に基づく頻度論的統計学の仮説検定は誤解や誤用が多いと警鐘を鳴らしています。ベイズ統計学はこの代替となる検定手法を提供します。\n２．p値とベイズファクター：頻度論の検定指標のp値は帰無仮説の棄却の可否を判定するだけですが、ベイズの検定指標のベイズファクターは帰無仮説と対立仮説の確からしさの比を連続値で相対評価します。このため、どちらの仮説がどれほど確からしいかがわかるので、検定結果に基づいて柔軟に対策が立てられます。\n３．ベイズの仮説検定手法：現実の問題解決に適用可能なベイズの仮説検定手法として、１サンプルのｔ検定、対応のある２サンプルのｔ検定、独立した２サンプルのｔ検定、一元配置分散分析、適合性の検定、独立性の検定を、完全に習得します。",
      "target_audience": [
        "学業や業務でデータ分析を必要としている方、将来データアナリストを目指す方、データサイエンスに興味のある方であればどなたでも。",
        "データ分析の初心者から学び直しの中級者。"
      ]
    },
    {
      "title": "[2024년 개정] 데이터분석 전문가(ADP) 자격증 따기 필기 - 5과목 데이터 시각화편",
      "url": "https://www.udemy.com/course/2024-adp-5/",
      "bio": "데이터 분석에 필요한 시각화와 분석을 위한 기초 및 응용 지식을 함께 배워봅시다.",
      "objectives": [
        "데이터 시각화 1 - 시각화 인사이트",
        "데이터 시각화 2 - 시각화 디자인",
        "데이터 시각화 3 - 시각화 구현",
        "데이터 시각화 - 문제풀이"
      ],
      "course_content": {
        "데이터분석 전문가(ADP) 자격증 따기 필기 - 5과목 데이터 시각화편": [
          "1 데이터 시각화 1 - 시각화 인사이트",
          "2 데이터 시각화 2 - 시각화 디자인",
          "3 데이터 시각화 3 - 시각화 구현",
          "4 데이터 시각화 - 문제풀이",
          "..."
        ]
      },
      "requirements": [
        "누구나 수강할 수 있습니다."
      ],
      "description": "안녕하세요, ITGO 입니다.\n\n\n본 강의는 [2024년 개정] 데이터분석 전문가(ADP) 자격증 따기 필기 - 5과목 데이터 시각화편 입니다.\n\n\n본 과정을 통해 학습자는 데이터분석 전문가 자격증 필기 시험 5과목에 해당하는 시각화 파트에 합격하기 위한 이론을 습득할 수 있습니다.\n\n\n또한 데이터 분석에 필요한 시각화 등 분석을 위한 기초 및 응용 지식을 습득할 수 있습니다.\n\n\n\n\n누구를 위한 강의인가요?\n\n\n데이터분석전문가 자격증을 취득하고자 하는 누구나\n\n\n실무 데이터 분석에 필요한 이론 및 지식을 이해하고 싶은 분\n\n\n\n\n무엇을 배우나요?\n\n\n데이터 시각화 1 - 시각화 인사이트\n\n\n데이터 시각화 2 - 시각화 디자인\n\n\n데이터 시각화 3 - 시각화 구현\n\n\n데이터 시각화 - 문제풀이\n\n\n\n\n[2024년 개정] 데이터분석 전문가(ADP) 자격증 따기 필기 - 5과목 데이터 시각화편 강의에 입문해봅시다~!\n\n\n그럼 강의에서 만나요 :)",
      "target_audience": [
        "데이터분석전문가 자격증을 취득하고자 하는 누구나",
        "실무 데이터 분석에 필요한 이론 및 지식을 이해하고 싶은 분"
      ]
    },
    {
      "title": "YOLOv5+DeepSORT多目标跟踪与计数精讲",
      "url": "https://www.udemy.com/course/yolov5deepsort/",
      "bio": "计算机视觉多目标跟踪实战",
      "objectives": [
        "掌握YOLOv5+DeepSORT多目标跟踪与计数方法",
        "深入理解多目标跟踪原理",
        "DeepSORT代码解读"
      ],
      "course_content": {
        "课程介绍": [
          "课程介绍"
        ],
        "基础篇": [
          "多目标跟踪任务介绍",
          "多目标跟踪数据集与评估指标"
        ],
        "实践篇（windows系统）": [
          "安装软件环境",
          "安装PyTorch",
          "下载项目及安装",
          "行人多目标跟踪与计数演示",
          "训练行人ReID数据集",
          "训练车辆ReID数据集",
          "车辆多目标跟踪与计数演示"
        ],
        "实践篇（ubuntu系统）": [
          "安装软件",
          "下载项目及安装",
          "行人多目标跟踪与计数演示",
          "训练行人ReID数据集",
          "训练车辆ReID数据集",
          "车辆多目标跟踪与计数演示"
        ],
        "原理篇": [
          "马氏距离",
          "匈牙利算法",
          "卡尔曼滤波器",
          "SORT论文解读",
          "DeepSORT论文解读1",
          "DeepSORT论文解读2"
        ],
        "代码解析篇": [
          "项目目录结构",
          "DeepSORT流程图",
          "detection.py代码解析",
          "nn_matching代码解析",
          "iou_matching.py代码解析",
          "linear_assignment.py代码解析",
          "preprocessing代码解析",
          "kalman_filter.py代码解析",
          "track.py代码解析",
          "tracker.py代码解析",
          "deep_sort.py代码解析",
          "objdetector.py代码解析",
          "objtracker.py代码解析",
          "demo.py代码解析",
          "行人和车辆计数代码解析"
        ],
        "课程更新和补充": [
          "课程更新",
          "摄像头的使用"
        ]
      },
      "requirements": [
        "熟悉Python和PyTorch"
      ],
      "description": "YOLOv5是目前流行的强悍的基于深度学习的目标检测技术。DeepSORT是流行的多目标跟踪方法。本课程使用YOLOv5和DeepSORT对视频中的行人、车辆做多目标跟踪和计数，开展YOLOv5目标检测器和DeepSORT多目标跟踪器方法强强联手的应用。\n课程分别在Windows和Ubuntu系统上做项目演示，对DeepSORT原理进行讲解，并对DeepSORT的代码做详细解读（使用PyCharm单步调试讲解）。\n\n课程内容分为：基础篇、实践篇、原理篇和代码解析篇。\n- 基础篇包括多目标跟踪的任务介绍、多目标跟踪的常用数据集和多目标跟踪的评估指标；\n- 实践篇中分别在Win10和Ubuntu系统上对YOLOv5+DeepSORT的多目标跟踪和计数具体的实践操作步骤进行演示，特别是讲解了对行人、车辆的ReID数据集上使用深度学习网络的训练方法；\n- 原理篇中详细讲解了马氏距离、匈牙利算法、卡尔曼滤波器的原理，并解读了SORT论文和DeepSORT论文；\n- 代码解析篇中使用PyCharm单步调试对DeepSORT的代码逐个文件进行讲解。\n本课程提供注释后的deepsort代码以及行人和车辆ReID数据集下载（百度网盘和谷歌网盘）。\n\n\n【相关课程】\n本人推出了有关YOLOv5目标检测的系列课程。请持续关注该系列的其它视频课程，包括：\n《YOLOv5目标检测实战：训练自己的数据集》\n《YOLOv5目标检测：原理与源码解析》\n《YOLOv5目标检测实战：Flask Web部署》\n《YOLOv5目标检测实战：TensorRT加速部署》\n《YOLOv5目标检测实战：Jetson Nano部署》\n《YOLOv5实战口罩佩戴检测》\n《YOLOv5实战交通标志识别》\n《YOLOv5实战垃圾分类目标检测》\n《YOLOv5+DeepSORT多目标跟踪与计数精讲》",
      "target_audience": [
        "希望学习YOLOv5+DeepSORT多目标跟踪与计数方法的学员"
      ]
    },
    {
      "title": "PENTAHO PDI e APACHE SUPERSET: ETL e visualização de dados",
      "url": "https://www.udemy.com/course/pentahopdi-e-apache-superset-etl-e-visualizacao-de-dados/",
      "bio": "Produza poderosos pipelines e visualizações de dados incríveis",
      "objectives": [
        "PENTAHO PDI: O que é o Pentaho PDI",
        "PENTAHO PDI: Entendendo sobre fluxos de trabalho e pipelines",
        "PENTAHO PDI: Entendendo sobre projetos e ambientes",
        "PENTAHO PDI: Instalando o Pentaho PDI",
        "PENTAHO PDI: Criando pipelines com arquivos texto",
        "PENTAHO PDI: Realizando tratamento de dados para entendimento do processo de engenharia de dados",
        "PENTAHO PDI: O que são transformações, Jobs e ações dentro de um pipeline",
        "PENTAHO PDI: Construindo um workflow com Jobs, orquestrador da sequência das operações",
        "PENTAHO PDI: Entendendo os menus principais e o seu GUI e seus componentes",
        "PENTAHO PDI: Comp. pipelines: Sort, Select value, CSV file input, Value mapper, Filter rows, Dummy, Unique rows, Merge Join, Text File Output, Row Normaliser",
        "PENTAHO PDI: Entendendo como podem ser depurados os dados via output, logs",
        "PENTAHO PDI: Number Range, Concat Field, String Operations, Replace in String, IF Field Value is Null, Split Fields, CSV File Input, Mail, File Exist",
        "PENTAHO PDI: Leitura de dados em uma API: Rest Client, JSON Input, JSON Output",
        "PENTAHO PDI: Construindo Workflow com execução de pipelines",
        "PENTAHO PDI: Entendo o uso de variáveis globais no PENTAHO PDI",
        "PENTAHO PDI: Automatização de pipeline ou workflow",
        "PENTAHO PDI: Construindo pipelines em banco de dados Postgresql: Table Input, Table Output, Configurando conexão",
        "PENTAHO PDI: Instalação de banco de dados Postgresql, uso do PGAdmin",
        "PENTAHO PDI: Automatização de JOBs e Transformações com o Kitchen e Pan",
        "PENTAHO PDI: Construção do projeto de dados a sua escolha e correção com o uso do Pentaho PDI",
        "SUPERSET: Plataforma de exploração e visualização de dados criada com base no Apache Superset de código aberto",
        "SUPERSET: Permite a criação de gráficos e dashboards, permitindo a construção de visualização sem código",
        "SUPERSET: É possível executar uma análise mais profunda usando o editor SQL nativo",
        "SUPERSET: Permite a conexão com diversas fontes de dados como Data Warehouse, Data Lake, planilhas, tudo 100% na nuvem",
        "SUPERSET: Possui um ambiente fácil de usar, onde você cria uma workspace de trabalho e constrói seus projetos",
        "SUPERSET: Permite carregar seus dados de diversos bancos de dados e origens diferentes, acessando os dados de forma transparente",
        "SUPERSET: Permite a criação de gráficos (CHART) dos mais variados e com requisitos de filtros e ajustes de campos, podendo gerar novos atributos",
        "SUPERSET: Permite que você utilize o SQL LAB para explorar seus dados via SQL",
        "SUPERSET: Possui um fluxo de trabalho que organiza a construção das análises de dados",
        "SUPERSET: Preset - APACHE Superset é possível conectar seus dados, criar um conjunto de dados, criar gráficos, construir um painel e compartilhar seus insights",
        "SUPERSET: Possui um espaço de trabalho para armazenamento das informações a serem desenvolvidas",
        "SUPERSET: Permite a construção de gráficos diversos: tabela, setores, heatmap, treemap, box plot, linha, sunburst, dentre outros",
        "SUPERSET: Permite a construção de previsões utilizando técnicas como FORECAST",
        "SUPERSET: Permite a colaboração e compartilhamento de gráficos e dashboard"
      ],
      "course_content": {
        "PENTAHO PDI: ETL-Integração e Ingestão de dados": [
          "Apresentação sobre o Pentaho PDI",
          "INFORMAÇÕES IMPORTANTES - Leia antes de começar o curso",
          "Introdução ao Pentaho PDI",
          "Instalação do JAVA",
          "Site de instalação do PDI",
          "Instalação do Pentaho PDI",
          "Entendo como funciona a área de trabalho",
          "Pipeline de Tratamento: arquivo vinhos",
          "Pipeline de tratamento: filtragem e gravação de arquivo ajustado- arquivo vinhos",
          "Pipeline de tratamento: seleção de atributos - arquivos vinhos",
          "Pipeline de tratamento: ordenação, agrupamento - arquivos vinhos",
          "Aula extra - 01 - Leitura de arquivo sem cabeçalho e sem delimitador",
          "Pipeline Merge dos dados: leitura arquivos de entrada e sort dados - 4 arquivos",
          "Pipeline Merge dos dados: componente merge e sort dados - 4 arquivos",
          "Pipeline Merge dos dados: seleção de campos e gravação arquivo - 4 arquivos",
          "Aula extra - 02 - Tratamento e leitura de arquivo colunar",
          "Pipeline Tratamento de dados: arquivo cliente veículos e replace dados",
          "Pipeline Tratamento de dados: operação string, categorias e componente IF null",
          "Pipeline Tratamento de dados: componentes cut e split fields",
          "Pipeline Tratamento de dados: componente number range e concat fileds",
          "Utilizando Debug do PDI dentro de um pipeline",
          "Tratando dados em um Pipeline lendo WebService",
          "Construindo JOB - encadeamento de pipelines",
          "Instalação do Postgres",
          "Carregado dados tabela AUTOR e gerando novos dados tabela NOVO_TB_AUTOR",
          "Aula extra - 03 - Criando JOB para movimentação de arquivos e pastas",
          "Automatizando Jobs e Transformações - Kitchen e Pan",
          "Aula Final - Construa o seu projeto de dados",
          "Vamos responder ao nosso Quiz?"
        ],
        "Preset - APACHE Superset: Dashboard e Visualização de Dados": [
          "Apresentação - O que é o APACHE Superset",
          "Entendendo sobre Preset - APACHE Superset",
          "Criação da conta no ambiente da nuvem do Preset",
          "Realizando a carga dos dados para o treinamento",
          "Criação do Chart - Tabela",
          "Criação do Chart - Treemap",
          "Criação do Chart - Série Temporal e Previsão com FORECAST",
          "Criação de uma nova coluna customizada",
          "Criação do Chart - KPI",
          "Criação do Chart - Sunburst",
          "Criação do Chart - Graph Chart - Gráfico de rede",
          "Criação do Chart - Box Plot",
          "Criação de dashboard - gerando insights e entendo as análises",
          "Criação de filtros nos dashboards",
          "Criação de query pelo SQL LAB",
          "Aula Final - entrega de atividade"
        ]
      },
      "requirements": [
        "Importante ter conhecimento sobre banco de dados, arquivos de dados",
        "Importante que você conheça lógica de programação",
        "Conhecimento elementar de SQL"
      ],
      "description": "Você é apaixonado por análise de dados e busca por uma ferramenta completa que possibilite criar dashboards dinâmicos e ter acesso a informações precisas em tempo real? Então, você não pode perder a oportunidade de realizar o curso de Pentaho PDI e Apache Superset!\nCom essa solução incrível, você poderá integrar dados de diferentes origens e simplificar a gestão do processo de integração, além de criar gráficos, tabelas e mapas de forma simples e intuitiva. Com o Apache Superset, você terá acesso a um mundo de possibilidades para personalizar suas visualizações de dados de acordo com as necessidades da sua empresa ou projeto.\nO curso de Pentaho PDI e Apache Superset irá te ensinar tudo o que você precisa saber sobre essas ferramentas fantásticas. Você aprenderá desde a instalação e configuração das ferramentas até a criação de dashboards avançados, utilizando recursos como drill-down, filtros e outras funcionalidades, tais como:\nO Pentaho PDI é uma ferramenta ETL (Extract, Transform and Load) open source, que permite a integração de dados de diversas fontes em um ambiente centralizado. Suas principais funções são:\nExtração de dados: permite extrair dados de diferentes fontes, como bancos de dados, planilhas e arquivos;\nTransformação de dados: possibilita realizar transformações nos dados extraídos, tais como limpeza, enriquecimento e alteração de formatos;\nCarregamento de dados: permite carregar os dados transformados em um ambiente centralizado, como um data warehouse ou um ambiente de análise de dados.\nJá o Apache Superset é uma plataforma de visualização de dados, que permite criar dashboards interativos e personalizados. Suas principais funções são:\nCriação de dashboards: possibilita criar dashboards personalizados com gráficos, tabelas e mapas;\nExploração de dados: permite explorar as informações presentes nos dashboards de forma intuitiva, utilizando recursos como drill-down e filtros;\nCompartilhamento de informações: possibilita compartilhar as informações contidas nos dashboards com outras pessoas, garantindo que todos tenham acesso às informações relevantes.\nEm conjunto, essas duas ferramentas oferecem uma solução completa para integração e visualização de dados, permitindo que as empresas possam tomar decisões mais assertivas e baseadas em informações precisas.\nCom aulas teóricas e práticas, você poderá se capacitar e ampliar seus conhecimentos em análise de dados, tornando-se um especialista em integração de dados e visualização de informações. Não perca mais tempo e inscreva-se agora mesmo para realizar esse curso empolgante que vai mudar completamente a forma como você realiza a análise de dados!",
      "target_audience": [
        "Estudantes e profissionais de computação, Informática, estatística, data science, analista de dados, engenheiro de dados",
        "Pessoas interessadas em aprender os conceitos sobre ferramentas de ingestão de dados, ou que gostariam adentrar na área de engenharia de dados",
        "Profissionais que, de alguma forma, utilizam dados no seu dia a dia",
        "Profissionais que desejam construir dashboards e gráficos"
      ]
    },
    {
      "title": "Sıfırdan Zirveye Uygulamalarla Veri Bilimi",
      "url": "https://www.udemy.com/course/sfrdan-zirveye-uygulamalarla-veri-bilimi/",
      "bio": "Python ile Veri Bilimi. En güncel alıştırmalarla ile uygulayarak Veri Bilimi öğrenin!",
      "objectives": [
        "Veri Bilimi alanında iş bulabilecek kapasitede olacaksınız",
        "Bol bol proje geliştirerek en güncel bilgileri öğreneceksiniz",
        "Geleceğin mesleklerinden biri olan veri analisti olabileceksiniz",
        "Veri biliminde proje yönetimi yapabileceksiniz"
      ],
      "course_content": {
        "Giriş": [
          "Gerekli Kurulumlar"
        ],
        "Pandas Nedir ?": [
          "Pandas"
        ],
        "Veri Yapıları": [
          "Series Veri Yapısı",
          "Series Veri Yapısı 2",
          "Series Veri Yapısı 3 (Örnek Program)",
          "Data Frame Veri Yapısı",
          "Data Frame 2 ( Del Metodu)",
          "Data Frame 3 ( Örnek Program )",
          "Data Frame 4 ( Örnek Program 2 )",
          "Önemli Metodlar"
        ],
        "Aritmetik İşlemler": [
          "Aritmetik İşlemler - 1",
          "Bir Fonksiyonu Veri Setinde Uygulamak"
        ],
        "Sıralama (Sorting)": [
          "Sıralama (Sorting)",
          "Sıralama(Sorting) - Uygulama"
        ],
        "Veri Okuma Yazma": [
          "Veri okuma",
          "Veri okuma yazma-2",
          "Veri okuma yazma- 3",
          "Veri okuma yazma - 4"
        ],
        "Eksik Veri": [
          "Eksik Veri Nedir ?",
          "Eksik Veri-2 ( Uygulama )"
        ]
      },
      "requirements": [
        "Daha önceden programlama becerisine ihtiyacınız yoktur. Bu kurs ile veri analizini sıfırdan öğrenebilirsiniz."
      ],
      "description": "Sıfırdan Zirveye Uygulamalarla Veri Bilimi kursumuza hoş geldiniz !\n\n\nVeri bilimi günümüz dünyasında oldukça rağbet görmekte olan bir alan haline geldi. Gelişen teknoloji ile birlikte her geçen gün artan verileri analiz yaparak hayatımızı kolaylaştırmamız gerekiyor. Bu ihtiyaçlara yönelik olarak veri bilimi öğrenmenin avantajları oldukça fazla. İş arayışından okul hayatına yazılım geliştiriciliğinden üst düzey yöneticiliğe kadar tüm düzeylerde veri bilimini bu kurs sayesinde rahatlıkla anlayıp uygulayabileceksiniz.\nBu eğitim Python ile Veri Bilimi eğitimidir. Veri Bilimi Python programlama dili üzerinden Jupyter Notebook geliştirme ortamı aracılığı ile anlatılmıştır.\n\n\nGerekli ortam kurulumları kurs içeriğinde anlatılmış ve indirilebilir doküman halinde de sizlere sunulmuştur.\nKursun içeriği her düzeye uygun olarak hazırlanmıştır. Kursu almadan önce bilmeniz gereken herhangi bir programlama becerisine ihtiyaç duymayacaksınız. Kullanılan komutlar yazılan kodların tamamı detaylı bir şekilde açıklanmıştır. Veri biliminde öne geçmek istiyorsanız bu kurs tam size göre. Yazılım dünyasına yeni girmiş ve yolunuzu henüz bulamadıysanız veri bilimi ile bu kurs sayesinde tanışmak geleceğinizi yönlendirmek açısından size rehber olabilecek niteliktedir. Veri bilimini en temelden anlayıp ileri düzeye kadar kendinizi geliştirebileceğiniz bu kursta bol bol uygulama yaparak kod yazma pratiğinizi geliştirebilirsiniz.\n\n\nKursta ele alınacak modül ve konular:\nPandas\nNumpy\nVeri Özetleme\nVeri Görselleştirme\nZaman Serileri\n\n\n\n\nÖğreneceklerinizi pekiştirmenin en verimli yolu alıştırma yapmaktır. Bu kurs ile yeni ve güncel bilgiler öğrenecek, bildiklerinizi pekiştirecek ve Veri Bilimi için gerekli olan becerileri kazanmış olacaksınız.",
      "target_audience": [
        "Veri bilimi alanında çalışmak, proje geliştirmek ve üst düzey bir yazılım geliştiricisi olmak isteyen herkes."
      ]
    },
    {
      "title": "IPK: İstatistiksel Proses Kontrol Kartları",
      "url": "https://www.udemy.com/course/ipk-istatistiksel-proses-kontrol-kartlari/",
      "bio": "X-R Kartı Doldurma ve Yorumlama",
      "objectives": [
        "Makine ve Proses yeterlik bilgisi",
        "SPC / İPK Mantığı",
        "Makine Yeterlilik Çalışması",
        "Proses Yeterlilik Çalışması",
        "XR Kartı kullanımı ve yorumlanması"
      ],
      "course_content": {
        "Giriş": [
          "Giriş",
          "SPC / IPK Amacı",
          "Tanımlar",
          "Genel Hatlar",
          "IPK Yöntemleri - 1",
          "IPK Yöntemleri - 2",
          "X-R Kartı",
          "Hesaplama Yöntemleri",
          "Kapanış"
        ]
      },
      "requirements": [
        "Başlangıç seviyesinde bilgisayar bilgisi",
        "Temel Excel Programı Kullanım Bilgisi"
      ],
      "description": "Kalite kontrol yöntemleri içerisinde önemli bir yer tutan İstatistiksel çalışmanın bir adımı olan X-R kartlarının kullanılması ve yorumlanması hakkında bilgi sahibi olmak isteyen kullanıcılar için hazırlamış olduğumuz bu eğitim ile, temel istatistiksel tanımların neler olduğunu, Makine ve Proses yeterlilik çalışmalarının nasıl yapıldığını, X-R kartının nasıl doldurulduğunu ve yorumlandığını öğrenebileceksiniz.\nIPK sürecini Kalite derneğinin yorumlamasıyla incelemek gerektiğinde \"Değişkenlik birçok işte ve proseste kalitesizliğe yol açan en büyük etkendir. Proseslerin kontrol altına alınabilmesi, öncelikle değişkenlik kaynaklarının doğru belirlenebilmesi ve doğasının anlaşılabilmesine bağlıdır. Bu da doğru veri toplayabilme ve sağlıklı bir şekilde analiz etme yeteneğini gerektirir. İstatistiksel Proses Kontrol ile değişkenlikleri sürekli azaltarak sorunsuz bir çalışma ortamına ulaşmak, kaliteyi yükseltmek, fireleri, gereksiz ayarları yok ederek maliyeti düşürmek ve en üst seviyede müşteri tatminini sağlamak mümkün olmaktadır.\" olarak ifade edilmektedir. Eğitim boyunca bu alanda değerlendirmeler yapacağız.\nEğitim içeriğinde SPC / IPK amacı anlatılmakta ve bu aşamada IPK yı nerede kullanabileceğiniz hakkında önerilerde bulunulmaktadır. Tanımlamalar bölümünde, Eğitim boyunca anlatılan içeriklerin kısa özetleri ve anlamları belirtilmektedir. Genel hatlar bölümünde ise, IPK temellerini oluşturan bölümler hakkında bilgi verilmektedir. Dersin devamında iki ayrı bölümde IPK yöntemleri gösterilmektedir. Nihai bölümde IPK kartının nasıl doldurulduğu anlatılmaktadır.\nEğitim sonunda Excel formatında hazırlanmış olan Makine Yeterlilik, Proses Yeterlilik ve XR Kartı formatlarına sahip olacak ve örnekleri kendiniz uygulayabileceksiniz.",
      "target_audience": [
        "Üretim hattında kalite kontrol işlemi yapanlar",
        "Kalite kontrol metotlarını öğrenmek isteyenler",
        "Yeni mühendisler ve mühendis adayları",
        "Üretim hattını kontrol altına almak isteyenler"
      ]
    },
    {
      "title": "Conceitos de DS e ML para Problemas Científicos",
      "url": "https://www.udemy.com/course/conceitos-de-ds-e-ml-para-problemas-cientificos/",
      "bio": "Aprenda os fundamentos e princípios de dados e aprendizado de máquina aplicado em problemas científicos",
      "objectives": [
        "Entender os conceitos de ciência de dados e aprendizado de máquina e como eles podem ser usados na ciência",
        "Conhecer os principais algoritmos utilizados em tarefas de classificação, regressão e agrupamento",
        "Conhecer as principais arquiteturas de redes neurais",
        "Entender como usar algoritmos em projetos científicos"
      ],
      "course_content": {
        "Introdução ao curso": [
          "Bem vindo ao curso!",
          "Como assistir o curso"
        ],
        "Conceitos sobre dados, variáveis, e introdução á Machine learning": [
          "Diferentes tipos de variáveis e transformação de variáveis",
          "O objetivo de ciência de dados e machine learning",
          "Normalização/escalonamento de dados",
          "Aprendizado supervisionado e não supervisionado",
          "Detalhes sobre treinamentos de modelos",
          "Visualizando dados com gráficos: parte 1",
          "Visualizando dados com gráficos: parte 2",
          "Análise de componentes principais (PCA)",
          "Estudo de caso real: PCA em análise de poluição por metais pesados no solo",
          "Questões"
        ],
        "Machine learning: classificação": [
          "Introdução à classificação",
          "O algoritmo Naïve Bayes: introdução",
          "O algoritmo Naive Bayes : aplicações/exemplos na ciência",
          "Árvores de decisão: uma introdução",
          "Árvores de decisão: Conceito de entropia",
          "Floresta aleatória (Random Forest): Um incremento sobre as árvores de decisão",
          "Introdução a classificação com KNN",
          "O problema de números extremos de K e outros métodos de calcular distância",
          "Algoritmo KNN: exemplos científicos",
          "Support Vector Machines (SVM): Introdução",
          "A margem no SVM",
          "SVM: exemplos científicos",
          "Estudo de caso real: diagnóstico de diabetes com random forests",
          "Questões"
        ],
        "Machine learning: regressão": [
          "Regressão: introdução",
          "Regressão linear simples: introdução",
          "Regressão linear múltipla: introdução",
          "Estudo de caso real: regressão para estimar poluição por material particulado",
          "Questões sobre regressão"
        ],
        "Machine learning: agrupamento": [
          "Algoritmo K-means: Uma introdução e funcionamento",
          "Exemplos cientificos para agrupamento",
          "Introdução ao clustering e algoritmo K-means",
          "Agrupamento hierárquico: introdução",
          "Agrupamento hierárquico na taxonomia",
          "Estudo de caso real: agrupamento meteorológico baseado em dados geográficos",
          "Questões sobre agrupamento"
        ],
        "Introdução as redes neurais": [
          "Introdução às redes neurais básicas e inspiração biológica",
          "Introdução às redes neurais básicas: parte 2",
          "Introdução às redes neurais básicas: parte 3",
          "Redes neurais convolucionais e sua inspiração no córtex visual",
          "Redes neurais convolucionais: parte 2",
          "Redes neurais recorrentes: parte 1",
          "Redes neurais recorrentes: parte 2",
          "Redes de Hopfield como modelo da memória humana: uma breve visão geral",
          "Redes neurais líquidas: uma introdução",
          "Estudo de caso real: Redes convolucionais para detecção de objetos astronômicos",
          "Questões sobre redes neurais artificiais",
          "Conclusão",
          "AULA BÔNUS"
        ]
      },
      "requirements": [
        "Conhecimento básico de matemática é desejável"
      ],
      "description": "O curso \"Princípios de Ciência de Dados e Aprendizado de Máquina para Ciências Naturais\" foi desenvolvido para conectar disciplinas científicas tradicionais aos campos em rápido crescimento da Ciência de Dados (DS) e do Aprendizado de Máquina (ML). À medida que a pesquisa depende cada vez mais de grandes conjuntos de dados e métodos computacionais avançados, torna-se essencial que cientistas saibam como utilizar técnicas de DS e ML para melhorar seu trabalho.\nEste curso oferece uma introdução aos conceitos-chave de Ciência de Dados e Aprendizado de Máquina, especificamente voltados para cientistas e pesquisadores em áreas como Biologia, Química, Física e Ciências Ambientais. Você aprenderá os fundamentos da análise de dados, incluindo tratamento e visualização de dados, antes de avançar para algoritmos de aprendizado de máquina que ajudam a identificar padrões e fazer previsões a partir dos dados.\nO curso não exige conhecimentos prévios em programação e foca nos conceitos teóricos fundamentais. Ele está estruturado em seis seções principais:\n\nIntrodução\nComeçaremos apresentando o curso, cobrindo suas principais características, conteúdo e como acompanhar as aulas.\nConceitos Básicos de DS/ML\nAbordaremos conceitos fundamentais como variáveis, escalonamento de dados, treinamento, conjuntos de dados e visualização de dados.\nClassificação\nNesta seção, discutiremos os principais algoritmos de classificação, como árvores de decisão, florestas aleatórias, Naïve Bayes e KNN, com exemplos de como podem ser aplicados na pesquisa científica.\nRegressão\nFaremos uma breve introdução à regressão linear e múltipla, discutindo os conceitos principais e fornecendo exemplos relevantes para as ciências.\nAgrupamento (Clustering)\nEsta seção se concentrará em métodos de agrupamento padrão e hierárquico, juntamente com exemplos práticos para aplicações científicas.\nRedes Neurais\nPor fim, introduziremos redes neurais, discutindo sua inspiração biológica e arquiteturas comuns, como Redes Neurais Feedforward (FNN), Redes Neurais Convolucionais (CNN), Redes Neurais Recorrentes (RNN) e Redes Hopfield.",
      "target_audience": [
        "Pessoas das áreas de Ciência, Tecnologia, Engenharia e Matemática interessadas em entender os conceitos de DS/ML",
        "Pessoas de TI/Ciência da Computação interessadas em saber como os algoritmos podem ser usados em projetos científicos",
        "Pessoas com formação em matemática interessadas em entender conceitos de DS/ML e ciências"
      ]
    },
    {
      "title": "人工智能基础入门：走进AI世界",
      "url": "https://www.udemy.com/course/ai-dudec/",
      "bio": "让人工智能为你所用",
      "objectives": [
        "看懂AI",
        "通俗易懂",
        "了解AI的原理",
        "获得启发，为未来深入学习人工智能打下基础"
      ],
      "course_content": {
        "课程导读": [
          "课程介绍",
          "讲师介绍",
          "课程大纲"
        ],
        "Part 1:走进Al-看懂人工智能": [
          "01-Al是什么?",
          "02-AI是如何学习的?",
          "03-AI能够做什么?",
          "04-如何看待人工智能?"
        ],
        "Part 2:思维进阶篇-让你“懂”AI": [
          "01-什么是机器学习?",
          "02-什么是深度学习?",
          "03-“大模型”的本质是什么?",
          "04-如何找到AI落地场景？"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "无经验"
      ],
      "description": "在当今快速发展的科技行业中，人工智能（AI）已经成为引领变革的核心力量。然而，对于许多初学者和跨行业从业者来说，AI的复杂性和广阔性常常令人望而生畏。为了解决这个问题，我们推出了《人工智能基础入门：走进AI世界》课程，旨在帮助学员快速掌握AI的核心概念与实际应用。\n本课程首先带你走进AI的世界，解读AI的本质、学习方式以及它能做什么。接着，通过进阶篇的深入学习，你将理解机器学习、深度学习的基本概念，并探索“大模型”的本质。更重要的是，我们将教你如何找到AI的落地场景，让你的知识不再停留在理论层面。\n如果你在工作中遇到与AI相关的挑战，或者你希望在未来职业生涯中抢占先机，那么这门课程将是你的不二之选。通过系统学习，你将能够建立对AI的全面认知，为未来的工作和学习打下坚实的基础。现在就加入我们，一起走进AI的奇妙世界吧！\n对于企业来说，真正做好人工智能（AI）方向的价值是巨大的。\n提升业务效率与自动化水平：\n引入AI技术可以自动化处理大量重复性、繁琐的任务，释放人力资源，使员工能够专注于更具创造性和价值的工作。AI通过数据分析和预测模型，可以优化业务流程，减少错误，提高生产效率和服务质量。\n创新商业模式与产品服务：\nAI技术为企业提供了创新产品和服务的机会，通过智能推荐、个性化定制等功能，增强用户体验，提升客户满意度。AI可以助力企业开发新的商业模式，如基于大数据分析的精准营销、智能客服等，为企业带来新的增长点。\n增强企业竞争力与创新能力：\n领先的AI技术可以使企业在激烈的市场竞争中脱颖而出，通过智能化产品和服务吸引更多客户。AI技术可以帮助企业持续创新，不断迭代优化产品和服务，保持行业领先地位。AI技术还可以降低企业的运营成本，提高盈利能力，从而增强企业的整体竞争力。\n课程内的知识点\nAI是未来社会的基础设施\nAI是生活/工作中的必需品\nAI是获取和处理海量信息的必备\n人工智能的六个能力\nAI的本质是什么？\nAI学习的五种方法\nAI能够做什么？\nAI和人的边界\n机器学习的原理\n深度学习 VS 机器学习\n大模型为什么能够理解我们的语言？\n为什么和王老师学习？\n深厚的学术背景与实战经验：王老师作为北京邮电大学的本硕毕业生，在智能科学领域有着扎实的学术基础。这种学术背景使得王老师能够深入浅出地讲解人工智能的原理和理论，确保学员能够系统地掌握相关知识。同时，王老师作为国内首批语言模型落地实践者，拥有丰富的实战经验，能够将理论知识与实际应用相结合，让学员更好地理解和应用AI技术。\n一线大厂产品专家的经验：王老师曾在腾讯、滴滴等一线大厂担任产品专家，这意味着他具备将AI技术应用于实际产品中的丰富经验。在学习过程中，王老师可以分享这些宝贵经验，帮助学员了解AI技术在产品中的应用场景、挑战和解决方案，为学员未来的职业发展提供有力支持。\n丰富的辅导经验：王老师辅导过120+AI创业项目，这充分展示了他在AI领域的广泛影响力和丰富经验。在课程中，王老师可以分享这些项目的案例和心得，让学员更加直观地了解AI技术的实际应用和效果。同时，王老师还可以根据学员的实际情况，提供个性化的指导和建议，帮助学员更好地规划自己的职业道路。\n关注AI落地：王老师作为《AI落地》一书的作者，对AI技术的实际应用和落地有着深入的思考和实践。在课程中，王老师将重点关注AI落地方面的内容，帮助学员了解如何将AI技术应用于实际场景中，解决实际问题。这种关注实际应用的教学方式将使学员在学习过程中更加具有针对性和实用性。",
      "target_audience": [
        "非技术专业、无算法背景的职场人",
        "想了解AI、落地AI的企业老板、高管",
        "不懂技术，希望学会使用AI的“普通人”"
      ]
    },
    {
      "title": "Fundamental Data Science and Data Analyst",
      "url": "https://www.udemy.com/course/fundamental-data-science-and-data-analyst/",
      "bio": "Data Science and Data Analyst for Beginners",
      "objectives": [
        "Pengenalan & Instalasi KNIME",
        "Pengenalan Machine Learning",
        "Studi Kasus dan Analisis Korelasi Tingkat Penjualan Product Terhadap Karakteristik Konsumen",
        "Studi Kasus dan Analisis Prediksi Tingkat Penjualan Product Menggunakan dengan Mudah Machine Learning",
        "Data analyst role 101",
        "Data Sources",
        "Data visualisation",
        "Data in business: descriptive statistic, experiment with data",
        "Mengenal skill data analyst (Python/ R/ SQL/etc)",
        "TABLEAU Workspace",
        "Aggregation and Calculated Fields",
        "Choosing The Best Chart for Your Needs",
        "Dashboard Visualisation and Layouting",
        "Job hunting tips"
      ],
      "course_content": {
        "Tips dan Trik Memulai Karir Sebagai Data Analyst": [
          "Introduction",
          "Data Roles",
          "Data Analyst 101",
          "Learning Path",
          "Analytics Process",
          "Case Study",
          "Example",
          "Kuis",
          "Job Hunting 101",
          "Tanya Jawab"
        ],
        "Story Telling Data with TABLEAU": [
          "Why Tableau",
          "Why We Use Visualization?",
          "Introduction to TABLEAU #1",
          "Introduction to TABLEAU #2",
          "Introduction to TABLEAU #3",
          "Where Should I Start",
          "Tanya Jawab"
        ],
        "Belajar Data Science dan Kuasai Dunia Digital": [
          "Online Learning Knime Introduction",
          "Fundamental Machine Learning #1",
          "Fundamental Machine Learning #2",
          "Fundamental Machine Learning #3",
          "Learning Method in Data Mining Algorithms",
          "Cross-Industry Process for Data Mining"
        ],
        "Fundamental KNIME for Data Science": [
          "Introduction",
          "KNIME",
          "Algorithm & Machine Learning",
          "Example Report Data Science",
          "Customer Segmentation",
          "KNIME Analytics Platform, Dataset & KNIME Workflow",
          "Practice Using KNIME #1",
          "Practice Using KNIME #2",
          "Practice Using KNIME #3",
          "Tanya Jawab"
        ]
      },
      "requirements": [
        "Koneksi internet yang stabil",
        "Laptop",
        "Meng-install TABLEAU"
      ],
      "description": "Profesi data analyst dan data scientist tentunya memiliki kesamaan dan perbedaan. Keduanya sama-sama berkutat pada urusan data dan banyak diminati oleh perusahaan maupun pencari kerja. Dikutip dari data McKinsey Global Institute bahwa di tahun 2019 ada setidaknya 200 ribu lowongan untuk profesi ini. Namun dua bidang ini ternyata tidak bisa disamaratakan begitu saja.\nMemangnya apa sih tugas seorang data analyst dan data scientist? Tugas data analyst berfokus untuk menganalisa data yang dimiliki perusahaan agar mendapatkan insight yang berguna bagi pengembangan strategi perusahaan. Sedangkan data scientist bertugas untuk menganalisa data dengan menggunakan algoritma maupun cara-cara lainnya seperti bahasa pemrograman. Data scientist dan data analyst sama-sama bertujuan untuk mencari pola, tren dan perilaku konsumen.\nSeorang data analyst akan berkutat dengan banyak data setiap harinya. Diantara tools yang wajib dikuasai oleh seorang data analyst adalah TABLEAU. TABLEAU akan membantu kamu untuk memvisualisasikan sebuah data menjadi lebih menarik dan mudah untuk dipresentasikan sehingga data yang disajikan dapat dipahami bersama.\nLain halnya dengan data analyst, mempelajari machine learning dan big data menjadi sebuah keharusan untuk seorang data scientist. Hal tersebut dapat mempermudah dan mempercepat stakeholder mengambil keputusan yang tepat. Oleh karena itu, memiliki skill set tersebut menjadi keunggulan dalam dunia kerja dan bisnis saat ini. Baik itu sebagai karyawan, manager, leader, business owner bahkan fresh graduate yang baru mulai cari kerja.\n\nYuk bekali diri kamu agar punya nilai plus di dunia kerja dengan skill set ekstra yang kamu punya. Pelajari video course ini dan dapatkan ilmu seputar TABLEAU dan KNIME yang bisa kamu gunakan untuk menganalisa customer behaviour.",
      "target_audience": [
        "Siapa saja yang tertarik mempelajari data science dan data analyst"
      ]
    },
    {
      "title": "강화학습 기초",
      "url": "https://www.udemy.com/course/build_reinforcement_learning/",
      "bio": "가장 쉬운 파이썬 언어로 어려운 개념도 쉽게 설명하는 영상을 보시고 따라해보세요. 만들어 보면, 인공지능이 뭔지 감 잡을 수 있습니다!",
      "objectives": [
        "머신러닝 중 강화학습 기초",
        "주어진 문제를 해결하기 위해 강화학습을 적용해가는 과정",
        "파이썬 웹 프레임워크 플라스크를 사용하는 방법"
      ],
      "course_content": {
        "강좌 소개": [
          "소개"
        ],
        "강화학습 이론": [
          "환경",
          "스테이트, 에이전트, 리워드",
          "마르코프 결정 과정 1",
          "마르코프 결정 과정 2",
          "리워드 결정 방법",
          "강화학습 방법",
          "2.7 학습된 머신이 플레이하는 방법"
        ],
        "개발환경 설치": [
          "개발환경 설치"
        ],
        "RLkit 소개": [
          "RLkit 동작 이해 1",
          "RLkit 동작 이해 2",
          "강화학습 훈련 방법 1",
          "강화학습 훈련 방법 2"
        ],
        "강화학습 구현": [
          "ML 클래스에 learn 함수 구현"
        ],
        "실습하기": [
          "틱택톡 게임 실행하기"
        ],
        "맺는말": [
          "앞으로 과제"
        ]
      },
      "requirements": [
        "파이썬을 기초 이상으로 다루셔야 합니다."
      ],
      "description": "강화학습 기초 강좌입니다.\n환경, 에이전트, 리워드 개념을 이해할 수 있습니다.\n마르코프 결정 과정을 알아봅니다.\n3 x 3 오목을 두는 인공지능을 만들어 볼 수 있습니다.",
      "target_audience": [
        "파이썬 기초를 배우고 다음 단계로 실습 프로젝트를 찾으시는 분",
        "인공지능을 만들어보고 싶은 분",
        "알파고를 보고 저런 걸 만들어보고 싶다고 생각하셨 던 분"
      ]
    },
    {
      "title": "Welcome2KI Teil 3: KI Deep Learning Projekte selbst umsetzen",
      "url": "https://www.udemy.com/course/welcome2ki-teil-3-ki-deep-learning-projekte-selbst-umsetzen/",
      "bio": "PyTorch und fastai ermöglichen einen state-of-the-art Deep Learning Klassifizierer mit nur 8 Zeilen Code. Unvorstellbar?",
      "objectives": [
        "Umsetzung eines Bildklassifizierers mit dem fastai Framework und PyTorch",
        "Aufbau eines individuellen Bilddatensets für unser Deep Learning System",
        "Implementierung eines individuellen Biildklassifizierers end-2-end.",
        "Wir generieren das Datenset, erstellen das Modell und fügen mit voila ein graphisches Userface hinzu. Im Anschluß erfolgt das kostenfreie Deployment im Web.",
        "Wie kann ich vom dem Thema Data Augmentation profitieren? Wie hilft das fastai Framework bei kleinen Datensets?",
        "Einsatz und Verwendung der google colab Plattform, um mithilfe einer kostenfreien GPU schnell neuronale Netze trainieren zu können.",
        "Woher kommt die Idee neuronaler Netze? Wie sind neuronale Netze aufgebaut?",
        "Worin liegt der große Vorteil von neuronalen Netzen?",
        "Worin besteht der Unterschied zwischen einer Metrik und einer loss-Funktion?",
        "Was ist das Approximations-Theorem? Etwas Mathematik, leicht verständlich erklärt.",
        "In-Depth Analyse. Was passiert hinter den Kulissen von fastai? Wir implementieren sämtliche Schritte mit purem Python nach!",
        "Warum kann ich Deep Learning Modelle auch für Datensets mit nur wenigen Elementen einsetzen?",
        "Was genau bedeutet das Konzept \"Transfer-Learning\" und wie kann ich es für einen Bild-Klassifizierer nutzen?",
        "Wie funktioniert der (Stochastic) Gradient Descent Algorithmus? Wie kann ich diesen mit reinem Python Code implementieren?",
        "Was genau ist ein Multi-Label Klassifizierer und wie baue ich eine solche Architektur selbst mithilfe von PyTorch und fastai?",
        "Was verbirgt sich hinter dem Begriff \"Image Regression\"?",
        "Wir implementieren ein Modell, das die Koordinaten der Mitte des Gesichts auf einem Portraitfoto errechnet."
      ],
      "course_content": {
        "Einleitung": [
          "Einleitung",
          "Infrastruktur - Arbeiten mit Google Colab",
          "Infrastruktur - Editier- vs. Befehlssicht in Colab",
          "Infrastruktur - Plotten von Funktionen direkt im Notebook",
          "User Interfaces mithilfe von IPython Widgets direkt im Notebook umsetzen",
          "Laufzeitumgebung auf GPU ändern, Verknüpfung mit gdrive herstellen",
          "Shell / Terminalbefehle direkt im Notebook ausführen",
          "Erstelle dein erstes Notebook mithilfe von Google Colab"
        ],
        "Einführung Neuronale Netze / Deep Learning Frameworks": [
          "Einführung in Deep Learning - Historie, Anwendungsbereich, ...",
          "Deep Learning Frameworks",
          "Quiz Neuronale Netze Einführung"
        ],
        "Das erste Deep Learning Modell": [
          "Deep Learning Cats vs. Dogs Classifier - Code Walk Through 1",
          "Das erste Deep Learning Modell",
          "Deep Learning Cats vs. Dogs Classifier - Code Walk Through 2",
          "Cats vs Dog Dataset Quiz",
          "Testen unseres Classifiers",
          "Cats vs. Dogs Klassifizierer - Theoretischer Hintergrund 1/2",
          "Cats vs. Dogs Klassifizierer - Theoretischer Hintergrund 2/2",
          "Overfitting",
          "Auswahl der Daten für das Validation-Set",
          "Metrik vs. Loss-Funktion",
          "Was lernt unser Modell eigentlich intern?",
          "Das erste Deep Learning Modell - Quiz"
        ],
        "Die nächsten Schritte - Ein vollständiger Bild-Classifier mit eigenem Datenset": [
          "Einführung",
          "Bilder für unser Datenset mithilfe von Bing-Image Search laden",
          "Download der Bilder für unseren Classifier",
          "Persistentes Speichern unseres Bilddatensets",
          "Vorbereitungen für den CNN Learner",
          "Dataloaders erstellen",
          "Trainieren unseres CNN-Learners",
          "Inference - Prognosen mit unserem Classifier erstellen",
          "User Interface mithilfe von IPyWidgets",
          "Deployment mit Voila und dem Binder Service",
          "Data Augmentation Techniken",
          "Confusion Matrix",
          "Image Data Cleaner",
          "Eigenes DL-Modell erstellen",
          "Analyse deines DL-Modells"
        ],
        "Deep Learning Grundlagen": [
          "Das MNIST MachineLearning Datenset",
          "MNIST Berechnung der Durchschnittswerte",
          "Ermittlung der Distanz zur perfekten 3 / 7 (Theorie)",
          "L1-Norm und MSE im Code",
          "NumPy Arrays vs. PyTorch Tensors",
          "Einführung Handling von Tensoren",
          "Validierung unseres Modells mittels Broadcasting",
          "Stochastic Gradient Descent - Basics",
          "Berechnung der Gradienten mit PyTorch",
          "Wahl der richtigen Learningrate",
          "Vollständiges Optimierungsbeispiel mithilfe von Gradient Descent",
          "Zusammenfassung Gradient Descent",
          "Stochastic Gradient Descent mit MNIST",
          "Eigener Optimizer",
          "Nicht-Linearität zu unserem Modell hinzufügen",
          "Zusammenfassung, Abschluss"
        ],
        "Hinter den Kulissen des Learner Objekts in fastai": [
          "Erweitertes Data Augmentation - Image Presizing",
          "Image Presizing - Codebeispiel",
          "Vom Binärlkassifizierer zum Multi-Label-Klassifizierer",
          "Die Cross-Entropy-Loss-Function",
          "Die Bedeutung vom Logarithmus in unserer Loss-Funktion",
          "Interpretation unseres Multi-Label Modells",
          "Die Learning Rate als Hyperparameter. Learning-Rate-Finder aus fastai",
          "Unfreezing und Discriminative Learning Rates",
          "Anzahl Trainings-Epochs, Wahl der Basis-Modell-Architektur",
          "Outro - Zusammenfassung"
        ],
        "Multi-Label Classifier": [
          "Das Datenset für den Multi-Label Classifier",
          "Multi-Label Classifier erstellen."
        ],
        "Erstellen eine Image-Regression Models": [
          "Image-Regression: Unser Datenset",
          "Image-Regression: Der Learner"
        ],
        "Techniken, um das Trainingsergebnis zu verbessern": [
          "Normalisierung der Trainingsdaten",
          "Normalisierung der Trainingsdaten Codebeispiel",
          "Progressive Resizing",
          "Test-Time-Augmentation",
          "Test-Time-Augmentation 2",
          "MixUp Augmentation Technik",
          "Mix Up Augmentation 2",
          "Label Smoothing"
        ],
        "Abschlussprojekt": [
          "Optimierung deines DL-Modells"
        ]
      },
      "requirements": [
        "Du solltest zumindest über Basisprogrammierkenntnisse in Python verfügen.",
        "Wir empfehlen parallel unsere anderen Kurs aus der Reihe welcome2ki zu absolvieren. So schaffst du dir einen guten Überblick über das gesamte Thema \"künstliche Intelligenz\"."
      ],
      "description": "Der dritte Teil aus der Reihe \"Welcome 2 KI\" bringt uns zur Entwicklung von künstlicher Intelligenz mittels Deep Learning Techniken. Zum Einsatz kommen PyTorch und das darauf aufbauende fastai Framework.\nLerne, wie du mit fastai und PyTorch mit nur 8 Zeilen Code einen state-of-the-art Bildklassifizierer bauen kannst!\nWir lernen die kostenfreie Entwicklungsumgebung für Deep Learning von google: colab Notebooks kennen. Auf diese Weise können wir sehr einfach mit einer kostenfreien GPU arbeiten.\nIm Anschluß entwickeln wir eine eigene Deep Learning Applikation und arbeiten den gesamten Prozess end-to-end bis zur Erstellung eines User-Interfaces für unsere Deep Learning Anwendung durch.\nNeben den Techniken und der geschichtlichen Entwicklung von neuronalen Netzen, besprechen wir das Thema Data Augmentation.\nWie können wir aus einem beschränkten Datenset ein größeres machen und warum macht der Einsatz von verschiedenen Data Augmentation Techniken Sinn? Wie unterstützt uns fastai und PyTorch bei der Data Augmentation?\nWir stellen unser eigenes Trainingsdatenset automatisiert zusammen,\nbauen ein Deep Learning Modell mithilfe neuronaler Netze,\ntesten unser Modell mit einem Validierungsdatenset und\nDeployment unser Modell als Webapplikation kostenfrei im Internet\nDanach gehen wir technisch in die Tiefe und implementieren sämtliche Schritte, die für das Trainieren eines Deep Learning Modells erforderlich sind mit reinem Python Code nach. Auf diese Weise verstehen wir die Zusammenhänge, was hinter den Kulissen einer künstlichen Intelligenz auf Basis von Deep Learning abläuft.\nWas passiert genau während des \"Lernens\" unseres Modells?\nWorin liegt der Unterschied zwischen der Metrik (zB Genauigkeit) unseres Modells und der Loss-Funktion (ZB Mean-Squared-Error). Warum benötigen wir beides?\nWie hilft uns ein Dataloader beim Umgang mit unseres Trainingsdaten?\nWie initialisieren wir die Gewichte (Parameter) in unserem neuronal Netz?\nWas passiert während des Trainings mit den Gewichten des neuronalen Netzes?\nWie können wir PyTorch nutzen, um die Gradienten unserer Gewichte im neuronalen Netz zu berechnen?\nWir implementieren den Gradient-Descent Algorithmus \"from-the-scratch\" und\nintegrieren Mini-Batches, um zu Stochastic-Gradient-Descent zu gelangen.\nWir implementieren unseren eigenen Optimizer für die Verwendung mit PyTorch.\nWarum benötigen wir \"Nicht-Linearität\", um ein komplexes neuronales Netz aufbauen zu können?\nWas genau ist eine \"Aktivierungsfunktion\" wie zB. ReLU (Rectified Linear Unit)?\nWie kann ich mithilfe der Confusion Matrix das Ergebnis meines Modells bewerten?\nWie funktioniert ein Multi-Label Klassifizierer?\nSämtlicher Code, der im Kurs besprochen wird, ist via github verfügbar und kann direkt in google colab geöffnet und ausgeführt werden. Der Link zum Kurs-Repository befindet sich direkt in den Kursressourcen.\nDies ist nicht der Machine Learning Kurs von jannis seemann - es werden aber vergleichbare Inhalte vermittelt. Allerdings unter Verwendung von PyTorch und fastai.",
      "target_audience": [
        "Python Entwickler mit Interesse an künstlicher Intelligenz, deep learning und PyTorch / fastai."
      ]
    },
    {
      "title": "ANALYSE STATISTIQUE DE DONNEES PAR JAMOVI [Français]",
      "url": "https://www.udemy.com/course/apprentissage-des-statistiques-avec-jamovi/",
      "bio": "Analyse quantitative d’une étude concrète de A a Z",
      "objectives": [
        "Analyser une étude quantitative de A a Z",
        "Installer et importer des données sous Jamovi",
        "Présentation de l'outil et du corpus",
        "Traitement, imputation des données manquantes",
        "Vérification de la qualité des données et de la normalité des distributions",
        "Analyse corrélationnelle",
        "Analyse factorielle",
        "Extraction des facteurs a partir de l'analyse factorielle exploratoire",
        "L'interprétation qualitative des facteurs identifiés et justification statistique",
        "Etude et interprétation de la fiabilité des facteurs",
        "Analyse inférentielle",
        "Calcul des scores individuels",
        "Réaliser un test t de Student"
      ],
      "course_content": {
        "Introduction": [
          "Présentation du cours"
        ],
        "Prise en main de Jamovi": [
          "Installer et importer des données sous Jamovi",
          "Présentation de l'outil et du corpus",
          "Clarification des données : partie 1",
          "Clarification des données : partie 2",
          "Prise en main de Jamovi"
        ],
        "Statistiques descriptives": [
          "Traitement, imputation des données manquantes",
          "Vérification de la qualité des données et de la normalité des distributions",
          "Statistiques descriptives"
        ],
        "L'analyse corrélationnelle": [
          "Calculer et interpréter des corrélations dans Jamovi",
          "L'analyse corrélationnelle"
        ],
        "L'analyse factorielle": [
          "Aspects généraux : À quoi correspondent les options présentes sur l’interface",
          "Extraction des facteurs a partir de l'analyse factorielle exploratoire",
          "L'interprétation qualitative des facteurs identifiés/ justification statistique",
          "Etude et interprétation de la fiabilité des facteurs",
          "L'analyse factorielle"
        ],
        "L'analyse inférentielle": [
          "Calcul des scores individuels",
          "Test t de Student : À quoi correspondent les options présentes sur l’interface",
          "Réaliser et interpréter un test t de Student"
        ]
      },
      "requirements": [
        "Avoir un ordinateur ! C’est tout !"
      ],
      "description": "Bonjour et bienvenue dans cette introduction au cours sur les statistiques avec Jamovi, l'objectif de cette formation va être de vous apprendre à faire une analyse quantitative d’une étude de A à Z, pour cela nous allons étudier les différents types de variables, leurs échelles de mesures et voir comment les encoder correctement dans Jamovi, et le traitement, imputation des données manquantes, nous allons également apprendre à utiliser les différents indicateurs statistiques et les différents types de graphiques, en va voir aussi dans ce cours l’analyse corrélationnelle, l’analyse factorielle avec l’extraction des facteurs, L'interprétation qualitative des facteurs et l’étude de fiabilité. En va voir aussi une partie consacrée à l’analyse inférentielle avec le calcul des scores individuels et les tests de Student et de Mann Withny, le test d’homogénéités…etc.\nCe cours est basé sur une modalité pédagogique du micro-learning, cette méthode d’apprentissage privilégiant des modules très courts de moins de 5 minutes, Chacun d’eux étant centré sur une notion précise. Et l’apprentissage passe toujours par la pratique sur une étude concrète.\nJ'espère que cette formation vous conviendra bien à vos attentes, si vous avez la moindre question n'hésitez surtout pas à m'envoyer un message.\nyassine.karroumi@gmail.com\n\n\nContenu de la formation :\n•Chapitre 1_Prise en main de Jamovi\n1_Installer et importer des données sous Jamovi\n2_Présentation de l'outil et du corpus\n3_Clarification des données : partie 1\n4_Clarification des données : partie 2\n•Chapitre 2_Statistiques descriptives\n1_Traitement, imputation des données manquantes\n2_Vérification de la qualité des données et de la normalité des distributions\n•Chapitre 3_Analyse corrélationnelle\n1_Calculer et interpréter des corrélations dans Jamovi\n•Chapitre 4-Analyse factorielle\n1_Aspects généraux-À quoi correspondent les options présentes sur l’interface\n2_Extraction des facteurs a partir de l'analyse factorielle exploratoire\n3_L'interprétation qualitative des facteurs identifiés et justification statistique\n4_Etude et interprétation de la fiabilité des facteurs\n•Chapitre 5-. Analyse inférentielle\n1_Calcul des scores individuels\n2_Test t de Student-À quoi correspondent les options présentes sur l’interface\n3_Réaliser et interpréter un test t de Student",
      "target_audience": [
        "les étudiants de licence/master qui expérimentent avec les analyses statistiques, ainsi que les chercheurs qui utilisent des logiciels statistiques ponctuels."
      ]
    },
    {
      "title": "AutoKerasによる分類モデル/回帰モデル作成講座 : 【AutoML/Python/SIGNATE】",
      "url": "https://www.udemy.com/course/autokeras-rc-learning-pythonsignate/",
      "bio": "Learn how to create Supervised Regression model with AutoKeras(AutoML) & Participate in SIGNATE",
      "objectives": [
        "AutoKeras(AutoML)を使用した回帰モデルの作成",
        "AutoKeras(AutoML)を使用した分類モデルの作成",
        "Argumentsの設定方法",
        "Signateに向けたモデルの作成"
      ],
      "course_content": {
        "はじめに": [
          "講座の概要",
          "AutoKerasとは ／ インストール"
        ],
        "回帰【Tips】": [
          "データセットの取得",
          "AutoKerasの実行",
          "Arguments【column_names】",
          "Arguments【column_types】",
          "Arguments【output_dim】",
          "Arguments【loss】",
          "Arguments【metrics】",
          "Arguments【project_name】",
          "Arguments【max_trials】",
          "Arguments【directory】",
          "Arguments【objective】",
          "Arguments【tuner】",
          "Arguments【overwrite】",
          "Arguments【seed】",
          "Arguments【max_model_size】",
          "Arguments【callbacks】",
          "Arguments【validation_split】",
          "Arguments【validation_data】"
        ],
        "回帰【Diamonds】": [
          "Test",
          "Answer"
        ],
        "二値分類【Titanic】": [
          "データセットの取得",
          "二値分類モデルの作成"
        ],
        "マルチクラス分類【Iris】": [
          "Test",
          "Answer"
        ],
        "Challenge for SIGNATE 【山火事の消失面積予測】": [
          "コンペの参加",
          "データの取得",
          "モデルの作成と予測結果の提出"
        ],
        "Challenge for SIGNATE 【毒キノコの分類】": [
          "データの取得",
          "モデルの作成と予測結果の提出"
        ]
      },
      "requirements": [
        "Jupyter NotebookによるPythonの基本操作",
        "データセットの読込、データ前処理、モデル作成、モデルの評価の経験",
        "AutoKerasの使用が未経験な方、又は経験が浅い方"
      ],
      "description": "本講座はAutomated Machine Learning Tool（自動機械学習モデル）であるAutoKerasパッケージを使用し、公式ドキュメントを参考にしながら AutoKerasのArguments、そして回帰/分類モデルの作成、やモデルの予測精度向上のコツを学習していくコースとなっています。\nまた、講義の中では分析コンペティション（SIGANTE）にも参加し、機械学習モデル自動化ツールがどれほどの予測精度を出すか確認できます。\n手軽で実用的なツールなため、機械学習に苦手意識を持っていた方でもお勧めです。\n\n\nコース内容は以下の通りです。\nSection1:はじめに\nSection2:回帰【Tips】\nSection3:回帰【Diamonds】\nSection4:二値分類【Titanic】\nSection5:マルチクラス分類【Iris】\nSection6:Challenge for SIGNATE 【山火事の消失面積予測】\nSection7:Challenge for SIGNATE 【毒キノコの分類】",
      "target_audience": [
        "AutoKerasパッケージの使用に関心を持つ方",
        "AutoKerasを使用し回帰・分類モデルの作成を行いたい方",
        "機械学習をツールとして使いこなしたい方",
        "AutoKerasに興味があるけど、始め方が分からない方",
        "AutoMLで何らかの問題を解決したい方",
        "AIコンペの参加に関心がある方"
      ]
    },
    {
      "title": "Deep Learning con Python",
      "url": "https://www.udemy.com/course/deep-learning-con-python-g/",
      "bio": "Programaciones de Redes Neuronales con Pytorch y TensorFlow con Ejemplos Prácticos",
      "objectives": [
        "Python",
        "Deep Learning",
        "Pytorch",
        "TesnsorFlow",
        "Creación de Redes Neuronales de Aprendizaje Profundo",
        "Keras"
      ],
      "course_content": {
        "Introducción": [
          "Introducción"
        ],
        "Python Básico": [
          "Google Colab",
          "Variables en Python",
          "Manejo de Strings",
          "Condicionales en Python",
          "Listas en Python",
          "Bucles",
          "Funciones",
          "Funciones Recursivas",
          "Funciones Lambda",
          "Lectura y escritura de archivos",
          "Uso de Try - Except",
          "Manejo de Clases"
        ],
        "Python Programacion Orientada a Objetos": [
          "Clases & Instancias",
          "Variables de Instancias & Variables de Clases",
          "Herencia",
          "Metodos de Clases"
        ],
        "Pandas": [
          "Introduccion a Pandas",
          "¿Que es un DataFrame?",
          "Filtros en DataFrames",
          "Agrupaciones",
          "Filtros en DataFrames.Ejemplo Real",
          "Copy,Reshape y Concatenate"
        ],
        "Numpy": [
          "¿Como crea un Array?",
          "Operaciones con Arrays",
          "Manipulación de Arrays y Filtros",
          "Métodos útiles",
          "Estadística con Numpy",
          "Manipulación de imágenes"
        ],
        "Introducción a Redes Neuronales": [
          "¿Que es una red neuronal?",
          "Funciones de Activación",
          "Conceptos básicos",
          "Perceptron.Teoría",
          "Perceptron Ejemplo Práctico parte 1",
          "Perceptron Ejemplo Práctico parte 2",
          "Perceptron Ejemplo Práctico parte 3",
          "Introducción a las librerías mas empleadas en Deep Learning"
        ],
        "Aplicaciones de Redes Neuronales": [
          "Clasificación y Regresión . Ejemplo Real",
          "Ejemplo de Clasificación usando Redes Neuronales",
          "Entrenamiento de una Red Neuronal",
          "Ejemplo Mnist",
          "¿Que es el PCA?",
          "Ejemplo de Mnist con PCA"
        ],
        "Introducción al Deep Learning": [
          "¿Que es el Deep Leaning? Ejemplos",
          "¿Que es un Tensor?",
          "Pooling layer",
          "Normalization layer",
          "Regularization Layer",
          "Convolutional layer",
          "Mnist con Deep Learning",
          "Reconocimiento de caras"
        ],
        "Pytorch": [
          "¿Ques Pytorch?",
          "Operaciones con Pytorch",
          "Derivadas en Pytorch",
          "Ejemplo Práctico de Reconimiento de Imágenes con Pytorch. Introducción",
          "Ejemplo Práctico Fase de Entrenamiento",
          "Ejemplo Práctico. Fase de Validación"
        ]
      },
      "requirements": [
        "Tener ganas de Aprender"
      ],
      "description": "Perfecto. Aquí tienes una versión optimizada, más extensa y con enfoque de marketing, manteniendo el tono claro, directo y atractivo:\n\"¿Quieres adentrarte en el fascinante mundo del Deep Learning y no sabes por dónde empezar? Este curso ha sido diseñado para guiarte paso a paso en tu camino hacia el aprendizaje profundo, sin importar si partes desde cero. Nuestro objetivo es que adquieras una base sólida en los conceptos clave, las herramientas más utilizadas y las técnicas prácticas que te permitirán implementar tus primeros modelos de Deep Learning por ti mismo.\nEl contenido está estructurado de forma progresiva, comenzando con lo esencial y avanzando hacia el desarrollo de redes neuronales profundas. No necesitas conocimientos previos, solo las ganas de aprender y de llevar tu carrera o proyectos personales al siguiente nivel.\nA lo largo del curso trabajaremos con los siguientes módulos:\nProgramación en Python: Aprenderás los fundamentos del lenguaje más utilizado en Ciencia de Datos e Inteligencia Artificial, con ejercicios prácticos para que te familiarices con su sintaxis y lógica.\nManejo de datos con Pandas: Dominarás una de las librerías esenciales para transformar, limpiar y manipular datos de manera eficiente, preparando los conjuntos de datos para el entrenamiento de modelos.\nOperaciones matemáticas con NumPy: Profundizaremos en operaciones matriciales y cálculos numéricos, fundamentales para comprender cómo funcionan las redes neuronales desde el punto de vista técnico.\nIntroducción a las redes neuronales: Conocerás la estructura y funcionamiento de las redes neuronales, entendiendo cómo procesan información y aprenden patrones a partir de los datos.\nDeep Learning con Keras y TensorFlow: Te enseñaremos a implementar redes de aprendizaje profundo con estas dos poderosas librerías, explorando sus técnicas más utilizadas para resolver problemas reales.\nPyTorch aplicado: Aprenderás a trabajar con una de las librerías más populares en el ecosistema del Deep Learning, construyendo un proyecto real que te permitirá aplicar los conceptos adquiridos.\nEste curso combina teoría clara y ejemplos prácticos, para que no solo entiendas los conceptos, sino que también sepas aplicarlos en proyectos reales. Trabajarás directamente en Python y descubrirás cómo llevar una idea desde el papel hasta su implementación en un modelo funcional de Deep Learning.\nAl finalizar, serás capaz de:\nComprender los fundamentos del aprendizaje profundo y su relación con el Machine Learning.\nManipular y preparar datos de manera profesional para entrenar modelos.\nConstruir tus primeras redes neuronales con TensorFlow, Keras y PyTorch.\nAplicar estos conocimientos a casos reales, sentando las bases para proyectos más avanzados en Inteligencia Artificial.\nSi buscas un curso que te lleve de principiante a creador de tus propios modelos de Deep Learning, este es el camino perfecto para ti. Aprenderás las herramientas más demandadas, desarrollarás pensamiento crítico y tendrás la confianza para explorar el potencial del aprendizaje profundo en tus propios proyectos.\"**\n¿Quieres que también te prepare una versión corta (150-200 palabras) para plataformas tipo Udemy/Coursera (más directa y enfocada en captar al alumno rápido) y otra estilo “landing page” para vender el curso en tu propia web? Así tendrías las tres versiones listas. ¿Las hacemos?",
      "target_audience": [
        "Personas interesadas en Aprender Deep Learning",
        "Estudiantes de Ciencia de Datos",
        "Estudiantes y Profesionales del mundo IT"
      ]
    },
    {
      "title": "الذكاء الاصطناعي",
      "url": "https://www.udemy.com/course/artificial-intelligence-course/",
      "bio": "الذكاء الاصطناعي",
      "objectives": [
        "أهمية الذكاء الاصطناعي ودوره في التعلم والاستنتاج وحل العديد من المشكلات باستخدام طرق وخوارزميات معينه"
      ],
      "course_content": {},
      "requirements": [
        "اساسيات في البرمجة"
      ],
      "description": "كما نعلم أن الذكاء الاصطناعي يُعتبر أحد فروع علم الحاسوب، وإحدى الركائز الأساسية التي تقوم عليها صناعة التكنولوجيا في العصر الحالي. حيث  يتعلق بالقدرة على التفكير الفائق وتحليل البيانات أكثر من تعلقه بشكل معين أو وظيفة معينة. فهو يهدف إلى تعزيز القدرات والمساهمات البشرية بشكل كبير. مما يجعله أصلاً ذا قيمة كبيرة من أصول الأعمال.\nف يوجد العديد من الإيجابيّات والفوائد التي تترتب على استخدام الذكاء الاصطناعي، ومن هذه الفوائد الآتي:\nالعمل الدائم ، التطبيقات المهمة للحياة اليومية (كالهاتف الذكي) ، استخدام الذكاء الاصطناعي لتقديم الخدمات ، التخلص من الأعمال المتكررة ،  تقديم الرعاية الطبية ، القدرة على معالجة كم هائل من المعلومات ، الدقة وتقليل هامش الخطأ ، القيام بالأعمال الصعبة ، عدم تحكيم العاطفة.\nأما عن سلبيات التي تترتب على استخدام الذكاء الاصطناعي، منها ما يأتي :\nالتكلفة العالية التي تترتب على استخدامه ،عدم وعي أنظمة الذكاء الاصطناعي بالأخلاقيات والقيم البشرية ، عدم قدرتها على تغيير نظلم عمله  وتطويره في حال تلقيها نفس البيانات في كل مرة ، افتقار انظمة الذكاء الاصطناعي الى الاستجابة للظروف والتغيرات التي قد تحدث  في بيئة العمل  وأخيرا الاستغناء عن العديد من العمال والموظفين نتيجة الاعتماد عل انظمة الذكاء الاصطناعي بدلا من الانسان.\nلذا نطرح عليكم كورس الذكاء الاصطناعي والذي يتحدث عن أهمية الذكاء الاصطناعي ودوره في التعلم والاستنتاج وحل العديد من المشكلات باستخدام طرق وخوارزميات معينه من خلال سلسلة من الفيديوهات على اليوديمي مع أمثلة تشرح كل خوارزمية تساعد الطالب على حل أي مشكلة باستخدام هذه الطرق.\nوصف الماده\nيحتوي هذا الكتاب على العديد من المعلومات التي تتمتع بطابع الذكاء من خوارزميات البحث إلى تطبيقات الذكاء الاصطناعي وذكاء الآلات. حل المشاكل التي تواجهنا في الحياة الاعتيادية, الاستدلال من خلال مرتكزات من المعطيات والبيانات, التعلم الآلي, معالجة اللغات الطبيعية. منهجية هذا المقرر التدريسي تعتمد على توصيل المعلومات اللازمة لكي يصبح الطالب على قدر من المعرفة بكل المواضيع العائدة إلى هذا المقرر.\nأهداف الماده\nتوضيح المفاهيم الأساسية في علم الذكاء الاصطناعي. مناقشة حلول المشاكل الموجودة حاليا واقتراح حلول جديدة وهذا من هدفه تنمية قدرات البحث العلمي عند الطالب. الخوض بمفهوم الاستدلال وعرض العديد من المسائل بهذا الخصوص. مناقشة مسائل معالجة اللغات الطبيعية ومبادئ التوقعات والاحتمالات. الخوض بمفاهيم الطرق التي تهدف إلى عرض البيانات في طرق محددة بهدف اتخاذ القرارات",
      "target_audience": [
        "جميع طلاب كلية علوم الحاسوب بكافة فروعها"
      ]
    },
    {
      "title": "[PT] Masterclass de Engenharia de IA: Do Zero ao Herói da IA",
      "url": "https://www.udemy.com/course/masterclass-de-engenharia-de-ia-do-zero-ao-heroi-da-ia/",
      "bio": "Domine a Engenharia de IA: desenvolva, treine e implemente soluções escaláveis com projetos reais e prática aplicada(AI)",
      "objectives": [
        "Construa modelos de IA usando Python, TensorFlow e PyTorch para criar sistemas inteligentes capazes de resolver problemas do mundo real",
        "Pré-processe, limpe e analise conjuntos de dados complexos para garantir entradas de alta qualidade para o treinamento de modelos de IA e aprendizado de máquina",
        "Treine, avalie e otimize modelos de aprendizado de máquina para tarefas como regressão, classificação e agrupamento",
        "Projete, implemente e ajuste redes neurais, incluindo CNNs e RNNs, para aplicações avançadas de IA",
        "Aplique técnicas de Processamento de Linguagem Natural (PLN) para analisar, interpretar e gerar textos com linguagem semelhante à humana",
        "Aproveite o aprendizado por transferência para adaptar modelos de IA pré-treinados a novas tarefas, reduzindo tempo e recursos de desenvolvimento",
        "Implemente modelos de IA usando APIs escaláveis e ferramentas de conteinerização como Docker para integração fluida em aplicações",
        "Monitore o desempenho dos modelos de IA, detecte desvios de dados e estabeleça fluxos de retraining para manter a confiabilidade",
        "Resolva desafios técnicos e empresariais do mundo real utilizando abordagens baseadas em IA e sistemas inteligentes",
        "Desenvolva projetos de IA completos, desde a ideação e prototipagem até a implantação e manutenção a longo prazo"
      ],
      "course_content": {},
      "requirements": [
        "Conhecimentos básicos de programação: Familiaridade com Python é recomendada, mas não obrigatória.",
        "Curiosidade e entusiasmo: Ter paixão por IA e disposição para aprender é essencial.",
        "Acesso a um computador: Um computador com acesso à internet e potência suficiente para tarefas de IA.",
        "Nenhuma experiência prévia em IA necessária: O curso começa com conceitos fundamentais e avança gradualmente.",
        "Conhecimentos básicos de matemática: Compreensão de conceitos matemáticos do ensino médio (ex.: álgebra, estatística básica).",
        "Conexão de internet estável: Para acessar os materiais do curso, ferramentas e projetos práticos.",
        "Ferramentas opcionais: Instalação de Python, Jupyter Notebook e bibliotecas de IA relevantes (orientações fornecidas no curso).",
        "Mente aberta: Esteja pronto para explorar, experimentar e construir aplicações reais de inteligência artificial."
      ],
      "description": "Este curso foi traduzido por IA do inglês para o espanhol para que você possa aprender tecnologias de ponta no seu idioma nativo.\nBem-vindo à Masterclass de Engenharia de IA: Do Zero ao Herói da IA!\nEste curso completo foi criado para te levar em uma jornada emocionante, desde o nível iniciante até te tornar um Engenheiro de IA confiante, equipado com as habilidades necessárias para construir, treinar e implantar soluções de Inteligência Artificial. Seja você um iniciante ou alguém que deseja consolidar seus conhecimentos, esta masterclass oferece um caminho passo a passo rumo ao sucesso.\nNa Masterclass de Engenharia de IA, você começará pelos fundamentos da inteligência artificial, explorando programação em Python, pré-processamento de dados e os conceitos básicos de machine learning. À medida que avança, você mergulhará em tópicos avançados como redes neurais, deep learning, processamento de linguagem natural (NLP) e visão computacional. Você também terá experiência prática com frameworks modernos como TensorFlow, PyTorch e Hugging Face, criando soluções de IA prontas para produção.\nEsta masterclass enfatiza habilidades práticas, com projetos reais integrados em cada módulo. Você aprenderá a resolver problemas de negócios reais usando tecnologias de IA, otimizar modelos e implementar soluções escaláveis.\nPor que escolher esta Masterclass de Engenharia de IA?\nCurrículo acessível para iniciantes: Comece do zero e torne-se um especialista\nProjetos práticos: Construa aplicações reais para desafios do mundo real\nDomine os frameworks de IA: Aprenda TensorFlow, PyTorch e Hugging Face\nFormação completa: Inclui Python, Machine Learning, Deep Learning, NLP e Deploy\nRoteiro Do Zero ao Herói da IA: Aprendizado estruturado para dominar a engenharia de IA\nAo final desta masterclass, você terá não apenas dominado as habilidades essenciais da engenharia de IA, mas também estará preparado para inovar, liderar projetos e transformar sua empresa ou startup com soluções inteligentes.\nSe você é um futuro engenheiro de IA, um entusiasta apaixonado ou alguém que deseja ingressar na indústria da Inteligência Artificial, esta masterclass é o seu recurso definitivo para ir do Zero ao Herói da IA.\nJunte-se à revolução da IA hoje mesmo — inscreva-se na Masterclass de Engenharia de IA: Do Zero ao Herói da IA e dê o primeiro passo rumo à maestria em Inteligência Artificial!",
      "target_audience": [
        "Futuros Engenheiros de IA: Pessoas que desejam iniciar uma carreira em IA com habilidades práticas e projetos reais.",
        "Cientistas de Dados e Analistas: Profissionais que buscam expandir sua expertise na construção e implantação de modelos de IA.",
        "Desenvolvedores de Software: Programadores ansiosos para integrar capacidades de IA em seus aplicativos e sistemas.",
        "Pessoas em Transição de Carreira: Indivíduos de áreas não técnicas prontos para entrar no setor de inteligência artificial.",
        "Estudantes de Pós-Graduação: Alunos de ciência de dados, ciência da computação ou áreas afins em busca de conhecimento prático em IA.",
        "Empreendedores de Tecnologia: Fundadores e CTOs que exploram a IA para inovação de produtos e crescimento de negócios.",
        "Entusiastas de IA: Qualquer pessoa apaixonada por IA e interessada em construir sistemas inteligentes do zero.",
        "Profissionais de Negócios: Líderes que desejam entender a IA para tomada de decisões estratégicas e crescimento organizacional."
      ]
    },
    {
      "title": "Ciencia de datos con Python Aplicado a Proyectos Reales",
      "url": "https://www.udemy.com/course/ciencia-de-datos-con-python-aplicado-a-proyectos-reales/",
      "bio": "WebScraping y Procesamiento del Lenguaje Natural. Análisis y Visualización de datos.",
      "objectives": [
        "Analisis de datos",
        "Programación en Python",
        "Visualización de datos con Matplotlib y Seaborn",
        "Uso de Pandas para el tratamiento de datos",
        "Solucionar Problemas Reales",
        "Web Scrapping",
        "Procesamiento del Lenguaje Natural",
        "Limpieza de datos"
      ],
      "course_content": {},
      "requirements": [
        "Tener ganas de aprender"
      ],
      "description": "¿Quieres aprender a sacarle el máximo  partido a tus datos?\nEste curso está diseñado para que te conviertas  en un experto trabajando los datos de manera profesional\nEl curso está estructurado  de manera totalmente práctica con multitud de ejemplos y ejercicios para que puedas desarollar tus habilidades de la mejor manera. A lo largo del curso iremos avanzando desde la parte más básica de programación en Python hasta Finalmente llegar  implementar un proyecto real donde usaremos todas las herramientas vistas durante el curso\nLa estructura del curso es:\nProgramación Básica en Python, pensado para todas aquellas personas que quieren aprender python\nUso de Pandas, veremos una de las librerías mas empleadas en Python para el tratamiento y manipulación de datos\nNumpy, Estudiaremos esta librería pensada para explotar al máximo las funciones matématicas.\nVisualización de datos con Matplotlib y Seaborn, estudiaremos los diferentes gráficos que existen, cuales son los mejores en función de los datos que tenemos y como podemos implementarlo de manera sencilla.\nWebScrapping, aprenderemos que es el webscrapping y como podemos sacarle el máximo  provecho a esta herramienta para automatizar  tareas y extraer datos haciendo uso de Selenium, BeautifulSoap y Requests.\nProcesamiento del Lenguaje Natural , aprenderemos los conceptos básicos de esta discplina y como podemos aplicarla a nuestros proyectos.\nAnalisis de datos de un E-Commerce, Aprenderemos como resolver un problema de Analisis de datos desde cero aplicando los conceptos vistos en el curso.\nProjectos real en Instagram, Realizaremos un proyecto completo desde la fase de Extracción de datos aplicando Webscraping, Limpieza de datos hasta finalmente sacar diferentes conclusiones.",
      "target_audience": [
        "Personas interesadas en Analisis de datos",
        "Estudiantes de Informatica",
        "Desarolladores de Python principiantes con interés en ciencia de datos",
        "Perfiles no tecnicos que quieran incorporar nuevas habilidades en sus curriculums",
        "Personas interesadas en el Procesamiento de Lenguaje Natural"
      ]
    },
    {
      "title": "10 Proyectos de Ciencia de Datos y Machine Learning Reales",
      "url": "https://www.udemy.com/course/proyectos-reales-ciencia-datos-aprendizaje-automatico/",
      "bio": "Aprende Ciencia de Datos, Machine Learning y Deep Learning con 10 Proyectos del Mundo Real y Aplicaciones Web en Flask",
      "objectives": [
        "Fundamentos de la Ciencia de Datos y el Aprendizaje Automático",
        "Técnicas de preprocesamiento de datos",
        "Algoritmos de regresión y su aplicación",
        "Implementación de modelos predictivos",
        "Evaluación y mejora de modelos",
        "Visualización de datos para análisis descriptivo"
      ],
      "course_content": {
        "Proyecto-1: Predicción de la Generación de Energía Renovable": [
          "Introducción",
          "Procesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Predicción de Valores Futuros",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-2: Predicción del Precio de Venta de Diamantes Usando Métodos de Regre": [
          "Introducción",
          "Procesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-3: Predicción de Precios de Ethereum": [
          "Introducción",
          "Procesamiento de Datos",
          "Creación y Procesamiento de Características",
          "Entrenamiento del Modelo de ML",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-4: Detección de Niveles de Estrés con Redes Neuronales Artificiales Usa": [
          "Introducción",
          "Procesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-5: Detección de Tumores Cerebrales usando Redes Neuronales Convoluciona": [
          "Introducción",
          "Procesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-6: Predicción de Edad y Género usando Radiografías de Tórax": [
          "Introducción",
          "Preprocesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-7: Detección de Covid usando Tomografías Computarizadas (CT Scans)": [
          "Introducción",
          "Preprocesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-8: Detección de DeepFake": [
          "Introducción",
          "Preprocesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-9: Reconocimiento de Placas de Matrícula": [
          "Introducción",
          "Preprocesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Aplicación Flask",
          "Descargar los archivos del proyecto"
        ],
        "Proyecto-10: Segmentación de Terreno usando U-Nets": [
          "Introducción",
          "Preprocesamiento de Datos",
          "Entrenamiento del Modelo de ML",
          "Predicción de Valores Futuros",
          "Descargar los archivos del proyecto"
        ]
      },
      "requirements": [
        "Conocimientos básicos de ciencia de datos y aprendizaje automático"
      ],
      "description": "En el mundo actual, la ciencia de datos está a la vanguardia en la generación de conocimientos, innovaciones y soluciones en diversas industrias. Este curso ofrece un enfoque práctico, brindando a los participantes la oportunidad de trabajar en proyectos reales que abarcan aplicaciones variadas de la ciencia de datos. Desde la predicción de energía renovable hasta el procesamiento de imágenes médicas y la detección de deepfakes, este curso te ayudará a construir habilidades esenciales que son altamente demandadas en el campo.\nLa ciencia de datos es una disciplina en rápida evolución, impulsada por los avances en machine learning, inteligencia artificial y tecnologías de big data. Las organizaciones buscan cada vez más profesionales capacitados en análisis de datos y modelado predictivo para tomar decisiones basadas en datos y mejorar sus operaciones. Al trabajar en proyectos que reflejan desafíos reales, los estudiantes no solo fortalecerán sus habilidades técnicas, sino que también desarrollarán habilidades para resolver problemas y entender en profundidad cómo se puede aplicar la ciencia de datos para crear soluciones impactantes.\nA lo largo del curso, trabajarás en los siguientes proyectos:\nPredicción de Generación de Energía Renovable\nPredicción de Precios de Venta de Diamantes mediante Regresión\nPredicción de Precios de Ethereum\nDetección de Niveles de Estrés con Redes Neuronales Artificiales y Sensores PPG\nDetección de Tumores Cerebrales usando Redes Neuronales Convolucionales (CNN)\nPredicción de Edad y Género usando Radiografías de Tórax\nDetección de Covid mediante Tomografías Computarizadas (CT Scans)\nDetección de DeepFake\nReconocimiento de Placas de Matrícula\nSegmentación de Terreno usando U-Nets\nCada proyecto ha sido seleccionado para exponerte a diferentes técnicas de ciencia de datos, como regresión, clasificación, redes neuronales, procesamiento de imágenes y segmentación. Al finalizar el curso, contarás con un portafolio diverso que demuestra tu capacidad para aplicar la ciencia de datos en diferentes dominios, mostrando tu versatilidad y experiencia.\nLa ciencia de datos tiene un futuro prometedor, con aplicaciones cada vez mayores en campos como la salud, las finanzas, los sistemas autónomos, la ciberseguridad y las ciencias ambientales. Completar este curso te pondrá en un camino sólido hacia una carrera en ciencia de datos, equipándote con habilidades que los empleadores valoran, como el pensamiento analítico, la competencia técnica y el conocimiento de dominio.\nEste curso es ideal para cualquiera que desee adentrarse en la ciencia de datos o avanzar en sus habilidades existentes. Ya seas un principiante o un profesional experimentado, la experiencia práctica obtenida a través de estos proyectos te ayudará a mantenerte a la vanguardia en un campo donde el cambio es constante y las oportunidades continúan emergiendo. Aprovecha el futuro de la ciencia de datos y prepárate para contribuir a un mundo impulsado por los datos.",
      "target_audience": [
        "Principiantes en ciencia de datos y aprendizaje automático"
      ]
    },
    {
      "title": "ChatGPT: Yapay Zeka ile Para Kazanma & Freelance'de Uzmanlaş",
      "url": "https://www.udemy.com/course/chatgpt-yapay-zeka-dunyasnn-yeni-yldz-ile-tansn/",
      "bio": "Yapay Zeka Dünyasının En Yenilikçi Aracıyla Para Kazanmaya Başlayın, Freelance İşlerde Uzmanlaşma ve Gelişim Sağlayın!",
      "objectives": [
        "Yapay zeka ve ChatGPT'nin temellerini öğrenme: ChatGPT ile yapay zeka ve doğal dil işleme alanlarında temel bilgiler öğrenilir.",
        "Yapay zeka uygulamaları tasarlama yeteneği: ChatGPT ile yapay zeka uygulamaları tasarlama yetenekleri geliştirilir",
        "Doğal Dil İşleme becerileri: ChatGPT ile doğal dil işleme becerileri geliştirmeleri, dil modellemesi, kelime önerme gibi alanlarda uzmanlaşmalara yardımcı olur",
        "Veri Analizi: ChatGPT eğitimi, öğrencilere veri analizi konularında bilgi sağlar.",
        "Proje Yönetimi: ChatGPT eğitimi, öğrencilere proje yönetimi becerileri kazandırır. Bu sayede, proje yönetimi becerilerini geliştirilir.",
        "Yenilikçi Fikirler Geliştirme: ChatGPT eğitimi ile yenilikçi fikirler geliştirme konusunda farklı uygulamalar tasarlanabilir."
      ],
      "course_content": {
        "ChatGPT'nin Temeline Birlikte Göz Atalım": [
          "ChatGPT'ye Giriş: Nasıl Üye Olunur",
          "Freelance İşler İçin ChatGPT'nin Gücünden Nasıl Yararlanacağız?",
          "ChatGPT'nin Diğer Kullanım Alanlarına Hakim Olalım",
          "Pratik"
        ],
        "ChatGPT ile İşi Elimize Alalım": [
          "ChatGPT ile Nasıl Metin/Makale Yazarız?",
          "Bu ChatGPT Dil Çevirisi de Yapabiliyor Mu?",
          "ChatGPT ile bir de Veri Analizi Yapalım",
          "ChatGPT ile Tasarım Alanlarında Nasıl İşler Yapabiliriz?",
          "Her Şeyi Planlamamıza da Yardım Etsin",
          "Gündelik Hayatımızda Bir Rehbere mi İhtiyaç Var? ChatGPT Var"
        ],
        "Kapanış, Tebrikler Sertifikanız Hazır": [
          "Kapanış, tebrikler kursu tamamladın dostum!"
        ]
      },
      "requirements": [
        "ChatGPT eğitimi, herhangi bir ön bilgi veya tecrübe gerektirmez.",
        "ChatGPT kullanımı hakkında herhangi bir ön bilgiye sahip olmanız gerekmese de, bu kursta tam anlamıyla yararlanabilmeniz için temel bir bilgisayar becerisi, internet bağlantısı ve İngilizce dil seviyesinde basit seviyede bilgi sahibi olmanız önerilir. Bu, eğitim videolarımızı izlemenizi, ChatGPT'yi nasıl kullanacağınızı anlamanızı ve kursun sonunda öğrendiklerinizi pratikte uygulamanızı kolaylaştıracaktır.",
        "Ayrıca, kursa kaydolmak için herhangi bir özel yazılım veya uygulama indirmenize gerek yoktur. ChatGPT'yi kullanmak için sadece bir web tarayıcısı ve internet bağlantısı yeterlidir. Bu nedenle, bu kursa katılmak için özel bir donanım veya yazılım satın almanız gerekmez.",
        "Genel olarak, bu kursa katılmak için herhangi bir ön koşul yoktur. Sadece ilgi duymalı ve ChatGPT ile çalışmayı öğrenmek istemelisiniz. Kurs boyunca öğreneceğiniz tüm becerileri ve bilgileri, ChatGPT'yi kullanarak farklı platformlarda metin yazarak para kazanmak için kullanabilirsiniz."
      ],
      "description": "Yapay zeka teknolojileri, son yıllarda hızlı bir şekilde gelişmiş ve hayatımızın birçok alanında kullanılmaya başlanmıştır. Bu gelişmeler, insanların yapay zeka teknolojileri hakkında daha fazla bilgi edinmesine ve bu alanda kendilerini geliştirmelerine olanak sağlamaktadır. Bu bağlamda ChatGPT, özellikle metin işleme konusunda son derece başarılı bir yapay zeka aracı olarak öne çıkmaktadır.\nKursumuz, yapay zekanın en yenilikçi ve etkili aracı olan ChatGPT'yi keşfetmek isteyen herkes için tasarlanmıştır. Eğitim videolarımızda ChatGPT'nin ne olduğunu, nasıl üye olabileceğinizi, nasıl kullanabileceğinizi, kullanım alanlarını ve nasıl para kazanabileceğinizi detaylı bir şekilde anlatıyoruz. Bu kurs, kendi kendini geliştiren metin araçlarını öğrenmek isteyen, yapay zeka teknolojisine merak duyan ve freelancer olarak çalışarak ek gelir elde etmeyi hedefleyen herkes için uygundur.\nKursun tamamlanmasıyla birlikte, standart düzeyde ChatGPT kullanımı konusunda bilgi sahibi olacak ve farklı platformlarda metin oluşturarak para kazanabileceksiniz. Eğitim videolarımız size ChatGPT'nin ne kadar etkili bir araç olduğunu gösterecektir. ChatGPT'yi kullanarak etkili metinler oluşturma konusunda ihtiyaç duyulan tüm becerileri sağlayacak ve işinizi daha verimli hale getirebileceksiniz. Bu kurs, kendi işinizi kurmak isteyenler, freelance yazarlar, metin yazarları ve diğer birçok meslek grubundan insanlar için idealdir. Kursumuza kaydolarak, ChatGPT'nin gücünü keşfedin ve metin yazarlığı konusunda kendinizi geliştirin. Yapay zeka teknolojilerine meraklıysanız veya metin oluşturma konusunda daha fazla bilgi edinmek istiyorsanız, hemen kaydolun ve ChatGPT'nin farkını görün!",
      "target_audience": [
        "Eğitim, makine öğrenimi, doğal dil işleme ve yapay zeka konularına ilgi duyan herkes için faydalı olabilir. Bu konulara önceden ilgi duyanlar veya bu alanlarda bir kariyer yapmayı planlayanlar özellikle faydalı bulabilirler.",
        "Bu alanda kariyer yapmak isteyenler, mevcut işlerinde yapay zeka teknolojilerini kullanmak isteyenler, yazılım geliştiricileri, veri bilimcileri, makine öğrenimi uzmanları, robotik mühendisleri ve benzeri teknik alanlarda çalışanlar için yararlıdır.",
        "Kursun amacı, öğrencilerin yapay zeka teknolojisine dair bilgilerini artırmak, ChatGPT'nin potansiyelini keşfetmek ve alternatif sitelerde para kazanmak için gereken becerileri kazandırmaktır. Bu kursu alarak, yapay zeka teknolojilerinin en yenilikçi ve etkili araçlarından biri olan ChatGPT'yi öğrenecek ve işinizi daha verimli hale getirmek için kullanabileceksiniz."
      ]
    },
    {
      "title": "Coze：零基础开发Agent智能体",
      "url": "https://www.udemy.com/course/cozeagent/",
      "bio": "本课程专为零基础学员打造，通过系统的教学和实战项目，教您如何利用Coze开发Agent智能体，让您轻松掌握AI领域的关键技能。",
      "objectives": [
        "掌握核心技术原理：学员将深入了解结合Coze开发Agent智能体的开发流程，了解其背后的技术原理，从而能够更精准、高效地开发和优化Agent智能体。",
        "拓展应用视野：本课程将带领学员探索Coze在多个领域的应用场景，如天气查询、智能客服等，帮助学员拓展应用视野，激发创新思维，找到更多创新点。",
        "紧跟发展潮流：通过学习本课程，学员将能够体验最新AI技术工具之一：Coze，并学习如何运用Coze来开发更先进的Agent智能体。",
        "提升实践能力：课程中不仅包含理论知识，更有大量的实战案例和实用技巧，学员将通过实践项目，提升在实际项目中应用Coze的能力，增强职场竞争力，为未来的职业发展奠定坚实基础。"
      ],
      "course_content": {},
      "requirements": [
        "无需经验"
      ],
      "description": "在当今数字化时代，人工智能技术的飞速发展使得Agent智能体成为各行业的新宠。然而，许多企业和个人在开发Agent智能体时面临技术门槛高、缺乏实战经验的困境。\n为此，三节课邀请了具有丰富开发经验的郭旭老师讲解本次课程，旨在帮您快速掌握Agent智能体的开发技巧，解决行业中的技术难题。\n本课程从零基础出发，帮助学员快速上手Coze开发，解决工作中的实际难题。通过课程的学习，您将了解如何运用Coze开发各类Agent智能体，从而提升工作效率。",
      "target_audience": [
        "职场精英：提升工作效率",
        "企业主：建立智能客服机器人",
        "学生：助力论文写作 AI",
        "爱好者：实现专属的AIGC机器人"
      ]
    },
    {
      "title": "Visualización Interactiva con Python",
      "url": "https://www.udemy.com/course/visualizacion-interactiva-con-python/",
      "bio": "Aprende a crear Dashboards y Aplicaciones Web de manera Interactiva utilizando Dash y Plotly",
      "objectives": [
        "Creación de Aplicaciones Web Interactivas",
        "Tecnicas de Visualización",
        "Python",
        "Procesamiento de datos con Pandas",
        "Creación de Dashboards",
        "Dash y Plotly"
      ],
      "course_content": {},
      "requirements": [
        "Ser una persona proactiva en el aprendizaje"
      ],
      "description": "Este curso está diseñado para  que aprendas a crear aplicaciones web enfocadas a la visualización de datos. y para ello lo haremos de una forma totalmente práctica e intuitiva haciendo uso de Python y de la librería Dash-Plotly.\nEl curso está estructurado en los siguiente módulos:\nProgramación en Python, veremos los fundamentos de la programación haciendo uso de uno de los lenguajes mas populares en la actualidad\nPandas, una de la librerías estrella a la hora de manipulación y tratamiento de datos\nIntroducción a Dash y Plotly, veremos para que se utiliza y los conceptos más importantes que debemos conocer para utilizarlas.\nLayout. Estudiaremos su uso e integración con Dash\nCallbacks. Veremos que son, los diferentes tipos que existen y como podemos usarlas en nuestros proyectos\nGráficos en Plotly, Aprenderemos  los distintos tipos de gráficos que existen y como crearlos y customizarlos usando Plotly.\nProyecto Final, crearemos un DashBoard-Aplicacción web interactivo y lo pondremos en producción.",
      "target_audience": [
        "Personas que se inicien en el mundo de Python",
        "Personas que sabiendo Python quieran aprender a crear aplicaciones web usando Python",
        "Personas interesadas en crear Visualizaciones Interactivas",
        "Analistas de datos",
        "Te interesa el Desarollo Web"
      ]
    },
    {
      "title": "Reinforcement Learning Avanzado: DQNs avanzadas",
      "url": "https://www.udemy.com/course/reinforcement-learning-dqns-avanzadas/",
      "bio": "Construye agentes de Inteligencia Artificial (IA) utilizando Deep Reinforcement Learning y PyTorch",
      "objectives": [
        "Domina algunos de los algoritmos de Reinforcement Learning más avanzados.",
        "Aprende a crear inteligencias artificiales que puedan actuar en un entorno complejo para alcanzar sus objetivos.",
        "Crea desde cero agentes avanzados de Reinforcement Learning utilizando las herramientas más populares de Python (PyTorch Lightning, OpenAI gym, Optuna).",
        "Aprende a realizar ajuste de hiperparámetros (selección de las mejores condiciones experimentales para que nuestra inteligencia artificial aprenda).",
        "Comprende de manera fundamental el proceso de aprendizaje para cada algoritmo.",
        "Depura y amplía los algoritmos presentados.",
        "Comprende e implementa nuevos algoritmos a partir de artículos de investigación."
      ],
      "course_content": {
        "Introducción": [
          "Introducción",
          "Serie sobre Reinforcement Learning",
          "Google Colab",
          "Dónde empezar",
          "Código completo",
          "Conecta conmigo en redes sociales"
        ],
        "Repaso: El proceso de decisión de Markov (MDP)": [
          "Visión general del módulo",
          "Elementos comunes a todas las tareas de control",
          "El proceso de decisión de Markov (PDM)",
          "Tipos de proceso de decisión de Markov",
          "Trayectoria y episodio",
          "Recompensa vs retorno",
          "Factor de descuento",
          "Política",
          "Valor de un estado v(s) y valor de un estado-acción q(s,a)",
          "Ecuaciones de Bellman",
          "Resolver un proceso de decisión de Markov"
        ],
        "Repaso: Q-Learning": [
          "Visión general del módulo",
          "Métodos de diferencias temporales (TD)",
          "Resolver tareas de control con métodos de diferencias temporales",
          "Q-Learning",
          "Ventajas de los métodos de diferencias temporales"
        ],
        "Repaso: Breve introducción a las redes neuronales": [
          "Visión general del módulo",
          "Aproximadores de funciones",
          "Redes neuronales artificiales",
          "Neuronas artificiales",
          "Cómo representar una red neuronal",
          "Descenso gradiente estocástico (SGD)",
          "Optimización de redes neuronales"
        ],
        "Repaso: Deep Q-Learning": [
          "Visión general del módulo",
          "Deep Q-Learning",
          "Repetición de experiencia (experience replay)",
          "Red target (target network)"
        ],
        "PyTorch Lightning": [
          "PyTorch Lightning",
          "Enlace a la libreta de código",
          "Introducción a PyTorch Lightning",
          "Crear la Deep Q-Network",
          "Crear la política",
          "Crear la memoria de repetición",
          "Crear el entorno",
          "Definir la clase para el algoritmo Deep Q-Learning",
          "Definir la función 'play_episode'",
          "Preparación del 'data loader' y el optimizador",
          "Definir la función 'train_step'",
          "Definir la función 'train_epoch_end'",
          "[Importante] Corrección de la lección",
          "Entrenar el algoritmo Deep Q-Learning",
          "Explorar el agente resultante"
        ],
        "Ajuste de hiperparámetros con Optuna": [
          "Ajuste de hiperparámetros con Optuna",
          "Link a la libreta de código",
          "Registro del retorno promedio",
          "Definición de la función objetivo",
          "Creación y lanzamiento del ajuste de hiperparámetros",
          "Exploración del mejor ensayo"
        ],
        "Double Deep Q-Learning": [
          "Sesgo de maximización y Double Deep Q-Learning",
          "Enlace a la libreta de código",
          "Implementación del algoritmo Double Deep Q-Learning",
          "Exploración del agente resultante"
        ],
        "Dueling Deep Q-Network": [
          "Dueling Deep Q-Networks",
          "Enlace a la libreta de código",
          "Creación de la Dueling Deep Q-Network",
          "Normalización de recompensas y observaciones",
          "Creación del entorno - Parte 1",
          "Creación del entorno - Parte 2",
          "Implementación del algoritmo Deep Q-Learning",
          "Exploración del agente resultante"
        ],
        "Repetición de Experiencia Priorizada (Prioritized Experience Replay)": [
          "Repetición de Experiencia Priorizada",
          "Enlace a la libreta de código",
          "DQN para inputs visuales (imágenes)",
          "Memoria de reproducción priorizada",
          "Creación del entorno",
          "Deep Q-Learning con Repetición de Experiencia Priorizada",
          "Errata lección anterior",
          "Lanzamiento del entrenamiento",
          "Exploración del agente resultante"
        ]
      },
      "requirements": [
        "Sentirse cómodo programando en Python.",
        "Completar nuestro curso \"Reinforcement Learning de principiante a maestro\" o estar familiarizado con los conceptos básicos del Reinforcement Learning (o ver las secciones introductorias incluidas en este curso).",
        "Conocimientos básicos de estadística (media, varianza, distribución normal)."
      ],
      "description": "Este es el curso más completo de Reinforcement Learning Avanzado en Udemy. En él, aprenderás a implementar algunos de los algoritmos más poderosos de Deep Reinforcement Learning en Python utilizando PyTorch y PyTorch Lightning. Implementarás desde cero algoritmos adaptativos que resuelven tareas de control basadas en la experiencia. Aprenderás a combinar estas técnicas con Redes Neuronales y métodos de Deep Learning para crear agentes de Inteligencia Artificial adaptativos capaces de resolver tareas de toma de decisiones.\nEste curso te introducirá al estado del arte en técnicas de Reinforcement Learning. También te preparará para los próximos cursos en esta serie, donde exploraremos otros métodos avanzados que destacan en otros tipos de tareas.\nEl curso se enfoca en el desarrollo de habilidades prácticas. Por lo tanto, después de aprender los conceptos más importantes de cada familia de métodos, implementaremos uno o más de sus algoritmos en cuadernos Jupyter, desde cero.\n\n\nMódulos de nivelación:\n\n\n- Repaso: El proceso de decisión de Markov (MDP).\n- Repaso: Q-Learning.\n- Repaso: Breve introducción a las Redes Neuronales.\n- Repaso: Deep Q-Learning.\n\n\nReinforcement Learning avanzado:\n\n\n- PyTorch Lightning.\n- Ajuste de hiperparámetros con Optuna.\n- Reinforcement Learning con entradas de imágenes.\n- Doble Aprendizaje Profundo Q.\n- Redes Neuronales Q de Duelo.\n- Reproducción Experiencial Priorizada (PER).\n- Redes Neuronales Q Distribucionales.\n- Redes Neuronales Q Ruidosas.\n- Aprendizaje Profundo Q de N pasos.\n- Aprendizaje Profundo Q Rainbow.",
      "target_audience": [
        "Desarrolladores que deseen conseguir un trabajo en Machine Learning.",
        "Científicos/analistas de datos y profesionales de Machine Learning que buscan ampliar su amplitud de conocimientos.",
        "Estudiantes e investigadores en robótica.",
        "Estudiantes e investigadores de ingeniería."
      ]
    },
    {
      "title": "Pythonで体系的に学ぶデータサイエンスとAIの初歩 Vol.2 確率と統計・ベイズ統計・データサイエンス・AI",
      "url": "https://www.udemy.com/course/pythonai-vol2/",
      "bio": "確率と統計（平均、メジアン、標準偏差、相関、正規分布、ランダムウォーク、モンテカルロ法）、ベイズ統計（ベイズの定理、ベイズ的意思決定、ベイズ更新、ベイズフィルター）、データサイエンスとAIの初歩（人工知能、回帰、分類、クラスタリング）",
      "objectives": [
        "Pythonの基礎知識を得て、簡単なプログラムを作成できるようになります。",
        "データサイエンスやAIの学習に必要な数学・統計などの初歩的基礎知識を体系的に得ることができます。",
        "データサイエンスとAIを本格的に学ぶにあたって必要な初歩的事項を身に付けることができます。",
        "本Vol.2では特に「確率と統計」、「ベイズ統計」、「データサイエンスとAIの初歩」についての基礎知識を得ることができます。"
      ],
      "course_content": {
        "コースの紹介": [
          "コースの紹介"
        ],
        "Anacondaのインストール方法": [
          "Anacondaのインストール方法 Windows向け",
          "Anacondaのインストール方法 Mac向け"
        ],
        "テキスト": [
          "テキスト"
        ],
        "確率と統計": [
          "テンプレートプログラム",
          "テンプレートプログラムの使い方 Windows向け",
          "テンプレートプログラムの使い方 Mac向け"
        ],
        "確率の計算": [
          "集合の和と積1",
          "集合の和と積2"
        ],
        "加法定理と乗法定理": [
          "加法定理と乗法定理"
        ],
        "バースデーパラドックス": [
          "バースデーパラドックス1",
          "バースデーパラドックス2"
        ],
        "基本的統計量": [
          "平均",
          "メジアン",
          "分散と標準偏差",
          "相関と因果関係",
          "正規分布",
          "補足　実際の講義から　シックスシグマ"
        ],
        "統計の話題": [
          "アンスコム（anscombe）のデータセット1",
          "アンスコム（anscombe）のデータセット2"
        ],
        "乱数": [
          "サイコロ投げ",
          "一様分布乱数",
          "補足　実際の講義から　相関係数"
        ]
      },
      "requirements": [
        "簡単なプログラミングの経験があった方が望ましいです。",
        "中学生レベル、できれば高校生レベルの数学的素養があると理解が早いです。"
      ],
      "description": "■「Pythonで体系的に学ぶデータサイエンスとAIの初歩」シリーズ全4巻ではデータサイエンスとAIの初歩を学ぶのに必要な数学、統計学などの初歩的基礎知識を体系的に学んでいきます。高校の新学習指導要領の「情報I」および「情報II」の内容も多く含んでいます。\n■高校生程度の数学の知識があれば理解できるように説明しているので、いきなりデータサイエンスやAIのコースは荷が重いという方にお勧めです。実際に中高生向けのプログラミング教室での実績があります。\n■Anacondaをインストールして、Jupyter Notebookでプログラミングを行なっていきます。\n■学習内容\nVol.1ではPythonの基礎、数と暗号、関数と微分\nVol.2では確率と統計、ベイズ統計、データサイエンスとAIの初歩\nVol.3では線形代数、主成分分析、自然言語処理\nVol.4ではネットワークの基礎知識、ブロックチェーン\nについて学んでいきます。\n■4コースともPDF形式のテキスト（約100ページ）が付属しています。\nまた、タイピングが大変だったり時間がないという方のためにある程度コードを記載したテンプレートプログラムをつけてあります。完成したサンプルプログラムもついています。\n■Vol.2の内容は以下の通りです。\n確率と統計\nベイズ統計\nデータサイエンスとAIの初歩",
      "target_audience": [
        "データサイエンスやAIに関心を持つが、いきなり本格的なAIプログラミングは困難なPythonプログラミング初心者。",
        "高校の新学習指導要領「情報I」、「情報II」レベルのプログラミングを学びたい方。",
        "高校の新学習指導要領「情報I」、「情報II」レベルのプログラミングを教えたい教師。"
      ]
    },
    {
      "title": "코드리스 데이터 분석",
      "url": "https://www.udemy.com/course/kqfogepv/",
      "bio": "데이터 분석 배워서 어디에 쓰나? 직장인이라면 한 번쯤 해보았을 고민 \"요즘 트렌드가 뭐지?\" 한큐에 해결해 줄 소셜분석 완전 정복! 코드리스 데이터 분석 클래스",
      "objectives": [
        "공공데이터, 웹데이터, 소셜데이터 크롤링을 누구나 할 수 있게 됩니다.",
        "별도 프로그램 없이 딱 엑셀 하나로만 데이터 크롤링분석을 할 수 있게 됩니다.",
        "데이터 분석에 필요한 기초 개념을 정리할 수 있게 됩니다.",
        "듣기만 해도 어려운 통계 분석! 비전문가 눈높이에 맞춰서 개념부터 차근차근 배워보고, 실습까지 함께 하게 됩니다."
      ],
      "course_content": {
        "코드리스 데이터 분석": [
          "공공데이터 수집하는 방법",
          "코딩없이 빅데이터 수집하는 법",
          "코딩없이 빅데이터 수집하는 법",
          "분석을 위한 데이터 준비의 기술1",
          "분석을 위한 데이터 준비의 기술2",
          "데이터 분석의 기초, 통계분석 1",
          "데이터 분석의 기초, 통계분석 2",
          "코드리스 데이터 분석 1",
          "코드리스 데이터 분석 2",
          "소셜분석",
          "소셜분석과 인사이트 도출"
        ]
      },
      "requirements": [
        "코딩 못해도 누구나 뚝딱! 노코드로 진행되기 때문에 코딩 지식이 필요하지 않습니다."
      ],
      "description": "코딩 못해도 누구나 뚝딱! 노코드 시대의 시작!\nSW 개발 인력난이 만든 노코드, 로우코드 시대! 코딩 없이도 프로그래밍을 할 수 있는 플랫폼이 많아지고 있어요.\n\n\n쉽다고 생각하고 접했던 파이썬, 코딩이 너무 어려워 포기하고 싶었다면?\n더이상 불필요한 학습에 금쪽같은 시간을 낭비하지 마세요. 노코드로 쉽게 데이터 분석하는 방법을 알려드립니다.\n\n\n우리는 이런 것들을 할 수 있게 됩니다!\n\n\n엑셀을 이용하여 데이터 활용을 하고싶은데, 어떻게 시작하면 좋을까요?\n- 엑셀을 활용해 공공데이터부터 웹데이터를 모아오고, 실시간 연동을 위한 스킬을 공유해요.\n\n\n데이터 분석 강의를 들었지만 어떻게 활용해야 할지 막막해요. 활용까지 가능한 강의일까요?\n- 데이터를 모았다면 어떻게 분석해야 할지 고민해야 할 때! 엑셀을 활용해 데이터를 함께 반죽해요.\n\n\n통계 분석이 너무 어렵게 느껴지는데 제가 과연 끝까지 들을 수 있을까요?\n- 듣기만 해도 어려운 통계 분석! 비전문가 눈높이에 맞춰서 개념부터 차근차근 배워보고, 실습까지 함께 해요.\n\n\n비전공자를 위한 노코드 데이터 전문 에이블런에서 인증한 바로 그 강의!\n지금 바로 만나보세요 :)",
      "target_audience": [
        "요즘 다들 코딩코딩 하길래 들어봤더니 문과생임에 감사할 따름인 분들을 위해! 문과생 맞춤형 데이터 분석 강의 여기 있어요!"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第七部 AI之眼",
      "url": "https://www.udemy.com/course/lang_graph/",
      "bio": "關於YOLO， ultralytics， scikit-learn， supervision， opencv-python",
      "objectives": [
        "掌握如何利用YOLO",
        "掌握如何利用Ultralytics",
        "掌握如何利用OpenCV-Python",
        "掌握如何利用Supervision",
        "掌握如何利用Scikit-learn"
      ],
      "course_content": {
        "介紹": [
          "什麼是物件檢測Object Detection?",
          "課程工具準備"
        ],
        "如何使用Yolo實現物件檢測Object Dection": [
          "如何使用Poetry",
          "使用Yolo製作第一個Object Detection程序"
        ],
        "無顯卡如何訓練Yolo模型": [
          "如何在線上訓練Yolo模型",
          "學員將學會如何在使用OpenCV輸出和輸入視頻"
        ],
        "如何追蹤影像中的人和物": [
          "如何在使用SuperVision處理Detections",
          "如何在使用ByteTrack追蹤每一個對象",
          "如何在保存追蹤數據到數據庫",
          "如何添加Annotation到每一個球員與裁判",
          "如何在添加Track ID到每一個球員腳下",
          "如何填補缺失的Frame"
        ],
        "如何用AI分類足球的隊伍": [
          "如何獲取隊服顏色",
          "如何分配Team ID",
          "如何獲得帶球足球員的ID"
        ]
      },
      "requirements": [
        "一台電腦"
      ],
      "description": "論你是足球愛好者、數據科學家還是AI開發者，這門課程將為你打開一扇全新的大門！我們將帶你深入了解如何利用YOLO、Ultralytics、Scikit-learn、Supervision和OpenCV-Python來製作精確的視頻處理系統，實現對足球運動員、裁判、足球等目標的自動標註與識別。\n為什麼選擇這門課程？\n實戰技能：課程內容設計專為實戰應用，從基礎理論到高級實踐，你將學會如何建立一個完整的AI視頻分析系統，讓AI能夠識別足球場上的各種情況，幫助你或你的團隊做出更準確的判斷。\n前沿技術：本課程深入探討YOLO (You Only Look Once) 和Ultralytics等最先進的物體檢測技術，結合Scikit-learn的機器學習模型，以及OpenCV-Python的視頻處理能力，讓你站在技術的最前沿。\n應用場景廣泛：這門課程不僅適用於足球比賽的分析，還可以擴展到其他運動、安防監控等多種場景。只要掌握了這些技術，你的職業發展將擁有無限的可能性\n課程亮點：\nYOLO模型訓練與優化：深入理解YOLO架構，學習如何使用Ultralytics進行模型訓練和優化，實現高精度的目標檢測。\n視頻處理與標註：運用OpenCV-Python實現視頻的讀取、處理與標註，自動識別足球場上的各種目標。\n機器學習模型應用：結合Scikit-learn，掌握如何建立並應用機器學習模型進行數據分析與預測。\n實時應用：學習如何將這些技術應用到實時系統中，實現即時視頻分析與反應。\n\n\n立即報名，開啟你的AI足球視頻分析之旅！\n趕快加入我們的課程，不僅能提升你的技術水平，還能拓展你的職業視野，成為這個領域的佼佼者。席位有限，抓住機會，讓你的職業生涯更進一步！",
      "target_audience": [
        "想了解AI視覺的開發者",
        "對視頻處理與分析感興趣的工程師"
      ]
    },
    {
      "title": "Data Science - علوم البيانات",
      "url": "https://www.udemy.com/course/data-science-ar/",
      "bio": "الشامل في علوم البيانات بالعربي",
      "objectives": [
        "تحليل البيانات, تصور البيانات , لغة بايثون"
      ],
      "course_content": {
        "تقديم": [
          "Introduction",
          "Who is a data scientist - من هو عالم البيانات",
          "Who is a Data Analyst - من هو محلل البيانات",
          "Who is a Data Engineer - من هو مهندس البيانات",
          "Notebooks"
        ],
        "SQL - لغة سيكول": [
          "Select From ِClause - العبارة اختر من",
          "Where Clause - العبارة حيث",
          "Distinct - مختلف",
          "Order by - رتب ب",
          "Group by"
        ],
        "Basics in Python - الأساسيات في بايثون": [
          "List",
          "Tuple",
          "Set",
          "Dictionary - القاموس",
          "String"
        ],
        "numpy library - مكتبة نامباي": [
          "Numpy Arrays vs Python Lists",
          "Numpy Arrays - المصفوفات في نامباي"
        ],
        "Pandas library - مكتبة بانداس": [
          "Series & Data Frames - السلاسل و اطارات البيانات"
        ],
        "Data visualisation - تصور البيانات": [
          "matplotlib",
          "plotly",
          "Seaborn"
        ],
        "Exploratory Data Analysis (EDA) - تحليل البيانات الاستكشافي": [
          "EDA Introduction - EDA تحليل البيانات الاستكشافي"
        ],
        "Machine Learning - تعلم الاليات": [
          "Regression - الانحدار",
          "Classification - التصنيف",
          "Clustering - التجميع"
        ]
      },
      "requirements": [
        "العزيمة"
      ],
      "description": "بسم الله الرحمان الرحيم و الصلاة على خاتم المرسلين\nمرحبا بكم في كورس علوم البيانات\nThis course is in arabic !\n\n\nThis course is all you need to start your learning about data science\nهذا الكورس هو كل ما تحتاجه من اجل بدأ مسيرتك الخاصة في مجال علوم البيانات\nThis course is still under construction\nالكورس لازال طور التطوير\nif you have any comment and recommendation please don't hesitate to contact me :)\nلا ترددو اذا ما كانت لديكم افكار او اراء حول الكورس\nسوف تجدون مختلف النوت بوكس في الغيتهاب الخاص بي\n:) لا تنسو ترك نجمة",
      "target_audience": [
        "اي شخص مهتم بمجال علوم البيانات"
      ]
    },
    {
      "title": "Bootcamp Data Science dan Data Analyst menggunakan Python",
      "url": "https://www.udemy.com/course/bootcamp-data-analyst-dengan-menggunakan-python-untuk-pemula/",
      "bio": "Bootcamp Data Science dan Data Analyst dengan menggunakan Python untuk pemula",
      "objectives": [
        "Peserta akan mempelajari sintaks Python, tipe data, operasi dasar, dan struktur kontrol seperti pengulangan dan percabangan.",
        "Mampu menulis skrip Python sederhana untuk memecahkan masalah dasar.",
        "Peserta akan belajar bagaimana membaca, memanipulasi, dan membersihkan data menggunakan pustaka Python standar.",
        "Menggunakan alat seperti Google Colab atau Jupyter Notebook untuk menjalankan kode Python dan mendokumentasikan analisis.",
        "Peserta akan memahami cara menggunakan pustaka Numpy untuk manipulasi array dan operasi matematika yang efisien.",
        "Mampu melakukan operasi numerik seperti perhitungan statistik dasar, pengindeksan array, dan operasi matriks.",
        "Peserta akan mempelajari cara menggunakan pustaka Pandas untuk mengolah data dalam format tabel, seperti CSV dan Excel.",
        "Peserta mampu melakukan data cleaning dengan Pandas, termasuk menangani data kosong, duplikat, dan mengubah tipe data.",
        "Peserta akan memahami cara membuat visualisasi data menggunakan Matplotlib dan Seaborn untuk menyajikan analisis secara menarik.",
        "Peserta akan membuat proyek nyata seperti analisis dataset, laporan interaktif, dan dashboard sederhana menggunakan Python."
      ],
      "course_content": {
        "Pengantar": [
          "Introduction",
          "Apa itu data analitycs",
          "Pengenalan Python",
          "Tools yang dibutuhkan",
          "Pengenalan Google Colaboratory",
          "Setup dan overview google colaboratory",
          "Pengenalan Jupyter ( jupyter notebook dan jupyter lab )",
          "Tata cara setup dan instalasi jupyter",
          "Setup dan overview jupyter",
          "Course Repository",
          "Cara Akses Repository dan Data Bootcamp"
        ],
        "Python dasar": [
          "Pengantar",
          "Variable",
          "Kuis Variable",
          "Type data umum di python",
          "Kuis tipe data umum",
          "String Formatting",
          "Operator Dasar",
          "List, dictinary, tupple dan sets",
          "Conditional Statement",
          "Soal pembantu pemahaman materi",
          "Operator Logika",
          "Soal Jawab Analisis Cerita",
          "Membership Operator",
          "Looping atau perulangan",
          "Kontrol Loop",
          "Soal analisis pemahaman materi",
          "Jawaban soal",
          "Functions",
          "List comprehension",
          "Module",
          "Library"
        ],
        "Numpy": [
          "Pengenalan numpy",
          "Membuat array dari list",
          "Membuat array dari np.arange",
          "Membuat array dengan np.zeros dan np.ones",
          "Mengatur dimensi array",
          "Indexing dan slicing pada array numpy",
          "Mengubah nilai element array",
          "Operasi matematika dasar pada array",
          "Atriibut pada array",
          "Fungsi statistik pada array",
          "Distribusi probabilitas"
        ],
        "Pandas": [
          "Pengenalan pandas",
          "Instalasi pandas",
          "Struktur data pandas - Series",
          "Struktur data pandas - DataFrame",
          "Display",
          "Membuat DataFrame dari files",
          "Melihat sebagian data",
          "Mengecek struktur, bentukan dan statistik data",
          "Pandas set_option() untuk mengatur tampilan data",
          "Memilih kolom dari dataframe",
          "Memilih baris data dengan iloc",
          "Memilih baris data dengan loc",
          "Slicing baris dengan loc dan iloc",
          "Slicing kolom dan baris",
          "Mengatur dan mereset index",
          "Filter data berdasarkan kondisi",
          "Menggunakan query untuk filter data",
          "Menggunakan isin method untuk filter data",
          "Menggunakan contains untuk filter data",
          "Menggunakan startwith untuk filter data",
          "Memanfaatkan operator logika pada filter",
          "Sorting data",
          "Menambah kolom baru",
          "Menambah kolom dengan value berdasarkan value kolom lain",
          "Menghapus kolom",
          "Mengganti nama kolom",
          "Bekerja pada data tanggal",
          "Groupby dan aggregation",
          "Merge di Pandas",
          "Join di Pandas"
        ],
        "Pandas: Data Cleaning": [
          "Konsep dan Pengenalan Data Cleaning",
          "Pengenalan Missing Value",
          "Menghapus missing value",
          "Mengisi missing value dengan value tertentu seperti 0",
          "Mengisi missing value dengan method ffill dan bfill",
          "Mengisi missing value dengan mean, median, mode",
          "Mengisi missing value dengan interpolate",
          "Mengisi missing value dengan replace method",
          "Function yang dapat digunakan untuk handling missing value",
          "Mengatasi data duplikat",
          "Standarisasi format Data"
        ],
        "Data Visualization": [
          "Konsep Dasar Data Visualization"
        ],
        "Matplotlib": [
          "Pengenalan Matplotlib",
          "Line Plot",
          "Membuat Line Plot",
          "Customisasi plot di matplotlib",
          "Bar Plot",
          "Membuat Bar Plot",
          "Scatter Plot",
          "Membuat Scatter plot",
          "Pie Chart",
          "Membuat Pie Chart",
          "Histogram",
          "Membuat Histogram"
        ],
        "Pandas Visualization": [
          "Pengenalan pandas visualization",
          "Customisasi dengan matplotlib",
          "Plot yang tersedia di pandas"
        ],
        "Seaborn": [
          "Pengenalan Seaborn",
          "Mengelola Palette warna",
          "Dataset yang tersedia di seaborn",
          "Relational: Histogram Plot",
          "Realtional: Scatter Plot",
          "Relational: Rel Plot",
          "Categorical: Bar Plot",
          "Categorical: Strip Plot",
          "Categorical: Swarm Plot",
          "Categorical: Box Plot",
          "Categorical: Violin Plot",
          "Regression Plot",
          "Matrix Plot : Heatmap"
        ]
      },
      "requirements": [
        "Tidak perlu pengetahuan tentang pemograman, anda akan belajar semuanya disini"
      ],
      "description": "Bootcamp Data Analyst dengan Python\nBootcamp ini dirancang untuk pemula yang ingin memulai karier di bidang data analitik atau meningkatkan keterampilan analitik mereka. Anda akan belajar dasar-dasar pemrograman Python, termasuk cara menggunakan alat populer seperti Jupyter Notebook dan Google Colaboratory untuk menjalankan kode dan mendokumentasikan analisis data.\nDalam bootcamp ini, Anda akan mempelajari pustaka Python yang penting seperti Numpy untuk manipulasi data numerik, Pandas untuk membersihkan dan mengolah data dalam format tabel, serta Matplotlib dan Seaborn untuk membuat visualisasi data yang menarik dan mudah dipahami.\nBootcamp ini menggunakan pendekatan praktis dengan latihan langsung dan real-world project. Anda akan mengerjakan proyek analisis data sederhana, yang melibatkan proses pengumpulan, pembersihan, manipulasi, analisis, dan penyajian data dalam bentuk yang dapat ditindaklanjuti.\nSiapa yang Cocok Mengikuti Bootcamp Ini?\nPemula tanpa pengalaman pemrograman sebelumnya.\nMahasiswa atau profesional yang ingin beralih ke bidang analitik data.\nSiapa saja yang ingin mempelajari alat-alat utama dalam analisis data menggunakan Python.\nMateri yang Akan Dipelajari:\nPython: Dasar-dasar pemrograman, struktur kontrol, tipe data, dan operasi dasar.\nJupyter Notebook & Google Colaboratory: Cara menulis, menjalankan, dan mendokumentasikan kode Python.\nNumpy: Manipulasi array, operasi numerik, dan perhitungan statistik.\nPandas: Data cleaning, pengolahan dataset, dan pengolahan data dalam format tabel (CSV, Excel, JSON).\nMatplotlib & Seaborn: Membuat visualisasi data yang informatif dan menarik.\nReal-World Project: Proyek analisis data sederhana yang mencakup pembersihan data, analisis, dan pembuatan laporan visual.\nHasil Akhir:\nSetelah menyelesaikan bootcamp, Anda akan memiliki dasar yang kuat dalam analitik data menggunakan Python. Anda akan mampu:\nMenangani dataset sederhana hingga menengah.\nMelakukan pembersihan dan manipulasi data dengan Pandas.\nMembuat visualisasi data menggunakan Matplotlib dan Seaborn.\nMengerjakan proyek analisis data dunia nyata.\nDengan bootcamp ini, Anda akan siap untuk melanjutkan perjalanan di bidang data analitik ke topik yang lebih kompleks seperti Machine Learning atau Big Data Analysis!\nTentang Saya\nSaya adalah seorang pengajar yang berpengalaman dalam bidang pemrograman dan analitik data. Dengan latar belakang yang kuat dalam Python dan berbagai alat analitik seperti Numpy, Pandas, dan Matplotlib, saya telah membantu banyak siswa memahami konsep dasar hingga tingkat lanjut dalam analitik data. Pendekatan saya dalam mengajar berfokus pada praktik langsung dan studi kasus yang relevan dengan kebutuhan dunia nyata. Saya percaya bahwa belajar analitik data bukan hanya tentang memahami teori, tetapi juga tentang membangun keterampilan yang dapat diterapkan untuk menyelesaikan masalah nyata. Dengan pengetahuan ini, saya ingin membantu Anda mencapai potensi terbaik Anda.",
      "target_audience": [
        "Seseorang yang ingin memulai karir sebagai data analyst",
        "Pengembang python yang ingin belajar ilmu data",
        "Pemula dalam dunia data analyst"
      ]
    },
    {
      "title": "Réseaux de neurones à partir de zéro",
      "url": "https://www.udemy.com/course/reseau-neurones/",
      "bio": "Implementation de réseaux de neurones en partant de zéro (Python)",
      "objectives": [
        "Ce que sont les réseaux de neurones",
        "Implémenter un réseau de neurones en partant de zéro (Python, Java, C, ...)",
        "Entraînement de réseaux de neurones",
        "Fonctions d'activation et le théorème d'approximation universel",
        "Renforcer vos connaissances en Machine Learning et Data Science",
        "Astuces d'implémentations: produit vectoriel Jacobien & log-sum-exp trick"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Réseau de neurones: explication intuitive"
        ],
        "Propagation avant (forward pass)": [
          "Propagation avant: explication",
          "Propagation avant: implémentation",
          "Fonction d'activation (ReLU)",
          "Réseau de neurones séquentiel"
        ],
        "Inférence: classification d'images": [
          "Sauvegarder et charger les paramètres du réseau de neurones",
          "Classification d'images: partie1",
          "Classification d'images: partie2",
          "Fonction d'activation: softmax"
        ],
        "Propagation arrière (backward pass)": [
          "Optimisation par descente de gradient",
          "Matrice jacobienne",
          "Théorème de dérivation des fonctions composées (chain rule)",
          "Théorème de dérivation des fonctions composées: implémentation"
        ],
        "Régression": [
          "Erreur quadratique moyenne (MSELoss)",
          "Entraînement d'un réseau de neurones",
          "Optimiseur",
          "Problème de régression: mesure quantitative de la progression d'une maladie"
        ],
        "Classification": [
          "LogSoftmax",
          "Log-sum-exp trick",
          "Erreur de vraisemblance logarithmique négative (NLLLoss)"
        ],
        "Améliorations": [
          "Traitement par lots (batching): partie1",
          "Traitement par lots (batching): partie2",
          "Produit vectoriel jacobien (jacobian-vector product)",
          "Initialisation de Xavier"
        ],
        "Classification d'images et conclusion": [
          "Classification d'images",
          "Conclusion"
        ]
      },
      "requirements": [
        "Connaissances de base en programmation, algèbre et analyse"
      ],
      "description": "Dans ce cours, nous allons implémenter un réseau de neurones en partant de zéro, sans librairies dédiées. Ainsi, bien que nous allons utiliser le langage de programmation python, à l'issue de ce cours, vous aurez la capacité d'implémenter un réseau de neurones, dans n'importe quel langage de programmation.\n\n\nNous verrons le fonctionnement des réseaux de neurones de manière intuitive, puis mathématique. Nous verrons également certaines astuces primordiales, qui permettent de stabiliser l'entraînement des réseaux de neurones (log-sum-exp trick), et d'empêcher que la mémoire utilisée pendant l'entraînement ne croisse de manière exponentielle (jacobian-vector product). Sans ces astuces, la plupart des réseaux de neurones ne pourraient être entraînés.\n\n\nNous allons entraîner nos réseaux de neurones sur des problèmes réels de classification d'images et de régression. Pour cela, nous allons implémenter différentes fonctions de coûts, ainsi que plusieurs fonctions d'activations.\n\n\nCe cours s'adresse aux développeurs qui aimeraient implémenter un réseau de neurones en partant de zéro ainsi qu'à ceux qui veulent comprendre comment fonctionne un réseau de neurones de A à Z.\n\n\nCe cours est enseigné en utilisant le langage de programmation Python et requiert des connaissances de base en programmation. Si vous n'avez pas les bases requises, je vous recommande de vous mettre à jour en suivant un cours accéléré de programmation. Il est également recommandé d'avoir des connaissances en Algèbre et en Analyse afin de profiter au mieux de ce cours.\n\n\nConcepts abordés :\nLes réseaux de neurones L'implémentation de réseaux de neurones en partant de zéro\nDescente de gradient et matrice Jacobienne\nLa création de Modules qui peuvent s'imbriquer dans le but de créer une architecture neuronale complexe\nLe log-sum-exp trick\nProduit vectoriel jacobien\nLes fonctions d'activations (ReLU, Softmax, LogSoftmax, ...)\nLes fonctions de coûts (MSELoss, NLLLoss, ...)\n\n\nCe cours est fréquemment mis à jour, avec l'ajout de bonus.\n\n\nN'attendez plus avant de vous lancer dans le monde du machine learning!",
      "target_audience": [
        "Aux développeurs qui aimeraient implémenter un réseau de neurones sans utiliser de librairies dédiées",
        "À ceux qui étudient le machine learning et aimeraient renforcer leurs connaissances sûr les réseaux de neurones et les frameworks de différentiation automatique",
        "À ceux qui préparent des entretiens en data science",
        "Aux enthousiastes de l'intelligence artificielle"
      ]
    },
    {
      "title": "Машинное обучение для людей",
      "url": "https://www.udemy.com/course/ml_for_people/",
      "bio": "Всё про все современные методы машинного обучения самыми простыми словами",
      "objectives": [
        "Машинное обучение",
        "Искусственные нейронные сети",
        "Обучение с подкреплением",
        "Ансамблевые методы"
      ],
      "course_content": {
        "Логистика курса": [
          "Логистика курса"
        ],
        "Введение": [
          "Зачем обучать машины",
          "Три составляющие машинного обучения",
          "Обучение или Интеллект",
          "Карта мира машинного обучения",
          "Тест к теме «Введение»"
        ],
        "Классическое обучение": [
          "Классическое обучение",
          "Обучение с учителем",
          "Классификация",
          "Регрессия",
          "Обучение без учителя",
          "Кластеризация",
          "Уменьшение размерности (обобщение)",
          "Поиск правил (ассоциация)",
          "Тест к теме «Классическое обучение»"
        ],
        "Обучение с подкреплением": [
          "Обучение с подкреплением. Часть 1",
          "Обучение с подкреплением. Часть 2",
          "Обучение с подкреплением. Часть 3",
          "Тест к теме «Обучение с подкреплением»"
        ],
        "Ансамбли": [
          "Ансамбли. Стекинг",
          "Бэггинг",
          "Бустинг",
          "Тест к теме «Ансамбли»"
        ],
        "Нейросети и глубокое обучение": [
          "Нейросети и глубокое обучение. Часть 1",
          "Нейросети и глубокое обучение. Часть 2",
          "Свёрточные нейросети",
          "Рекуррентные нейросети",
          "Тест к теме «Нейросети и глубокое обучение»"
        ],
        "Заключение": [
          "Когда на войну с машинами?"
        ]
      },
      "requirements": [
        "Базовая математическая подготовка была бы полезна, но не обязательно"
      ],
      "description": "Машинное обучение — это очень широкое направление исследований и прикладных разработок в области Искусственного Интеллекта. Сегодня машинное обучение представлено не только статистическими и дедуктивными методами, но и массой новых индуктивных подходов, одним из главных среди которых являются искусственные нейронные сети. Из нашего курса вы узнаете, какие методы машинного обучения есть, как и для каких задач их можно применять и что для этого нужно. После того как вы пройдёте курс, вы сможете абсолютно осознанно подходить к выбору решений, на экспертном уровне разговаривать с разработчиками, да и вообще уровень вашей сексуальности повысится на порядок.",
      "target_audience": [
        "Студенты и школьники",
        "Бизнесмены и менеджеры",
        "Чиновники и лица, принимающие решения",
        "Журналисты и технологические блогеры"
      ]
    },
    {
      "title": "Машинное обучение на Python. Метод линейной регрессии",
      "url": "https://www.udemy.com/course/pyqt5-bi/",
      "bio": "Это курс для тех, кто хочет познакомиться с методами линейной регрессии.",
      "objectives": [
        "Создать модель одномерной или многомерной линейной регрессии"
      ],
      "course_content": {
        "Введение": [
          "Какие библиотеки нам понадобятся",
          "Метод наименьших квадратов",
          "Одномерная линейная регрессия",
          "Одномерная линейная регрессия",
          "Многомерная линейная регрессия",
          "МетодМетод Lasso",
          "Метод Lasso",
          "Метод Lasso"
        ]
      },
      "requirements": [
        "Вы должны знать язык программирования Python."
      ],
      "description": "Вы узнаете, как на языке Python применять для анализа данных метод линейной регрессии. Вы узнаете:\nКакие разновидности этого метода встречаются.\nКак реализовать его на языке Python.\nКак построить регрессионную модель по имеющимся данным.\nКак выполнить прогноз методом линейной регрессии.\nКак выполняется селекция признаков.\nКак оценить качество построенной модели и прогноза на ее основе.",
      "target_audience": [
        "Разработчики программного обеспечения",
        "Специалисты по анализу данных"
      ]
    },
    {
      "title": "Python e Pandas - Tratamento de Dados com Interface Gráfica",
      "url": "https://www.udemy.com/course/python-e-pandas-tratamento-de-dados-com-interface-grafica/",
      "bio": "Crie um programa com Interface Gráfica com Python, Tkinter e Pandas e trate seus dados",
      "objectives": [
        "Introdução ao Pandas: aprenda os conceitos básicos do Pandas",
        "Interface Gráfica: Aprenda como criar um interface gráfica com o Python e Tkinter",
        "Manipule e trate os dados com o Pandas através de uma Inface Gráfica",
        "Amplie seus conhecimentos em Python"
      ],
      "course_content": {
        "Sistema com Python e Pandas": [
          "Apresentação",
          "Dica!",
          "Download e Instalação do Anaconda",
          "Primeiros Passos - Criando a Tela Inicial",
          "Criando Menu Arquivo e Label",
          "Criando Todos os Menus e Iniciando a Função para Abrir o Arquivo",
          "Carregando Dados de Excel para o Treeview com o Menu Abrir",
          "Soma Colunas com Valores",
          "Renomear Coluna",
          "Remover Linhas em Branco",
          "Remover Linhas Alternadas",
          "Remover Duplicados",
          "Remover Coluna",
          "Filtrar",
          "Group",
          "Merge Inner Join",
          "Merge Inner Join Resumindo Dados",
          "Merge Join Full",
          "Merge Left Join",
          "Merge Outer",
          "Salvando o Arquivo no Excel",
          "Editar Item da Treeview",
          "Consolidar Arquivos",
          "Quebrando Arquivos de Acordo com uma Coluna"
        ],
        "Mensagem Final do Curso": [
          "Parabéns você completou o curso"
        ]
      },
      "requirements": [
        "Programação em Python: é importante que o aluno já tenha conhecimentos básicos de programação em Python."
      ],
      "description": "O Pandas é uma biblioteca poderosa e amplamente utilizada em Python para manipulação e análise de dados. Nesse curso abrangente, você aprenderá a utilizar o Pandas para trabalhar com dados, desde a leitura e gravação de arquivos até técnicas avançadas de manipulação e análise de dados criando um programa com interface gráfica, facilitando a vida de seus clientes e usuários.\n\n\nAo longo do curso, você irá aprender:\n\n\nDownload e Instalação do Anaconda\nPrimeiros Passos - Criando a Tela Inicial\nCriando Menu Arquivo e Label\nCriando Todos os Menus e Iniciando a Função para Abrir o arquivo\nCarregando Dados de Excel para o Treeview com o Menu Abrir\nSoma Colunas com Valores\nRenomear Coluna\nRemover Linhas em Branco\nRemover Linhas Alternadas\nRemover Duplicados\nRemover Coluna\nFiltrar\nGroup\nMerge Inner Join\nMerge Inner Join Resumindo Dados\nMerge Join Full\nMerge Left Join\nMerge Outer\nSalvando o Arquivo no Excel\nEditar Item da Treeview\nConsolidar Arquivos\nQuebrando Arquivos de Acordo com uma Coluna\n\n\nPara quem é este curso?\nEste curso é literalmente para qualquer pessoa, de cientistas de dados a estudantes, médicos, músicos e novos programadores em potencial.\n\n\nPreciso de conhecimento prévio de Python?\nSim. Você precisa conhecer o básico do Python, que são variáveis, tipos de dados, funções, condicionais e loops. Este curso não cobre isso porque você pode encontrar esse conteúdo facilmente no YouTube.\n\n\nEste curso cobre Python 2 ou Python 3?\nPython 3.\n\n\nCom esses conhecimentos você dará o ponta pé inicial para criação de sistemas e automação de processos usando Pandas e criando uma Interface Gráfica amigavel para seus usuários.\n\n\nSatisfação garantida ou seu dinheiro de volta\n\"E se eu não gostar do curso?” A Udemy oferece uma garantia de reembolso, essa é mais uma garantia de qualidade e um incentivo a mais para você começar já! Após a compra você terá 30 dias para testar o produto, e se não gostar, basta solicitar o reembolso.\n\n\nJunte-se a mais de 5 mil alunos que já fizeram e aprovaram meus outros cursos! Comece agora mesmo esse curso!",
      "target_audience": [
        "Pessoas que desejam trabalhar com manipulação e análise de dados usando a linguagem Python.",
        "Para estudantes, cientistas de dados, engenheiros de dados, analistas de dados, programadores e qualquer pessoa que esteja interessada em aprender a utilizar a biblioteca Pandas em Python para manipulação e análise de dados."
      ]
    },
    {
      "title": "Numpy, Pandas를 이용한 데이터 분석 실습",
      "url": "https://www.udemy.com/course/ai-xiceq/",
      "bio": "[강의 교안 제공] Numpy, Pandas를 이용한 머신러닝을 위한 데이터 전처리",
      "objectives": [
        "리스트 구조",
        "딕셔너리 다루기",
        "넘파이 배열",
        "브로드 캐스팅"
      ],
      "course_content": {
        "인공지능(AI) 프로그래밍 - Machine Learning(머신러닝)을 위한 데이터전처리 (2022)": [
          "강좌 소개",
          "리스트 구조",
          "리스트 다루기",
          "리스트 반복문",
          "딕셔너리 다루기",
          "패키지",
          "날짜 시간 데이터",
          "넘파이 배열",
          "넘파이 배열 슬라이싱",
          "넘파이 배열 변경",
          "벡터화 연산",
          "브로드 캐스팅",
          "난수",
          "판다스 시리즈 생성",
          "데이터프레임 인덱싱",
          "파일 입출력",
          "데이터 참조",
          "데이터 조작",
          "데이터프레임 합성",
          "데이터 그룹화"
        ]
      },
      "requirements": [
        "별도의 요구 사항이나 필요 조건은 없습니다."
      ],
      "description": "넘파이, 판다스를 이용한 머신러닝을 위한 데이터 전처리에 대해 학습합니다.\n데이터 구조에 대한 설계 및 변경에 대해 학습을 진행합니다.\n\n\n데이터분석을 위한 자료에 대해 전처리를 진행할 수 있습니다.\n분석 목적에 적합한 형태로 데이터를 변형할 수 있습니다.\n\n\n[HD]인공지능(AI) 프로그래밍 - Machine Learning(머신러닝)을 위한 데이터전처리 (2022)\n\n\n강좌 소개\n리스트 구조\n...",
      "target_audience": [
        "데이터 분석 입문자",
        "파이썬 기초 문법 이수자"
      ]
    },
    {
      "title": "Prédiction des séries temporelles en deep learning - Partie1",
      "url": "https://www.udemy.com/course/modeles-avances-des-series-temporelles-avec-keras-partie-1/",
      "bio": "Concepts d'attention dans les modèles Seq2Seq. Codage du modèle multivarié DA-RNN sous Python avec Keras / Tensorflow.",
      "objectives": [
        "Comprendre et coder le concept d'attention spatio-temporelle afin d'améliorer les prédictions sur les séries temporelles",
        "Coder des réseaux de prédictions de séries temporelles de type séquence-vers-séquence univariés et multivariés",
        "Coder les attentions de Badhanau et de Luong dans les modèles de prédiction",
        "Coder un réseau de prédiction de type DA-RNN"
      ],
      "course_content": {},
      "requirements": [
        "Python, Keras et Tensorflow.",
        "Avoir de bonnes connaissances en deep learning et en particulier sur les modèles récurrents et leur mise en application dans les séries temporelles",
        "Avoir accès aux TPU avec sous Google Colab",
        "Il est préférable d'avoir suivi la partie consacrée aux séries temporelles dans ma première formation généraliste consacrée au deep learning"
      ],
      "description": "Ce cours fait suite à ma première formation \"Formation au deep learning avec Python (Keras / Tensorflow)\" qui vous a permis de faire une initiation sur la prédiction des séries temporelles.\nPour aller plus loin, cette formation va vous permettre d'introduire des concepts d'attention utilisés pour réaliser différents modèles de prédiction. Vous allez découvrir en particulier le modèle DA-RNN. Nous utiliserons les librairies telles que Tensorflow, Keras, Pandas, Numpy, Scikit learn, ...\nLes travaux sont accessibles et exploitables en ligne grâce à l'utilisation des carnets Jupyter avec Google Colab. Aucune installation de logiciel spécifique sur son ordinateur n'est requise car tout le travail se fait en ligne.\nA chaque étape d'apprentissage de ce cours, de nouveaux concepts sont introduits. Des explications claires permettent de bien les comprendre à travers 6 thèmes d'étude :\nQuelques rappels sur les modèles basiques de prédictions de séries temporelles qui utilisent des couches récurrentes\nComprendre et coder les concepts de base de l'attention dans les modèles basiques.\nCoder et mettre en œuvre  le modèle End-to-End Memory Network\nAppliquer les modèles de type Séquence vers Séquence (Seq2Seq) à la prédiction de l'énergie produite par des panneaux photovoltaïques\nComprendre et coder l'attention de Badhanau et de Luong aux modèles Séquence vers Séquence\nComprendre et coder un modèle multivarié à attention spatio-temporelle : le modèle DA-RNN\nLes activités en Python expliquent clairement comment les exploiter. D'une durée totale de 7h, ce cours vous permettra d'être à l'aise dans l'utilisation avancée de Keras et Tensorflow pour coder vos propres couches et modèles, à partir de classes héritées. Les thèmes d'étude s'appuient sur des documents issus de la recherche scientifique et proposent des concepts importants qui ont été développés ces dernières années.\n=== Prérequis ===\nCe cours fait suite à ma première formation \"Formation au deep learning avec Python (Keras / Tensorflow)\".\nSi vous avez déjà une expérience en Deep Learning, vous découvrirez certainement de nouveaux thèmes d'étude qui vous permettront d'élargir vos compétences.\n== Aide en ligne ===\nQuelque soit votre niveau, je suis disponible pour vous aider dans votre progression. Vous pourrez éventuellement rencontrer des difficultés en Mathématiques ou en programmation car il est bien évident que vous avez tous un bagage différent en fonction de votre parcours (études ou professionnel).\nDans ce sens, il ne faudra surtout pas hésiter à me poser vos questions et je m'engage à y répondre dans un délai raisonnable. Votre motivation est essentielle pour réussir cette formation.\n=== Thèmes étudiés dans la formation ===\n#1. Rappels sur les modèles basiques du deep learning utilisés pour la prédiction des séries temporelles\nQuand on veut prédire ou juste analyser l’évolution d’une certaine quantité dans le temps, (le cours de la bourse par exemple) on est très vite confronté un type de données assez particulier : Les séries temporelles.\nL’importance des séries temporelles vient de son omniprésence. En effet, dans presque tous les domaines on les retrouve. Que ce soit en finance, en marketing, en traitement du signal, en physiques, etc...\nNous reverrons rapidement dans cette première partie les résultats essentiels vus dans ma première formation sur le deep learning.\n\n\n#2. Comprendre et coder les concepts de base de l'attention dans les modèles basiques\nDans sa forme la plus simple, telle que découverte et popularisée par Dzmitry Bahdanau, Kyunghyun Cho et Yoshua Bengio dès 2014, un mécanisme d’attention est défini comme étant un mini-réseau neuronal capable d’évaluer les proportions d’attention qu’il faut accorder à des éléments dans un ensemble ou une liste.\nC'est ce concept d'attention qui a permis au domaine de traitement naturel du langage de faire un pas de géant dans les années 2015. Depuis, ce concept a été porté dans le domaine des modèles de prédiction des séries temporelles par différents chercheurs.\n\n\n#3. Coder et mettre en œuvre  le modèle End-to-End Memory Network\nUn modèle de ce type est un modèle qui tente de modéliser le fonctionnement d'un cerveau animal. Ce type de model a été publié dans les documents de Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus.\n\n\n#4. Appliquer les modèles de type Séquence vers Séquence (Seq2Seq) à la prédiction de l'énergie produite par des panneaux photovoltaïques\nUn modèle de type séquence vers séquence est un réseau de neurones récurrent qui convertit une séquence de données d'un domaine en entrée vers une nouvelle séquence de données dans un autre domaine en sortie.\nGénéralement, un modèle séquence à séquence est implémenté en utilisant deux réseaux de neurones récurrents, un premier réseau est un encodeur et le second est un décodeur. On parle aussi d'une architecture encodeur-décodeur. Dans ces modèles, l'entrée et la sortie ne sont pas nécessairement de la même longueur. Un bon exemple d'utilisation d'un modèle séquence à séquence est la traduction neuronale d'une phrase d'une langue d'origine vers une langue d'arrivée.\n\n\n#5. Comprendre et coder l'attention de Badhanau et de Luong aux modèles Séquence vers Séquence\nCes deux attentions sont utilisées dans les modèles séquence vers séquence. Ce sont les deux principales méthodes qui introduisent le concept d'attention, très largement utilisé dans le domaine du traitement naturel du langage, de la vision artificielle, de la prédiction des séries temporelles ...\nCes deux méthodes sont très importantes ar ce sont sur elles que reposent les modèles de prédictions de séries temporelles à attentions spatio-temporelles les plus avancés de ces dernières années.\n\n\n#6. Comprendre et coder un modèle multivarié à attention spatio-temporelle : le modèle DA-RNN\nLe modèle DA-RNN est un modèle de référence dans le domaine. Très souvent, les modèles issus des recherches scientifiques actuelles comparent leurs résultats à ceux obtenus avec le modèle DA-RNN.\nIl est donc essentiel de bien comprendre ce modèle afin de pouvoir approfondir ses connaissances et évoluer vers des modèles plus performants par la suite.",
      "target_audience": [
        "Etudiants ou professionnels souhaitant approfondir leurs connaissances dans le domaine de la prédiction des séries temporelles à l'aide du deep learning."
      ]
    },
    {
      "title": "Learn Python for Data Science in Arabic (Part 1)",
      "url": "https://www.udemy.com/course/learn-python-for-ds-for-beginners-in-arabic-l1/",
      "bio": "تعلم بايثون بالتفصيل وأبدأ رحلتك في مجال تحليل البيانات وعلم البيانات",
      "objectives": [
        "ماهي لغة بايثون",
        "لماذا نتعلم لغة بايثون لمجال تحليل وعلم البيانات",
        "أساسيات البرمجة",
        "كيفية إنشاء متغير في لغة بايثون",
        "أنواع البيانات المستخدمة في لغة بايثون",
        "كيفية إنشاء لووب في بايثون",
        "كيفية إنشاء إف ستيتمنت في بايثون",
        "كيف تقوم بإنشاء داتا فريم في بايثون",
        "كيف تقوم بإنشاء لست في بايثون",
        "كيف تقوم بإنشاء دكشنري في بايثون",
        "كيف تقوم بعمل تابل في بايثون",
        "كيف تتعامل مع النصوص ( سترينج ) في بايثون",
        "كيف تقوم بعمل عمليات حسابية في بايثون",
        "ستتعلم أساسيات برنامج جيوبتر نوت بوكس"
      ],
      "course_content": {
        "Introduction": [
          "نظرة عامة على الدورة",
          "تقييم الدورة",
          "لماذا يجب عليك ان تقيم الدورة",
          "تعرف على مقدم الدورة",
          "نظرة عامة على كيفية مشاهدة هذه الدورة"
        ],
        "Getting ready for the course": [
          "Installing Anaconda Distribution",
          "Google Colab Overview",
          "Replit Overview",
          "What is Python and why learn it",
          "Why Choose Python for Data Science"
        ],
        "Jupyter Notebook Crash Course": [
          "What is Jupyter Notebook",
          "Edit-Command-Mode",
          "Opening Jupyter Notebook",
          "Autocomplete",
          "Menus Overview",
          "What is Markdown",
          "Markdown: Headings",
          "Markdown: Blockquotes",
          "Markdown: Math Symbols",
          "Markdown: Line Break",
          "Markdown: Bold Text",
          "Markdown: Italic Text",
          "Markdown: Horizontal line",
          "Markdown: Ordered list",
          "Markdown: Unordered list",
          "Markdown: Internal Link",
          "Markdown: External Link",
          "Markdown: Adding Image",
          "Markdown: Adding Video"
        ],
        "Basics of Programming | أساسيات البرمجة": [
          "What Is a Programming",
          "What Is A Programming Language",
          "What is an IDE",
          "Variables (Theory)",
          "Variables (Practice)",
          "Variables (Exercise)",
          "Variables (Exercise -Solution)",
          "Data Types (Theory + Practice)",
          "Data Types (Exercise)",
          "Data Types (Exercise-Solution)",
          "String (Theory)",
          "String (Practice)",
          "String (Exercise)",
          "String (Exercise-Solution)",
          "String Negative Indexing (Practice)",
          "String Positive - Negative Indexing (Exercise)",
          "String Positive - Negative Indexing(Exercise-Solution)",
          "Operators (Theory)",
          "Arithmetic Operators (Practice)",
          "Arithmetic Operators (Exercise)",
          "Arithmetic Operators (Exercise-Solution)",
          "Logical Operators (Practice)",
          "Logical Operators (Exercise)",
          "Logical Operators (Exercise-Solution)",
          "Comparison Operators (Practice)",
          "Comparison Operators (Exercise)",
          "Comparison Operators (Exercise-Solution)",
          "Loops (Theory)",
          "Loops (Practice)",
          "Loops (Exercise)",
          "Loops (Solution)",
          "IF Statement (Theory)",
          "IF Statement (Practice)",
          "IF Statement (Exercise)",
          "IF Statement (Solution)",
          "Case Sensitive",
          "Type of variables",
          "Converting Data types",
          "Override Separator",
          "Override Separator ( Exercise)",
          "Override Separator ( Solution)",
          "Override End",
          "Override End (Exercise)",
          "Override End (Solution)",
          "Dynamic vs Static Typing",
          "Dynamic vs Static Binding",
          "Creating Multiple Variables",
          "Taking User Input"
        ],
        "Basics of Python Programming": [
          "Iterables Data Types",
          "List (Theory)",
          "List (Practice)",
          "List (Exercise)",
          "List (Solution)",
          "Tuple (Theory)",
          "Tuple (Practice)",
          "Tuple (Exercise)",
          "Tuple (Solution)",
          "Set (Theory)",
          "Set (Practice)",
          "Set (Exercise)",
          "Set (Solution)",
          "Dictionary (Theory)",
          "Dictionary (Practice)",
          "Dictionary (Exercise)",
          "Dictionary (Solution)",
          "Review",
          "Functions (Theory)",
          "Functions (Practice)",
          "Functions (Exercise)",
          "Functions (Solution)"
        ],
        "Data Science Crash Course |أساسيات علم البيانات": [
          "ماهو علم البيانات",
          "الفرق بين ذكاء الإصطناعي والتعلم الآلة والتعلم العميق",
          "نوع الأسئلة الذي يقوم عالم البيانات بإجابتها",
          "التعلم بإشراف والتعلم بدون إشراف",
          "تطبيقات علم البيانات على الأرض الواقع",
          "مراحل عملية علم البيانات",
          "مصادر البيانات",
          "أنواع البيانات",
          "الفرق بين عالم البيانات ومحلل البيانات ومهندس البيانات وعالم التعلم الآلة-جزء 1",
          "الفرق بين عالم البيانات ومحلل البيانات ومهندس البيانات وعالم التعلم الآلة-جزء 2",
          "المهارات المطلوبة لكي تصبح عالم البيانات",
          "ماهي منصة كيجل؟",
          "نظرة مفصلة على منصة كيجل",
          "Quiz for Data Science Course"
        ],
        "BONUS SECTION": [
          "BONUS LECTURE"
        ]
      },
      "requirements": [
        "يجب ان تكون لديك صلاحية تحميل البرامج من الانترنت",
        "لا يوجد اي متطلبات متعلقة بالبرمجة حيث ستتعلم كل شي بالتفصيل ومن البداية في هذه الدورة"
      ],
      "description": "تُعتبر دورة \"تعلم بايثون في مجال علم البيانات\" من الدورات الأساسية والمهمة لمن يرغبون في دخول عالم تحليل وعلم البيانات والتعلم الآلة والتعلم العميق. تتجلى أهمية تعلم لغة بايثون في قدرتها على توفير بيئة تطوير سهلة ومرنة تتيح للمبرمجين والعلماء استخدامها بكفاءة في تحليل ومعالجة البيانات. بايثون تمتاز بقوتها في التعامل مع هياكل البيانات والمكتبات المتخصصة في علم البيانات والذكاء الاصطناعي.\n\n\nمن خلال هذه الدورة، سيتعلم المتدربون أساسيات البرمجة باستخدام لغة بايثون، مع التركيز بشكل خاص على تطبيقاته في مجال علم البيانات، ثم سيتعلمون كيفية تحليل البيانات باستخدام مكتبة بانداز ومن ثم أساسيات تصوير البيانات باستخدام مكتبة ماتبلوت ليب وسيبورن\nتتميز هذه الدورة ان جميع المواضيع تم شرحها اكثر من مرة وبشكل مفصل وكل درس يحتوي على تمرين مع حل التمرين وجميع الفيديوهات في هذه الدورة تم تسجيلها بجودة عالية\nباختتام هذه الدورة، سيكون لدى المتدربين قاعدة قوية في البرمجة بايثون وفهم واضح لكيفية استخدامها في تحليل البيانات. سيكونون قادرين على التعامل مع البيانات واستخدامها لاكتشاف الأنماط واتخاذ القرارات المستندة إلى البيانات. هذه الدورة هي خطوة أساسية لمن يسعون لبناء مستقبل واعد في مجال علم البيانات وتحقيق النجاح في فهم وتحليل البيانات الكبيرة والمعقدة.\nفي هذه الدورة ستتعلم:\n· ماهي لغة بايثون\n· لماذا نتعلم لغة بايثون لمجال تحليل وعلم البيانات\n· أساسيات البرمجة\n· كيفية إنشاء متغير في لغة بايثون\n· أنواع البيانات المستخدمة في لغة بايثون\n· كيفية إنشاء لووب في بايثون\n· كيفية إنشاء إف ستيتمنت في بايثون\n· كيف تقوم بإنشاء داتا فريم في بايثون\n· كيف تقوم بإنشاء لست في بايثون\n· كيف تقوم بإنشاء دكشنري في بايثون\n· كيف تقوم بعمل تابل في بايثون\n· كيف تقوم بعمل عمليات حسابية في بايثون\n· ستتعلم أساسيات برنامج جيوبتر نوت بوكس",
      "target_audience": [
        "أي احد مهتم في دراسة لغة بايثون في مجال علم البيانات",
        "أي احد يرغب في دراسة مجال تحليل و علم البيانات",
        "طلبة دراسات عليا في مجال الذكاء الإصطناعي وعلم البيانات",
        "الباحثين في مجال الذكاء الإصطناعي وعلم البيانات"
      ]
    },
    {
      "title": "【初心者向け】LLMをファインチューニングして独自AIモデルを作ろう！Webページの情報を元に回答できるAIを作ろう！",
      "url": "https://www.udemy.com/course/fine-tuning/",
      "bio": "ファインチューニング・RAG・プロンプトエンジニアリングの違いやファインチューニングの種類について学ぼう！OpenAIのGPTモデルをファインチューニングし自分だけのAIモデルを作れるようになろう！Pythonでの様々な処理方法も学ぼう！",
      "objectives": [
        "ファインチューニングとRAGとプロンプトエンジニアリングの違い",
        "ファインチューニングの種類",
        "OpenAIのモデルをプラットフォーム上からファインチューニングする方法",
        "OpenAIのモデルをPythonからファインチューニングする方法",
        "Webサイトの情報を元にファインチューニングする方法"
      ],
      "course_content": {
        "はじめに": [
          "イントロダクション",
          "ファインチューニングとRAGとプロンプトエンジニアリングの違い"
        ],
        "Pythonの使い方": [
          "Pythonを学ぼう！",
          "Pythonの実行環境",
          "Google Colabの使い方",
          "演算子の種類を学ぼう！",
          "Pythonの変数と型を学ぼう！",
          "list型(リスト)を学ぼう！",
          "dict型(辞書)を学ぼう！",
          "print関数を学ぼう！",
          "文字列の操作方法を学ぼう！",
          "条件分岐のif文を学ぼう！",
          "繰り返し処理のfor文を学ぼう！",
          "関数の作り方と使い方を学ぼう！"
        ],
        "ファインチューニングをGUI上で実行してみよう！": [
          "OpenAIのAPIキー取得方法と次レクチャー以降の参照URL",
          "ファインチューニングのアプローチの違い：SFT・DPO・RFT",
          "ファインチューニングにインプットするデータセットの作成",
          "【注意】ファインチューニングのベースとなるモデルについて",
          "ファインチューニングを実行しよう！",
          "ファインチューニングモデルの出力と元のモデルの出力を比較してみよう！"
        ],
        "ファインチューニングをPythonで実行してみよう！": [
          "Pythonを使ってファインチューニング用のデータをアップロードしていこう！",
          "Pythonでファインチューニングを実行していこう！",
          "ジョブの確認や学習状況の確認など様々なコマンドを実行してみよう！",
          "ファインチューニング済みのモデルに質問して出力を確認しよう！"
        ],
        "Webサイトの情報をベースにファインチューニングしたモデルを作ろう！": [
          "特定のWebページの情報をスクレイピングで取得してテキストに整形しよう！",
          "スクレイピングした情報をテキストに整形していこう！",
          "ファインチューニングのデータセットを作成しよう！",
          "データセットをJSONL形式で保存しよう！",
          "ファイルをアップロードし、ファインチューニングを実行しよう！",
          "ファインチューニング済みモデルの出力を確認しよう！",
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "Pythonの基礎から学びますのでプログラミングの知識は特に必要ありません"
      ],
      "description": "ファインチューニング講座へようこそ！\n\n\nこの講座では、既存のAIモデルを独自のデータで再学習させるファインチューニングというアプローチについて学んでいきます。\n\n\nまずファインチューニング・RAG・プロンプトエンジニアリングの違いについて学び、その後にファインチューニングの種類について学んでいきます。\n\n\nそして実際にOpenAIのプラットフォーム上でファインチューニングを実行した後、Pythonでファインチューニングを行います。\n最後に特定のWebサイトの情報を元にファインチューニングを行いWebページの情報に返答できるようなモデルを作っていきます。\n\n\nファインチューニングを使いこなして自分だけの独自のモデルを作れるようになりましょう！",
      "target_audience": [
        "ファインチューニングに興味のある方",
        "既存のモデルでは上手くタスクが処理出来ず困っている方",
        "独自のデータを用いて独自のAIモデルを作りたい方"
      ]
    },
    {
      "title": "Curso DBT: Modelado y Pipelines en el Modern Data Stack",
      "url": "https://www.udemy.com/course/curso-dbt-modelado-y-pipelines-en-el-modern-data-stack/",
      "bio": "Flavio, experto en DBT y analítica moderna, te guiará paso a paso para transformar,modelar y desplegar datos eficazmente",
      "objectives": [
        "Comprender qué es DBT y su rol en la transformación de datos moderna.",
        "Diseñar proyectos escalables utilizando el Modern Data Stack y herramientas como DBT y Snowflake.",
        "Aplicar buenas prácticas de modelado de datos, incluyendo modelos staging, intermediate y marts.",
        "Utilizar macros, pruebas y snapshots en DBT para garantizar la calidad y trazabilidad de los datos.",
        "Implementar estrategias incrementales para el procesamiento eficiente de grandes volúmenes de datos.",
        "Automatizar, monitorear y mantener pipelines DBT en producción utilizando herramientas de documentación y alertas."
      ],
      "course_content": {
        "Introducción": [
          "Bienvenido a DataBoosters",
          "Presentación del curso",
          "Presentación del instructor y contenido del curso",
          "Archivos descargables"
        ],
        "Módulo 1: Introducción a DBT y a Modern Data Stack": [
          "1.1 Bienvenida al Mundo de la Ingeniería Analítica",
          "1.2 Qué es la Ingeniería Analítica y por qué DBT",
          "1.3 El Ciclo de Vida del Análisis de Datos",
          "1.4 Roles Clave en el Mundo del Dato",
          "1.5 El Modern Data Stack - La Nueva Arquitectura del Dato",
          "1.6 DBT - El Corazón del Modern Data Stack",
          "1.7 DBT y Snowflake - Una Pareja Poderosa",
          "1.8 Práctica Inicial - Configurando tu Proyecto DBT y Conexión a Snowflake"
        ],
        "Modelado de Datos con DBT": [
          "2.1 Fundamentos del Modelado Dimensional para OLAP",
          "2.2 Modelado Monolítico vs Modular - La Importancia de la Modularidad",
          "2.3 Construyendo Modelos Modulares con DBT",
          "2.4 Organización de un Proyecto DBT - Carpetas y Archivos Clave",
          "2.5 Práctica - Modelos Staging y Dependencias en dbt",
          "2.6 Profundizando en Modelos Intermedios y Marts",
          "2.7 Buenas Prácticas en el Modelado de Datos con DBT"
        ],
        "Módulo 3: Transformaciones Avanzadas con DBT": [
          "3.1 Macros en DBT - Funciones SQL Reutilizables",
          "3.2 Tests en DBT - Asegurando la Calidad de tus Datos",
          "3.3 Snapshots en DBT - Rastreando la Evolución de tus Datos",
          "3.4 Jinja en DBT - Lógica Condicional y Control de Flujo",
          "3.5 Profundizando en Jinja - Ejemplos Avanzados y Casos de Uso"
        ],
        "Módulo 4: Implementación y Despliegue de Pipelines DBT": [
          "4.1 Materializaciones de Modelos en DBT - Tablas, Vistas y Más",
          "4.2 Estrategias Incrementales - Append, Merge y Otras",
          "4.3 Cuándo Elegir Cada Estrategia Incremental - Casos de Uso",
          "4.4 Environments en dbt Cloud - Desarrollo, Despliegue, Staging y Producción",
          "4.5 Práctica - Modelos Incrementales y Entorno en dbt Cloud",
          "4.6 Despliegue de Pipelines DBT en Producción"
        ],
        "Módulo 5: Mantenimiento y Monitoreo de Pipelines DBT": [
          "5.1 Documentación en DBT - La Clave para la Mantenibilidad",
          "5.2 Pruebas Continuas - El Pilar del Mantenimiento",
          "5.3 Organización del Código para Proyectos DBT Escalables",
          "5.4 Gestión de Dependencias con Packages en DBT",
          "5.5 Monitoreo de Pipelines DBT y Alertas",
          "5.6 Refactorización y Optimización de Modelos DBT"
        ],
        "Finalización de curso": [
          "Cierre del curso"
        ]
      },
      "requirements": [
        "No se requiere experiencia previa en DBT.",
        "Conocimientos básicos de SQL son altamente recomendados.",
        "Familiaridad con conceptos generales de bases de datos y análisis de datos será útil.",
        "Acceso a una cuenta gratuita de dbt Cloud y, opcionalmente, una cuenta de prueba de Snowflake."
      ],
      "description": "En el mundo actual de los datos, donde la velocidad, la escalabilidad y la trazabilidad son esenciales, DBT (Data Build Tool) se ha posicionado como una herramienta fundamental para los equipos de analítica moderna. Este curso está diseñado para llevarte de cero a experto en DBT, guiándote paso a paso en el uso de esta potente herramienta en el contexto del Modern Data Stack.\nA lo largo de este programa, descubrirás qué es la ingeniería analítica, por qué es crucial hoy en día y cómo DBT se convierte en el corazón de esta transformación. Comenzarás configurando tu primer proyecto DBT, conectándote con Snowflake y creando modelos simples. Luego, explorarás cómo construir modelos modulares, organizar tus carpetas y archivos, y aplicar buenas prácticas de desarrollo.\nEntrarás al mundo de las transformaciones avanzadas, aprendiendo a usar macros, tests y snapshots, así como a implementar Jinja para lógicas condicionales. También dominarás la materialización de modelos, estrategias incrementales como merge y append, y la gestión de entornos en dbt Cloud.\nFinalmente, el curso te preparará para mantener y optimizar tus pipelines en producción mediante documentación automatizada, pruebas continuas, monitoreo y refactorización de modelos.\nEste curso combina teoría, práctica y laboratorios guiados para garantizar una experiencia completa y aplicable en contextos reales. Si deseas convertirte en un verdadero Analytics Engineer y dominar la transformación de datos con DBT y Snowflake, este es el curso para ti.",
      "target_audience": [
        "Analistas de datos que buscan automatizar y escalar sus procesos de transformación.",
        "Científicos de datos y BI Analysts interesados en mejorar la calidad y trazabilidad de sus datasets.",
        "Ingenieros de datos que deseen incorporar DBT en sus pipelines de datos.",
        "Estudiantes o profesionales del área de datos que quieran aprender sobre el Modern Data Stack.",
        "Equipos de analítica que buscan colaborar en proyectos organizados y versionados."
      ]
    },
    {
      "title": "Mengenal Firebase untuk Pemrograman, Web Hosting, Database",
      "url": "https://www.udemy.com/course/mengenal-firebase-untuk-pemrograman-web-hosting-database/",
      "bio": "Mengoptimalkan Firebase untuk Web Hosting, Pemrograman Database, NoSQL, Storage, Pyrebase, dan Authentication",
      "objectives": [
        "Mengenal dan memulai menggunakan Firebase",
        "Membuat website dengan menggunakan Firebase",
        "Menggunakan Python dan Firebase untuk membangun aplikasi",
        "CRUD dengan menggunakan Python dan Pyrebase",
        "Cara updating dan rollback pada website",
        "Pengaturan service account dan cloud firestore",
        "Membuat dokumen, menggunakan merge, dan Field",
        "Operasi CRUD database menggunakan Firestore",
        "Update data berbentuk array dan dictionary",
        "Membuat username, password, dan telephone field",
        "Mengenal Firebase Storage dan cara mengunggah dan mendownload image",
        "Password reset dan email verification"
      ],
      "course_content": {
        "Pengantar": [
          "Pengantar",
          "Bagaimana Mendapatkan e-Certificate?"
        ],
        "Memulai Firebase": [
          "Membuat Project Baru",
          "Register App Baru dan Install Node.js",
          "Menyiapkan Website dan Initiate Project",
          "Publikasi Website",
          "Update dan Rolled Back",
          "Tugas Membuat website sendiri"
        ],
        "Firebase dan Python": [
          "Instalasi Python dan Firebase Admin",
          "Service Account",
          "Cloud Firestore",
          "Membuat Document",
          "Merge untuk Penambahan Field",
          "Kuis Seputar Dasar-Dasar Layanan Firebase"
        ],
        "CRUD": [
          "Create Data",
          "Read Data",
          "Read Data dan Query",
          "Merge dan Dictionary Sebagai Value",
          "Update Data",
          "Update Data Array dan Dictionary",
          "Update tanpa Document ID",
          "Menghapus Document dan Field",
          "Menghapus Data dengan Query"
        ],
        "Authentication": [
          "Membuat User dengan Email dan Password",
          "Menggunakan Parameter phone_number",
          "Parameter uid",
          "Mengambil Data User",
          "Kuis Seputar Database dan Authentication"
        ],
        "Storage": [
          "Firebase Storage",
          "Upload File",
          "Artikel: Kode Program untuk Mengunggah File",
          "Tugas: Unggah File-File Anda ke dalam Storage"
        ],
        "Pyrebase": [
          "Instalasi dan Konfigurasi Pyrebase",
          "Upload dan Download File",
          "Pyrebase Authentication",
          "Artikel: Kode program untuk register user baru",
          "Email Verification",
          "Password Reset",
          "CRUD dengan Realtime Database",
          "Artikel: Kode Program CRUD Realtime",
          "Menghapus Data",
          "Kuis Akhir tentang Firebase"
        ]
      },
      "requirements": [
        "Memiliki akun Google"
      ],
      "description": "Apakah Anda ingin menjadi pengembang database, web hosting, dan menggunakan layanan Firebase?\n\n\nJadi, mulailah dari video course ini. Video ini adalah online course yang paling lengkap, to-the point, dan mudah dipahami untuk mempelajari Firebase di Udemy. Tidak hanya itu, semua penjelasan di dalam video course ini disampaikan dalam Bahasa Indonesia dengan jargon-jargon awam. Tidak memandang apakah Anda ingin mengambil profesi sebagai web developer dengan spesialisasi NoSQL atau ingin membuat aplikasi dengan layanan Firebase, video course ini dirancang tentu saja untuk Anda. Dalam kursus ini, kami akan mengajari Anda menggunakan Firebase secara efektif dan singkat, tepat, serta jelas.\nDilengkapi dengan berjam-jam video course, kursus komprehensif ini mengupas cara membuat aplikasi dengan menggunakan Firebase. Anda bisa mengoperasikannya menggunakan MS Windows. MacOS, dan Linux selama memiliki Visual Studio Code.\nVideo course  ini akan mengajarkan Anda memahami cara membangun aplikasi dengan Firebase dengan setiap bab dilengkapi screencast yang mengajarkan sintaks, fungsi, dan aturan pemrograman lengkap dan penjelasan-penjelasan yang menarik.\nAdapun penjelasan lebih lengkap adalah:\nMengenal dan memulai menggunakan Firebase\nMembuat website dengan menggunakan Firebase\nMenggunakan Python dan Firebase untuk membangun aplikasi\nCRUD dengan menggunakan Python dan Pyrebase\nCara updating dan rollback pada website\nPengaturan service account dan cloud firestore\nMembuat dokumen, menggunakan merge, dan Field\nOperasi CRUD database menggunakan Firestore\nUpdate data berbentuk array dan dictionary\nMembuat username, password, dan telephone field\nMengenal Firebase Storage dan cara mengunggah dan mendownload image\nPassword reset dan email verification\n\n\nJangan khawatir tentang kualitas video course ini sebab Udemy menawarkan garansi 30 hari uang kembali tanpa syarat. Jika Anda tidak puas, silakan ajukan refund tanpa syarat.",
      "target_audience": [
        "Pengembang website",
        "Programmer Python",
        "Programmer NoSQL Database"
      ]
    },
    {
      "title": "เรียน Python for Data Science ฉบับตั้งไข่ เรียนง่ายเป็นเร็ว",
      "url": "https://www.udemy.com/course/pythondatasci/",
      "bio": "เรียนเขียนโปรแกรมภาษาไพธอน Python Programming for Data Science สำหรับผู้เริ่มต้น Zero to Hero",
      "objectives": [
        "ผู้เรียนจะเข้าใจโครงสร้างพื้นฐานของการเขียนโปรแกรมภาษา Python สำหรับงาน Data Scicence",
        "ผู้เรียนจะได้รับความเข้าใจอย่างถี่ถ้วนเกี่ยวกับเนื้อหาของการเขียนโปรแกรมภาษา Python สำหรับงาน Data Science อย่างเข้มข้น",
        "ผู้เรียนสามารถเขียนโปรแกรมภาษา Python สำหรับงาน Data Science ตั้งแต่ระดับพื้นฐานไปจนถึงสามารถต่อยอดประยุกต์ใช้งานจริงได้",
        "ผู้เรียนสามารถนำความรู้ที่ได้ไปประยุกต์ใช้ทำ Project ต่าง ๆ ในการเรียนและการทำงานจริงได้"
      ],
      "course_content": {
        "เริ่มต้นเขียนโปรแกรมภาษา Python สำหรับงาน Data Science": [
          "เริ่มต้นเขียนโปรแกรม Python สำหรับงาน Data Science",
          "ใช้ Python เป็นเครื่องคำนวณทางคณิตศาสตร์",
          "ใช้ Python กำหนดค่าตัวแปร Variable Assignment",
          "ใช้ Python ทำการคำนวณค่าตัวแปร Variables",
          "ใช้ Python เก็บค่าและแสดงข้อมูลของตัวแปรชนิดต่าง ๆ",
          "ใช้ Python สร้าง Operations เป็นตัวดำเนินการ",
          "ใช้ Python แปลงชนิดข้อมูล Type Conversion",
          "ทดสอบความเข้าใจข้อใดแสดงผลได้ถูกต้อง"
        ],
        "เรียนเขียนโปรแกรมใช้งาน Python Lists สำหรับงาน Data Science": [
          "ใช้ Python สร้าง List เก็บข้อมูลแบบรายการ",
          "ใช้ Python สร้าง List เก็บข้อมูลแบบรายการต่างชนิดกัน",
          "ใช้ Python สร้าง List แบบต่าง ๆ",
          "ใช้ Python สร้าง List of Lists",
          "ใช้ Python สร้าง Subset & Conquer",
          "ใช้ Python สร้าง Subset & Calculate",
          "ใช้ Python สร้าง Slicing &. Dicing",
          "ใช้ Python สร้าง Replace List Elements",
          "ใช้ Python ทำการ Extend a List",
          "ใช้ Python ทำการ Delete List Elements",
          "ใช้ Python ทำการ Inner Workings of Lists"
        ],
        "เรียนเขียนโปรแกรมใช้งาน Python Functions สำหรับงาน Data Science": [
          "ใช้ Python เรียกใช้ฟังก์ชันพื้นฐาน",
          "ใช้ Python สร้าง Multiple Arguments",
          "ใช้ Python สร้าง String Methods",
          "ใช้ Python สร้าง List Methods 1",
          "ใช้ Python สร้าง List Methods 2",
          "ใช้ Python ทำการ Import Package",
          "ใช้ Python User Defined Functions 1",
          "ใช้ Python User Defined Functions 2",
          "ใช้ Python User Defined Functions 3"
        ],
        "เรียนเขียนโปรแกรมใช้งาน Python NumPy สำหรับงาน Data Science": [
          "ใช้ Python ทำการเก็บค่าความสูง",
          "ใช้ Python แปลงหน่วยความสูง",
          "ใช้ Python คำนวณค่า BMI",
          "ใช้ Python เรียกใช้ Subsetting NumPy Array",
          "ใช้ Python สร้างเงื่อนไขตรวจสอบ",
          "ใช้ Python สร้าง Arithmetic Operators",
          "ใช้ Python สร้าง 2D NumPy Array",
          "ใช้ Python เรียกใช้ Subsetting 2D NumPy",
          "ใช้ Python คำนวณ 2D Arithmetic Operators",
          "ใช้ Python คำนวณค่าทาง Statistics",
          "ใช้ Python คำนวณ Statistics และ Correlation",
          "ใช้ Python คำนวณค่าทาง Statistics ของนักฟุตบอล"
        ],
        "เรียนเขียนโปรแกรมใช้งาน Python Data Visualization สำหรับงาน Data Science": [
          "ใช้ Python สร้าง Single Line Chart แบบ Number",
          "ใช้ Python สร้าง Single Line Chart แบบ String",
          "ใช้ Python สร้าง Multiple Line Chart",
          "ใช้ Python นำเข้า Local File",
          "ใช้ Python สร้าง Single Bar Chart แนวตั้ง",
          "ใช้ Python สร้าง Single Bar Chart แนวนอน",
          "ใช้ Python สร้าง Multiple Bar Chart แนวตั้ง",
          "ใช้ Python สร้าง Multiple Bar Chart แนวนอน",
          "ใช้ Python สร้าง Multiple Stacked Bar Chart แนวตั้ง",
          "ใช้ Python สร้าง Multiple Stacked Bar Chart แนวนอน",
          "ใช้ Python เพิ่ม Label ด้าน Top",
          "ใช้ Python เพิ่ม Label ปรับ Alignment",
          "ใช้ Python เพิ่ม Label ด้าน Center",
          "ใช้ Python สร้าง Stacked Area Chart",
          "ใช้ Python สร้าง Scatter Plot",
          "ใช้ Python สร้าง Pie Chart",
          "ใช้ Python สร้าง Histogram ผ่าน URL",
          "ใช้ Python สร้าง Histogram ผ่าน Local File",
          "ใช้ Python สร้าง Data Visualization Applications"
        ]
      },
      "requirements": [
        "ผู้เรียนไม่จำเป็นต้องมีความรู้ด้านการเขียนโปรแกรมมาก่อน คุณจะได้เรียนรู้ทุกสิ่งที่คุณอยากเรียน"
      ],
      "description": "หลักสูตร เรียน Python for Data Science ฉบับตั้งไข่ เรียนง่ายเป็นเร็ว\nสามารถเรียนได้ทุกคน ไม่จำเป็นต้องมีความรู้ด้านการเขียนโปรแกรมมาก่อน\nเนื้อหาการเรียนเขียนโปรแกรมภาษา Python สำหรับงาน Data Science\nแบ่งเป็น 5 ส่วน\nส่วนที่ 1: เริ่มต้นเขียนโปรแกรมภาษา Python สำหรับงาน Data Science\n1. เริ่มต้นเขียนโปรแกรม Python สำหรับงาน Data Science\n2. ใช้ Python เป็นเครื่องคำนวณทางคณิตศาสตร์\n3. ใช้ Python กำหนดค่าตัวแปร Variable Assignment\n4. ใช้ Python ทำการคำนวณค่าตัวแปร Variables\n5. ใช้ Python เก็บค่าและแสดงข้อมูลของตัวแปรชนิดต่าง ๆ\n6. ใช้ Python สร้าง Operations เป็นตัวดำเนินการ\n7. ใช้ Python แปลงชนิดข้อมูล Type Conversion\n8. ทดสอบความเข้าใจข้อใดแสดงผลได้ถูกต้อง\nส่วนที่ 2: เรียนเขียนโปรแกรมใช้งาน Python Lists สำหรับงาน Data Science\n9. ใช้ Python สร้าง List เก็บข้อมูลแบบรายการ\n10. ใช้ Python สร้าง List เก็บข้อมูลแบบรายการต่างชนิดกัน\n11. ใช้ Python สร้าง List แบบต่าง ๆ\n12. ใช้ Python สร้าง List of Lists\n13. ใช้ Python สร้าง Subset & Conquer\n14. ใช้ Python สร้าง Subset & Calculate\n15. ใช้ Python สร้าง Slicing &. Dicing\n16. ใช้ Python สร้าง Replace List Elements\n17. ใช้ Python ทำการ Extend a List\n18. ใช้ Python ทำการ Delete List Elements\n19. ใช้ Python ทำการ Inner Workings of Lists\nส่วนที่ 3: เรียนเขียนโปรแกรมใช้งาน Python Functions สำหรับงาน Data Science\n20. ใช้ Python เรียกใช้ฟังก์ชันพื้นฐาน\n21. ใช้ Python สร้าง Multiple Arguments\n22. ใช้ Python สร้าง String Methods\n23. ใช้ Python สร้าง List Methods 1\n24. ใช้ Python สร้าง List Methods 2\n25. ใช้ Python ทำการ Import Package\n26. ใช้ Python User Defined Functions 1\n27. ใช้ Python User Defined Functions 2\n28. ใช้ Python User Defined Functions 3\nส่วนที่ 4: เรียนเขียนโปรแกรมใช้งาน Python NumPy สำหรับงาน Data Science\n29. ใช้ Python ทำการเก็บค่าความสูง\n30. ใช้ Python แปลงหน่วยความสูง\n31. ใช้ Python คำนวณค่า BMI\n32. ใช้ Python เรียกใช้ Subsetting NumPy Array\n33. ใช้ Python สร้างเงื่อนไขตรวจสอบ\n34. ใช้ Python สร้าง Arithmetic Operators\n35. ใช้ Python สร้าง 2D NumPy Array\n36. Python Lists เขียนโปรแกรมสร้างโครงสร้างข้อมูลแบบลิสต์ภาษาไพธอน 1\n37. Python Lists เขียนโปรแกรมสร้างโครงสร้างข้อมูลแบบลิสต์ภาษาไพธอน 2\n38. Python Lists เขียนโปรแกรมสร้างโครงสร้างข้อมูลแบบลิสต์ภาษาไพธอน 3\n39. Python Lists เขียนโปรแกรมสร้างโครงสร้างข้อมูลแบบลิสต์ภาษาไพธอน 4\n40. Python Lists เขียนโปรแกรมสร้างโครงสร้างข้อมูลแบบลิสต์ภาษาไพธอน 5\nส่วนที่ 5: เรียนเขียนโปรแกรมใช้งาน Python Data Visualization สำหรับงาน Data Science\n41. ใช้ Python สร้าง Single Line Chart แบบ Number\n42. ใช้ Python สร้าง Single Line Chart แบบ String\n43. ใช้ Python สร้าง Multiple Line Chart\n44. ใช้ Python นำเข้า Local File\n45. ใช้ Python สร้าง Single Bar Chart แนวตั้ง\n46. ใช้ Python สร้าง Single Bar Chart แนวนอน\n47. ใช้ Python สร้าง Multiple Bar Chart แนวตั้ง\n48. ใช้ Python สร้าง Multiple Bar Chart แนวนอน\n49. ใช้ Python สร้าง Multiple Stacked Bar Chart แนวตั้ง\n50. ใช้ Python สร้าง Multiple Stacked Bar Chart แนวนอน\n51. ใช้ Python เพิ่ม Label ด้าน Top\n52. ใช้ Python เพิ่ม Label ปรับ Alignment\n53. ใช้ Python เพิ่ม Label ด้าน Center\n54. ใช้ Python สร้าง Stacked Area Chart\n55. ใช้ Python สร้าง Scatter Plot\n56. ใช้ Python สร้าง Pie Chart\n57. ใช้ Python สร้าง Histogram ผ่าน URL\n58. ใช้ Python สร้าง Histogram ผ่าน Local File\n59. ใช้ Python สร้าง Data Visualization Applications",
      "target_audience": [
        "ผู้เริ่มต้นเรียนรู้การเขียนโปรแกรม นักวิทยาศาสตร์ข้อมูล วิศวกรข้อมูล นักวิเคราะห์ข้อมูล นักพัฒนาโปรแกรม โปรแกรมเมอร์ ที่สนใจงานด้าน Data Science"
      ]
    },
    {
      "title": "Inteligência Artificial Moderna",
      "url": "https://www.udemy.com/course/inteligencia-artificial-moderna/",
      "bio": "Aplicações de Deep Learning, usando Keras, Google Colab e HuggingFace",
      "objectives": [
        "Identificar os principais conceitos e aplicações da IA.",
        "Explicar a estrutura e funcionamento das redes neuronais artificiais.",
        "Demonstrar como as CNNs são utilizadas para tarefas de visão por computador.",
        "Aplicar LSTMs para análise de séries temporais, como dados financeiros e previsão de temperatura.",
        "Modelos de tipo \"Transformer\" (incluíndo modelos GPT ) e suas aplicações em linguagem.",
        "Implementar modelos de IA para gerar conteúdo criativo, como imagens e estilos artísticos.",
        "Saber usar Keras, uma das frameworks mais elegantes, simples e poderosas para implementar modelos de Inteligência Artificial baseados em Deep Learning",
        "Manter-se permanentemente atualizado em relação aos novos modelos, arquiteturas e plataformas que vão surgindo (curso em permante atualização)"
      ],
      "course_content": {
        "Apresentação Geral do Curso": [
          "Vídeo de boas-vindas",
          "A primeira pergunta do Curso, e uma das mais importantes"
        ],
        "Parte 1A - Introdução à Inteligência Artificial Moderna e ao Deep Learning": [
          "Introdução à Parte 1A e informações sobre os recursos do Curso",
          "Localização dos recursos das lições",
          "O que distingue a Inteligência Artificial Moderna",
          "De onde vem o termo \"Moderna\" no tipo de Inteligência Artificial deste Curso",
          "Temas que serão abordados no Curso",
          "Qual a ordem pela qual os temas serão abordados no Curso?",
          "Exemplos de pessoas importantes na IA (com algum \"drama\" à mistura)",
          "Quem são os três pesquisadores mencionados na lição como importantes para a IA?",
          "Inspiração da IA Moderna: o cérebro humano e os seus neurónios (células)",
          "Qual é a principal inspiração para os modelos de redes neuronais em IA?",
          "Transcrição de conceitos da biologia cerebral para os modelos de IA atuais",
          "O que é uma \"sinapse\" em termos de redes neurais artificiais?",
          "Noção de \"função de ativação\" dos neurónios artificiais",
          "Qual é o papel da função de ativação em um neurónio artificial?",
          "Inicio da explicação dos processos básicos em redes neuronais artificiais",
          "Qual é um dos processos básicos em redes neurais artificiais?",
          "Continuação da explicação dos processos básicos em redes neuronais artificiais",
          "Algoritmo fundamental nas redes neuronais contemporâneas mais populares",
          "O algoritmo fundamental da IA Moderna: retropropagação (\"error backpropagation\")",
          "Qual é a principal função da retropropagação?",
          "Noção de função de perda (\"loss function\")",
          "Para que serve a função de perda em redes neuronais?",
          "Explicação complementar do algoritmo da retropropagação",
          "Qual é um aspeto importante da retropropagação explicado nesta lição?",
          "Perspetiva mais geral, intuitiva e aplicada sobre poder de retropropagação",
          "Porque é que a retropropagação é crucial para o treino de redes neuronais?",
          "Aprofundamento do conceito de função de ativação e seus tipos",
          "Qual das seguintes é uma função de ativação comum em redes neuronais?",
          "Recapitulação dos conceitos abordados até agora",
          "Qual conceito é fundamental para entender o treino de redes neuronais?",
          "Importância da não-linearidade em redes neuronais",
          "Importância da não-linearidade em redes neuronais",
          "Visão geral do processo completo de treino de redes neuronais",
          "Qual é o objetivo principal do treino de redes neuronais?",
          "Ajustamento de uma rede neuronal. Noções de \"underfitting\" e de \"overfitting\"",
          "O que é \"overfitting\" em redes neuronais?",
          "Complementos para entender melhor os conceito de ajustamento / \"fit\" da rede",
          "Qual é um sinal de que uma rede neuronal está numa situação de \"underfitting\"?",
          "Processo de otimização e de \"gradient descent\" em redes neuronais",
          "O que é \"gradient descent\" em redes neuronais?",
          "Representações visuais e intuitivas sobre otimização e \"gradient descent\"",
          "Qual é o objetivo da representação visual em \"gradient descent\"?",
          "Integração dos conceitos abordados até agora: otimização, e retropropagação",
          "Como a retropropagação e a otimização estão relacionadas?",
          "Conceito de ritmo de mudança dos pesos da rede neuronal (\"learning rate\")",
          "O que é a taxa de aprendizagem (\"learning rate\") em redes neuronais?",
          "Como dividir os dados de treino, para nos adaptarmos aos recursos de hardware",
          "Porque é importante dividir os dados de treino em batchs?",
          "Técnicas de regularização (evitar o sobre-ajustamento e permitir generalização)",
          "Qual é o objetivo das técnicas de regularização em redes neuronais?",
          "Métricas de performance de uma rede neuronal",
          "Qual das seguintes é uma métrica de desempenho comum para redes neuronais?",
          "Diferença entre exatidão (\"accuracy\") e precisão (\"precision\")",
          "Qual é a principal diferença entre exatidão e precisão em redes neuronais?",
          "Sensibilidade (\"sensitivity\" ou \"recall\") e especificidade (\"specificity\")",
          "O que mede a sensibilidade (\"sensitivity\" ou \"recall\") em redes neuronais?",
          "Medida F1 (\"F1 Score\")",
          "O que é a medida F1 em redes neuronais?",
          "Panorâmica final e geral da secção introdutória",
          "Qual foi o foco principal da secção introdutória do curso?"
        ],
        "Parte 1B - Inteligência Artificial aplicada a Tarefas de Visão": [
          "Apresentação e Recursos da Parte 1B",
          "Qual é o principal objetivo desta parte do curso?",
          "Inteligência Artificial Moderna aplicada a tarefas de visão: qual é a novidade?",
          "Qual é a principal vantagem das redes neuronais convolucionais (CNNs) ?",
          "Partes principais de uma rede neuronal convolucional (CNN)",
          "Qual é a função dos filtros em uma rede neuronal convolucional (CNN)?",
          "Imagens como matrizes de números e papel dos filtros",
          "Como uma imagem é representada numa rede neuronal convolucional?",
          "Noção de convolução e de camada convolucional de uma rede CNN",
          "O que é uma convolução em uma rede neuronal convolucional?",
          "Aplicação de função de ativação \"ReLU\" nas redes CNN",
          "Qual é o propósito da função de ativação ReLU em redes CNN?",
          "Noção de \"pooling\" em redes CNN",
          "O que é a operação de \"max pooling\" em redes CNN?",
          "Revisão integrada das principais etapas de uma CNN (incluindo código de exemplo)",
          "Quais são as principais etapas de uma rede neuronal convolucional (CNN)?",
          "Panorâmica final e geral da secção sobre redes CNN aplicadas a tarefas visuais"
        ],
        "Parte 2 - Deep Learning e Séries Temporais": [
          "Apresentação e Recursos da Parte 2",
          "Qual é o foco principal da Parte 2 do curso?",
          "Introdução ao conceito de rede neuronal recorrente (RNN)",
          "Características das redes neuronais recorrentes (RNNs) e aplicações associadas",
          "Redes neuronais recorrentes melhoradas: \"Long Short Term Memory\" (LSTM)",
          "Qual é o papel das células de memória nas redes LSTM?",
          "Exemplo prático aprofundado: modelo financeiro de previsão de preços na Bolsa",
          "Indicadores utilizados no modelo financeiro de previsão de preços na Bolsa?",
          "Explicação passo-a-passo do modelo e de como são usados todos os indicadores",
          "Qual é a função da operação de escalonamento (scaling) dos dados no modelo LSTM?",
          "Experiências com o modelo, e análise dos resultados empíricos",
          "Porque é importante dividir os dados em conjuntos de treino e teste?",
          "Considerações sobre o modelo e como poderia ser melhorado para fins práticos",
          "Qual é uma técnica comum para melhorar o desempenho de um modelo LSTM?",
          "Visualização e análise dos resultados do modelo",
          "Qual é a vantagem de visualizar as previsões do modelo junto com os dados reais?"
        ],
        "Parte 3 - Inteligência Artificial Moderna e Linguagem": [
          "Apresentação e Recursos da Parte 3",
          "Qual é o foco principal da Parte 3 do curso?",
          "Introdução aos Transformers (modelo em que se baseia o ChatGPT, por exemplo)",
          "Qual é a principal vantagem dos Transformers em relação às RNNs e LSTMs?",
          "Representação de palavras como vetores (\"embeddings\")",
          "Qual é a dimensão típica dos vetores de embeddings utilizados com Transformers?",
          "Codificação posicional (manter informação sobre ordem das palavras)",
          "Porque é que a codificação posicional é necessária nos Transformers?",
          "Introdução ao conceito de \"atenção\", a base dos Transformers",
          "Qual é a função principal do mecanismo de atenção nos Transformers?",
          "Codificador do Transformer: detalhes sobre \"queries\", \"keys\" e \"values\"",
          "Qual é a função das \"queries\" no mecanismo de atenção dos Transformers?",
          "Descodificador do Transformer e Transformer Completo",
          "Qual é a diferença entre o codificador e o descodificador no Transformer?",
          "Exemplo Prático (Python/Colab): Transformer para tradução Inglês->Espanhol",
          "Qual é a importância de usar uma GPU no treino do modelo Transformer?",
          "Breve revisão da arquitetura do Transformer, para entender melhor o código",
          "Qual a componente responsável por capturar a relação entre palavras?",
          "Divisão dos dados em conjunto de treino, validação e teste",
          "Porque é importante dividir os dados em conjuntos de treino, validação e teste?",
          "Preparação para vetorização do vocabulário: codificação das palavras como \"int\"",
          "Qual é o propósito da vetorização das palavras no contexto dos Transformers?",
          "Passagem das frases para o tipo \"dataset\" (da biblioteca \"tensorflow\")",
          "Transformação dos dados para o tipo \"dataset\" no TensorFlow.",
          "Confirmação de que processo de representação dos dados está correto, até agora",
          "Verificar a representação correta dos dados antes do treino do modelo",
          "Codificador do Transformer - explicação detalhada do código",
          "\"Residual connections\" no contexto do codificador do Transformer",
          "Descodificador do Transformer - explicação detalhada do código",
          "Papel da máscara causal na camada de self-attention do descodificador",
          "Explicação da opção usada para fazer \"codificação posicional\" neste modelo",
          "Por que é importante a codificação posicional nos Transformers?",
          "Parte núclear do código: uso encadeado de preparação de dados e Transformer",
          "Preparação dos dados no pipeline do Transformer",
          "Função para obtenção da tradução através do Transformer, uma palavra de cada vez",
          "Função para obtenção da tradução através do Transformer",
          "Treino e teste do Transformer (com gravação de melhor modelo até ao momento)",
          "Processo de gravação de melhor modelo até ao momento"
        ],
        "Parte 4 - Inteligência Artificial Generativa e Criatividade": [
          "Apresentação e Recursos da Parte 4",
          "Qual é o foco principal da Parte 4 do curso?",
          "Como esta parte do Curso estará sempre em evolução / atualização",
          "Porque é importante manter esta parte do Curso atualizada?",
          "Bases dos Transformers generativos (como o usado no ChatGPT)",
          "Bases dos Transformers generativos",
          "Um primeiro exemplo - Transformer generativo de linguagem, treinado de raíz.",
          "Transformer generativo treinado de raiz",
          "Transferência de estilo (exemplo: foto \"pintada ao estilo\" de van Gogh)",
          "Transferência de estilo"
        ]
      },
      "requirements": [
        "Saber o básico de Python, ou ter vontade de aprender essa linguagem (tutorial para iniciantes de Python incluído)",
        "Ser muito curioso/a e estar motivado/a para aprender sobre Inteligência Artificial Moderna, de forma simples, prática e rigorosa"
      ],
      "description": "Aprenda os princípios e aplicações práticas da Inteligência Artificial Moderna, incluindo modelos de Deep Learning e Transformers (a base dos modelos \"GPT\"), entre outros.\nUsaremos a biblioteca Keras, que simplifica a construção de modelos de Deep Learning, oferecendo recursos para utilizadores de todos os níveis, desde iniciantes até especialistas. Privilegiaremos exemplos do mundo real, explorando aplicações em Saúde, Arte, Finanças e muitas outras áreas.\nUtilizaremos a plataforma Colab, da Google, que permite usar GPUs de forma gratuita, sem preocupações de configuração e sem a necessidade de introduzir informações de cartão de crédito (basta ter uma conta Google). Os exemplos iniciais, baseados em código aberto do próprio autor da Keras, François Chollet, serão complementados cada vez mais por código que tira proveito de plataformas como a HuggingFace e a versão mais recente da Keras (3.0), que permite usar diferentes frameworks de Deep Learning como base (TensorFlow, JAX, PyTorch).\nUma das grandes vantagens de aprender Keras é que ficará apto/a a trabalhar com as frameworks de Deep Learning mais populares sem ter de aprender as idiossincrasias de cada uma: basta aprender uma sintaxe (a da Keras) e terá a liberdade de trabalhar sem preocupação com a framework de base (seja TensorFlow, a mais popular até há pouco, PyTorch, que é cada vez mais usada, ou mesmo JAX, valorizada por empresas de vanguarda como a OpenAI).\nEste curso está em permanente atualização, e você pode contar com a introdução de novos conteúdos ao longo do tempo, acompanhando as novidades da Indústria e da Academia, bem como os interesses dos alunos expressos no fórum público do Curso ou através de mensagens com pedidos de melhoria ou adição de conteúdos.",
      "target_audience": [
        "Qualquer pessoa com curiosidade e vontade de aprender sobre este tema fascinante e que está a marcar o presente, e promete definir o futuro.",
        "Estudantes que desejam aprofundar os seus conhecimentos nas fomas mais recentes de IA",
        "Profissionais de IT interessados em aplicar técnicas de IA no seu trabalho",
        "Desenvolvedores de software que querem entender como aplicar IA para resolver problemas do mundo real"
      ]
    },
    {
      "title": "Beginner’s Guide to Data Visualization",
      "url": "https://www.udemy.com/course/beginners-guide-to-data-visualization-d/",
      "bio": "Học kể chuyện bằng dữ liệu với biểu đồ và phương pháp ADME",
      "objectives": [
        "Nắm vững khái niệm và vai trò của trực quan hóa dữ liệu trong phân tích và truyền đạt thông tin",
        "Phân biệt và chọn đúng loại biểu đồ phù hợp với từng loại dữ liệu.",
        "Áp dụng các nguyên tắc và quy trình trực quan hóa hiệu quả (AMDE: Audience - Message - Drawing - Explanation)",
        "Nhận diện các lỗi phổ biến trong biểu đồ và biết cách khắc phục để truyền đạt dữ liệu chính xác hơn."
      ],
      "course_content": {
        "Giới thiệu về Data Visualization": [
          "Giới thiệu",
          "Trò chơi khởi động & sức mạnh của hình ảnh",
          "Ứng dụng thực tế của Data Visualization"
        ],
        "Các nguyên tắc cơ bản để vẽ biểu đồ rõ ràng và hiệu quả": [
          "Tại sao biểu đồ lại hiệu quả hơn văn bản?",
          "Các dạng biểu diễn biểu đồ — Phổ thông và Phức tạp",
          "Nguyên tắc quan trọng khi thiết kế biểu đồ"
        ],
        "Quy trình xây dựng biểu đồ AMDE": [
          "A – Audience: Hiểu đúng đối tượng người xem",
          "Hiểu Câu hỏi",
          "Cách lựa chọn biểu đồ phù hợp với dữ liệu"
        ],
        "Các loại biểu đồ phổ biến và cách sử dụng": [
          "Biểu đồ chứa thông tin (Big Number)",
          "Biểu đồ so sánh (Pie chart, Bar chart)",
          "Biểu đồ xu hướng và thay đổi (Line chart, Scatter plot)",
          "Biểu đồ mô tả mối quan hệ (Histogram, Scatter Plot)",
          "Biểu đồ tổ chức (Flowchart, Mindmap)",
          "Công cụ vẽ biểu đồ – Từ tay đến code",
          "E – Storytelling qua biểu đồ"
        ],
        "Các lỗi thường gặp và cách tránh khi trực quan hóa": [
          "Tránh sai lầm phổ biến khi vẽ biểu đồ"
        ],
        "Tổng kết khóa học và tài liệu tham khảo": [
          "Tổng hợp kiến thức và sách hữu ích"
        ]
      },
      "requirements": [
        "No programming experience needed. You will learn everything you need to know"
      ],
      "description": "Khóa học này hướng dẫn bạn từng bước để nắm vững kỹ năng trực quan hóa dữ liệu từ cơ bản đến thực hành. Bạn sẽ được giới thiệu các khái niệm nền tảng, tầm quan trọng của việc trực quan hóa trong phân tích dữ liệu và ra quyết định trong công việc. Khóa học giúp bạn hiểu rõ cách chọn biểu đồ phù hợp với từng loại dữ liệu như dạng số, phân loại hay chuỗi thời gian, đồng thời áp dụng các nguyên tắc thiết kế đơn giản, rõ ràng, dễ đọc và trực quan.\nBạn cũng sẽ được làm quen với các lỗi phổ biến thường gặp trong quá trình trực quan hóa và cách phòng tránh để biểu đồ không gây hiểu nhầm.\nNgoài ra, bạn còn được khám phá kỹ thuật kể chuyện bằng dữ liệu (storytelling) - một yếu tố quan trọng giúp biểu đồ trở nên cuốn hút, dễ ghi nhớ và thuyết phục hơn. Khóa học phù hợp cho người mới bắt đầu, sinh viên, nhân viên phân tích dữ liệu, marketing, kinh doanh hoặc bất kỳ ai muốn nâng cao kỹ năng trình bày dữ liệu một cách chuyên nghiệp và hiệu quả.",
      "target_audience": [
        "Sinh viên, nhân viên phân tích dữ liệu, marketing, hoặc quản lý muốn truyền đạt thông tin hiệu quả hơn qua biểu đồ.",
        "Những ai đang làm việc với báo cáo dữ liệu, dashboard, hoặc dữ liệu kinh doanh và cần biểu diễn trực quan dễ hiểu.",
        "Người học tự do, freelancer, hoặc học sinh cấp cao muốn tiếp cận Data Science thông qua trực quan hóa."
      ]
    },
    {
      "title": "빅데이터분석기사 완전정복 필기편 : Part.1 빅데이터 분석 기획(1)",
      "url": "https://www.udemy.com/course/part1-1-x/",
      "bio": "빅데이터 이해하기! 빅데이터 개요 및 활용부터 기술 제도까지 함께 배워봅시다.",
      "objectives": [
        "빅데이터 분석 기사 시험 안내",
        "빅데이터의 이해",
        "빅데이터 개요 및 활용",
        "빅데이터 기술 및 제도",
        "예상 문제 풀이"
      ],
      "course_content": {
        "빅데이터 분석 기획 : 빅데이터의 이해(빅데이터 개요 및 활용)": [
          "1 빅데이터 분석 기사 시험 안내",
          "2 빅데이터의 이해-빅데이터 개요 및 활용(데이터와 정보, 데이터 구분, 데이터 유형, 데이터 기능, 지식창조 메커니즘)",
          "3 빅데이터의 이해-빅데이터 개요 및 활용(DIKW피라미드, 데이터베이스 정의, DBMS 종류, SQL개념, 데이터베이스 특징, 장",
          "4 빅데이터의 이해-빅데이터 개요 및 활용(OLTP, OLAP의 개념 및 차이점, 데이터 웨어하우스 개념, 특징, 구성요소)",
          "5 빅데이터의 이해-빅데이터 개요 및 활용(빅데이터 개요, 등장과 변화, 특징, 3V, +2V, 활용을 위한 3요소, 기본 테크닉)",
          "6 빅데이터의 이해-빅데이터 개요 및 활용(빅데이터 가치, 역할, 기능과 효과, 가치 측정 어려움, 영향)",
          "7 빅데이터의 이해-빅데이터 개요 및 활용(데이터 산업의 진화 5단계의 개념과 각 단계의 특징)",
          "8 빅데이터의 이해-빅데이터 개요 및 활용(빅데이터 조직 및 인력 필요성, 역할, 구성, 조직의 형태, 데이터 사이언스 개념)",
          "9 빅데이터의 이해-빅데이터 개요 및 활용(데이터 사이언티스트 개념, 직무, 빅데이터 관련 직업군 5가지, 데이터 분선 수준 진단",
          "10 빅데이터의 이해-빅데이터 개요 및 활용(예상 문제 풀이)"
        ],
        "빅데이터 분석 기획 : 빅데이터의 이해(빅데이터 기술 및 제도)": [
          "1 빅데이터의 이해-빅데이터 기술 및 제도(빅데이터 플랫폼 개념, 계층 구조 3가지, 구성 요소 4가지, 아파치 하둡 개념)",
          "2 빅데이터의 이해-빅데이터 기술 및 제도(하둡 에코 시스템의 개념, 기능, 구성도, 척와, 플럼, 스크라이브, 스쿱, 히호, HD",
          "3 빅데이터의 이해-빅데이터 기술 및 제도(분산 데이터 처리 기술 맵리듀스 개념, 기능, 분산 데이터베이스 기술 HBase개념, 기",
          "4 빅데이터의 이해-빅데이터 기술 및 제도(얀, 아파치 스파크, 피그, 하이브, 머하웃, 임팔라, 타조의 개념과 특징)",
          "5 빅데이터의 이해-빅데이터 기술 및 제도(우지, 주키퍼의 개념과 특징, 인공지능, 범위, 머신러닝, 딥러닝의 개념)",
          "6 빅데이터의 이해-빅데이터 기술 및 제도(개인정보보호 정의, 필요성, 빅데이터 개인정보 가이드 라인, 데이터 3법, 개인정보보호법",
          "7 빅데이터의 이해-빅데이터 기술 및 제도(개인정보보호법 2항, 개인정보 유출 통지, 개인정보보호원칙, GDPR, 가명정보, K-익",
          "8 빅데이터의 이해-빅데이터 기술 및 제도(t-근접성, m-유일성, 마이 데이터의 개념 및 특징, 예시, 원칙)",
          "9 빅데이터의 이해-빅데이터 기술 및 제도 예상 문제 풀이-1",
          "10 빅데이터의 이해-빅데이터 기술 및 제도 예상 문제 풀이-2"
        ]
      },
      "requirements": [
        "한번에 합격하겠다는 의지! 데이터 통계와 데이터 보안 관련 분야 지식이 있으면 좋습니다."
      ],
      "description": "안녕하세요, ITGO 입니다.\n\n\n본 강의는 빅데이터분석기사 완전정복 필기편 : Part.1 빅데이터 분석 기획(1) 강의입니다.\n\n\n빅데이터 분석 기사는 국가 기술 자격증으로 필기와 실기 시험이 있습니다.\n\n\n본 강의는 빅데이터 분석 기사 자격증 취득을 위한 필기 시험 대비 강의입니다.\n\n\n빅데이터 분석 기사 필기 시험 핵심 개념부터 예상 문제 풀이까지 제대로 배워 한번에 합격합시다!\n\n\n\n\n누구를 위한 강의인가요?\n\n\n빅데이터 분석기사(필기) 자격증 취득을 원하시는 분들\n\n\n데이터 분석 직무로의 취업 및 이직을 준비하시는 분들\n\n\n데이터 분석에 대해 공부하고자 하시는 분들\n\n\n\n\n무엇을 배우나요?\n\n\n빅데이터 분석 기사 시험 안내\n\n\n빅데이터의 이해\n\n\n빅데이터 개요 및 활용\n\n\n빅데이터 기술 및 제도\n\n\n예상 문제 풀이\n\n\n\n\n빅데이터분석기사 완전정복 필기편 : Part.1 빅데이터 분석 기획(1) 강의에 입문해봅시다~!\n\n\n\n\n그럼 강의에서 만나요 :)",
      "target_audience": [
        "빅데이터 분석기사(필기) 자격증 취득을 원하시는 분들",
        "데이터 분석 직무로의 취업 및 이직을 준비하시는 분들",
        "데이터 분석에 대해 공부하고자 하시는 분들"
      ]
    },
    {
      "title": "Análise estatística de experimentos em R",
      "url": "https://www.udemy.com/course/estatistica-experimentos-r/",
      "bio": "Analise de dados de forma fácil e descomplicada!",
      "objectives": [
        "Análises de diferentes delineamentos amostrais",
        "Avaliação de pressupostos estatísticos",
        "Aplicações de métodos paramétricos e não paramétricos",
        "Interpretações práticas e objetivas dos resultados",
        "Adquirir autonomia para análise estatística de dados de experimentos"
      ],
      "course_content": {
        "Apresentação e materiais do curso": [
          "Sobre o curso",
          "Material: dados e script com códigos"
        ],
        "Estatística Descritiva": [
          "Medidas de posição e dispersão, quartil, curtose"
        ],
        "Delineamentos Experimentais": [
          "Testes de média para delineamentos DIC, DBC, DQL, Fatorial"
        ],
        "Análises exploratórias e diagnósticos": [
          "Tendências dos dados, testes de normalidade e diagnóstico de outliers"
        ],
        "Testes paramétricos e não paramétricos": [
          "Testes paramétricos: T, ANOVA, Tukey, Skott-Knott, SNK, Duncan, Scheffé, Dunnet",
          "Não Paramétricos: Wilcoxon, Kruskal-Wallis, Friedman, Dunn,Conover, Scheirer-Ray"
        ],
        "Análise de regressão": [
          "Regressão linear simples",
          "Regressão linear múltipla"
        ],
        "Modelos mistos para análise de efeitos fixos e aleatórios": [
          "Modelos mistos generalizados e ANOVAs (aninhadas, hierárquicas)"
        ]
      },
      "requirements": [
        "É desejável conhecimento básico de R (ex: instalação, uso de pacotes). Porém, o conteúdo é acessível a todos os públicos."
      ],
      "description": "A linguagem computacional do software R se destaca mundialmente no fornecimento de pacotes para os mais diversos tipos de análises estatísticas, permitindo o usuário organizar e processar seus dados com facilidade e eficiência. Isso fez com que habilidades de executar análises em R passassem a se tornar requisitos obrigatórios nas vagas ofertadas por vários dos melhores cargos disponíveis em variados setores.\nTodavia diversas pessoas tem dificuldade em avançar no uso da linguagem R devido em parte considerável ao excesso de informações não essenciais ao propósito de aprendizado. Isso levanta a necessidade de cursos com abordagens mais objetivas e práticas.\nEsse curso se baseia numa abordagem simples, direta e de rápida assimilação que combina teoria com execuções práticas de análises estatísticas no ambiente de programação do R. Com isso, esse curso é delineado para focar na execução prática das análises suas interpretações, capacitando você no uso R para analisar os dados obtidos a partir de diferentes tipos de experimentos. Além disso, é explicado de forma demonstrativa em que contexto se aplica cada método estatístico abordado no curso.\nO conhecimento adquirido permitirá a você trilhar o seu caminho de independência na análise de dados em diferentes contextos, uma qualidade altamente desejada e procurada em diferentes setores acadêmicos e empresariais.",
      "target_audience": [
        "Pesquisadores, professores, pós-graduandos, analistas de dados, analistas de negócio, estatísticos. Empreendedores e profissionais em geral que desejam enriquecer o currículo."
      ]
    },
    {
      "title": "R을 이용한 데이터 분석",
      "url": "https://www.udemy.com/course/data-analysis-using-r/",
      "bio": "월스트리트 CIO 출신에게 제대로 배우는 R데이터 분석",
      "objectives": [
        "더 이상 엑셀에서 Copy & Paste의 지루한 작업을 반복해서 하지 않아도 됩니다.",
        "엑셀에 넣을 수 없는 크기의 데이터를 이용해 데이터 분석 업무를 프로그래밍을 이용해 할 수 있습니다.",
        "R프로그래밍을 활용하여 업무 성과 향상이 가능합니다.",
        "데이터분석에 대하여 자신감이 생기며, R프로그래밍을 다양하게 활용할 수 있습니다.",
        "머신러닝등 좀 더 심화된 데이터분석과정을 이해할 수 있는 기본 지식을 쌓을 수 있습니다."
      ],
      "course_content": {
        "[챕터1]": [
          "데이터 사이언스의 열린 공간, 깃과 깃허브 활용하기",
          "R을 더 유용하게 만드는 패키지와 나만의 함수 만들기",
          "파일을 읽고 쓰는 다양한 방법",
          "유럽의 이커머스 데이터를 불러오고 살펴보기",
          "데이터 분석 워밍업 : 이커머스 데이터 summary하기"
        ],
        "[챕터2]": [
          "R에서 데이터 구조를 자유롭게 다루기",
          "R을 이용한 기본 Group 데이터 분석",
          "데이터 클리닝, 분석 전 여러 데이터를 붙이거나 제거해 준비하기",
          "gdp 데이터의 구조 변경하기",
          "gdp 데이터의 기본 분석하기"
        ],
        "[챕터3]": [
          "간단한 데이터 시각화로 데이터 이해하기",
          "정교한 그래프로 모델링을 위한 데이터 성격 이해하기",
          "Scatter plot을 이용해 두 데이터 비교하기",
          "두 개 이상의 변수 관계를 그래프를 통해 파악하기",
          "서베이 데이터 분석을 그래프로 그리기"
        ],
        "[챕터4]": [
          "실전편 : tidyverse를 이용한 2016년 클린턴과 트럼프의 선거 데이터 분석",
          "필요한 변수를 만들고, 이에 해당하는 통계치를 그룹별로 구하기",
          "데이터를 여러 형식의 테이블로 표현하기",
          "ggplot을 이용해 선거 결과 그래프 그리기"
        ]
      },
      "requirements": [
        "R프로그래밍에 대한 기초 지식"
      ],
      "description": "지금은 빅데이터 시대!\n데이터 이해, 가공, 분석부터 그래프 그리기까지 효율적인 데이터 분석은 물론, 빅데이터 전문가가 되기 위한 필수 과정!\n빅데이터의 분석과 관리를 통해 사람들의 행동 패턴이나 시장 경제 상황을 예측하고, 수많은 데이터 속에서 부가가치가 높은 결과물을\n도출해내는 빅데이터 전문가가 되고 싶다면,\n글로벌 최고 수준의 강사진이 제공하는 깊이 있고 체계적인 커리큘럼 'R을 이용한 데이터 분석' 강좌에서 시작해 보세요!\n\n\n비싼 통계 패키지를 사용하지 않아도 다양한 분야에서 R프로그래밍을 통해 빅데이터를 원하는 형태로 활용할 수 있습니다.\n1. R을 활용한 데이터 분석 기법 습득하기\n- 데이터를 이루고 있는 변수나 데이터의 기본 구조를 이해(엑셀 또는 csv파일)\n- 데이터 분석에서 가장 많이 이용하는 유럽의 이커머스 데이터를 활용\n2. R을 활용하여 원하는 형태로 데이터 가공하기\n- 데이터 구조의 변형 및 요약, 분할, 재조합을 통한 데이터 분석\n- 국가별 GDP데이터 파일과 이커머스데이터 파일을 접목시키는 예시를 활용\n3. R로 만들 수 있는 다양한 그래프 만들기\n- 그래프를 이용해 데이터 분석 결과를 보여주고, 또 다른 데이터를 비교하는 학습\n- 실무에서 흔히 접하게 되는 서베이 결과를 보여주는 그래프를 만드는 실습\n4. Tidyverse 패키지를 활용하여 데이터 분석하기\n- Tidyverse를 이용하여 2016년 트럼프와 클린턴의 대선 데이터를 분석\n- Ggplot 패키지를 이용한 그래프 만들기",
      "target_audience": [
        "R을 실제 업무와 일상의 예에 적용시키고, 활용하고 싶은 사람",
        "R프로그래밍 스킬을 향상시키고 싶은 사람",
        "데이터 분석의 고수가 되고 싶은 사람",
        "데이터분석을 그래픽을 이용하여 시각화 하고 싶은 사람",
        "R프로그래밍 입문 과정을 수강한 사람"
      ]
    },
    {
      "title": "如何用Python实现最实用的数据挖掘算法",
      "url": "https://www.udemy.com/course/python-q/",
      "bio": "数据挖掘入门",
      "objectives": [
        "1.了解机器学习基本概念",
        "2.了解Python实现机器学习算法的方式：scikit-learn库",
        "3.掌握决策树模型在Python的具体实现方法",
        "4.掌握K-means模型在Python的具体实现方法"
      ],
      "course_content": {
        "介绍": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "课程内容": [
          "第一节：机器学习简介",
          "第二节：决策树模型以及Python实现",
          "第三节：K-means聚类以及Python实现"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "具备一定编程能力，会用Python"
      ],
      "description": "数据挖掘是一种决策支持过程，它通过高度自动化地分析企业的数据，做出归纳性的推理，从中挖掘出潜在的模式，帮助决策者调整市场策略，减少风险，做出正确的决策。这对于一个企业的发展十分重要。\n数据挖掘的核心知识点是机器学习建模。本门课程，我们特别邀请到历任数据科学家、高级数据与应用科学家、策略运营专家的张宇晖老师，带你回顾机器学习的基本概念，并为你详细介绍使用Python实现机器学习算法的方式：scikit-learn库。接下来则重点为你讲解一个简单的监督学习模型（决策树）和一个简单无监督学习模型（K-means）在Python中是如何具体实现的。\n\n\n本节课程是由授课老师与三节课合作制作的。在此，要特别感谢老师的辛苦付出！经历了课程立项、设计、开发中的众多环节，我们才能最终为你呈现现在的这门课程。无论是授课老师还是三节课团队，都希望这门课程能够让你有所收获，希望同学们结合个人工作情况，学以致用。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "1.数据分析方向从业者",
        "2.想要提升数据分析能力的互联网从业者",
        "3.对大数据分析感兴趣的学习者"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第四部（後端） - AI x UI = 生成式UI",
      "url": "https://www.udemy.com/course/generative-ui/",
      "bio": "關於ChatGPT，LangGraph，AgentState，LangServe，Poetry",
      "objectives": [
        "學會Langchain Graph 整個流程",
        "學會如何使用Langchain Agent在LangGraph中連結工具和Prompt",
        "學會如何使用Conditional Edge靈活處理不同條件",
        "學習掌握Langchain Serve部署和處理LangServe錯誤糾正"
      ],
      "course_content": {},
      "requirements": [
        "一台電腦",
        "有一定Python語言基礎",
        "學習過Generative AI第一到第三部內容"
      ],
      "description": "人工智能的快速發展，讓我們的世界變得更加智能化與高效化。尤其是生成式人工智能（Generative AI），更是為用戶界面（UI）的設計帶來了無限可能。現在，您有機會搶先學習這一前沿技術，並應用於實際項目中！\n課程亮點：\n1. 最新熱門技術：本課程將Generative AI（生成式人工智能）與UI（用戶界面）結合，教授如何為不同用戶生成不同的UI，或者為同一個用戶生成多樣化的UI設計。\n2. 全面覆蓋：課程分為伺服器端和前端兩部分，您現在報名的是伺服器端部分。我們將專注於後端技術，為您打下堅實的基礎。\n3. 深入講解：我們將詳細介紹如何使用Langchain Agent、AgentState、LangGraph、Edge、LangServe、LangChain Tools、Poetry等工具來構建強大的LLM（Large Language Model）後端Endpoint。\n\n\n課程內容：\n* Langchain Agent：了解如何構建和管理多代理系統，實現高效的任務分配與執行。\n* AgentState：掌握代理狀態管理，確保系統的穩定性和可靠性。\n* LangGraph：學習圖形化編程，實現複雜的數據處理與分析。\n* LangServe：深入探討如何使用LangServe快速構建Fast API Endpoing。\n* LangChain Tools：學習如何在Langchain中自定義工具。\n* Poetry：學習Python的依賴管理工具，確保項目的可維護性與擴展性。\n\n\n搶佔未來技術制高點！提升您的專業技能，讓您在職場中脫穎而出！\n不要錯過這個學習前沿技術的絕佳機會！立即行動，成為生成式人工智能與UI設計的先鋒！",
      "target_audience": [
        "有一定前端開發經驗，想要提升技術並學習最新的AI應用和技術",
        "希望深入了解如何利用AI技術來改進和創新用戶界面設計的設計師",
        "對生成式AI技術和大型語言模型有興趣，並希望應用到實際項目中的研究人員和工程師",
        "對前沿技術充滿好奇，並希望在這個領域中尋找職業機會或創新項目的技術愛好者和學生",
        "希望了解生成式AI技術如何影響和改進產品設計和用戶體驗的產品經理"
      ]
    },
    {
      "title": "AI - ML-NET (C#) الذكاء الاصطناعي",
      "url": "https://www.udemy.com/course/ai-ml-net-c/",
      "bio": "ML-NET دورة تعليمية شاملة عن الذكاء الاصطناعي في بيئة عمل دوت نت باستخدام مكتبة",
      "objectives": [
        "تعلم اساسيات الذكاء الاصطناعي",
        "التعرف على الشبكات العصبية",
        "التعرف على اللغات البرمجية المستخدمة في الذكاء الاصطناعي",
        "ML-NET استخدام مكتبة",
        "تطوير نموذج تصنيف النصوص لتصنيف التعليقات السيئة من الجيدة",
        "تطوير نموذج التنبيء بالقيم",
        "تطوير نموذج تصنيف الصور",
        "تطوير نموذج اكتشاف الكائنات"
      ],
      "course_content": {},
      "requirements": [
        "C# Language",
        "VS2019"
      ],
      "description": "ما هو الذكاء الاصطناعي؟\nهو سلوك وخصائص معينة تتسم بها البرامج الحاسوبية، تجعلها تحاكي القدرات الذهنية البشرية وأنماط عملها. من أهم هذه الخصائص القدرة\nعلى التعلم والاستنتاج ورد الفعل على أوضاع لم تبرمج في الآلة.\n\n\nما هي مكتبة ML-NET\nML-NET هي مكتبة تعلم آلي برمجية مجانية للغات البرمجة C # و F #. كما أنه يدعم نماذج Python عند استخدامها مع ملفات\n\n\nماذا تقدم لك هذه الدورة؟\nتقدم لك هذه الدورة الاساسيات الخاصة في الذكاء الاصطناعي بطريقة مبسطة وملخصة وايضا بشكل تطبيقي على نماذج عمل\n\n\nلمن هذه الدورة؟\nلمطوري بيئة الدوت نت وايضا لكل شخص يريد ان يتعلم الذكاء الاصطناعي بشكل عملي ومبسط\n\n\n*** محتوى الدورة ***\nتتضمن الدورة التالي:\nتعريف عام بالذكاء الاصطناعي\nانواع الذكاء الاصطناعي\nاشهر اللغات المستخدمة\nالتعرف على الشبكات العصبية وكيف تعمل\nالتعرف على مكتبات الذكاء الاصطناعي\nالتعرف على مكتبة ML-NET\nالتعرف على نموذج تصنيف النصوص\nتطوير نموذج التعرف على التعليقات السيئة والجيدة\nالتعرف على نموذج التنبئي\nتطوير نموذج التنبيء بالقيم\nالتعرف على نموذج تصنيف الكائنات\nتطوير نموذج تصنيف الصور\nالتعرف على نموذج التعرف على اجزاء الكائنات\nتطوير نموذج التعرف على الكائنات ضمن الصور\nالتعرف على مواقع تجهيز قواعد البيانات\nالتعرف على برامج تهيئة التعرف على الكائنات\n***************\nيشكل الذكاء الاصطناعي تحديا والهاما لعلم الفلسفة ؛ لزعمه القدرة على إعادة خلق قدرات العقل البشري\nهل هناك حدود لمدى ذكاء الآلات؟ هل هناك فرق جوهري بين الذكاء البشري والذكاء الاصطناعي؟ وهل يمكن أن يكون للآلة عقل ووعي؟ عدد قليل من أهم الإجابات\nآلات الحساب والذكاء \"قانون تورنغ\"\nإذا كان الجهاز يعمل بذكاء يضاهي الإنسان، إذافذكائه يماثل ذكاء الإنسان. تفيد نظرية آلان تورنغ أنه، في نهاية المطاف، لا يسعنا إلا أن نحكم على ذكاء الآلة بناء على أدائها. هذه النظرية تشكل أساسا لاختبار تورنغ\nأطروحة دارتموث\n\"يمكن وصف كل جانب من عملية التعلم أو غيرها من مظاهرالذكاء بدقة شديدة تمكن الإنسان من تصميم آلة تحاكيه\". طبع هذا التأكيد في الأطروحة المقدمة لمؤتمر دارتموث عام 1956، وهو يمثل موقف معظم الباحثين في مجال الذكاء الاصطناعي\nنظام الرموز المادية (فرضية نظام نويل وسيمون للرموز المادية)\n\"نظام الرموز المادية لديه الوسائل الضرورية والكافية للأفعال الذكية بوجه عام\". مفاد هذه الجملة هو أن جوهر الذكاء يكمن في المقدرة على معالجة الرموز على عكس ذلك، يعتقد أوبير دريفوس أن الخبرات البشرية تتشكل بشكل غريزي لا واعي ولا تعتمد على التلاعب بالرموز بشكل واعي ؛ فهي تتطلب أن يكون لدى الإنسان \"شعور\" بالموقف حتى وان لم تكن لديه المعرفة الكافية بالرموز.",
      "target_audience": [
        "للمبتدئين - سنتعلم كل شيء من الصفر",
        "لمطوري الدوت نت",
        "لمطوري سي شارب"
      ]
    },
    {
      "title": "Traitement du langage naturel avec Deep Learning 1",
      "url": "https://www.udemy.com/course/traitement-du-langage-naturel-avec-deep-learning-1/",
      "bio": "De la langue à l'information",
      "objectives": [
        "Comprendre les bases du traitement de texte, y compris les techniques de tokenization, la normalisation du texte, et la gestion des données textuelles.",
        "Maîtriser les concepts fondamentaux de la modélisation de langage, leur application dans la prédiction de mots et la génération de text",
        "Être en mesure d'appliquer des techniques avancées telles que la correction orthographique pour améliorer la qualité du texte",
        "Acquérir des compétences pratiques dans la classification de texte, ainsi que dans l'analyse de sentiment"
      ],
      "course_content": {},
      "requirements": [
        "ce cours est accessible à un large éventail de personnes, des débutants aux apprenants plus expérimentés, et nous nous efforçons de rendre l'apprentissage aussi accessible que possible."
      ],
      "description": "Bienvenue dans notre cours de traitement du langage naturel ! Ce programme vous guidera à travers des concepts fondamentaux et des projets pratiques pour maîtriser l’analyse et la manipulation de textes à l'aide de techniques avancées d'intelligence artificielle.\nSemaine 1 : Introduction au Traitement du Langage Naturel\nComprendre les bases du NLP et ses applications.\nAperçu des outils et bibliothèques utilisés dans le domaine.\nSemaine 2 : Manipulation et Vectorisation de Texte\nTechniques de prétraitement de texte : tokenisation, nettoyage et normalisation.\nIntroduction aux techniques de vectorisation : TF-IDF, Word2Vec, etc.\nSemaine 3 : Premier Modèle de Classification de Texte\nComprendre les algorithmes de classification de texte.\nCréation de votre premier modèle de classification à l’aide de données textuelles.\nSemaine 4 : La Modélisation du Langage\nExploration des modèles de langage : n-grammes et modèles probabilistes.\nIntroduction aux réseaux neuronaux dans la modélisation du langage.\nSemaine 5 : Représentation Vectorielle des Mots\nApprentissage des représentations vectorielles des mots (Word Embeddings).\nDifférences entre les modèles classiques et modernes de représentation des mots.\nSemaine 6 : Réseaux Neuronaux Récurrents (RNN)\nComprendre le fonctionnement des réseaux neuronaux récurrents.\nApprentissage des applications des RNN en traitement du langage naturel.\nSemaine 7 : Réseaux Seq2Seq avec Mécanisme d'Attention\nIntroduction aux réseaux Seq2Seq pour la traduction et autres applications.\nCompréhension du mécanisme d'attention pour améliorer les performances des modèles.\nProjets Pratiques\nProjet 1 : Analyse de Sentiments à partir de Données Textuelles\nUtilisation des techniques de classification pour prédire les sentiments dans des textes.\nProjet 2 : Détection d'Entités Nommées sur des Données Textuelles\nDéveloppement d'un modèle pour extraire des informations clés comme des noms, des dates, etc.\nProjet 3 : Traduction Automatique du Langage Naturel\nMise en œuvre d'un système de traduction automatique avec des modèles Seq2Seq.\nRejoignez-nous dans ce parcours fascinant pour transformer vos compétences en NLP et créer des modèles puissants pour analyser et comprendre le langage humain !",
      "target_audience": [
        "Étudiants en informatique : Les étudiants en informatique qui souhaitent acquérir des compétences en traitement de texte, modélisation linguistique et analyse de texte trouveront ce cours bénéfique pour leur formation.",
        "Professionnels de l'informatique : Les professionnels de l'informatique, tels que les développeurs de logiciels et les ingénieurs en données, peuvent améliorer leurs compétences en traitement de texte pour des applications telles que la recherche d'information et l'analyse de données textuelles.",
        "Analystes de données : Les analystes de données cherchant à étendre leurs compétences dans le domaine de l'analyse de texte et de la modélisation linguistique peuvent utiliser ce cours comme ressource de formation.",
        "Curieux et autodidactes : Les personnes intéressées par le traitement de texte et l'analyse de texte, qu'elles aient une formation en informatique ou non, peuvent également bénéficier de ce cours pour développer une compréhension solide de ces domaines."
      ]
    },
    {
      "title": "Машинное обучение: кластеризация и аномалии на Python",
      "url": "https://www.udemy.com/course/ittensive-machine-learning-clustering/",
      "bio": "Прокачаться в машинном обучении без учителя и научиться выделять кластеры в данных и искать аномалии",
      "objectives": [
        "Процесс и модель машинного обучения",
        "Заполнение пропусков в данных",
        "Разведочный анализ данных",
        "K-средних",
        "Расстояние Махаланобиса и GMM",
        "Агломеративная кластеризация",
        "DBSCAN и HDBSCAN",
        "OPTICS",
        "Самоорганизующиеся карты Кохонена",
        "Расширяющийся нейронный газ",
        "Спектральная кластеризация",
        "pAUC и поиск аномалий",
        "Тест Смирнова-Граббса",
        "Эллипсоидальная аппроксимация",
        "LOF и ABOD",
        "COPOD",
        "iForest",
        "Классификация через кластеризацию"
      ],
      "course_content": {
        "Введение": [
          "Приветствие",
          "Задачи машинного обучения",
          "Обучение без учителя",
          "Задачи кластеризации",
          "Задачи машинного обучения"
        ],
        "Часть 1. Процесс машинного обучения": [
          "Модель и процесс машинного обучения",
          "Что такое ETL",
          "Процесс машинного обучения",
          "Что такое EDA",
          "Подготовка данных",
          "Подготовка данных",
          "Разбиение выборки",
          "Оптимизация гиперпараметров",
          "Недообучение и переобучение",
          "Смещение, разброс и ошибка данных",
          "Обучение модели"
        ],
        "Линейные модели": [
          "Метод максимального правдоподобия",
          "Метод наименьших квадратов",
          "Метод наименьших квадратов",
          "Аппроксимация пропусков в данных",
          "Аппроксимация данных",
          "Среднеквадратичная ошибка",
          "Метрики и расстояния",
          "Метрики и расстояния",
          "Линейная регрессия и L1/L2-регуляризация",
          "Линейная регрессия"
        ],
        "Часть 2. Базовая кластеризация": [
          "Внешние метрики кластеризации",
          "Внутренние метрики кластеризации",
          "К-средних",
          "Агломеративная кластеризация",
          "FOREL",
          "Расстояние Махаланобиса",
          "Базовая кластеризация"
        ],
        "Практикум: кластеризация объявлений": [
          "Прогноз срока экспозиции объявления",
          "Очистка и предобработка данных",
          "Обогащение данных",
          "Выделение факторов",
          "K-средних",
          "Агломеративная кластеризация",
          "GMM",
          "Метрики кластеризации",
          "Оптимальное число кластеров"
        ],
        "Часть 3. Продвинутая кластеризация": [
          "DBSCAN",
          "OPTICS",
          "Affinity Propagation",
          "Диаграмма Вороного",
          "Расширяющийся нейронный газ",
          "Самоорганизующиеся карты Кохонена (SOM)",
          "Матрица Кирхгофа",
          "Спектральная кластеризация",
          "Продвинутая кластеризация"
        ],
        "Практикум: Кластеризация для классификации": [
          "DBSCAN",
          "HDBSCAN",
          "OPTICS",
          "Affinity Propagation",
          "Самоорганизующиеся карты Кохонена",
          "Спектральная кластеризация",
          "Классификация через кластеры",
          "Классификация объявлений"
        ],
        "Часть 4. Задача поиска аномалий": [
          "Обнаружение аномалий",
          "Эксцесс и асимметрия",
          "Тест Смирнова-Граббса",
          "Метрика pAUC",
          "Эллипсоидальная аппроксимация",
          "Локальный фактор выброса (LOF)",
          "ABOD",
          "COPOD",
          "Лес изоляции (iForest)",
          "Проблема балансировки классов",
          "SMOTE",
          "ADASYN",
          "Автокодировщики",
          "Обнаружение аномалий"
        ],
        "Практикум: фильтрация аномалий": [
          "Статистические выбросы",
          "Эллипсоидальная аппроксимация",
          "LOF",
          "ABOD",
          "COPOD",
          "iForest",
          "Восстановление значений",
          "Прогноз срока экспозиции объявления"
        ]
      },
      "requirements": [
        "Продвинутый Python",
        "Основы математической статистики"
      ],
      "description": "Внимание: для доступа к курсам ITtensive на Udemy напишите, пожалуйста, на support@ittensive.com с названием курса или группы курсов, которые хотите пройти.\n\n\nЭто второй курс из серии Машинное обучение без учителя. На нем вы освоите работу с кластеризацией данных и поиском аномалий на примере задача хакатона Яндекс.Недвижимости по прогнозу срока экспозиции объявлений.\nКурс разбит на 4 части. В первой части мы последовательно пройдем все этапы работы с данными: от видов задач и их постановки до работы с моделями машинного обучения для минимизации предсказательной ошибки. Дополнительно рассмотрим фундаментальные основы построения моделей машинного обучения, базовые метрики и наиболее простые модели - линейную регрессию, а также ансамбли машинного обучения.\nВторая часть посвящена базовым моделям кластеризации:\nИзучите внешние и внутренние метрики кластеризации.\nРазберете модели К-средних и FOREL и потренируетесь в их применении.\nРассмотрите принципы работы агломеративной кластеризации и используете ее на практике.\nУзнаете про расстояние Махаланобиса и работу GMM.\nВ качестве задания соберем простую модель кластеризации исходных данных.\nВ третьей части перейдем к продвинутой кластеризации:\nПогрузитесь в различия моделей DBSCAN, HDBSCAN и OPTICS.\nРазберете особенности модели распространения близости.\nПосмотрите на расширяющийся нейронный газ.\nЗапустите и обучите самоорганизующиеся карты Кохонена (SOM).\nСтолкнетесь с матрицей Кирхгофа и спектральной кластеризацией.\nИ соберем ансамбль из несколько моделей кластеризации.\nВ завершении:\nИзучите поиск аномалий и метрику pAUC.\nИспользуете тест Смирнова-Граббса на практике.\nПотренируетесь в эллипсоидальной аппроксимации.\nРазберете разницу между LOF и ABOD.\nОбучите и используете модель COPOD.\nВырастите как iForest, как и расширенный лес изоляции.\nВ финале соберем свое решение задачи Хакатона 2020 года.",
      "target_audience": [
        "Аналитики Python, изучающие машинное обучение",
        "Программисты больших данных",
        "Исследователи больших данных"
      ]
    },
    {
      "title": "Hands-On AWS S3 mit Boto3 (AWS SDK / Python) - 2025",
      "url": "https://www.udemy.com/course/hands-on-aws-s3-mit-boto3-python-092022/",
      "bio": "Entwicklung vollumfänglicher Funktionen in Python (Boto3) zum programmatischen Arbeiten mit AWS S3",
      "objectives": [
        "Erstellen eines S3 Buckets mit Python und Boto3",
        "Löschen eines S3 Buckets mit Python und Boto3",
        "Uploaden eines Files in einen S3 Bucket mit Python und Boto3",
        "Downloaden eines Files in einen S3 Bucket mit Python und Boto3",
        "Kopieren eines Files von einem S3 Bucket in ein anderen mit Python und Boto3",
        "Bucket Policy's abfragen / hinzufügen / löschen mit Python und Boto3",
        "Tags von Files & Buckets abfragen / hinzufügen / löschen mit Python und Boto3",
        "Presigned URL für Files erstellen zum versenden mit Python und Boto3",
        "Bucket Versionierung abfragen / hinzufügen / löschen mit Python und Boto3"
      ],
      "course_content": {
        "Einführung": [
          "HINWEIS zu den Advanced Lektionen",
          "Road Map",
          "KURSMATERIALIEN",
          "Kursaufbau und -struktur",
          "Was ist AWS S3?",
          "Was ist Boto3?"
        ],
        "AWS Konto einrichten": [
          "Bei Amazon AWS registrieren",
          "IAM User anlegen"
        ],
        "Basic: S3 Bucket erstellen": [
          "Ressource vs. Client",
          "Install Anaconda, PIP, AWS Config Datei",
          "Notebook: Bucket erstellen",
          "Script: Klasse - Bucket erstellen - erstellen",
          "Script: Verbesserungen",
          "Script: CLI Anbindung - Bucket erstellen",
          "Readme: Bucket erstellen"
        ],
        "Basic: File in S3 Bucket hochladen": [
          "Notebook: File Uploaden",
          "Script: Klasse - File Uploader - erstellen",
          "Script: CLI Anbindung - File Uploader",
          "Readme: File Uploader"
        ],
        "Basic: S3 Bucket auflisten": [
          "Notebook: Auflisten von Buckets",
          "Script: Klasse - Auflisten von Buckets - erstellen",
          "Script: CLI Anbindung - Auflisten von Buckets",
          "Readme: Auflisten von Buckets"
        ],
        "Basic: Files in S3 Bucket auflisten": [
          "Notebook: Erklärung Auflisten von Files",
          "Script: Klasse - Auflisten von Files - erstellen",
          "Script: CLI Anbindung - Auflisten von Files",
          "Readme: Auflisten von Files"
        ],
        "Basic: S3 Bucket löschen": [
          "Notebook: Bucket löschen",
          "Script: Klasse - Bucket löschen - erstellen",
          "Script: CLI Anbindung - Bucket löschen",
          "Readme: Bucket löschen"
        ],
        "Basic: File in S3 Bucket löschen": [
          "Notebook: Erklärung Löschen von Files",
          "Script: Klasse - Löschen von Files - erstellen",
          "Script: CLI Anbindung - Löschen von Files",
          "Readme: Löschen von Files"
        ],
        "Basic: File in S3 Bucket kopieren": [
          "Notebook: Erklärung Kopieren von Files",
          "Info: Bucket Problem Lösung",
          "Script: Klasse - Kopieren von Files - erstellen",
          "Script: CLI Anbindung - Kopieren von Files",
          "Readme: Kopieren von Files",
          "Hinweis: Umbenennen von Files = Kopieren von Files"
        ],
        "Basic: File von S3 Bucket downloaden": [
          "Notebook: Erklärung File Downloader",
          "Script: Klasse - File Downloader - erstellen",
          "Script: CLI Anbindung - File Downloader",
          "Readme: File Downloader"
        ]
      },
      "requirements": [
        "Python Grundkentnisse nötig",
        "Objekt-orientierte Programmierkenntnisse von Vorteil",
        "Basic CLI Grundkenntnisse",
        "Eine Kreditkarte zum erstellen eines AWS Accounts (keine Kosten)"
      ],
      "description": "Wusstest du, dass die KOMPLETTE Infrasturuktur von NETFLIX auf AWS basiert? Und alles beginnt mit einem geeigneten Speicherplatz! Und um genau diesen Speicherplatz, kümmern wir uns hier.\nIn diesem Tutorial lernen wir nicht nur AWS kennen, wir entwickeln komplett fertige Scripts die du immer und überall in beliebig vielen Projekten verwenden kannst. Wenn du Buckets erstellen, Daten hochladen und Tags vergeben willst, bist du hier genau richtig. Hierfür programmieren wir mit Python und Boto3 die passenden Klassen. Und nicht nur Das, wir erstellen zudem passende Lösungen, damit deine Klassen zudem mittels der CLI ausgeführt werden können, wenns mal etwas schneller gehen muss.\nDenn Amazon Simple Storage Service (Amazon S3) ist ein Objektspeicherservice, der branchenführende Skalierbarkeit, Datenverfügbarkeit, Sicherheit und Leistung bietet. Firmen aller Größen und Branchen können Amazon S3 für die Speicherung und den Schutz beliebiger Datenmengen für eine Reihe von Anwendungsfällen verwenden, wie Data Lakes, Websites, mobile Anwendungen, Backup und Wiederherstellung, Archivierung, Unternehmensanwendungen, IoT-Geräte und Big-Data-Analysen. Amazon S3 bietet Verwaltungsfunktionen, mit denen du Zugriff auf deine Daten optimieren, organisieren und konfigurieren kannst, um deine spezifischen geschäftlichen, organisatorischen und Compliance-Anforderungen zu erfüllen.\nIn diesem Sinne, lass uns keine Zeit verlieren und direkt loslegen, denn nach S3 kommt Lambda und nach Lambda kommt EC2. Du siehst, im Cloud Computing gibt es immer was zu tun.\n\n\nViele Grüße,\nStefan",
      "target_audience": [
        "Python Entwickler",
        "AWS Cloud Entwickler",
        "Cloud Developer",
        "Boto3 Entwickler"
      ]
    },
    {
      "title": "Prompt Engineering für KI: ChatGPT, Claude, Gemini und LLM",
      "url": "https://www.udemy.com/course/prompt-engineering-fuer-ki-chatgpt-claude-gemini-und-llm/",
      "bio": "Meistere Text-to-Text AI für Business, Produktivität und Alltag mit Tools wie GPT-4o, GPTs, Gemini und Neuroflash.",
      "objectives": [
        "Lerne verschiedene LLM's kennen: ChatGPT, Claude, Gemini und Neuroflash.",
        "Verstehe die Funktionsweise: Was ist ein Prompt und wie funktioniert die KI-Tools?",
        "Lerne, wie du effektive Prompts formulierst und die besten Ergebnisse erzielst.",
        "Entdecke, wie du KI-Tools gezielt in deinen Alltag und Beruf integrieren kannst.",
        "Nutze eine Vielzahl an praxisnahen Prompts für Marketing, Produktivität und mehr.",
        "Erfahre, wie du strukturierte und unstrukturierte Prompts gezielt einsetzt.",
        "Lerne, wie du mit Prompts klare Rollen und Aufgaben für KI definierst.",
        "Entwickle deinen eigenen GPT und passe ihn individuell an deine Bedürfnisse an.",
        "Verstehe die Unterschiede zwischen ChatGPT, Claude, Gemini und Neuroflash.",
        "Lerne, KI-Tools für kreative Projekte wie Blogartikel oder Bücher zu nutzen.",
        "Finde heraus, wie du mit KI effizient E-Mails und Bewerbungsschreiben erstellst.",
        "Erfahre, wie du Text-to-Image-Funktionen für visuelle Projekte anwendest.",
        "Erstelle beeindruckende Bilder mit KI-Tools wie DALL-E und Neuroflash.",
        "Entdecke, welche KI-Tools sich für spezifische Projekte am besten eignen.",
        "Verstehe, wie LLMs funktionieren und nutze ihr Potenzial (inkl. fertiger Prompts).",
        "Kreiere professionelle SEO-Texte und Produktbeschreibungen mit der Hilfe von KI.",
        "Entdecke kreative Anwendungsbereiche, wie die Erstellung von Geschichten und mehr.",
        "Entdecke, wie du mit KI Geschäftsideen generierst und weiterentwickelst.",
        "Lerne, komplexe Projekte mit der Unterstützung von KI effizient zu organisieren und zu planen.",
        "Erfahre, wie du KI nutzt, um Social-Media-Content in Rekordzeit zu erstellen (z. B. für Instagram).",
        "Verstehe, wie du mit KI eine detaillierte Zielgruppenanalyse für dein Unternehmen durchführst.",
        "Entdecke geheime KI-Hacks, die deine Produktivität und Kreativität auf ein neues Level bringen."
      ],
      "course_content": {
        "Einführung": [
          "Herzlich willkommen",
          "Bevor es losgeht - ein wichtiger HINWEIS vorab!",
          "Mehr Inhalte? Dann schaue UNBEDINGT dieses Video!",
          "Haftungsausschluss/Disclaimer"
        ],
        "Einstieg in die KI-Welt mit ChatGPT": [
          "Grundbegriffe: Was sind überhaupt \"Prompts\"?",
          "KOSTENLOSEN Account bei ChatGPT erstellen",
          "Einführung in ChatGPT... lass uns beginnen!",
          "Benötige ich die PLUS-Version von ChatGPT?",
          "Warum sollte ich überhaupt KI-Tools verwenden?",
          "Du bist nicht allein – Ich bin an deiner Seite!"
        ],
        "Erste Prompts bei ChatGPT eingeben": [
          "So VERBESSERT du jeden deiner Prompts",
          "#1 – Beispiele in die Prompts integrieren...",
          "#2 – ChatGPT eine Rolle zuteilen...",
          "Strukturierte VS. unstrukturierte Prompts - Was ist was?",
          "Tipp: Benutzerdefinierte Anweisungen an ChatGPT erstellen"
        ],
        "Verschiedene Ausgaben von ChatGPT": [
          "Textausgabe (Standard)",
          "Tabelle & Liste",
          "HTML-Code"
        ],
        "Einführung in Google Gemini": [
          "Kostenlosen Account bei Google Gemini erstellen",
          "Einführung in Google Gemini: Los geht's!",
          "Benötige ich Gemini Pro oder reicht die kostenlose Version?",
          "Erste Prompts bei Google Gemini eingeben..."
        ],
        "Einführung in Claude": [
          "Kostenlosen Account bei Claude erstellen",
          "Einführung in Claude: Los geht's!",
          "Benötige ich Pro-Version oder reicht die kostenlose Version?",
          "Erste Prompts bei Claude eingeben..."
        ],
        "Alltags-Prompts mit ChatGPT": [
          "Was wirst du hier finden?",
          "#1 – Entscheidungsfindung mit ChatGPT",
          "#2 – Terminplanung mit ChatGPT",
          "#3 – Einkaufsliste (o.a.) erstellen",
          "#4 – Geburtstagskarten schreiben (inkl. Gedichte und mehr!)",
          "#5 – ChatGPT als virtueller Reiseassistent und -planer"
        ],
        "Produktivitäts-Prompts mit Gemini": [
          "#1 – Effektives Zeitmanagement (dank Gemini!)",
          "#2 – Tagesplanung und ToDo-Listen",
          "#3 – Minimalismus im Alltag",
          "#4 – \"Zeitfresser\"/Ablenkungen minimieren"
        ],
        "Berufs- und Karriere-Prompts mit Gemini": [
          "#1 – Gemini als virtueller Büro-Assistent (z. B. für E-Mails)",
          "#2 – Top 3 Prioritäten in deinem Berufsalltag",
          "#3 – Veranstaltungen und Events planen",
          "#4 – Jobsuche mit Gemini leicht gemacht...",
          "#5 – Bewerbungsschreiben & Lebenslauf",
          "#6 – Gehaltserhöhung verhandeln - aber mit Gemini!",
          "#7 – Fortbildungsmöglichkeiten im Beruf"
        ],
        "Schul-, Ausbildungs- und Studium-Prompts mit ChatGPT": [
          "#1 – Sprach- und Grammatikprüfung mit ChatGPT",
          "#2 – Gedichte, Geschichten und Co. analysieren",
          "#3 – Matheaufgaben lösen (mit Lösungsweg!)",
          "#4 – Geschichtsfragen klären (z. B. historischer Kontext)",
          "#5 – Zusammenfassungen von Büchern",
          "#6 – Recherchearbeit mit ChatGPT meistern",
          "#7 – Gliederung für wissenschaftliche Arbeiten",
          "#8 – Literaturverzeichnis erstellen",
          "#9 – Arbeitsprotokolle und Berichte schreiben",
          "#10 – Berufsspezifische Software erlernen",
          "#11 – Ausbildungsverträge verstehen"
        ]
      },
      "requirements": [
        "Alles was du brauchst, wirst du in diesem Kurs lernen."
      ],
      "description": "Träumst du davon...\ndie Möglichkeiten von KI-Tools wie ChatGPT, Claude, Gemini und Neuroflash voll auszuschöpfen, um produktiver und kreativer zu arbeiten?\n\n\nmit präzisen Prompts die besten Ergebnisse aus generativer KI herauszuholen und deinen Workflow zu optimieren?\n\n\nPrompts zu erstellen, die für verschiedene Anwendungen wie Content-Erstellung, Produktivität, Marketing oder Automatisierung maßgeschneidert sind?\n\n\ndeine Fähigkeiten im Umgang mit Large Language Models (LLM) zu erweitern, um innovative Lösungen für persönliche und berufliche Herausforderungen zu entwickeln?\n\n\nDann ist dieser Kurs „Prompt Engineering für KI: ChatGPT, Claude, Gemini und LLM“ genau das Richtige für dich! Von der Einführung in die grundlegenden Funktionen bis hin zu fortgeschrittenen Strategien zeigt dir dieser Kurs, wie du die volle Leistung moderner KI-Tools nutzen kannst. Lerne, wie du mit gezielten Prompts Ergebnisse erzielst, deine Arbeitsprozesse vereinfachst und sogar neue kreative Ansätze entwickelst. Entdecke, wie generative KI nicht nur deine Projekte voranbringt, sondern auch eine neue Dimension von Effizienz und Innovation eröffnet.\n\n\nWas du lernen wirst:\nEinführung in ChatGPT, Gemini und Claude: Du erhältst eine Einführung in die führenden KI-Tools ChatGPT, Google Gemini und Claude. Der Kurs zeigt dir, wie du einen kostenlosen Account erstellst, dich mit den Benutzeroberflächen vertraut machst und erste Prompts eingibst. Außerdem erfährst du, wie du mit jedem Tool gezielt arbeiten kannst, unabhängig von deiner bisherigen Erfahrung.\nErste Prompts bei LLM's eingeben: Lerne, wie du effektive Prompts formulierst, die dir präzise und hilfreiche Ergebnisse liefern. Du erfährst, wie du Beispiele in deine Prompts integrierst, KI-Tools eine bestimmte Rolle zuweist und strukturierte sowie unstrukturierte Prompts erkennst und einsetzt. Zusätzlich wirst du die verschiedenen Ausgabeformate der Modelle kennenlernen.\nVerschiedene Prompts: Entdecke eine breite Palette an Prompts für unterschiedliche Anwendungsbereiche. Ob für Alltagsaufgaben, berufliche Herausforderungen, Produktivitätssteigerung oder kreative Projekte – du bekommst viele Beispiele, die du sofort nutzen kannst. Egal ob Texte verfassen, Finanzmanagement oder Marketing – für alles gibt es den passenden Prompt.\nEigenen GPT entwickeln: Erfahre, wie du deinen eigenen GPT in ChatGPT entwickelst und für deine individuellen Anforderungen anpasst. Du lernst die Grundlagen, den Konfigurationsprozess und die Veröffentlichung deines GPT. So kannst du spezielle KI-Modelle für deine persönlichen oder geschäftlichen Projekte erstellen.\nEinführung in Neuroflash: Du erhältst eine Einführung in Neuroflash, ein Tool, das dir bei der Erstellung von Texten und Bildern hilft. Der Kurs zeigt dir die Grundlagen der Bedienung und gibt dir praktische Tipps, wie du das Dashboard effektiv nutzt, um deine Projekte noch kreativer und produktiver zu gestalten.\nGemeinsame Projekte: Wir zeigen dir, wie du KI-Tools bei verschiedenen Projekten einsetzt: von Blogartikeln über Bücher und E-Commerce bis hin zu E-Mails und Bewerbungsschreiben. Du lernst, wie du die Stärken der Tools kombinierst, um gemeinsam mit der KI produktive und inspirierende Ergebnisse zu erzielen.\nBilder erstellen mit KI: Tauche in die Welt der Bildgenerierung ein und entdecke die Möglichkeiten von DALL-E, Gemini, Claude und Neuroflash. Du lernst, wie du Text-to-Image-Funktionen nutzt, strukturierte Prompts erstellst und visuelle Inhalte für kreative und berufliche Projekte generierst.\nChatGPT vs. Gemini, Claude und Neuroflash: Vergleiche die Stärken und Schwächen der verschiedenen KI-Tools. Der Kurs zeigt dir, welches Tool für welche Aufgaben am besten geeignet ist und gibt dir Orientierung, um das optimale Modell für deine Bedürfnisse auszuwählen.\nPrompts, auf die du lieber verzichten solltest: Lerne, welche Prompts du aus rechtlichen und ethischen Gründen vermeiden solltest. Dazu gehören illegale, sensible oder unethische Inhalte sowie Prompts, die medizinische oder rechtliche Beratung erfordern. So nutzt du KI-Tools sicher und verantwortungsvoll.\n\n\nWarum genau dieser Kurs:\nPraxisnah und umfassend: Egal, ob du zum ersten Mal mit LLMs arbeitest oder deine Fähigkeiten ausbauen möchtest.\nPraxisorientiert und vielfältig: Von Text- bis Bildgenerierung, von Alltagsaufgaben bis Unternehmensprojekten – hier ist für jeden etwas dabei.\nZeitgemäß und umfassend: Bleibe auf dem neuesten Stand mit den führenden KI-Tools und Methoden. Setze das Gelernte sofort in realen Projekten um.\nFlexibles Lernen: Passe dein Lerntempo individuell an und greife jederzeit auf die Kursinhalte zu. Egal, ob du nebenbei arbeitest, studierst oder anderen Verpflichtungen nachgehst – dieser Kurs ermöglicht es dir, dein Wissen flexibel und bequem von überall aus zu erweitern.\nStarte deine Reise in die Welt des Prompt Engineerings und entdecke, wie du mit KI-Tools wie ChatGPT, Claude, Gemini und Neuroflash das Beste aus generativer KI herausholst. Schreibe dich jetzt in den Kurs „Prompt Engineering für KI: ChatGPT, Claude, Gemini und LLM“ ein und profitiere von praxisnahen Anleitungen, einer umfangreichen Sammlung an Prompts und wertvollen Strategien, die deinen Alltag und Beruf revolutionieren können.\nBis gleich! Dein Maximilian.",
      "target_audience": [
        "Personen, die ihre Fähigkeiten im Prompt Engineering für KI-Modelle wie ChatGPT, Claude, Gemini und andere LLMs erweitern möchten.",
        "Arbeitnehmer, die mit präzisen Prompts ihren Arbeitsalltag effizienter gestalten wollen.",
        "Unternehmer, die KI-Modelle nutzen möchten, um Prozesse zu automatisieren und Innovationen voranzutreiben.",
        "Alle, die die volle Leistung moderner KI-Tools ausschöpfen und ihre Produktivität steigern wollen."
      ]
    },
    {
      "title": "쉽게 접근하는 데이터 리터러시",
      "url": "https://www.udemy.com/course/data_garam/",
      "bio": "데이터 기초 지식부터 실무에 활용하는 분석 방법까지!",
      "objectives": [
        "초보자도 시작할 수 있는 데이터 분석의 프로세스를 알게 됩니다.",
        "필요한 지표를 구체화하고 적절한 형태로 가공하는 방법을 배웁니다.",
        "비즈니스 분석에서 자주 사용하는 지표의 종류를 알게 됩니다.",
        "실무에 바로 적용할 수 있는 데이터 분석 방법을 예제와 함께 배웁니다."
      ],
      "course_content": {
        "INTRO": [
          "강사&강의 소개",
          "데이터 활용에 어려움을 겪는 이유"
        ],
        "CHAPTER 1. 데이터를 마주하기 전 준비 단계": [
          "분석의 목적과 방향 설정하기"
        ],
        "CHAPTER 2. 필요한 데이터 구체화": [
          "활용할 수 있는 데이터 파악하기",
          "필요한 지표 설정",
          "목적에 맞는 분석 방법 선택하기"
        ],
        "CHAPTER 3. 데이터 정리 및 해석": [
          "효과적인 방식으로 데이터 정리 & 시각화하기",
          "데이터 읽고 해석하기"
        ],
        "CHAPTER 4. 데이터 분석 실습": [
          "신규 셀러 현황 파악: 지표 설정 연습",
          "고객 세그멘테이션: RFM 분석",
          "신규 고객 잔존율 파악: 코호트 분석 활용"
        ]
      },
      "requirements": [
        "엑셀, 구글 스프레드시트를 사용하는 분들이 수강하시면 더 좋습니다!",
        "실습은 구글 스프레드시트로 진행해요! 엑셀과 크게 다르지 않아요:)"
      ],
      "description": "[강의 소개]\n많은 회사, 조직이 \"Data-Driven\" 하게 일하기를 강조합니다.\n그만큼 데이터를 활용하는 능력이 중요한 역량이 되었는데요,\n이 강의는 내 결과물의 퀄리티를 높여줄 '데이터 활용법'을 실무자의 관점으로 담았습니다.\n\n\n엑셀, SQL 등 데이터 도구 사용법이 아니라\n나에게 필요한 데이터를 구체적으로 정리하는 방법,\n방대한 데이터를 효율적으로 추출하고, 시각화하는 방법 등\n어떻게 하면 내 업무에 '데이터'를 더 잘 녹여낼 수 있는지에 초점을 맞춘 강의입니다.\n\n\n데이터 분석은 어디서부터 시작해야 하는지, 무엇을 먼저 고민하면 시간을 절약할 수 있는지\n제가 여러 번 실패하고 헤매면서 알게 된 것들을 여러분과 나누고 싶습니다.",
      "target_audience": [
        "현업에서 활용 가능한 데이터 분석 방법을 배우고 싶은 분",
        "방대한 데이터 중에서 필요한 것만 골라 효율적으로 정리하고 싶은 분",
        "SQL, 엑셀 등 툴 사용법은 배웠지만 막상 실무에 적용하기는 어려웠던 분"
      ]
    },
    {
      "title": "Data Engineering con Spark para Big Data: SQL, Delta Lakes",
      "url": "https://www.udemy.com/course/data-engineering-con-spark-para-big-data-sql-delta-lakes/",
      "bio": "Curso completo de Data Engineering (ingeniería del dato) con Apache Spark",
      "objectives": [
        "Introducción a big data y fundamentos de Apache Spark",
        "Spark en la nube con Azure y Databricks",
        "Funciones avanzadas con Apache Spark",
        "Spark Streaming",
        "Delta Lakes",
        "Databricks"
      ],
      "course_content": {
        "Introducción al curso": [
          "Como aprovechar al máximo el curso"
        ],
        "Fundamentos de Big Data y Apache Spark": [
          "Fundamentos de Spark",
          "Modo de ejecución de Apache Spark",
          "Ecosistema de Apache Spark",
          "Funcionamiento, administración de clusteres y arquitectura"
        ],
        "Instalación de Apache Spark": [
          "Descarga de Spark, Java y Anaconda",
          "Configuración de las variables de entorno",
          "Ejecución de Spark en Prompt y Jupyter Notebook"
        ],
        "Spark DataFrames y Apache Spark SQL": [
          "Fundamentos y ventajas de los DataFrames",
          "Características de los DataFrames y fuentes de datos",
          "Creación de DataFrames en PySpark",
          "Operaciones con DataFrames de PySpark",
          "Diferentes tipos de joins en DataFrames",
          "Consultas SQL en PySpark",
          "Funciones avanzadas para cargar y exportar datos en PySpark",
          "Ejercicio Práctico: Spark DataFrames y Apache Spark SQL",
          "Solución al ejercicio práctico"
        ],
        "Funciones avanzadas en Apache Spark": [
          "Funciones avanzadas y optimización del rendimiento",
          "BroadCast Join y almacenamiento en cache",
          "User Defined Functions (UDF) y funciones avanzadas de SQL",
          "Manejo e imputación de valores faltantes",
          "Particionamiento y catalogo de APIs"
        ],
        "Spark Streaming": [
          "Fundamentos de Spark Streaming",
          "Ejemplo práctico de contaje de palabras con Spark Streaming",
          "Configuraciones de Spark Streaming: modos de salida y tipos de operaciones",
          "Caso práctico_Operaciones con ventanas temporales en Spark Streaming",
          "Capacidades de Spark Streaming",
          "Caso práctico_Detección del fraude bancario en tiempo real Parte I",
          "Caso práctico_Detección del fraude bancario en tiempo real Parte II",
          "Ejercicio práctico Spark Streaming",
          "Solución al ejercicio práctico"
        ],
        "Como obtener un descuento en las certificaciones de Databricks": [
          "Como obtener un descuento en las certificaciones de Databricks"
        ],
        "BONUS": [
          "BONUS"
        ]
      },
      "requirements": [
        "No"
      ],
      "description": "Si estás buscando un curso práctico, completo y avanzado para aprender Data Engineering con Apache Spark , has venido al lugar correcto.\nEste curso está diseñado para preparar aprender todo lo relacionado con Apache Spark, desde RDDs, Spark SQL, Dataframes y Streaming, hasta Machine Lerning con Spark ML, analítica avanzada, visualización de datos, Spark Koalas y despliegue de clusters de Spark con Databricks.\n\n\nCon la formación teórica, las guías de estudio descargables, los ejercicios prácticos y los casos de uso reales, este es el único curso que necesitarás para aprender Apache Spark.\nLa ingeniería de datos o Data Engineering no es más que procesar los datos en función de nuestras necesidades posteriores. Necesitamos construir diferentes pipelines como Batch Pipelines, Streaming Pipelines, Data Factory pipelines, almacenar los datos en bases de datos o pre-procesar los datos con Apache Spark como parte de la Ingeniería de Datos. Todos los roles relacionados con el procesamiento de datos se consolidan en Data Engineering.\nLa ingeniería de datos con Apache Spark es una de las habilidades más valiosas hoy en día y este curso te enseñará todo lo que necesitas saber para posicionarte en el mercado laboral del big data.\nEn este curso te enseñaremos todas las habilidades de Spark, partiendo desde las bases hasta las funcionalidades más avanzadas. Para ello utilizaremos presentaciones visuales en Power Point, compartiendo explicaciones claras y útiles consejos profesionales.\n\n\nEste curso desarrolla los siguientes apartados:\nIntroducción a big data y fundamentos de Apache Spark\nInstalación de Apache Spark y librerías accesorias como Anaconda, Java, etc\nSpark Dataframes\nFunciones avanzadas con Apache Spark\nSpark Streaming\nSpark con Databricks\nDelta Lakes\nSpark Streaming en Databricks\nSpark en la nube (Azure)\n\n\nSi está listo para mejorar tus habilidades, aumentar tus oportunidades laborales y convertirte en un experto de Big Data, únete hoy y obtén acceso inmediato y de por vida a lo siguiente:\n• Guía completa de Apache Spark (e-book en PDF)\n• Archivos de proyecto de Spark descargables\n• Ejercicios prácticos y cuestionarios\n• Recursos de Spark como: Cheatsheets y resúmenes\n• Soporte experto 1 a 1\n• Foro de preguntas y respuestas del curso\n• 30 días de garantía de devolución de dinero\n\n\n¡Nos vemos allí!",
      "target_audience": [
        "Cualquiera que quiera aprender habilidades avanzadas de big data",
        "Cualquiera que conozca Python y quiera avanzar en un procesamiento de datos más rápido",
        "Cualquiera quiere hacer carrera como ingeniero de datos, analista de datos, científico de datos",
        "Interesado en aprender Apache Spark y pyspark para el análisis de Big Data",
        "Cualquiera quiere aprender tecnología de vanguardia en procesamiento de datos"
      ]
    },
    {
      "title": "初めての画像分類モデル開発( CNN/Vision Transformer/YOLO/AWS Rekognition)",
      "url": "https://www.udemy.com/course/image-classification-modeling/",
      "bio": "Python/Kerasを使ってディープラーニングを用いた画像分類にチャレンジしましょう！また、AWSのAutoMLも使ってノーコードの画像分類にも取り組みます。",
      "objectives": [
        "ディープラーニングの基礎",
        "畳み込みニューラルネットワーク（CNN）",
        "EfficientNetによる画像分類",
        "Vision Transformerによる画像分類",
        "事前学習済みモデルについて",
        "AutoMLのAWS Rekognitionを使った画像分類"
      ],
      "course_content": {
        "コース紹介": [
          "コース紹介",
          "サンプルコード・データのダウンロード",
          "コース準備レクチャー"
        ],
        "CNNベースのモデルによる画像分類": [
          "CNN",
          "AlexNet",
          "ResNet",
          "EfficientNet",
          "事前学習済みモデルを使う",
          "転移学習とファインチューニング",
          "EfficientNetによる画像分類演習の説明",
          "EfficientNetによる画像分類演習①",
          "Efficientnetによる画像分類演習②",
          "Efficientnetによる画像分類演習③"
        ],
        "Vision Transformerによる画像分類": [
          "Vision Transformerの概要",
          "インプットレイヤー",
          "エンコーダ",
          "MLPヘッドとまとめ",
          "Vision Transformerによる画像分類演習の説明",
          "Vision Transformerによる画像分類演習①",
          "Vision Transformerによる画像分類演習②",
          "Vision Transformerによる画像分類演習③"
        ],
        "YOLOv8による画像分類": [
          "YOLOの説明",
          "YOLOv8による画像分類演習①",
          "YOLOv8による画像分類演習②"
        ],
        "Amazon Rekognitionによる画像分類": [
          "Amazon Rekognitionを使った演習の説明",
          "Amazon Rekognitionの画面",
          "Amazon Rekognitionによる画像分類演習①",
          "Amazon Rekognitionによる画像分類演習②"
        ],
        "ボーナスレクチャー": [
          "ボーナス"
        ]
      },
      "requirements": [
        "Pythonや機械学習のの基本的な知識があるとよいです"
      ],
      "description": "本コースは画像分類に初めて取り組む方のためのコースで、大きく、CNNベースの画像分類、Vision Transformerによる画像分類、YOLOというライブラリを用いた画像分類、そして最後にAWSのAutoMLであるAmazon Rekognitionを用いた画像分類を行います。\n簡単なコーディングができる方は全て実行していただけますし、コーディングができないという方はAmazon Rekognitionだけ試すこともできますので、各自の好みに応じてチャレンジしていただければと思います。\n\n\n本コースの内容\nディープラーニングの基礎知識\n畳み込みニューラルネットワークCNNベースの画像分類（EfficientNet）\nVision Transformerによる画像分類\nYOLOによる画像分類\nAmazon Rekognitionによる画像分類\n\n\n実行環境\n実行環境はGoogle Colaboratoryを使用しますので、特に環境構築等は不要です。\nただし、Amazon Rekognitionを試される場合はAWSアカウントが必要になります（若干の課金が発生する可能性がありますので、そちらはご了承ください）。\n\n\n対象者\n本コースの対象は、画像分類に初めて取り組む方です。すでに業務などで画像分類などを行なっている方は対象外となります。\n\n\nその他\n- EfficientNet, Vision TransformerはKerasを用いてコードを書いていきます。\n- デプロイなどは行いません。あくまでモデルを作るところまで行います。\n- 機械学習の知識や精度評価などは話しませんし、詳しく取り扱いません。\n\n\n最近は仕事でも画像データを扱うケースも多くなってきたかと思います。\n本コースは1時間半程度にまとめられており、休みの日などにパッと見ることもできますから、ぜひこの機会にお試しください！",
      "target_audience": [
        "画像分類に興味があるがやったことがない方",
        "Transformerについて勉強してみたい方",
        "画像分類を仕事でやらないといけなくなった方",
        "Pythonで何かAIっぽいことをしてみたい方"
      ]
    },
    {
      "title": "TPOTによる分類モデル作成講座 : 【AutoML/Python/Kaggle/SIGNATE】①",
      "url": "https://www.udemy.com/course/automated-machine-learning-tpot-pythonkagglesignate1/",
      "bio": "Learn how to create Binary/Multi-Class Classification model with TPOTClassifier(AutoML) & Participate in Kaggle/SIGNATE",
      "objectives": [
        "TPOTClassifier (AutoML)を使用した分類モデルの作成",
        "TPOTClassifierのパラメータチューニング方法",
        "TPOTClassifierのAttributesやFunctionsを学習",
        "Kaggle・Signateに向けたモデルの作成"
      ],
      "course_content": {
        "はじめに": [
          "講座の概要",
          "TPOTとは",
          "TPOTのインストール"
        ],
        "二値分類【Titanic】": [
          "データセットの取得",
          "Parametersの学習①",
          "Parametersの学習②",
          "Drill",
          "Functionsの学習",
          "Attributesの学習"
        ],
        "二値分類【Breast Cancer】": [
          "Test",
          "Answer"
        ],
        "マルチクラス分類【Iris】": [
          "Test",
          "Answer"
        ],
        "マルチクラス分類【Wine】": [
          "Test",
          "Answer"
        ],
        "Challenge for SIGNATE 【天秤のバランス分類】": [
          "コンペの参加",
          "データの取得",
          "モデルの作成"
        ],
        "Practice in Kaggle 【アワビの年齢分類】": [
          "データセットの調査",
          "モデルの作成",
          "特徴量エンジニアリング"
        ]
      },
      "requirements": [
        "Jupyter NotebookによるPythonの基本操作",
        "データセットの読込、データ前処理、モデル作成、モデルの評価の経験",
        "TPOTClassifierの使用が未経験な方、又は経験が浅い方"
      ],
      "description": "本講座はAutomated Machine Learning Tool（自動機械学習モデル）であるTPOTライブラリを使用し、公式ドキュメントを参考にしながら TPOTClassifierのParametersやAttributes・Functions、そして分類モデルの作成を学習していくコースとなっています。\nまた、講義の中では分析コンペティション（SIGANTE）にも参加し、機械学習モデル自動化ツールがどれほどの予測精度を出すか確認できます。\n手軽で実用的なツールなため、機械学習に苦手意識を持っていた方でもお勧めです。\n\n\nコース内容は以下の通りです。\nSection1:はじめに\nSection2:二値分類【Titanic】\nSection3:二値分類【Breast Cancer】\nSection4:マルチクラス分類【Iris】\nSection5:マルチクラス分類【Wine】\nSection6:Challenge for SIGNATE 【天秤のバランス分類】\nSection7:Practice in Kaggle 【アワビの年齢分類】",
      "target_audience": [
        "TPOTライブラリの使用に関心を持つ方",
        "TPOTClassifierを使用し分類モデルの作成を行いたい方",
        "機械学習をツールとして使いこなしたい方",
        "TPOTに興味があるけど、始め方が分からない方",
        "AutoMLで何らかの問題を解決したい方",
        "AIコンペの参加に関心がある方"
      ]
    },
    {
      "title": "Pemrograman R Solusi Mudah Mengolah Data untuk Orang Awam",
      "url": "https://www.udemy.com/course/pemrograman-r-untuk-pemula/",
      "bio": "Programmer Olah Data yang Hampir Nyerah, Wajib Belajar R. Orang Awam Saja Bisa Paham. Dijamin Sip!",
      "objectives": [
        "Langkah Awal Menggunakan R (Tanpa Install Apapun)",
        "Tipe Data Variabel pada R",
        "Menggunakan Variabel",
        "Variabel Logika dan Operator",
        "Perulangan While pada R",
        "Perulangan For pada R",
        "Logika If untuk Pencabangan",
        "Mengenal dan Membuat Vector",
        "Mengakses Elemen-Elemen pada Vector",
        "Mengakses Elemen Menggunakan Perulangan For",
        "Matrix pada R dan Cara Pembuatannya",
        "Operasi Matematika pada Matrix",
        "Visualisasi Data pada Data-Data",
        "Membuat Chart dengan Mudah"
      ],
      "course_content": {
        "Pendahuluan": [
          "Pendahuluan",
          "Menggunakan R"
        ],
        "Prinsip Dasar R": [
          "Tipe Data Variabel pada R",
          "Menggunakan Variabel",
          "Mengenal Variabel",
          "Assignment Operator",
          "Variabel Logika dan Operator",
          "Latihan Koding R",
          "Kuis Dasar-Dasar Pemrograman R"
        ],
        "Perulangan dan Percabangan": [
          "Perulangan While",
          "Perulangan For",
          "Relational Operator",
          "Logika If",
          "Operator-Operator Logika"
        ],
        "Vector pada R": [
          "Mengenal Vector",
          "Membuat Vector",
          "Fungsi Length",
          "Mengakses Elemen",
          "Fungsi isna",
          "Mengakses Elemen Menggunakan For",
          "Kalkulasi terhadap Vector",
          "Fungsi-Fungsi pada Vector",
          "Tugas Bekerja dengan Vector"
        ],
        "Matrix pada R": [
          "Mengenal dan Membuat Matrix",
          "Fungsi rbind() dan cbind()",
          "Mengubah Nama Baris dan Kolom",
          "Kalkulasi pada Matrix",
          "Kuis tentang Vector dan Matrix"
        ],
        "Visualisasi Data": [
          "Membuat Pie Chart",
          "Membuat Bar Chart",
          "Parameter-Parameter untuk Membuat Judul pada Chart",
          "Membuat Line Chart",
          "Parameter-Parameter untuk Mengatur Font",
          "Tugas Membuat Bar Chart",
          "Kuis Seputar Chart"
        ]
      },
      "requirements": [
        "Memiliki komputer yang terhubung jaringan internet"
      ],
      "description": "Pemrograman R harus dikuasai oleh mereka yang ingin mempelari ilmu mengolah data.\nJelas, ilmu tentang mengolah data bukanlah bidang ilmu yang kaleng-kaleng. Justru dengan mempelajari cara pengolahan data sejak dini, Anda akan semakin dekat dengan bidang kecerdasan buatan, utamanya machine learning.\nNah, pemrograman R merupakan langkah awal untuk Anda mempelajari cara mengolah data.\nJadi, mulailah dari video course ini. Video ini adalah online course yang paling lengkap, to-the point, dan mudah dipahami untuk mengolah dan menganalisis data menggunakan R di Udemy! Tidak hanya itu, semua penjelasan di dalam video course ini disampaikan dalam Bahasa INDONESIA dengan jargon-jargon awam.\nTidak memandang apakah Anda belum pernah mengenal apa itu data sebelumnya, atau sudah mengetahui dasar-dasar pemrograman R, video course ini dirancang tentu saja untuk Anda! Dalam kursus ini, kami akan mengajari Anda mengolah data memakai R secara efektif dan singkat, tepat, serta jelas!\nDilengkapi dengan berjam-jam video course, latihan coding, dan artikel, kursus komprehensif ini tidak membutuhkan pengalaman apapun! Bahkan Anda bisa mengoperasikannya menggunakan MS Windows.\nJangan lupa pula, seluruh tools yang dibahas di sini dapat dimiliki GRATIS!\nVideo course  ini akan mengajarkan Anda mengolah dan menganalisis data dengan setiap bab dilengkapi screencast yang mengajarkan sintaks, fungsi, dan aturan pemrograman lengkap dan penjelasan-penjelasan yang menarik! Tunggu apa lagi, ayo belajarlah dengan cara  yang terbaik untuk Anda!\nKami akan memulainya dengan cara sederhana menulis kode-kode program R. Lantas, kami akan membantu Anda  topik-topik yang lebih spesifik, yaitu:\nLangkah Awal Menggunakan R (Tanpa Install Apapun)\nTipe Data Variabel pada R\nMenggunakan Variabel\nVariabel Logika dan Operator\nPerulangan While pada R\nPerulangan For pada R\nLogika If untuk Pencabangan\nMengenal dan Membuat Vector\nMengakses Elemen-Elemen pada Vector\nMengakses Elemen Menggunakan Perulangan For\nMatrix pada R dan Cara Pembuatannya\nOperasi Matematika pada Matrix\nVisualisasi Data pada Data-Data\nMembuat Chart dengan Mudah\n\n\nAyo, tunggu apa lagi! Jangan khawatir tentang kualitas video course ini sebab kami menawarkan GARANSI 30 HARI UANG KEMBALI TANPA SYARAT. Betul! Jika Anda tidak puas, silakan ajukan refund tanpa syarat.\nTetapi kami bisa mengatakan kepada Anda bahwa video course ini adalah yang terlengkap dan terbaik tentang R. Tidak percaya? Buktikan sendiri!",
      "target_audience": [
        "Orang yang ingin mempelajari Machine Learning",
        "Orang yang ingin mengolah data menggunakan pemrograman",
        "Peneliti dan Orang yang bekerja di bidang statistik",
        "Dosen olah data"
      ]
    },
    {
      "title": "【実戦で学ぶ基盤構築】ローカル端末で作り理解するエンジニアのための機械学習基盤の作成とMLOps",
      "url": "https://www.udemy.com/course/mlops-pf-and-ml/",
      "bio": "【データサイエンス/データエンジニアリングシリーズ】SparkMLを用いた機械学習モデルの作成からデプロイまで",
      "objectives": [
        "機械学習を行うデータアプリケーションの作成方法を学びます",
        "MLOpsとは何か？実現する方法はどのようなものがあるのか？を学びます",
        "大規模データセットでも機械学習モデルが作成できるSparkMLの利用方法を学びます",
        "データ分析基盤も含めたMLOpsを実現するデータの流れアプリケーションの作成方法を学びます"
      ],
      "course_content": {
        "紹介": [
          "紹介",
          "本コースの概要",
          "自己紹介",
          "学習ロードマップ",
          "レビューのお願い"
        ],
        "基本知識": [
          "コースで利用するシステムの全体像を理解しよう",
          "データアプリケーションとは",
          "データ分析基盤とは",
          "機械学習とは",
          "技術スタック紹介",
          "MLOpsとは"
        ],
        "機械学習基盤とモデル作成を通してMlOpsを体験する": [
          "仮説設定をしてKPIを定めてみよう",
          "環境構築と今回の環境についての説明",
          "データの取得と蓄積",
          "データのテストとSparkの使い方速習",
          "SparkMLで簡単なモデルを作成してみよう",
          "モデルの結果をデータベースに保存してみましょう",
          "自身でデータのテスト(バーリデーション)を行ってみよう"
        ],
        "データアプリケーションを作成しみよう": [
          "モデルの出力結果(mongodb)とデータアプリケーション(Nodejs)を連携してみよう",
          "施作効果を確認してみよう",
          "モデルの再学習を検討しよう",
          "モデルを継続的にデプロイしよう",
          "データを増やしてモデル作成 -> 再度デプロイをご自身で行ってみよう！"
        ],
        "ボーナスレクチャー": [
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "特にありません"
      ],
      "description": "コース概要：\nデータの民主化に伴い、エンジニアだけでなくアナリスト職のなどデータ活用を行う環境はここ数年で様変わりしました。\n現代では、従来の特定の人物だけの利用を想定したデータ分析環境では立ち行かなくなっています。\n本コースでは、データからモデルを作成し作成したモデルを継続的に利用する機械学習基盤の構築をDockerコンテナでご自身の端末で学んでみます。\n機械学習のモデルは一度作成しただけでは終わりではありません。刻々と変化するデータに応じて再学習を行いその結果をシームレスにシステムに反映させていくことが現代では求められています。それらを実現するのが機械学習基盤の役割でありMLOpsの考え方でもあります。本コースは、モデルを量産するコースではありませんが、一つのシンプルなモデルを通して効率よく機械学習モデルの実利用についてのエッセンスを学んでいきましょう。\n\n\nコース内では、クラウドだとどのような代替サービスがあるのか？についても言及しておりますので、本コースを通して学んだことをクラウド上の開発で活用していたくことも可能です。\nまた、混同を避けるためにデータ分析基盤との関係についても本コース内で言及します。\n\n\n本コースでは、ローカル端末とDockerコンテナを使って\n「機械学習基盤ってどう作るの？」\n「機械学習基盤ってそもそも何？」\n「機械学習基盤はどんな要素を含んでいるの？」\n「機械学習基盤を使ってどのような活動をするの？」\n「モデルを作成したけどどのように利用して良いかわからない」\n「ワンショット(一回限り)のモデルしか作成したことがないので継続的に適用していくイメージが湧かない」\nそんな疑問を解決するコースです。\n\n\nどんな人向け？\n「機械学習基盤に興味のある開発者」\n「モデルを作成後のモデル利用の流れとデータを利用した活動をイメージしたい方」\n「データ分析基盤と機械学習基盤の関係について知りたい方」\n\n\n登場する技術スタック：\nMongodb\nApache Spark\nApache SparkML\nNodeJs\n(Embulk)",
      "target_audience": [
        "機械学習をシステム的に実装する必要のある技術者",
        "MLOpsに興味のあるエンジニアやビジネスパーソン",
        "モデルを作成後のモデル利用の流れとデータを利用した活動をイメージしたい方",
        "データ分析基盤と機械学習基盤の関係について知りたい方"
      ]
    },
    {
      "title": "LLM (Büyük Dil Modeli) Fine-Tuning Uzmanlığı: GRPO, SFT, DPO",
      "url": "https://www.udemy.com/course/llms-buyuk-dil-modelleri-egitimi-uzmanlg-grpo-sft-dpo/",
      "bio": "DeepSeek yöntemi ile LLM eğitimi: SFT, LoRA, DPO ve GRPO ile Uygulamalı Fine-Tuning ve Reinforcement Learning",
      "objectives": [
        "Büyük Dil Modellerinin (LLM) temel prensiplerini ve eğitiminin genel altyapısını kavrayacaksınız",
        "Base model ile Instruct model arasındaki farklar ve bunlar için veri hazırlama yöntemlerini öğreneceksiniz",
        "Püf noktaları ile birlikte Veri ön işleme, modellerin beklediği özel tokenları, veri formatını bulmayı, ve veriye entegre etme yöntemlerini öğreneceksiniz",
        "LoRA ve Data Collator nasıl çalışır onları uygulamalı olarak detaylı bir şekilde öğreneceksiniz",
        "Eğitim için çok önemli olan hiperparemetrelerin ne iş yaradığını, nasıl çalıştığını detaylı bir şekilde öğreneceksiniz",
        "Eğitilen LoRa matrisleri ile asıl model nasıl birleştirilir, birleştirirken dikkat etmemiz gereken noktalar neler uygulamalı olarak detaylı bir şekilde öğrenec",
        "DPO (Direct Preference Optimization) nedir nasıl çalışır beklediği veri formatı ne, hangi durumlar kullanılır bunları öğreneceksiniz",
        "DPO için veri hazırlarken dikkat etmemiz gereken noktaları ve DPO data collator nasıl çalışır öğreneceksiniz",
        "DPO ile model eğitirken DPO ya özel hiperparametreler neler nasıl çalışır onları öğreneceksiniz",
        "Eğitim tamamlandıktan sonra modeli Hugging Face gibi platformlara yüklemek ve hiperparametre yönetimi yapmayı öğreneceksiniz",
        "GRPO (Group Relative Policy Optimization) Reinforcement Learning yöntemi nasıl çalışır ? öğrenme aşaması nasıl gerçekleşir detaylı bir şekilde öğreneceksiniz",
        "GRPO (Group Relative Policy Optimization) için veri hazırlamayı öğreneceksiniz",
        "GRPO (Group Relative Policy Optimization) için en kritik nokta olan ödül fonksiyonları yazmayı çeşitli örnek ödül fonksiyonlari ile öğreneceksiniz",
        "GRPO için ödül fonksiyonlarına veriler hangi formatta gidiyor ? Fonksiyonlar içinde verileri nasıl işleriz ?",
        "Fonksiyonlar içinde ödülleri nasıl tanımlarız template nasıl belirleriz ?",
        "Ham cevap içinden ödül vereceğimiz kısımları nasıl alıp ödül tanımlarız ? gibi bir çok detayı uygulamalı olarak öğreneceksiniz",
        "Instruct model GRPO ile düşünen diye tabir edilen \"chain of thoughts\" üreten modele nasıl dönüştürülür onu öğreneceksiniz",
        "Ve daha bir çok detayı uygulamalı olarak öğreneceksiniz"
      ],
      "course_content": {
        "Giriş": [
          "Giriş",
          "Kurs İçeriği Tanıtım",
          "Notebooklar"
        ],
        "Quantization, LoRa, SFT, Data Collator, Veri hazırlama...": [
          "Quantization nedir ? Boyuta ve Parametrelere etkileri nelerdir?",
          "Hugginface Hesap Açalım ve Token Alalım",
          "Colab Notebook Oluşturalım ve Kullanacağımız Kütüphaneleri Tanıyalım",
          "Modeli Quantize Ederek İndirelim",
          "Base Model ile Instruct Model Farkı",
          "Kullanılacak Veri Setini İndirelim ve İnceleyelim",
          "Veri Detini Hazırlama, Chat Template, Özel Tokenleri Entegre Etme",
          "Veri Seti Hazırlığı Devamı ve Tokenize Etme",
          "Data collator nedir? Nasıl çalışır ? Uygulamalı örnek.",
          "LoRa nedir ? Neden kullanılır ?",
          "LoRa Matrislerini Modele Entegre Edelim",
          "Training Args Ayarlayalım (Training Hiperparametreleri)",
          "Trainer Ayarlayıp Eğitimi Başlatalım ve Sonuçların Değerlendirelim",
          "Eğitilmiş LoRa Matrisleri ile Modeli birleştirelim",
          "Modeli Hugginface'e Yükleme ve Kullanım",
          "Çıktıları Etkileyen Hiperparametreler"
        ],
        "Tokenizer için yeni token ekleme ve template oluşturma": [
          "Modeli ve Tokenizeri İndirelim",
          "Tokenizer'a Yeni Özel Tokenler Ekleyelim",
          "Yeni Özel Tokenlar İle Template Oluşturup Verilere Entegre Edelim"
        ],
        "DPO (Direct Preference Optimization)": [
          "DPO nedir ? Beklediği Veri Formatı Nedir ?",
          "Modeli İndirme ve DPO Data Collator Nasıl Padding Yapar ?",
          "Veri Setini DPO İçin Hazırlayalım",
          "LoRa Matrislerini Modele Ekleyelim",
          "Training Args ( DPOConfig ile )",
          "Model Eğitim ve LoRa Matrislerini Birleştirelim"
        ],
        "GRPO (Group Relative Policy Optimization) Reinforcement Learning": [
          "\"Düşünen\" Model nedir? Nasıl Çalışır?",
          "GRPO Nedir ? Nasıl Uygulanır ?",
          "Unsloth ve VLLM nedir ? + Modeli İndirelim",
          "VeriSetini İnceleme ve İlk Hazırlık Adımları",
          "Veriden Belirli Kısımları Çekme Regex ve Grup İşlemleri",
          "Veriler Ödül Fonksiyonlarına Hangi Formatta Gider ?",
          "1. Ödül Fonksiyonu",
          "2. Ödül Fonksiyonu",
          "3. Ödül Fonksiyonu",
          "4. Ödül Fonksiyonu",
          "Training Hiperparametreleri ( GRPO Config ile)",
          "Trainer Nesnesi ve Eğitim",
          "Sonuçlar Tablosu ve Örnek Çıktılar"
        ]
      },
      "requirements": [
        "Temel Python programlama bilgisi",
        "Yapay zeka ve makine öğrenmesi kavramlarına giriş seviyesinde aşinalık",
        "İdeal olarak Jupyter Notebook veya Google Colab deneyimi"
      ],
      "description": "Bu kursta, Büyük Dil Modelleri (LLM) dünyasına adım atarak hem temel hem de ileri düzey optimizasyon yöntemlerini uçtan uca öğreneceksiniz. Eğitime SFT (Supervised Fine-Tuning) yaklaşımıyla başlayıp, veriyi doğru biçimde hazırlamayı, tokenizer ve data collator kullanarak özelleştirilmiş veri setleri oluşturmayı pratik örnekler eşliğinde göreceksiniz. SFT sürecinde LoRA (Low-Rank Adaptation) ve quantization yöntemleriyle büyük modelleri daha hafif ve daha verimli hale getirmenin püf noktalarını öğrenecek, projelerinizde nasıl kullanabileceğinizi adım adım keşfedeceksiniz.\n\n\nSFT temellerini sağlamlaştırdıktan sonra, DPO (Direct Preference Optimization) bölümüne geçeceğiz. DPO, kullanıcı geri bildirimlerini modele doğrudan yansıtarak kullanıcı odaklı sonuçlar elde etme imkânı sunar. Veriyi hangi formatta hazırlamamız gerektiğini, bu yöntemi uygularken nasıl bir ödül mekanizması kurgulayabileceğimizi ve Hugging Facegibi popüler platformlarda eğitilen modelleri nasıl paylaşabileceğinizi göreceksiniz. Ayrıca DPO süreçlerinde data collator mantığını daha derinlemesine anlayarak, farklı senaryolar için veri seti hazırlama ve dönüştürme pratiklerini öğreneceksiniz.\n\n\nKursun en önemli aşaması, giderek popülerleşen ve güçlü sonuçlar üreten GRPO (Group Relative Policy Optimization) tekniğidir. GRPO ile sadece bireysel değil, topluluk içi veya farklı kullanıcı grupları arasında da model davranışını optimize etmenin yöntemlerini öğreneceksiniz. Bu sayede büyük dil modellerinin farklı kitlelere veya farklı amaçlara hizmet etmesini sağlamak daha sistematik ve etkili hale geliyor. Kursta GRPO’nun temel prensiplerini öğrenecek, ardından gerçek veri setleri üzerinde uygulamalı örnekler yaparak tekniği pekiştireceksiniz.\n\n\nEğitim boyunca, LoRA, quantization, SFT, DPO ve özellikle GRPO gibi kilit başlıkları bir arada işleyerek, her birini proje odaklı uygulamalarla destekleyeceğiz. Sonuç olarak, bu kursu tamamladığınızda, uçtan uca veri hazırlamadan model ince ayarına ve grup bazlı politika optimizasyonuna kadar tüm aşamaları güvenle yönetebileceksiniz. Kendi projelerinizde hem performans hem de kullanıcı memnuniyeti odaklı, modern ve rekabetçi LLM çözümleri geliştirmek artık çok daha kolay olacak!",
      "target_audience": [
        "LLM eğitim tekniklerinde uzmanlık kazanmak isteyen veri bilimciler ve ML mühendisleri",
        "Veri hazırlamanın püf noktalarını öğrenmek isteyenler",
        "Kendi özelleştirilmiş dil modellerini geliştirmek isteyen yapay zeka geliştiricileri",
        "SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization), GRPO (Group Relative Policy Optimization) gibi ileri seviye teknikleri uygulamalı öğrenmek isteyenler",
        "GRPO (Group Relative Policy Optimization) tekniğini uygulamalı olarak öğrenmek isteyen",
        "Veri hazırlamanın püf noktalarını öğrenmek ve kendi özel veri setlerini oluşturarak dil modellerine uyarlamak isteyenler",
        "Reinforcement Learning yöntemlerine ilgisi olan, modeli kullanıcı geri bildirimine göre optimize etmek isteyenler."
      ]
    },
    {
      "title": "엑셀보다 쉽고, SPSS보다 예쁜, R 코맨더 데이터 시각화",
      "url": "https://www.udemy.com/course/r-commander-chart/",
      "bio": "클릭 몇 번으로 중요한 그래프들을 만들어 보세요.",
      "objectives": [
        "데이터의 종류에 어울리는 그래프를 선택하기",
        "R의 대표적인 그래프를 R 커맨더를 이용해서 쉽게 만들기",
        "R 커맨더로 R과 친숙해 지기",
        "엑셀로 만들기 어려운 몇 가지 그래프들을 쉽게 만들기"
      ],
      "course_content": {},
      "requirements": [
        "일반적인 컴퓨터 사용자라면 누구나 할 수 있습니다."
      ],
      "description": "엑셀로는 그릴 수 없는 boxplot, jitter plot, violin plot, beeswam plot, densitogram,  histogram,\nLOWESS,  Kaplarn Meier curve, ROC 을 그립니다.\n엑셀보다도 더 쉽게 클릭 몇 번으로 그릴 수 있습니다.\n점차 사용자가 늘어가는 R, 그러나 초보자들은 아무래도 코딩 방법이 접근이 쉽지 않습니다.\n그래서 R을 사용하되 엑셀이나 SPSS처럼 GUI 방식으로 시행하는 'R 코맨더'가 추천됩니다.\n'R 코맨더'의 여러 기능 중에서 그래프 만들기, 데이터 시각화 부분만 따로 떼서\n집중적으로 배워봅시다.\n통계는 어떻게 SPSS로 한다고 치더라도, 그래프만큼은 도무지 SPSS에 만족하기 힘든 사람들도\n들어도 좋은 강의 입니다.\n아예  'R 코맨더'로 통계를 시행하고 시각화까지 하려는 사람들에게도 추천드립니다.\n클릭 몇 번으로 쉽게 그리고 다양하게 꾸미는 방법을 배우게 됩니다.",
      "target_audience": [
        "데이터 시각화에 관심이 많은 사람",
        "엑셀로 그릴 수 없지만 많이 사용되는 그래프를 그리고 싶은 사람",
        "코딩에 소질이나 관심이 없는 사람"
      ]
    },
    {
      "title": "NumPy Essencial para Desenvolvedores Python",
      "url": "https://www.udemy.com/course/numpy-essencial-para-desenvolvedores-python/",
      "bio": "Domine tudo o que você precisa saber sobre o NumPy para análise numérica e cálculos científicos! Exercícios resolvidos!",
      "objectives": [
        "Reforçar seus conhecimentos em Python e aprimorar suas habilidades de programação para trabalhar com arrays NumPy em seus projetos",
        "Explorar a natureza poderosa dos arrays NumPy e seus atributos essenciais para manipulação eficiente de dados",
        "Criar, preencher e navegar em arrays NumPy, extraindo e modificando dados",
        "Utilizar métodos e funções para realizar operações complexas em seus arrays, otimizando seu código e explorando todo o potencial do NumPy",
        "Explorar conceitos fundamentais de ciência de dados, aprendendo a manipular e analisar dados de forma eficaz"
      ],
      "course_content": {
        "Introdução": [
          "Conteúdo do curso",
          "Mais sobre Análise de Dados",
          "Recursos para download"
        ],
        "NumPy x Listas": [
          "Criação de listas e arrays",
          "Tipos de dados",
          "Vetorização",
          "Broadcasting",
          "EXERCÍCIOS",
          "Solução"
        ],
        "ndarray e atributos": [
          "Array a partir de dados",
          "Array de 1 e 2 dimensões",
          "Array de n dimensões",
          "Atributos do array",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Arrays preenchidos": [
          "Arrays preenchidos 1",
          "Arrays preenchidos 2",
          "Arrays preenchidos 3",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Indexing, slicing e assigning": [
          "Indexing",
          "Slicing",
          "Assigning",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Métodos": [
          "Métodos de conversão",
          "Manipulação de formatos",
          "Seleção e manipulação de itens",
          "EXERCÍCIOS",
          "Solução",
          "Métodos para cálculo 1",
          "Métodos para cálculo 2",
          "Métodos para cálculo 3",
          "Métodos para cálculo 4",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Números aleatórios": [
          "Dados aleatórios simples",
          "Permutações",
          "Distribuições 1",
          "Distribuições 2",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Funções de manipulação": [
          "Agrupamento e divisão de arrays",
          "Adição de eixos",
          "Rearranjando elementos",
          "Adicionamento ou removendo elementos",
          "Obtendo valores únicos",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Funções de indexação": [
          "Funções de indexação 1",
          "Funções de indexação 2",
          "Funções de indexação 3",
          "EXERCÍCIOS",
          "Solução"
        ],
        "Funções universais": [
          "Funções matemáticas 1",
          "Funções matemáticas 2",
          "Funções trigonométricas",
          "Funções de comparação",
          "Funções float",
          "EXERCÍCIOS",
          "Solução"
        ]
      },
      "requirements": [
        "Lógica de programação e programação básica em Python, apesar de ser possível acompanhar o curso sem conhecimentos profundos dessas áreas"
      ],
      "description": "Este curso foi elaborado para desenvolvedores Python que desejam explorar os recursos poderosos da biblioteca NumPy. Através de aulas práticas, você vai adquirir as habilidades necessárias para trabalhar com arrays multidimensionais, realizar cálculos científicos complexos e manipular dados com eficiência.\nVamos abordar os seguintes tópicos:\n\nndarrays (a classe fundamental do NumPy) e seus atributos:\nCrie e manipule arrays multidimensionais com a classe ndarray\nExplore os atributos essenciais de ndarrays\nAprenda técnicas de indexação e fatiamento de arrays, e atribuição de valores\nCompreenda as diferentes maneiras de criar arrays preenchidos\n\nMétodos do ndarray:\nExtraia atributos e realize operações matemáticas em arrays\nUtilize métodos do ndarray para manipular dados de forma eficiente\n\nManipulação de arrays:\nUtilize funções de manipulação de arrays para modificar e transformar dados\nCombine arrays de diferentes formas para criar conjuntos de dados mais complexos\nDescubra como transpor, reorganizar e inverter arrays\nExplore técnicas avançadas de indexação para extrair informações específicas de arrays\n\nFunções NumPy para análise poderosa:\nUtilize funções de álgebra linear, que são úteis para resolver sistemas de equações, calcular matrizes inversas e muito mais\nAplique funções estatísticas para analisar dados, calcular medidas de tendência central e dispersão\nDomine funções universais do NumPy para realizar operações matemáticas em arrays\n\nE mais:\nGere números aleatórios com diferentes distribuições de probabilidade\nDescubra constantes NumPy úteis para cálculos científicos\nSalve e carregue arrays para persistência de dados\nAo concluir este curso, você estará apto a utilizar a biblioteca NumPy com confiança para análise numérica em Python, trabalhar com arrays multidimensionais de forma eficiente, realizar cálculos científicos complexos em arrays com precisão e rapidez, manipular dados de forma eficiente para extrair informações valiosas e integrar a biblioteca NumPy em seus projetos de desenvolvimento Python existentes. São mais de 8 horas de vídeos passo a passo e exercícios resolvidos ao final de cada seção!",
      "target_audience": [
        "Desenvolvedores Python interessados em aprender como otimizar operações envolvendo cálculos vetoriais e matriciais",
        "Estudantes de Ciência de Dados buscando o conhecimento essencial em uma das bibliotecas fundamentais para manipulação e análise de dados em Python",
        "Profissionais de Ciência de Dados que queiram se aprofundar sobre os principais conceitos e funcionalidades de uma das bibliotecas mais utilizadas na sua rotina de trabalho"
      ]
    },
    {
      "title": "データドリブン人材育成プログラム BRIDGE｜非エンジニアでもわかるデータ活用入門",
      "url": "https://www.udemy.com/course/bridge-data-dx/",
      "bio": "データ活用・データマネジメント・データリテラシー育成を1つに。非エンジニアでも理解できる、図解と事例で学ぶやさしい講座",
      "objectives": [
        "ビジネスにおけるデータ活用の現状と課題を理解し、自社のデータ活用力の現在地を見極める視点を身につけます。",
        "エンジニア以外の職種でもデータ活用に貢献できる理由を学び、自分の役割におけるデータとの向き合い方を考えます。",
        "データ活用の基本用語・技術・職種の違いをわかりやすく学び、会議やプロジェクトでの会話に自信を持てるようになります。",
        "実務で使えるデータ活用プロセスを身につけ、料理になぞらえた具体的なイメージで業務に活かせる力が養われます。"
      ],
      "course_content": {
        "紹介": [
          "Section0_データドリブン人材育成プログラム「BRIDGE」の概要",
          "Section1_イントロ"
        ],
        "データ活用の現在地": [
          "Section2.1_データ活用の現在地",
          "Section2.2_エンジニアではないあなたが必要な理由",
          "Section2.3_データを正しく活用できない理由"
        ],
        "データ活用の歴史": [
          "Section3_データ活用の歴史"
        ],
        "データ活用のプロセス": [
          "Section4_料理で読み解く、データ活用のプロセス"
        ],
        "今更聞けない専門用語": [
          "Section5.0_今更聞けない専門用語の概要まとめ",
          "Section5.1_今更聞けない専門用語_職種・役割編",
          "Section6_今更聞けない専門用語_製品・技術編",
          "Section7_今更聞けない専門用語_データ活用編",
          "Section8_今更聞けない専門用語_データ管理編"
        ],
        "おわりに": [
          "Section9_おわりに_これまでの仕組みとデータを活用したこれからの仕組み"
        ]
      },
      "requirements": [
        "プログラミングや統計の専門知識は不要です。データ活用の基本から丁寧に解説します。"
      ],
      "description": "「データ活用」と聞くと、難しそう・エンジニア向け・自分には関係ない——そんな風に思っていませんか？\n本講座「データドリブン人材育成プログラム BRIDGE」は、非エンジニアのビジネスパーソン向けに設計された、やさしく・実践的なデータ活用入門講座です。\n\n\n本コースでは、データ活用の「現在地」「課題」「歴史」「プロセス」などを、例え話や図解、ストーリーベースでわかりやすく解説。\nさらに、今さら聞けない用語（DWH・メタデータ・KPI・アドホック分析など）も、実務に即した解釈で丁寧に学べます。\n\n\n「料理になぞらえたデータプロセス」や「社内でよくある会話」など、現場目線を大切にした構成により、自分の仕事にどうデータを活かせるかが自然と身につきます。\n\n\nこのコースで得られること：\n・データ活用の基本構造と全体像の理解\n・エンジニア以外の職種でも必要な理由と役割\n・社内コミュニケーションで使える基礎用語の習得\n・DX推進に貢献するマインドセットと実践ヒント\n\n\n「データに苦手意識のある方」「DX推進や業務改善に関わる方」「チームでデータ活用を始めたい方」に、最初の一歩として最適です。",
      "target_audience": [
        "データに苦手意識のあるビジネス職・企画職など、非エンジニアの方を対象とした入門講座です。"
      ]
    },
    {
      "title": "INTELIGÊNCIA ARTIFICIAL: Sistema de Recomendação com Python",
      "url": "https://www.udemy.com/course/sistema-de-recomendacao-com-python/",
      "bio": "Inteligência Artificial para Ciência de Dados, Finanças, Economia, Análise de Dados, Estatística...",
      "objectives": [
        "Sistema de recomendação",
        "Sistema de recomendação por Mínimos Quadrados Alternados",
        "Recomendações por Decomposição em Valores Singulares",
        "Autovetores e autovalores",
        "KNN para sistema de recomendações",
        "K-Means para sistema de recomendação",
        "Sistema de recomendação por filtragem colaborativa",
        "Sistema de recomendação por conteúdo",
        "Sistema híbrido de recomendação",
        "Medidas de proximidades",
        "Medidas de similaridades",
        "Algoritmo SVD",
        "Aplicação de Inteligência Artificial"
      ],
      "course_content": {
        "Introdução": [
          "Boas vindas e apresentação do instrutor",
          "Apresentação do curso e da plataforma de estudos"
        ],
        "Fundamentos da Linguagem Python": [
          "A Linguagem Python",
          "Conhecendo o Google Colaboratory",
          "Instalação do Anaconda Python",
          "Conhecendo o Jupyter Notebook",
          "Primeiros passos",
          "Operadores Matemáticos",
          "Estrutura de Repetição",
          "Estrutura condicional",
          "Importações de bibliotecas e pacotes",
          "Listas, Tuplas e Dicionários",
          "Criação de Funções",
          "Função Lambda e função map",
          "List Comprehensions",
          "Arrays (vetores e matrizes)"
        ],
        "Conceitos do Sistema de Recomendação": [
          "Introdução ao Sistema de Recomendação",
          "Tipos de sistema de recomendação: Baseada em conteúdo e filtragem colaborativa",
          "Filtragem colaborativa: Baseada em modelo e baseada em memória."
        ],
        "Sistema de Recomendação com Matriz de Fatoração": [
          "Conceitos de matrizes",
          "Soma de Matrizes",
          "Multiplicação de Matrizes",
          "Operações com matrizes em Python",
          "Autovalores e Autovetores",
          "Autovalores e Autovetores em Python",
          "Decomposição em valores singulares (SVD): Conceitos",
          "Decomposição em valores singulares (SVD): Cálculos",
          "SVD como sistema de recomendação em Python",
          "SVD da biblioteca SURPRISE",
          "SVD no Surprise para recomendação de filmes em Python",
          "SVD no Surprise para recomendação de produtos bancários em Python",
          "ALS como sistema de recomendação em Python"
        ],
        "Sistema de Recomendação com KNN": [
          "Algoritmo KNN",
          "Medidas de Proximidade",
          "Medidas de proximidade em Python",
          "Similaridade por cosseno",
          "Similaridade por cosseno em Python",
          "Índice e distância de Jaccard",
          "Cálculos utilizados no algoritmo KNN em classificação",
          "KNN aplicado em sistema de recomendação",
          "KNN para recomendação de ativos"
        ],
        "Sistema de Recomendação com K-means": [
          "Conceitos sobre o algoritmo K-means",
          "Cálculo de Clusters no K-means",
          "Elbow Method (Método do cotovelo) e Algoritmo Elkan",
          "Sistema de recomendação com K-means",
          "Recomendação de produtos com K-means",
          "Recomendação de ativos com K-means",
          "Recomendação de ativos com K-means parte 2"
        ],
        "Finalização do curso": [
          "Encerramento"
        ],
        "Referências Bibliográficas": [
          "Referências e links úteis"
        ]
      },
      "requirements": [
        "Não há pré-requisitos"
      ],
      "description": "Com esse curso você terá a oportunidade de conhecer as principais técnicas dos sistemas de recomendação: Decomposição em Valores Singulares (SVD), Mínimos Quadrados Alternados (ALS), KNN (K vizinhos mais próximos) e K-means, os quais englobam todos os tipos de Sistema de Recomendação: Filtragem baseada em conteúdo, Filtragem colaborativa (baseada em modelo e em memória) e o sistema híbrido.\nO diferencial desse curso é que iremos obter o embasamento teórico de maneira clara e objetiva, com diversos exemplos práticos em Python e destacando a matemática que envolve todas as técnicas, como os cálculos com matrizes, autovalores e autovetores, distância Euclidiana, distância Manhattan, distância de Chebyshev, distância Minkowski, distância de Jaccard, índice de Jaccard e similaridade por cosseno. Todas as etapas são explicadas em detalhes, não é um curso onde somente serão apresentados os comandos utilizados, tudo será explicado detalhadamente.\nPara atender a todos os alunos, sem importar a área e o nível de conhecimento, a segunda seção é referente aos fundamentos da Linguagem Python.\nO curso é apresentado no sistema operacional Windows, mas usuários do Linux e Mac acompanham tranquilamente. Será utilizado o Google Colaboratory, mas como opção podem ser utilizados também o Jupyter Notebook, Spyder, Pycharm, VSCode ou qualquer outra IDE de Python.",
      "target_audience": [
        "Cientista de Dados",
        "Economista",
        "Estatístico",
        "Analista de Dados",
        "Engenheiro de Dados",
        "Pesquisadores",
        "Profissionais da área de finanças",
        "Administrador",
        "Estudante de pós-graduação",
        "Usuário de Inteligência Artifical"
      ]
    },
    {
      "title": "챗GPT와 파이썬으로 주식 자동매매 앱 및 웹 투자 리포트 만들기",
      "url": "https://www.udemy.com/course/gpt-trading/",
      "bio": "챗GPT 프로그래밍으로 자동매매 애플리케이션과 streamlit 웹 투자 리포트 만들기",
      "objectives": [
        "이 강의는 재테크를 메인 주제로 다루지 않습니다.",
        "챗GPT에 대한 기초와 초급 프롬프트 엔지니어링을 학습합니다.",
        "챗GPT를 통한 파이썬 프로그래밍의 전반적인 절차를 배웁니다.",
        "챗GPT 파이썬 프로그래밍으로 주식 자동매매 애플리케이션을 제작합니다.",
        "챗GPT 파이썬 프로그래밍으로Streamlit 기반 웹 투자 리포트를 제작합니다."
      ],
      "course_content": {
        "소개": [
          "인트로 및 본 강의를 학습하는 방법",
          "챗GPT 소개",
          "챗GPT에 효율적으로 질문하는 법 (순한맛)",
          "챗GPT와 소통하기 위해 주의해야 할 사항",
          "파이썬 가상환경 설정 및 VS Code 설치"
        ],
        "파이썬과 함께 하는 주식 자동매매 전략": [
          "매매전략1: 변동성 돌파 전략",
          "매매전략2: 머신러닝 기반 전략 (순한맛)"
        ],
        "증권사 API로 주식 자동매매 애플리케이션 구현하기": [
          "키움증권 Open API 소개 및 예수금과 종목 현재가 불러오기",
          "자동매매 애플리케이션 간단 설계도",
          "PyQt5로 GUI 윈도우 구성하기",
          "챗GPT로 자동매매 로직 구현하기",
          "코드 알고리즘 파악 및 디버깅1",
          "코드 알고리즘 파악 및 디버깅2",
          "코드 알고리즘 파악 및 디버깅3",
          "코드 알고리즘 파악 및 디버깅4",
          "코드 알고리즘 파악 및 디버깅5",
          "Slack 소개 및 파이썬으로 slack 메시지 전송하기",
          "자동매매 애플리케이션에 slack 메시지 전송 연동하기"
        ],
        "Streamlit 라이브러리로 웹 투자 대시보드 만들기": [
          "Streamlit 라이브러리 소개 및 웹 투자 대시보드 맛보기",
          "Streamlit 라이브러리 설치 및 실행하기",
          "Streamlit 기본 위젯과 레이아웃 알아보기 1",
          "Streamlit 기본 위젯과 레이아웃 알아보기 2",
          "키움증권 API로 매매일지 데이터 가져오기",
          "자동매매 애플리케이션과 연동하기",
          "Streamlit 매매일지 레이아웃 구성하기 1",
          "Streamlit 매매일지 레이아웃 구성하기 2",
          "Streamlit 매매일지 레이아웃 구성하기 3"
        ]
      },
      "requirements": [
        "프로그래밍 경험이 없어도 무방하지만, 가급적 파이썬 기초 지식을 학습한 후 본 강의를 듣는 것을 추천합니다."
      ],
      "description": "<이 클래스에서는 이런 걸 배워요>\n이 강의는 재테크를 메인 주제로 다루지는 않습니다.\n챗GPT를 이용한 파이썬 프로그래밍이 메인 주제입니다.\n\n\n1. 주식 자동매매 애플리케이션 제작\n변동성 돌파 전략을 기반으로 하는 나만의 재태크 앱 제작하기! 챗GPT의 도움을 받아 최소한의 프로그래밍 지식으로 파이썬 GUI 애플리케이션을 제작합니다. Slack을 통해 실시간 매매 현황을 모바일 알람으로 받아보세요.\n\n\n2. Streamlit 기반 웹 투자 리포트 제작\nHTML 기반 지식 없이도 손쉽게 웹앱 제작하기! 자동 매매 애플리케이션을 통해 거래한 매매일지를 바탕으로 나만의 웹 투자 리포트를 제작합니다. 무궁무진한 streamlit 웹 대시보드의 잠재력을 느껴보세요!\n\n\n<이런 수강생에게 추천해요!>\n챗GPT로 파이썬 프로그래밍을 맛보러 오신 분!\n나의 아이디어를 챗GPT와 파이썬을 통해 애플리케이션으로 만들고 싶으신 분!\n나만의 투자 관리 웹 앱을 만들고 싶은 분!\n\n\n<수강 후 얻을 수 있는 변화와 결과물>\n챗GPT 파이썬 프로그래밍 프로세스 및 디버깅 과정\n키움증권 Open API+를 이용한 자동매매 애플리케이션\nStreamlit을 이용한 나만의 데이터 웹 앱 개발 및 활용에 필요한 지식\n\n\n\n\n\n\n안녕하세요, 박가네 데이터랩입니다.\n나만의 애플리케이션을 만들고 싶지만 프로그래밍 언어를 배우기에 진입 장벽이 너무 높다고 생각하시지 않았나요?\n그런 분들을 위해 준비했습니다.\n인공지능(AI)을 적극 활용하여 최소한의 노력으로 파이썬 애플리케이션 제작하기!\n\n\n\n\n준비물 등 수강 전 참고사항\n실습 환경\n운영 체제 및 버전(OS): Windows\n사용 도구: 챗GPT (GPT-4.0), Visual Studio Code\n학습 자료\n보고 쉽게 따라할 수 있는 교안 제공 (PDF)\n강의에 사용된 python 코드가 그대로 담긴 소스코드 제공\n선수 지식 및 유의사항\n파이썬 기초 지식 (권장)\n챗GPT의 답변의 다양성으로 인하여 동일 프롬프트에 항상 동일하게 답변하지 않습니다. 강의 인트로의 \"이 강의를 학습하는 방법\"을 참고하세요!",
      "target_audience": [
        "챗GPT에 관심이 있으며, 챗GPT로 어디까지 할 수 있는지 궁금한 남녀노소",
        "챗GPT로 파이썬 프로그래밍을 하고자 하는 초보 개발자"
      ]
    },
    {
      "title": "Análisis de Datos con Python 3",
      "url": "https://www.udemy.com/course/analisis-de-datos-con-python-3/",
      "bio": "Aprenderás y desarrollarás la habilidad de analizar datos a través de herramientas y métodos de análisis y visualización",
      "objectives": [
        "Aprenderás y desarrollarás la habilidad de analizar datos a través de herramientas y métodos de análisis y visualización de datos reconocidos internacionalmente"
      ],
      "course_content": {
        "1. Lectura de Fuentes": [
          "Materiales",
          "1.1.1 Import pandas",
          "1.1.2 Creación de dataframes 1",
          "1.1.3 Atributos dataframes",
          "1.1.4 Columnas dataframes",
          "1.1.5 Creación de dataframes 2",
          "1.2.1 OS - listdir",
          "1.2.2 Read CSV",
          "1.2.3 Read CSV header",
          "1.2.4 Read CSV sep",
          "1.2.5 Read CSV otro parametros",
          "1.2.6 To CSV",
          "1.3.1 Read Excel",
          "1.3.2 Excel file",
          "1.3.3 Ejercicio Integración",
          "1.3.4 Ejercicio Integración - Solución",
          "1.4.1 Read JSON",
          "1.5.1 Read SQL",
          "1.6.1 Read HTML",
          "1.7.1 Read Clipboard"
        ],
        "2. Merging Dataframes": [
          "2.1.1 Append",
          "2.1.2 Ignore Reset Index",
          "2.1.3 Rename columns",
          "2.1.4 Concat",
          "2.1.5 Ejercicio propuesto",
          "2.1.6 Ejercicio Solución",
          "2.1.7 Ejercicio Solución 2",
          "2.2.1 Presentación Legos",
          "2.2.2 Merge",
          "2.2.3 Merge on",
          "2.2.4 Merge suffix columns",
          "2.3.1 Merge preparación",
          "2.3.2 Merge validación exploración",
          "2.3.3 Merge errores",
          "2.3.4 Merge group by",
          "2.3.5 Merge validate",
          "2.3.6 Merge Left Inner",
          "2.3.7 Ejercicio propuesto",
          "2.3.8 Ejercicio - Solución"
        ],
        "3. Análisis Exploratorio de Datos (EDA)": [
          "3.1 Introducción",
          "3.2 Info cantidad porcentaje nulos",
          "3.3 Value Counts",
          "3.4 Unique - Nunique",
          "3.5 Min Max",
          "3.6 Mean, Median y Mode",
          "3.7 Mean - Median: Diferencias",
          "3.8 Mean - Median: Dataframes y nulos",
          "3.9 Std - Var",
          "3.10 Describe",
          "3.11 Percentiles",
          "3.12 Histogramas",
          "3.13 Seaborn - Matplotlib",
          "3.14 Histogramas - Hue",
          "3.15 Boxplot",
          "3.16 Boxplot: x-y",
          "3.17 Histograma vs Boxplot",
          "3.18 Correlación - Intuición",
          "3.19 Correlación",
          "3.20 Heatmap",
          "3.21 Pairplot",
          "3.22 Ejercicio propuesto Forbes",
          "3.23 Ejercicio Forbes Solución"
        ],
        "4. Manipulación de dataframes": [
          "4.1 Introducción",
          "4.2 Filtros - Mascaras - Query",
          "4.3.1 Filtros AND",
          "4.3.2 Filtros Comparaciones",
          "4.3.3 Filtros ISIN",
          "4.3.4 Filtros ISNULL - NOTNULL",
          "4.3.5 Filtros Negación",
          "4.3.6 Filtros String",
          "4.4.1 Ejercicio Propuesto: Phelps - Bolt",
          "4.4.2 Ejercicio Phelps - Bolt Solución",
          "4.4.3 Ejercicio Propuesto: Ganadores",
          "4.4.4 Ejercicio Ganadores Solución",
          "4.5.1 Index loc - iloc",
          "4.5.2 Seleccionar Columnas",
          "4.5.3 Cambiar Nombres Columnas",
          "4.6.1 Transform: Operaciones Columnas",
          "4.6.2 Transform: Astype",
          "4.6.3 Transform: Apply Map",
          "4.7 Sort - Values",
          "4.8 Cuentas",
          "4.9 Drop Duplicates",
          "4.10 Dropna",
          "4.11.1 Group by",
          "4.11.2 Group by agg",
          "4.12 Pivot tables",
          "4.13 Pivot tables - margenes",
          "4.14 Pivot tables - Melt",
          "4.15 Ejercicio propuesto China",
          "4.16 Ejercicio China Solución",
          "4.17 Ejercicio Propuesto Mujeres Olimpiadas",
          "4.18 Ejercicio Mujeres Olimpiadas - Solución",
          "4.19 Fillna",
          "4.20 Fillna - Group by",
          "4.21 Datetimes",
          "4.22 Datetimes format"
        ],
        "5. Pandas para Big Data": [
          "5.1 Introducción",
          "5.2 Dtypes Category",
          "5.3 Chunks",
          "5.4 Dtypes Usecols",
          "5.5 For Apply Vectorize",
          "5.6 Intro - Dask",
          "5.7 Intro - Dask Dashboard"
        ],
        "Bonus Lecture": [
          "Bonus Lecture: DataHack"
        ]
      },
      "requirements": [
        "No, solo muchas ganas de aprender =)"
      ],
      "description": "Conoce el tratamiento de datos con Python. Trabaja con múltiples formatos de archivos de datos como csv, json y base de datos. Crea DataFrames que podrás manipular y analizar sin preocuparte por el performance de tus aplicaciones, todo esto muy fácil y rápido con Pandas.\n\n\nEste curso está dirigido para todas las personas que quieren aprender el tratamiento de datos con Python. Empezaremos con los conceptos básicos, desarrollando ejemplos simples (y no no son los de Iris ni de Titanic ?). Cada capítulo tiene un enfoque práctico con proyectos que te permitirán desarrollar un portafolio. Sí siempre quisiste utilizar Python con un enfoque en Data y Analytics esta es tu oportunidad de hacerlo a tu propio ritmo y sin instalar nada ?.\n\n\n¿Qué cosas vas aprender?\nLectura de datos con distintos formatos (csv, json, pickel, base de datos, cloud storage)\nManipulación de datos con Pandas\nAplicación de técnicas de aprendizaje de Máquina\nIntroducción DASK\n\n\nNuestro curso te permite aprender a tu ritmo, sin instalar algo en tu computadora. Nuestro docente tiene experiencia desarrollando proyectos de Analytics, por lo que si estás buscando aprender Python con un enfoque de análisis de datos esta es una gran oportunidad.\n\n\nÚnete a la comunidad de DataHackers, con más de 20 mil alumnos aprendiendo de temas de Data, Analytics y Cloud. Siempre nos preocupamos por tu satisfacción por eso estamos orgullosos de tener una calificación de 4.5/5 en Udemy.\nAprende, Aplica y Crece con DataHack.",
      "target_audience": [
        "Este curso está dirigido para todas las personas que deseen fortalecer sus cualidades de analistas de datos."
      ]
    },
    {
      "title": "Banco de Dados MySQL / MariaDb: Do Zero ao Avançado - 2025",
      "url": "https://www.udemy.com/course/banco-de-dados-mysql-mariadb-do-zero-ao-avancado/",
      "bio": "Aprenda a dominar consultas SQL, joins, funções, triggers, e mais. Ideal para estudantes, programadores e futuros DBAs!",
      "objectives": [
        "A criar um Banco de dados Mysql / MariaDb",
        "Inserir dados no Banco Mysql",
        "Deletar Dados no Banco Mysql",
        "Atualizar dados pré-existentes",
        "Selecionar (buscar) dados de forma avançada",
        "Linguagem SQL",
        "Joins - União entre tabelas",
        "Funções do Mysql",
        "Painel PHPMyAdmin",
        "Eventos no Mysql",
        "E Muito mais..."
      ],
      "course_content": {
        "Introdução ao Banco de dados Mysql / MariaDb": [
          "Introdução",
          "Conceitos",
          "Acessando um Banco de dados Mysql - ONLINE",
          "PhpMyAdmin no seu computador"
        ],
        "Criando Tabelas no seu Banco de dados": [
          "Creat Table",
          "Tipos de dados",
          "Auto Increment e Chave Primaria",
          "Criando tabelas pelo PhpMyAdmin",
          "Integridade de dados - Constraints",
          "Validação de Dados",
          "Validação por números de Caracteres",
          "Deletando uma Tabela SQL"
        ],
        "Gerenciando o Banco de Dados (CRUD)": [
          "CRUD",
          "INSERT",
          "INSERT com Datas",
          "Primary Key / Foreign Key",
          "UPDATE",
          "DELETE",
          "SELECT",
          "ORDER BY",
          "Múltiplas Ordenações",
          "LIMIT",
          "OFFSET",
          "DISTINCT",
          "WHERE",
          "WHERE com AND",
          "WHERE com OR",
          "Busca Avançada com LIKE",
          "Filtrando Datas",
          "Função Soma",
          "Função Contagem",
          "Funções Máx e Mín"
        ],
        "Joins - Unindo dados entre Tabelas": [
          "Buscando dados entre tabelas",
          "INNER e RIGHT JOIN",
          "LEFT e FULL JOIN"
        ],
        "TRIGGERS e Recursos Avançados": [
          "Group By",
          "TRIGGERS - Gatilhos SQL",
          "TRIGGERS - UPDATE",
          "TRIGGERS - DELETE",
          "TRIGGERS - INSERT",
          "Excluindo TRIGGERS"
        ],
        "Eventos - Tarefas Agendadas": [
          "Criando Eventos no Mysql",
          "Gerenciando Eventos"
        ],
        "Recursos Úteis": [
          "Importar / Exportar Dados"
        ],
        "Integrando Mysql com PHP e Node JS": [
          "Conectando Banco via PHP",
          "Inserindo Dados via Formulário",
          "Buscando Dados com PHP",
          "Deletando dados com PHP",
          "Atualizando dados com PHP",
          "Integrando Mysql com Node Js"
        ],
        "O que fazer Agora?": [
          "O que fazer Agora?"
        ]
      },
      "requirements": [
        "Vontade de aprender a criar e gerenciar Banco de dados Mysql / MariaDb"
      ],
      "description": "Você quer dominar o MySQL e aprender Banco de Dados de forma clara, prática e com foco no mercado? Este curso é para você!\nNeste curso completo de Banco de Dados com MySQL, você aprenderá desde os conceitos fundamentais até recursos avançados como joins, subconsultas, funções agregadas, triggers, views, eventos e transações. Tudo isso por meio de aulas didáticas, com exemplos reais de aplicação.\nAo longo do curso, você irá:\nCriar bancos de dados e tabelas com boas práticas\nInserir, atualizar, consultar e apagar dados com comandos SQL\nEntender e aplicar os principais tipos de JOIN\nUtilizar funções agregadas e subconsultas em cenários reais\nCriar procedures, funções e triggers para automatizar tarefas\nUtilizar eventos agendados e recursos de segurança\nAprender como fazer backup, restaurar dados e controlar acessos\nEste curso é ideal para:\nIniciantes em banco de dados e MySQL\nEstudantes de cursos técnicos ou universitários\nDesenvolvedores web e mobile que desejam integrar bancos de dados\nProfissionais que buscam atualização ou aprofundamento na área\nPré-requisitos: Nenhum conhecimento prévio é necessário! Basta ter vontade de aprender e um computador com acesso ao MySQL (pode ser via XAMPP).\nPronto para dominar banco de dados e impulsionar sua carreira na tecnologia? Então, inscreva-se agora!",
      "target_audience": [
        "Iniciantes em programação e TI",
        "Estudantes de cursos técnicos ou superiores",
        "Profissionais de outras áreas migrando para TI",
        "Desenvolvedores Front-end que desejam se tornar Full Stack",
        "Profissionais de suporte técnico ou infraestrutura",
        "Empreendedores e donos de pequenos negócios"
      ]
    },
    {
      "title": "파이썬(Python) 딥러닝(Deep Learning,DL) 프로젝트 - Flask 웹 서빙 CNN 프로젝트",
      "url": "https://www.udemy.com/course/maso-ds-python-onc75/",
      "bio": "딥러닝(Deep Learning)으로 하는 실무 완벽 재현! CNN(합성곱신경망)과 플라스크(Flask), Ngrok까지 활용하는 파이썬(Python) 실전 프로젝트로 딥러닝 전문가로 레벨업하는 과정.",
      "objectives": [
        "딥러닝 개발 과정 프로세스의 이해",
        "CNN 구성 요소와 모델의 원리 이해",
        "CNN 모델의 성능을 높여주는 OpenCV 이해",
        "CNN 모델 실습을 통한 딥러닝 활용 능력"
      ],
      "course_content": {
        "딥러닝 실무 프로젝트의 개요와 개발 프로세스": [
          "DLP001 딥러닝 실무 프로젝트의 개요",
          "DLP002 CNN 모델 프로세스",
          "DLP003 실무 프로젝트 개발 과정 프로세스"
        ],
        "데이터셋 구성과 데이터 전처리": [
          "DLP101 이미지 수집을 위한 Flickr API 연결",
          "DLP102 Flickr API를 이용한 이미지 수집",
          "DLP103 이미지 Numpy 형식 변환의 작업 순서",
          "DLP104 이미지 Numpy 형식으로 변환하기",
          "DLP105 원-핫 인코딩과 데이터 구분",
          "DLP106 이미지 데이터 전처리"
        ],
        "CNN 모델 정의 컴파일": [
          "DLP201 CNN 구조와 활성화 함수",
          "DLP202 컨볼루션 연산과 스트라이드",
          "DLP203 CNN 모델 도식화",
          "DLP204 CNN 모델 정의와 컴파일 설정"
        ],
        "CNN 모델 실행": [
          "DLP301 모델 평가와 시각화"
        ],
        "이미지 증식 모델 개선": [
          "DLP401 OpenCV를 활용한 이미지 증식",
          "DLP402 증식된 이미지를 활용한 모델 평가"
        ],
        "CNN 모델 예측하기": [
          "DLP501 직접 찍은 사진을 이용한 모델 예측"
        ],
        "Flask 프레임워크를 이용한 CNN 모델 웹서빙": [
          "DLP601 웹서빙을 위한 Flask 서버 구성",
          "DLP602 Flask 서버를 이용한 웹페이지 제작"
        ],
        "Ngrok과 Flask를 이용한 웹서빙": [
          "DLP701 Ngrok 프레임워크를 활용한 외부 연결",
          "DLP702 포트 연결 해제와 강의 핵심 요약"
        ]
      },
      "requirements": [
        "본 강의는 기본적인 파이썬 활용 능력을 요구합니다.",
        "마소캠퍼스의 [파이썬(Python) 실무 데이터 분석 프로젝트] 강의들을 먼저 수강하시는걸 추천드립니다."
      ],
      "description": "\"어제보다 성장하겠습니다. 그리고, 어제보다 성장하려는 사람을 돕겠습니다.\"\n\n\n마소캠퍼스의 진심과 소망을 담은 Actionable Content로,\n2013년부터 온오프라인으로 함께해 온 누적 강의 1억시간!\n이 소중한 경험과 시간은 언제나 마소캠퍼스와 수강생 모두의 성장의 원천입니다.\n\n\n-\n딥러닝 CNN 알고리즘으로 이미지 분류 모델을 직접 만들어보고 싶어요!\n알파고와 이세돌의 바둑 대결 기억하시나요? 인공지능(AI) 기술이 얼마나 발전했는지 전세계인들에게 보여준 사건이었습니다.\n많은 언론에서는 앞다투어 AI 관련 기사를 쏟아냈습니다. 특히 머신러닝/딥러닝도 함께 주목받았는데요.\n당시 국내에서도 엄청난 열풍이 불었습니다. 서점에선 관련 도서들이 베스트셀러 자리를 차지했고, 너도나도 파이썬이나 텐서플로우같은 언어를 배우겠다며 학원 문을 두드렸죠.\n그런데 시간이 흐르자 열기가 점차 사그라들기 시작했습니다.\n다들 배운다고 하는데 정작 주변엔 딥러닝을 배운다는 사람은 없었습니다. 대체 뭐가 문제일까요?\n그건 아마도 너무 어렵기 때문일 겁니다. 아무리 쉬운 단어라고 해도 전문용어이기 때문에 쉽게 와닿지 않습니다.\n그래서 저희 마소캠퍼스가 여러분께 딥러닝에서 꼭 필요한 부분 중 하나인 CNN에 대해 알아보고 이미지 분류 프로젝트를 진행하며 딥러닝에 푹 빠지실 수 있도록 도와드리겠습니다.\n이번 <파이썬(Python) 딥러닝(Deep Learning,DL) 프로젝트 - Flask 웹 서빙 CNN 프로젝트> 과정에서는 누구나 쉽게 딥러닝을 배울 수 있도록 CNN의 개념부터 실제 구현까지 전반적인 과정을 다룰 예정이니 부담없이 따라오셔도 됩니다.\n본 강의를 통해 딥러닝의 중요 핵심 모델 중 이미지 인식 AI 모델인 CNN을 활용한 이미지 분류 프로젝트를 제작해보고,\n이를 바탕으로 인터넷 상에 돌아다니는 강아지나 고양이 등 생김새가 비슷하여 구분하기 어려운 다양한 이미지 파일들을 딥러닝 CNN 알고리즘을 활용하면 쉽게 구분할 수 있습니다.\n개와 고양이 사진을 분류해보고 싶은 당신을 위해, 딥러닝 핵심 알고리즘인 CNN을 활용한 본 강의를 적극 추천드립니다!\n\n<파이썬(Python) 딥러닝(Deep Learning,DL) 프로젝트 - Flask 웹 서빙 CNN 프로젝트> 강의를 듣고 나면,\n여러분께서는 다음과 같은 역량을 확보하실 수 있습니다.\n딥러닝 개발 과정 프로세스의 이해\nCNN 구성 요소와 모델의 원리 이해\nCNN 모델의 성능을 높여주는 OpenCV 이해\nCNN 모델 실습을 통한 딥러닝 활용 능력\n\n\n분야에 상관 없이 압도적인 생산성 향상을 가져다 주는 딥러닝!\n딥러닝의 핵심 모델 중 하나인 CNN으로 이미지를 분류해보는 실무 프로젝트를 처음부터 끝까지 실습해보는 과정!\n\n\n\n\n-\n\n\n[ 강 사  소 개 ]\n\n\n김 진 숙\n現 마소캠퍼스 수석 교수\n컴퓨터시스템 공학 석사\n김진숙 교수는 마소캠퍼스에서 빅데이터 부분 수석 교수로 빅데이터(R, 파이썬), HTML5/CSS3, JQueryMobile, 스크래치, 앱인벤터, IoT 등의 최신 IT 관련 기술 과정들까지 다양한 기업과 기관의 수강생들을 대상으로 열정 넘치는 강의를 이어가고 있습니다. 김진숙 교수는 스마트팜 IoT 프로젝트, 카 셰어링 앱 프로젝트 등 다수 프로젝트 지도 경력까지 겸비한 전문가입니다.\n-",
      "target_audience": [
        "이미지 분류를 이용해 업무 활용을 시도하고 싶은 실무자",
        "IT업계로 창업/이직/입사 등 커리어를 쌓고 싶은 모든 사람",
        "사업에 인공지능을 도입하고 싶은 경영자, 실무자",
        "딥러닝 역량을 쌓기 위해 CNN의 핵심 기법을 제대로 익혀 시작하고 싶은 모든 사람"
      ]
    },
    {
      "title": "Kaggle Master が教える Polars データ分析入門〜実践的なハンズオンで大規模なデータ処理を加速！〜",
      "url": "https://www.udemy.com/course/data-science-with-polars/",
      "bio": "Kaggler Master の称号を持ち Polars 本の著者でもある講師が Python の超高速・軽量ライブラリ『Polars』の基礎から実践まで徹底解説！＆ハンズオンによるコード解説！",
      "objectives": [
        "まだ情報が Web 上に少ない Polars について、特に基本的なデータ分析利用について体系的に学べます",
        "Polarsの基本操作を理解します",
        "pandas と Polars の対応関係を理解し、すでに持つデータ分析のスキルを活かしたまま、Polars のメリットを享受できるような実装が実現できます",
        "Polars の特徴を活かした実装を通して、効率的なデータ分析実践について学べます"
      ],
      "course_content": {
        "はじめに": [
          "コース概要",
          "講師の自己紹介",
          "このコースの進め方"
        ],
        "Polars の基礎知識": [
          "セクション 2 で使用するサンプルコード",
          "このセクションで学ぶこと",
          "Polars の概要①",
          "Polars の概要②",
          "Polars のインストール①",
          "Polars のインストール②",
          "Polars のインストール③",
          "Polars の基本操作",
          "このセクションのまとめ"
        ],
        "式とエクスプレッション": [
          "セクション 3 で使用するサンプルコード",
          "このコースの進め方",
          "式とエクスプレッション",
          "select の概要、ハンズオン①",
          "select の概要、ハンズオン②",
          "with_columns の概要、ハンズオン",
          "filter の概要、ハンズオン",
          "group_by/agg の概要、ハンズオン①",
          "group_by/agg の概要、ハンズオン②",
          "式とエクスプレッションを組み合わせた実装の概要、ハンズオン①",
          "式とエクスプレッションを組み合わせた実装の概要、ハンズオン②",
          "式とエクスプレッションを組み合わせた実装の概要、ハンズオン③",
          "このセクションのまとめ"
        ],
        "ハイパフォーマンスな記述法": [
          "セクション 4 で使用するサンプルコード",
          "このコースの進め方",
          "pandas-like な記述法",
          "map_elements について",
          "Lazy API①",
          "Lazy API②",
          "Lazy API③",
          "このセクションのまとめ"
        ],
        "Polars を使った実践データ分析": [
          "セクション 5 で使用するサンプルコード",
          "このコースの進め方",
          "データの読み込み・EDA（可視化）①",
          "データの読み込み・EDA（可視化）②",
          "特徴量エンジニアリング①",
          "特徴量エンジニアリング②",
          "特徴量エンジニアリング③",
          "機械学習モデルのトレーニング",
          "モデルの性能評価",
          "このセクションのまとめ"
        ],
        "おわりに": [
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "Google Colabを使用するため、学習に使用できる Google アカウントを所持していること",
        "関数やクラスなどのPythonの基本を理解している方",
        "pandasの基本を理解している方"
      ],
      "description": "本コースの概要\nデータ分析や機械学習の現場で、より大量のデータを効率的に処理するニーズが高まる中、新しいデータフレームライブラリ ”Polars” が注目を集めています。\n\n\n本コースでは、『Polarsとpandasで学ぶデータ処理アイデアレシピ55』の著者であり、Kaggle Competitions Master の称号も持つ講師が、Python の定番ライブラリである pandas を超える高速性と効率性を備えた ”Polars” の使い方からデータ分析・機械学習への実践的活用方法まで徹底解説します！\n\n\n本コースのカリキュラム\nPolars の基礎知識\nPolars の概要\nPolars とは\nPolarsは何故速い？\nPolars の始め方\nPolars の始め方\nGoogle Colaboratory の使い方\nPolars の基本操作（ハンズオン）\n式とエクスプレッション\n式とエクスプレッションとは\nselect(...)\nwith_columns(...)\nfilter(...)\ngroup_by(...).agg(...)\n式とエクスプレッションを組み合わせた実装\nハイパフォーマンスな記述法\npandas-like な記述法\nmap_elements について\nlazy API\n遅延評価\n遅延評価機能 lazy API の使い方\nStreaming API\nlazy API の活用方針\nPolars を使った実践データ分析\nデータの読み込み・EDA（可視化）\n特徴量エンジニアリング\n機械学習モデルのトレーニング\n機械学習モデルの性能評価\n\n\n本コースで得られるもの\nPolarsを用いた高速データ処理スキル\n大規模データの効率的な分析手法\n機械学習モデルの前処理を最適化するテクニック",
      "target_audience": [
        "Polars について学びたいが、情報がなく困っている人",
        "Kaggle やデータ分析の現場で話題の Polars について簡単に触れたい人",
        "pandas の実行パフォーマンスをより良くするために、同じ処理内容を Polars に書き換えたい人",
        "データ分析の基礎実装を学びたい人"
      ]
    },
    {
      "title": "A-Z™ | Tensorflow ile Derin Öğrenme | 2023",
      "url": "https://www.udemy.com/course/a-ztm-tensorflow-ile-derin-ogrenme-2023/",
      "bio": "Sıfırdan Tensorflow ile Derin Öğrenmede Uzman Ol",
      "objectives": [
        "Derin öğrenme ve Evrişimsel Sinir Ağlarının Teorisini Öğreneceğiz.",
        "A-Z - Tensorflow ile Derin Öğrenme Algoritmalarının Nasıl Kodlanacağı Öğreneceğiz.",
        "Convolutional Neural Networks Algoritması Kullanarak Görüntü Sınıflandırmanın TensorFlow ile Nasıl Yapılacağını Öğreneceğiz.",
        "TensorFlow ile sıfırdan Yapay Sinir Ağlarının Nasıl İnşa Edileceğini Öğreneceğiz.",
        "Tensorflow ile Bonus Projeler Yapacağız.",
        "Weights & Biases (WandB) ile Tensorflow'da Modellerimizi Görselleştirip Nasıl Analiz Edileceğini Öğreneceğiz.",
        "Tensorflow ile Lite Object Detection Modellerinde Eğitim Yapıp Android'de Çalıştıracağız.",
        "NOT : Bu kurs çoğunlukla uygulama içerir. Proje ve uygulama üzerine oluşturulmuştur. Herşeyi sıfırdan yapıyoruz.",
        "Herşeyi kendi datamızla yapıyoruz. Kesinlekle hazır dataset kullanmıyoruz.",
        "Sizde izleyerek kendi datasetinizle proje yapabilirsiniz.",
        "Lite modellerde kendi verileriniz ile detection ve sınıflandırma eğitimleri yaparak android'de çalıştırabilirsiniz."
      ],
      "course_content": {
        "Giriş": [
          "Tanıtım",
          "Kurulumlar"
        ],
        "Derin Öğrenme Teori": [
          "Giriş",
          "Derin Öğrenme Nedir",
          "Yapay Sinir Ağları",
          "Aktivasyon Fonksiyonları",
          "Loss (Kayıp) Fonksiyonları",
          "Optimizasyon Algoritmaları"
        ],
        "CNN (Convolutional Neural Networks) Teori": [
          "Giriş",
          "Evrişim İşlemi",
          "CNN (Convolutional Neural Networks)",
          "Piksel Ekleme (Padding)",
          "Adım Kaydırma (Stride)",
          "Ortaklama (Pooling)"
        ],
        "Ek Teori": [
          "Epoch ve Batch Size",
          "Dropout",
          "Early Stopping",
          "Learning Rate"
        ],
        "Tensorflow ile Derin Öğrenme": [
          "Giriş",
          "Tensorflow Temelleri",
          "Veriyi Hazırlama",
          "Model Oluşumu Sequential",
          "Model Egitimi",
          "Model Testi | 1. Kısım",
          "Model Testi | 2. Kısım",
          "Modeli Kaydetme/Yükleme - Save/Load",
          "Model Sonuçlarını Görselleştirme",
          "Modelin Ara Katmalarını Görselleştirme",
          "Functional Bir Model Oluşturma",
          "Callbacks | 1. kısım",
          "Callbacks | 2. kısım",
          "Data Augmentation - Veri Arttırma | 1. Kısım",
          "Data Augmentation - Veri Arttırma | 2. Kısım",
          "Transfer Learning - VGG",
          "Hazır Model Kullanma - VGG"
        ],
        "Tensorflow ile Trafik İşaretlerini Sınıflandırma | Bonus Proje": [
          "Giriş",
          "Veriyi Hazırlama",
          "Model Eğitimi ve Test",
          "Real Time'da Test"
        ],
        "Tensorflow'da Weights & Biases (WandB) | Özel Veri": [
          "Wandb ile Keras'da Temel Fonsiyonlar",
          "Wandb ile Keras'da Sweepler",
          "Wandb ile Keras'da Sweep - Bonus Video"
        ],
        "Tensorflow Lite - Android App - Object detection - İmage Classification": [
          "EfficientDet Lite Model Eğitimi - Object detection",
          "EfficientDet Lite Modeli Android'de Çalıştırma 1 - Object detection",
          "EfficientDet Lite Modeli Android'de Çalıştırma 2 - Object detection",
          "EfficientDet Lite Model Testi Telefonda - Object detection",
          "EfficientDet Lite Model Eğitimi- İmage Classification",
          "Pose Estimation",
          "Pose Estimation Test - Telefonda"
        ]
      },
      "requirements": [
        "Öğrenme Arzusu ve Azim",
        "Temel Python Bilgisi",
        "Temel Seviyede Derin Öğrenme Bilgisi ve Konulara Aşinalık",
        "Temel Düzeyide Matematik Bilgisi"
      ],
      "description": "A-Z™ | Tensorflow ile Derin Öğrenme\nKursumuzda klasik ve derin öğrenme tabanlı yöntemlerini kullanarak sınıflandırma nasıl yapıldığını öğrenip, Tensorflow kütüphaneleriyle gerçek hayat projeleri yapacağız.\nProjelerle Yapay Zeka ve Bilgisayarlı Görü Kursu İçeriği\nGiriş Bölümü\nDerin Öğrenme Teori\nDerin Öğrenme Nedir\nYapay Sinir Ağları\nAktivasyon Fonksiyonları\nOptimizasyon Algoritmaları\nLoss (Kayıp) Fonksiyonları\nDerin Öğrenme Teori\nCNN (Convolutional Neural Networks) Teori\nEvrişim İşlemi\nCNN (Convolutional Neural Networks)\nPiksel Ekleme (Padding)\nAdım Kaydırma (Stride)\nOrtaklama (Pooling)\nEk Teori\nEpoch ve Batch Size\nDropout\nEarly Stopping\nLearning Rate\nTensorflow ile Derin Öğrenme\nTensorflow Temelleri\nVeriyi Hazırlama\nModel Oluşumu Sequential\nModel Egitimi\nModel Testi | 1. Kısım\nModel Testi | 2. Kısım\nModeli Kaydetme/Yükleme - Save/Load\nModel Sonuçlarını Görselleştirme\nModelin Ara Katmalarını Görselleştirme\nFunctional Bir Model Oluşturma\nCallbacks | 1. kısım\nCallbacks | 2. kısım\nData Augmentation - Veri Arttırma | 1. Kısım\nData Augmentation - Veri Arttırma | 2. Kısım\nTransfer Learning - VGG\nHazır Model Kullanma - VGG\nTensorflow ile Trafik İşaretlerini Sınıflandırma\nVeriyi Hazırlama\nModel Eğitimi ve Test\nReal Time'da Test\nTensorflow'da Weights & Biases (WandB) | Özel Veri\nWandb ile Keras'da Temel Fonsiyonlar\nWandb ile Keras'da Sweepler\nWandb ile Keras'da Sweep - Bonus Video\nTensorflow Lite - Android App - Object detection - İmage Classification\nEfficientDet Lite Model Eğitimi - Object detection\nEfficientDet Lite Modeli Android'de Çalıştırma 1 - Object detection\nEfficientDet Lite Modeli Android'de Çalıştırma 2 - Object detection\nEfficientDet Lite Model Testi Telefonda - Object detection\nEfficientDet Lite Model Eğitimi- İmage Classification\nPose Estimation\nPose Estimation Test - Telefonda\nMerhaba . Ben Yaşar Niyazoğlu. Uludağ Üniversitesi önlisans mekatronik mezunuyum. Üniversiteye başlamamdan itibaren derin öğrenme alanına büyük ilgi duyduğum için erken zamanda bu alana yöneldim. İlk olarak Makine Öğrenmesi - Veri bilimi ile başladım . Daha sonra ise Derin öğrenme ve Bilgisayarlı Görü Alanlarında kendimi geliştirdim ve halen de geliştirmekteyim. Özel bir şirkette Yapay Zeka ve Bilgisayarlı Görü Mühendisi olarak görev almaktayım. Birçok şirkete , kişilere ve Teknofestdeki gibi başta olmak üzere birçok yarışmaya katılan takımlara danışmanlık yapıyorum. Ülkemizde bu konularda kaynak olarak eksiklikler olduğu için, Udemy ve Youtube gibi platformlarda içerik üretiyorum. Ürettiğim içerikler Ülkemizde ve hatta Global çapta olmayan içerikler. Eğer siz de kendinizi bu alanlarda geliştirmek ve proje oluşturacak bilgi seviyesine ulaşmak istiyorsanız bu eğitim tam size göre olacaktır.",
      "target_audience": [
        "Geleceğin mesleklerinde yetkin olmak isteyen herkes",
        "Yapay zekaya ilgi duyan herkes",
        "Derin öğrenme konusundaki teorik ve uygulama bilgisiyle gerçek hayat problemlerini çözmek isteyenler",
        "Python programlama dili ile TensorFlow/Keras kütüphanelerini kullanarak kendi derin öğrenme modelini tasarlamak isteyenler",
        "Derin öğrenme yöntemleri ile sınıflandırmanın nasıl yapılacağını öğrenmek isteyenler",
        "Görüntü işlemenin Python ile nasıl kodlanacağını öğrenmek isteyenler",
        "Dünyada en çok kullanılan yapay zeka kütüphanelerinden olan TensorFlow/Keras kütüphanesini sıfırdan öğrenmek isteyenler"
      ]
    },
    {
      "title": "Herkes İçin Python: Adım Adım Programlama Becerileri",
      "url": "https://www.udemy.com/course/herkes-icin-python-101/",
      "bio": "Python ile Programlamaya Giriş: Temelden Uzmanlığa, Bolca Quiz sayesinde Sınavlara Hazırlık",
      "objectives": [
        "Temel Seviye Python Programlama / Kodlama Becerilerinin Kazanılması",
        "Python kullanılan ekiplerde yer alabilme",
        "Python'la düşünebilme, Python'ca düşünebilme",
        "Gündelik ve iş hayatınızdaki problemleri Python'la çözebilmek"
      ],
      "course_content": {
        "Giriş": [
          "Tanıtım",
          "Google Colab Tanıtım",
          "Python Tanıtım"
        ],
        "Temel Python Veri Yapıları": [
          "Python Obje oluşturma",
          "Python Variables Boolean",
          "Python Variables String-Numbers",
          "Python Örnekler ve Type Casting",
          "Python Variables Dictionary",
          "Python Variables List-Tuple-Set 1. Kısım",
          "Python Variables List-Tuple-Set 2. Kısım",
          "Python Variables Quiz"
        ],
        "Python Koşullar (If-Else) / Operatörler": [
          "Python Koşullar ve Operatörler Bölüm 1",
          "Python Koşullar ve Operatörler Bölüm 2",
          "Python Döngüler (For Döngüleri) Bölüm 1",
          "Python Döngüler (For Döngüleri) Bölüm 2",
          "Python Döngüler While - Python Exceptions",
          "Python If-Else / Operatörler Quiz"
        ],
        "Python Fonksiyonlar": [
          "Python Fonksiyonlar Bölüm 1",
          "Python Fonksiyonlar Bölüm 2",
          "Python Fonksiyon Sınavı"
        ],
        "Python Dosya Operasyonları": [
          "Python Dosyalara Yazma - Okuma",
          "Python Dosya Operasyonları Sınavı"
        ],
        "Python Nesne Tabanlı Programlama": [
          "Python Class",
          "Python Inhertance",
          "Python Nesne Tabanlı Programlama Sınav"
        ]
      },
      "requirements": [
        "Bilgisayar ve internet bağlantısı yeterli olacak, 101 kursu olduğu için kurs öncesinde programlama bilgisine ihtiyaç yoktur."
      ],
      "description": "Kursa Genel Bakış: Python, dünyanın en popüler programlama dillerinden biridir ve bu kurs, Python programlamaya giriş yapmak isteyen herkes için mükemmel bir başlangıç noktasıdır. Kurs içerisinde bol bol soru bulunmaktadır. \"Herkes İçin Python: Adım Adım Programlama Becerileri\" kursu, Python programlama dilinin temellerini adım adım öğretmeyi amaçlamaktadır. Sıfırdan başlayarak, Python'un temel yapılarını, veri türlerini, kontrol akışlarını ve fonksiyonlarını kapsamlı bir şekilde ele alacağız. Başlıca\n1- Giriş\n2- Python'a Giriş\n3- Python Temelleri\n4- Koşullu İfadeler ve Döngüler\n5- Veri Tipleri\n6- Fonksiyonlar\n7- Modüller ve Paketler\n8- Fonksiyonel Programlama\n9- Dosya İşlemleri\n10- Nesne Yönelimli Programlama\nkonularından bahsedeceğiz.\nBol bol quiz sorularıyla sınavlara hazırlanacaksınız.\nKime Hitap Ediyor? Bu kurs, programlamaya yeni başlayanlar ve temel programlama becerilerini geliştirmek isteyen herkes için idealdir. Özellikle, Python ile ilk adımlarını atmak isteyen öğrenciler, hobi olarak programlama öğrenmek isteyenler veya kariyerlerine yeni bir yön vermek isteyen profesyoneller için tasarlanmıştır.\n\n\nÖğrenim Sonuçları: Kursu tamamladığınızda, Python programlama dilinin temel prensiplerini anlayacak ve basit programlar yazabileceksiniz. Ayrıca, Python ile veri analizi ve manipülasyonu gibi daha ileri konulara geçiş yapmak için sağlam bir temel oluşturmuş olacaksınız.\n\nKatılım Şekli: Kurs, interaktif video dersleri, pratik örnekler, alıştırmalar ve gerçek dünya projeleri içermektedir. Öğrenciler, kurs boyunca aktif olarak kod yazacak ve öğrendiklerini pekiştireceklerdir.\n\nSizi Neler Bekliyor? Python programlamaya adım atmak ve bu heyecan verici dünyanın kapılarını aralamak için \"Herkes İçin Python: Adım Adım Programlama Becerileri\" kursuna katılın. Programlama becerilerinizi geliştirin, yeni fırsatların kapılarını aralayın ve teknoloji dünyasında yerinizi alın!\n\nEğitmeniniz: Python ve veri bilimi alanında geniş bir tecrübeye sahip olan Kaan Özbudak, bu kursu, öğrencilerin Python programlamayı kolay ve etkili bir şekilde öğrenmeleri için özel olarak tasarlamıştır.",
      "target_audience": [
        "Başlangıç düzeyinde Python öğrenmek isteyen herkes için uygun bir video serisi"
      ]
    },
    {
      "title": "Integração entre Google BigQuery e GA4",
      "url": "https://www.udemy.com/course/integracao-entre-google-bigquery-e-ga4/",
      "bio": "Aprenda a realizar consultas SQL em dados do Google Analytics 4",
      "objectives": [
        "Como realizar o cadastro no BigQuery",
        "Aprender sobre a interface do BigQuery",
        "Entender sobre a interface do Google Analytics 4",
        "Saber quais são as diferenças entre Universal Analytics e GA4",
        "Aprender a fazer a integração entre GA4 e BigQuery",
        "Aprender qual é a estrutura de exportação das tabelas do GA4 para o BigQuery",
        "Aprender sobre CROSS JOIN, UNNEST e _TABLE_SUFFIX",
        "Calcular métricas comuns do GA4, mas usando linguagem SQL"
      ],
      "course_content": {
        "Apresentação": [
          "Apresentação do Instrutor",
          "Módulos",
          "Ferramentas, Dicas e Contato"
        ],
        "Google BigQuery": [
          "Definição",
          "Cadastro na Plataforma",
          "Interface da Plataforma"
        ],
        "Integração com GA4": [
          "Interface da Plataforma",
          "Integração com BigQuery",
          "Estrutura das Tabelas"
        ],
        "Exemplos de Queries no BigQuery": [
          "Explicação do CROSS JOIN",
          "Explicação do comando UNNEST",
          "Explicação do _TABLE_SUFFIX",
          "Número de Sessões e Sessões Engajadas",
          "Taxa de Bounce",
          "Média de Tempo de Sessão",
          "Visualizações de Página por Sessão e por Usuário",
          "Número de Pedidos",
          "Taxa de Conversão"
        ]
      },
      "requirements": [
        "Saber SQL (pelo menos o básico)",
        "Força de Vontade",
        "Você não precisa ser da área de exatas para realizar esse curso"
      ],
      "description": "AULAS 100% ATUALIZADAS, EM 2024, COM A NOVA INTERFACE DO BIGQUERY E DO GOOGLE ANALYTICS\nCOM CERTIFICADO\n\n\nSOBRE O CURSO\nEsse NÃO é mais um curso complicado, sem explicações claras ou exemplos práticos para o mercado de trabalho.\n\n\nEsse curso É um jeito simples de você aprender a criar consultas SQL nos dados do GA4, através da integração entre BigQuery e Google Analytics 4.\n\n\nO GA4 (Google Analytics 4) é a versão mais recente da plataforma de análise web do Google, que oferece recursos avançados de rastreamento e relatórios. Ele utiliza uma abordagem baseada em eventos para coletar dados, permitindo um entendimento mais detalhado do comportamento do usuário.\n\n\nGA4 integra-se melhor com aplicativos e sites, proporcionando uma visão unificada dos dados. Além disso, ele oferece melhor conformidade com as regulamentações de privacidade e ferramentas aprimoradas de aprendizado de máquina para previsões e insights.\n\n\nVocê não precisa ter experiência na área de Dados ou exatas para acompanhar todo o curso, que foi pensado com didática simples e módulos progressivos para você avançar com segurança!\n\n\nComece hoje a explorar a área de Business Intelligence e Ciência de Dados com tranquilidade. Mesmo que já esteja na área, essa é a oportunidade para você melhorar suas habilidades com uma linguagem nova.\n\n\nCada vez mais o mercado de trabalho exige de vários profissionais o conhecimento para análise de dados! Aprenda a extrair informações de diferentes bases para relacionar e criar análises estratégicas.\n\n\nLEMBRE-SE: A área de dados é a MAIS QUENTE do mercado atualmente. Os salários podem chegar a 22 mil reais! Então investir em seu desenvolvimento é a melhor escolha para sua carreira de sucesso. Fonte: UOL\n\n\nAbaixo a trilha ideal em direção ao sucesso na área de dados:\nLinguagem SQL para Análise de Dados\nIntegração entre Google BigQuery e GA4 (este curso que você está visualizando)\nCriação com Dashboards com Looker Studio\nPython: Manipulação de Dados com Pandas\nMachine Learning: Clusterização com Linguagem Python\nMachine Learning: Classificação com Linguagem Python\nMachine Learning: Regressão com Linguagem Python\n\n\nSOBRE O INSTRUTOR\nMe chamo Caio Avelino, e o conhecimento que vou dividir com você nesse curso foi adquirido, principalmente, com minha experiência no mercado de trabalho. Atuo nas áreas de Business Intelligence, Ciência de Dados e Inteligência Artificial há anos e tive a oportunidade de desenvolver minhas habilidades em diversas startups.\n\n\nAté mais!",
      "target_audience": [
        "Iniciantes e Curiosos sobre Business Intelligence",
        "Alunos de Ciência de Dados",
        "Funcionários de uma empresa, de qualquer área, que gostariam de saber realizar queries nos dados do GA4",
        "Profissionais de Marketing que gostariam de trazer mais poder de análise",
        "Analistas de BI",
        "Qualquer pessoa, de qualquer área que deseja entrar no Mundo de Dados"
      ]
    },
    {
      "title": "Машинное обучение: выделение факторов на Python",
      "url": "https://www.udemy.com/course/ittensive-machine-learning-features/",
      "bio": "Выигрываем хакатон по выделению факторов: линейная регрессия, взаимная информация, PCA, ICA, NMF, MDS, t-SNE",
      "objectives": [
        "Процесс и модель машинного обучения",
        "Заполнение пропусков в данных",
        "Линейная регрессия и L1/L2 регуляризация",
        "Решающие деревья и ансамбли стекинга",
        "Корреляция и взаимная информация",
        "Метод главных компонент (PCA)",
        "Сингулярное разложение (SVD)",
        "Анализ независимых компонент (ICA)",
        "Многомерное шкалирование (MDS)",
        "t-SNE, UMAP, LargeVis"
      ],
      "course_content": {},
      "requirements": [
        "Продвинутый Python",
        "Основы математической статистики"
      ],
      "description": "Внимание: для доступа к курсам ITtensive на Udemy напишите, пожалуйста, на support@ittensive.com с названием курса или группы курсов, которые хотите пройти.\n\n\nМы разберем задачу хакатона 2020 года по выделению факторов, в наибольшей степени влияющих на продолжительность жизни в России, с точки зрения фундаментальных и прикладных подходов к понижению размерности данных. В заключении построим ансамбль моделей для предсказания продолжительности жизни, базируясь на выделенных факторах.\nКурс разбит на 4 части. В первой части мы последовательно пройдем все этапы работы с данными: от видов задач и их постановки до работы с моделями машинного обучения для минимизации предсказательной ошибки. Дополнительно рассмотрим фундаментальные основы построения моделей машинного обучения, базовые метрики и наиболее простые модели - линейную регрессии, решающие деревья и случайный лес. А также ансамбли машинного обучения.\nВо второй части на практике разберем:\nОчистку и предобработку данных - ETL\nЛинейную регрессию для экстраполяции данных\nЛинейную регрессию с регуляризацией для выделения факторов\nИнформационные критерии понижения размерности\nВ заключении создадим ансамбль стекинга из простых моделей понижения размерности.\nТретья часть посвящена матричным методам:\nМетод главных компонент (PCA)\nСингулярное разложение (SVD)\nАнализ независимых компонент (ICA)\nПоложительно-определенные матрицы (NMF)\nУточним решение задачи обучения без учителя через матричные методы.\nВ четвертой части рассмотрим нелинейные подходы:\nМногомерное шкалирование (MDS).\nt-SNE\nUMAP\nLargeVis\nСтабилизируем ансамбль понижения размерности и используем его для предсказания продолжительности жизни в России, основываясь на наиболее важных макроэкономических показателях.",
      "target_audience": [
        "Аналитики Python, изучающие машинное обучение",
        "Программисты больших данных",
        "Исследователи больших данных"
      ]
    },
    {
      "title": "Árbol de decisiones para machine learning en python.",
      "url": "https://www.udemy.com/course/arboles-de-desiciones-para-machine-learning-en-pyton/",
      "bio": "Programa tus propios árboles de decisión en python desde cero.",
      "objectives": [
        "Algoritmos de los árboles de decisión mas famosos.",
        "Codificar su propio árbol desde 0.",
        "Aplicar los algoritmos a varios dataset."
      ],
      "course_content": {
        "Introducción": [
          "Introducción",
          "Elementos del árbol de decisión",
          "Dataset y exploración",
          "Entropia clase",
          "Entropia por atributo y probabilidades",
          "Ganancia de información",
          "Construcción del árbol ID3",
          "¿Como usar otros dataset?",
          "Contenido",
          "Árbol C4.5 diferencias",
          "Split y Gain ratio",
          "Valores continuos 1",
          "Valores continuos 2",
          "Final árbol C4.5"
        ]
      },
      "requirements": [
        "Conocimiento básico de python",
        "Tener instalado python"
      ],
      "description": "Los árboles de decisión siguen siendo uno de los algoritmos mas potentes a día de hoy, además de sus buenos, éstos  dicen el por que dan una respuesta u otra de acuerdo a una serie de reglas. Otros algoritmos como redes neuronales son muy potentes pero no se sabe porque  llegan a  respuesta, por lo que es mas difícil corregir el verdadero problema.",
      "target_audience": [
        "Desarrolladores que quieren aprender sobre arboles decisiones"
      ]
    },
    {
      "title": "Neuronale Netze Optimieren und Evaluieren mit Tensorflow 2",
      "url": "https://www.udemy.com/course/neuronale-netze-optimieren-und-evaluieren-mit-tensorflow-2/",
      "bio": "Lerne warum dein Neuronales Netz schlecht performt und optimiere es anhand moderne Techniken",
      "objectives": [
        "Lerne die Grundlagen von Python",
        "Nutze NumPy, um Berechnungen durchzuführen und Daten zu generieren",
        "Erkenne Ursachen von schlechter Performance deiner Neuronalen Netze",
        "Nutze Tensorflow Callbacks um deine Netze zu optimieren",
        "Nutze Techniken wie Data Augmentation, Normalisierung & mehr.",
        "Analyse deine Netze mit Tensorboard und Callbacks"
      ],
      "course_content": {
        "Einleitung": [
          "Über mich",
          "Zusammenfassung Inhalt des Kurses",
          "Bedienung von Udemy",
          "Wie bewerte ich diesen Kurs?",
          "Kursmaterialien"
        ],
        "Einführung & Installations": [
          "Einführung Python und Visual Studio Code",
          "[Windows] Installation Python und Anaconda",
          "[MacOS] Installation Python Anaconda",
          "Einführung Tensorflow, Keras und andere Module",
          "Tensorflow CPU oder GPU?",
          "[Windows] Installation Tensorflow [CPU]",
          "[Windows] Installation Tensorflow [GPU]",
          "[Windows] Installation von VS Code",
          "[Mac] Installation Tensorflow",
          "[Mac] Installation von Python module",
          "[Mac] Installation VS Code"
        ],
        "Python Basics": [
          "Einführung VS Code",
          "Unsere erste Python-Datei",
          "Syntax",
          "Variablen",
          "Zahlen",
          "Casting",
          "Strings",
          "Operatoren",
          "Nutzer Eingaben nutzen mithilfe von input"
        ],
        "Datenstrukturen": [
          "Datenstruktur: Listen (1)",
          "Datenstruktur: Listen (2)",
          "Datenstruktur: Tupels",
          "Datenstruktur: Sets",
          "Datenstruktur: Dictionaries"
        ],
        "Schleifen und Kontrollstrukturen": [
          "Kontrollstruktur: if.. else",
          "Schleife: While",
          "Schleife: For",
          "List Comprehension (1)",
          "List Comprehension (2)"
        ],
        "NumPy": [
          "Jupyter Notebook",
          "Lists vs. Arrays",
          "For Schleife vs. Dot-Funktion",
          "Vektoren und Matrizen",
          "Automatisch Matrizen generieren",
          "Weitere Matrix operationen"
        ],
        "Use Case: MNIST": [
          "Einführung des Datasets",
          "Dataset Klasse (1) - Klasse anpassen",
          "Model (1) - Imports erläutern",
          "Model (2) - Layer definieren",
          "Model (3) - Trainieren und Evaluieren",
          "Evaluation: Confusion Matrix",
          "Evaluation: Bilder am Model testen"
        ],
        "Use Case: Katze vs. Hund": [
          "Einführung des Datasets",
          "Ordnerstruktur vorstellen",
          "Dataset Klasse (1) - Klasse anpassen",
          "Dataset Klasse (2) - Bilder einlesen",
          "Model (1) - Imports erläutern",
          "Model (2) - Layer definieren",
          "Model (3) - Kompilieren und Trainieren",
          "Evaluation: Loss und Accuracy untersuchen",
          "Vorwort: Data Augmentation",
          "Optimierung: Data Augmentation",
          "Evaluation: Bilder und Data Augmentation",
          "Evaluation: Bilder am Model testen"
        ],
        "Optimierung von Neuronale Netze": [
          "Evaluations-Tool: Tensorboard",
          "Integration von Tensorboard",
          "Optimierung: Normalisierung vs. Non-Normalisierung",
          "Overfitting: Vorteile von Data Augmentation",
          "Overfitting: Features/Netzwerk vergrößern",
          "Overfitting: Dropout",
          "Overfitting: Batch Normalization",
          "CNN: GlobalAveragePooling anstatt Fully Connected",
          "Einführung in Callbacks",
          "ModelCheckpoint: Gewichte abspeichern",
          "EarlyStopping: Training frühzeitig abbrechen",
          "ReduceLRonPlateu: Lernrate anpassen, genauigkeit erhöhen."
        ],
        "Fazit und wie geht es jetzt weiter?": [
          "Die nächsten Schritte"
        ]
      },
      "requirements": [
        "Neuronale Netze sollten dir ein Begriff sein.",
        "Lust etwas neues zu lernen :)"
      ],
      "description": "Deep Learning ist eines der angesagtesten Themen weit und breit. Insbesondere wird Deep Learning und Künstliche Neuronale Netze in vielen Technologien in deinem Umfeld eingesetzt, um dir ein noch angenehmeres Leben zu ermöglichen.\nMithilfe diesen Praxis-Kurs bringe ich dir bei wie man erkennen kann, ob Neuronale Netze schlecht performen. Du lernst an welchen Metriken du die Genauigkeit eines Netzes messen kannst. Natürlich lernst du auch Techniken wie du die Performance des Netzwerkes untersuchen und natürlich optimieren kannst!\nDas wirst du Lernen:\nLerne die Grundlagen von Python\nNutze NumPy, um Berechnungen durchzuführen und Daten zu generieren\nErkenne Ursachen von schlechter Performance deiner Neuronalen Netze\nNutze Tensorflow Callbacks um deine Netze zu optimieren\nNutze Techniken wie Data Augmentation, Normalisierung & mehr\nLerne wie du Overfitting und Underfitting im Netzwerk vermeidest\nLerne wie du die richtige Topologie deines Netzwerks wählst.\nAnalyse deine Netze mit Tensorboard und Callbacks\nWarum solltest du Tensorflow lernen? Tensorflow wird von den \"Big Five\" Unternehmen wie Apple, Google, Facebook, Amazon und Microsoft in vielen ihrer Produkte eingesetzt, um Machine Learning noch effizienter zu nutzen! Ebenfalls werde ich ihn auch immer auf dem neusten Stand der Technik und Wissenschaft halten.\nLerne wie du Tensorflow meisterst und schreibe dich JETZT ein!\nWarum solltest du Tensorflow lernen? Tensorflow wird von den \"Big Five\" Unternehmen wie Apple, Google, Facebook, Amazon und Microsoft in vielen ihrer Produkte eingesetzt, um Machine Learning noch effizienter zu nutzen! Ebenfalls werde ich ihn auch immer auf dem neusten Stand der Technik und Wissenschaft halten.\nLerne wie du Tensorflow meisterst und schreibe dich JETZT ein!",
      "target_audience": [
        "Softwareentwickler, Data Scientists und Machine Learning Experten",
        "Studenten der Informatik, Bioinformatik, IT-Sicherheit, Mathematik etc.",
        "Python Entwickler"
      ]
    },
    {
      "title": "[FR] De la Recette au Chef : Devenez Ingénieur en LLM",
      "url": "https://www.udemy.com/course/de-la-recette-au-chef-devenez-ingenieur-en-llm/",
      "bio": "Maîtrisez les LLM sans coder ! Apprenez l’IA avec des analogies culinaires amusantes. (AI)",
      "objectives": [
        "Comprenez ce que sont les grands modèles de langage (LLM) et comment ils fonctionnent à l’aide d’analogies concrètes",
        "Identifiez les ingrédients clés qui alimentent les LLM, tels que les données d’entraînement, la tokenisation et la qualité des données.",
        "Expliquez comment les LLM sont entraînés à l’aide de concepts tels que les lots, les époques et les fonctions de perte.",
        "Rédigez de meilleurs prompts en utilisant des techniques comme le zero-shot, le few-shot et le chain-of-thought.",
        "Personnalisez des modèles à l’aide du fine-tuning et d’outils comme Hugging Face et LoRA.",
        "Évaluez les performances du modèle à l’aide de mesures quantitatives et qualitatives.",
        "Déployez des LLM à l’aide d’API, de FastAPI/Flask et hébergez-les sur des plateformes comme Hugging Face Spaces.",
        "Créez des applications complètes alimentées par des LLM à l’aide d’outils no-code et de LangChain.",
        "Surveillez et améliorez vos modèles d’IA à l’aide de journaux, de boucles de rétroaction et de tests A/B.",
        "Surveillez et améliorez vos modèles d’IA à l’aide de journaux, de boucles de rétroaction et de tests A/B."
      ],
      "course_content": {
        "Qu’est-ce qui mijote ? Introduction aux LLM": [
          "Introduction à “Qu’est-ce qui mijote ? Introduction aux LLM”",
          "Qu’est-ce qu’un modèle de langage ?",
          "L’évolution des LLM – Des machines à écrire aux robots gastronomiques",
          "Comment les LLM “prédisent le mot suivant”(Préparation automatique de sandwichs)",
          "Différences entre LLMs et IA traditionnelle (Micro-ondes vs Cuisine de Chef)",
          "Tour d’horizon des LLM populaires: GPT, Claude, Gemini, LLaMA"
        ],
        "Les ingrédients comptent – Comprendre les données": [
          "Introduction à “Les ingrédients comptent – Comprendre les données”",
          "Qu’est-ce que les données d'entraînement ? (Remplir le garde-manger)",
          "Tokenisation – Découper le texte en morceaux digestes",
          "Jeux de données pour LLM : Wikipédia, livres, textes web",
          "Des données de mauvaise qualité = résultats médiocres",
          "Biais dans les données = Trop épicé pour les uns, trop fade pour les autres"
        ],
        "Cuisiner à grande échelle – Bases de l'entraînement des modèles": [
          "Introduction à “Cuisiner à grande échelle – Bases de l'entraînement des modèles”",
          "Que se passe-t-il lors de l'entraînement d’un modèle? (Mélanger, cuire, ajuster)",
          "Époques, lots et perte – Les cycles de cuisson",
          "GPUs et TPUs – Les fours industriels de l'entraînement",
          "Pré-entraînement vs Fine-tuning – Recette maîtresse vs Variante régionale",
          "Coût de l'entraînement – L’addition du supermarché des LLM"
        ],
        "Ingénierie de Prompt – Assaisonner pour un résultat parfait": [
          "Introduction à “Ingénierie de Prompt – Assaisonner pour un résultat parfait”",
          "Anatomie d’un prompt – Le mélange secret d’épices",
          "Styles de prompts: Zero-shot,Few-shot,Chain-of-Thought(Comme sel, piment, herbes",
          "Prompts en jeu de rôle – “Fais comme si tu étais un barista”",
          "Optimisation des prompts – De cru à bien cuisiné",
          "Évaluation des prompts – Test de goût des prompts"
        ],
        "Fine-Tuning – Personnaliser la recette": [
          "Introduction à “Fine-Tuning – Personnaliser la recette”",
          "Qu’est-ce que le Fine-Tuning ? – La touche de grand-mère à une recette classique",
          "Apprentissage par transfert– Réutiliser une base de gâteau et ajouter le glaçage",
          "Techniques : Fine-Tuning complet vs LoRA (Adaptation à faible rang)",
          "Fine-Tuning avec vos propres données (Votre cuisine, vos règles)",
          "Outils pour Fine-Tuning : Hugging Face, Google Colab, PEFT"
        ],
        "Évaluer les LLM – Test de goût": [
          "Introduction à “Évaluer les LLM – Test de goût”",
          "Pourquoi l’évaluation est importante – La dernière vérification du chef",
          "Métriques quantitatives : Perplexité, BLEU, ROUGE",
          "Métriques qualitatives : Retours humains, utilité, pertinence",
          "Hallucinations et erreurs du modèle – Saveurs inattendues",
          "Détection des biais – S’adapter aux préférences alimentaires"
        ],
        "Servir votre plat – Déployer les LLM": [
          "Introduction à “Servir votre plat – Déployer les LLM”",
          "Qu’est-ce que le déploiement ? – Ouvrir un restaurant éphémère",
          "Créer des APIs avec FastAPI ou Flask",
          "Utiliser Gradio/Streamlit pour les interfaces de démonstration",
          "Options d’hébergement : Hugging Face Spaces, AWS, GCP",
          "Mise à l’échelle et surveillance – Garder le buffet en marche en douceur"
        ],
        "Créer des applications avec LLM – Votre propre food truck": [
          "Introduction à “Créer des applications avec LLM – Votre propre food truck”",
          "Cas d’usage : chatbots, résumeurs, systèmes de recommandation",
          "Outils sans code : Modèles LangChain, GPT Builder, Voiceflow",
          "LLM + base de données : Le menu intelligent",
          "Chaînage avec LangChain – La chaîne de montage de l’IA",
          "Projet : Créez une application LLM complète avec une interface personnalisée"
        ],
        "Garder la fraîcheur – Suivi et amélioration": [
          "Introduction à “Garder la fraîcheur – Suivi et amélioration”",
          "Boucles de rétroaction – Comme des avis Yelp pour l’IA",
          "Journalisation et surveillance – Caméras intelligentes en cuisine",
          "Test A/B – Quel dessert gagne ?",
          "Dérive du modèle – Quand les goûts évoluent avec le temps",
          "Mise à jour des prompts, des jeux de données et des déploiements"
        ],
        "Devenir un grand chef – Carrière en ingénierie LLM": [
          "Introduction à “Devenir un grand chef – Carrière en ingénierie LLM”",
          "Métiers autour des LLM : ingénieur, architecte, spécialiste des prompts",
          "Créer votre portfolio – Votre livre de recettes d’IA",
          "Contribuer à l’open source : jeux de données, modèles, outils",
          "Conseils CV, entretiens et questions techniques",
          "Projet final : Créez et déployez votre propre application LLM"
        ]
      },
      "requirements": [
        "Aucune expérience en programmation requise – ce cours est conçu pour les débutants absolus.",
        "La curiosité pour l’IA et le fonctionnement des modèles de langage suffit largement pour débuter.",
        "Des compétences informatiques de base comme utiliser un navigateur, téléverser des fichiers et taper au clavier sont utiles.",
        "Un ordinateur portable ou de bureau avec accès à Internet – aucun matériel sophistiqué requis.",
        "Optionnel : une clé API gratuite d’OpenAI (pour des projets pratiques avec GPT).",
        "Optionnel : intérêt pour la création de chatbots, la rédaction de prompts ou l’exploration de carrières en IA.",
        "Tous les outils utilisés (comme Gradio, Google Colab ou les modèles LangChain) sont gratuits et adaptés aux débutants."
      ],
      "description": "Ce cours est traduit par l'IA de l'anglais vers le français afin que vous puissiez apprendre des technologies de pointe dans votre langue maternelle.\nDe la Recette au Chef : Devenez Ingénieur en LLM (avec des Analogies Culinaires) est un cours ludique et accessible aux débutants, qui vous apprend à maîtriser les Grands Modèles de Langage (LLMs) sans écrire une seule ligne de code. Que vous soyez curieux de l’intelligence artificielle, que vous souhaitiez entrer dans le monde des modèles de langage ou que vous visiez à devenir ingénieur en LLM, ce cours est votre porte d’entrée vers la compréhension et l’utilisation d’outils puissants comme ChatGPT, Claude, Gemini ou LLaMA. Nous simplifions les concepts techniques à l’aide de métaphores culinaires savoureuses—pour que vous passiez de cuisinier débutant à chef de l’IA en un rien de temps.\nVous explorerez comment les LLMs sont conçus, entraînés, déployés et évalués à travers des analogies faciles à comprendre. Imaginez la tokenisation comme la découpe de légumes, l’entraînement comme la cuisson en grande quantité, ou la création de prompts comme l’assaisonnement parfait d’un plat. Chaque module vous introduira à une nouvelle compétence : préparation des données, fine-tuning, évaluation et mise en production. À la fin du cours, vous comprendrez des concepts clés tels que l’architecture des modèles, le pré-entraînement, l’apprentissage par transfert, l’optimisation des prompts, les métriques d’évaluation comme la perplexité et le score BLEU, et vous saurez déployer vos propres applications alimentées par des LLMs à l’aide d’outils comme FastAPI, Gradio, Hugging Face Spaces et LangChain.\nCe cours s’adresse aux étudiants, enseignants, créateurs de contenu, entrepreneurs et professionnels issus de milieux non techniques qui souhaitent apprendre les bases de l’IA et développer des applications concrètes à l’aide de modèles de langage. Nous vous guiderons pas à pas tout au long du cycle de vie de l’IA—de « Qu’est-ce qu’un modèle de langage ? » à la création de votre propre chatbot, résumeur de texte ou moteur de recommandation. Vous apprendrez à utiliser des outils sans code, à expérimenter avec de vrais prompts, à ajuster des modèles existants, à évaluer les résultats, et même à explorer des carrières comme ingénieur en prompts, chef de produit IA ou architecte LLM.\nAucune expérience en programmation n’est requise. Vous apprendrez à communiquer avec les LLMs en langage naturel, à concevoir des prompts intelligents et efficaces, et à comprendre ce qui se passe en coulisses—de la collecte des données à la tokenisation, jusqu’au processus de prédiction du modèle et à ses besoins informatiques via GPU et TPU. Nous aborderons également la détection des biais, les hallucinations, les boucles de rétroaction et les stratégies pour surveiller et améliorer vos systèmes d’IA au fil du temps.\nÀ la fin du cours, vous disposerez d’une base solide en théorie des LLMs, d’un portfolio de projets pratiques en IA, et de la confiance nécessaire pour entrer dans le monde en pleine expansion de l’IA générative. Que vous souhaitiez créer votre propre produit IA, rejoindre une startup, contribuer à des projets open source ou simplement impressionner vos amis avec vos connaissances en machine learning, ce cours vous y mènera—avec une assiette bien garnie de savoir et une touche de plaisir.\nSi vous êtes prêt à passer de lecteur de recettes à chef en LLM, rejoignez-nous pour ce voyage gourmand au cœur des grands modèles de langage, où chaque concept est expliqué avec des analogies parlantes et des exemples concrets.",
      "target_audience": [
        "Débutants souhaitant découvrir le monde de l’IA sans jargon technique.",
        "Chefs de produit et dirigeants d’entreprise explorant des outils basés sur l’IA.",
        "Éducateurs, créateurs de contenu et conteurs cherchant à exploiter les LLM.",
        "Personnes en reconversion professionnelle souhaitant devenir ingénieurs LLM, concepteurs de prompts ou spécialistes de l’IA.",
        "Si vous aimez apprendre à travers des analogies concrètes (comme la nourriture !), des projets interactifs et une créativité pratique, ce cours vous paraîtra délicieusement enrichissant."
      ]
    },
    {
      "title": "내손으로 강화학습 with Python | 실리콘밸리 엔지니어 특강",
      "url": "https://www.udemy.com/course/codingxrl/",
      "bio": "핵심만 깔끔하게 배우고 기초부터 함께하는 프로젝트 실습",
      "objectives": [
        "강화학습의 주요 개념",
        "실전 알고리즘 실습과 프로젝트",
        "실제 문제에 적용할 수 있는 강화학습 Skill",
        "Value-based Reinforcement Learning",
        "Policy-based Reinforcement Learning",
        "DQN & DDQN",
        "Policy gradient",
        "REINFORCE",
        "Actor-Critic method",
        "DDPG",
        "OpenAI-Gym",
        "Stablebaseline",
        "tensorflow"
      ],
      "course_content": {
        "초압축 강화학습 핵심 개념": [
          "강화학습 기초",
          "강화학습 구성요소",
          "강화학습 설계"
        ],
        "강화학습 알고리즘 집중탐구": [
          "강화학습의 핵심"
        ],
        "내손으로 체화하는 강화학습 알고리즘 & 프로젝트": [
          "Value Based RL",
          "DQN & DDQN",
          "프로젝트 환경설정 & 맛보기",
          "DQN 실습 1",
          "DQN 실습 2",
          "Policy Gradient",
          "Policy Gradient 실습 1",
          "Policy Gradient 실습 2",
          "DDPG"
        ]
      },
      "requirements": [
        "이론 수업은 특별한 선수과목이 필요 없습니다.",
        "머신러닝 기초, 선형대수에 대한 지식이 있으면 훨씬 이해하기 수월합니다."
      ],
      "description": "남들보다 10년 먼저 배우는 인공지능 필수 SKILL\n실리콘밸리 AI 엔지니어 직강 \"내손으로 강화학습\"\n\n\n강화학습이란?\n- 컴퓨터의 빠른 연산력으로 스스로 학습하는 강화학습은, 특정 환경에 놓인 에이전트에게 행동에 따라\n새로운 상태와 보상을 주어 가장 많은 보상을 획득하는 최적의 방법을 찾는 인공지능 기술입니다.\n\n\n실습환경&언어\n- Jupyterlab, Python, TensorFlow, OpenAI\n\n\n핵심만 깔끔하게 끝내고 기초부터 함께하는 프로젝트\nStep 1. 초압축 강화학습 핵심\n- Basic of Reinforcement Learning\n- Key Concept in RL\n\n\nStep 2. 강화학습 알고리즘 집중탐구\n- Taxonomy of RL algorithms\n- Q learning(DQN,DDQN)\n- Policy Optimization(Reinforce,A2C,PPO)\n- DDPG\n\n\nStep 3. 내손으로 체화하는 실전 프로젝트\n- Tensorflow 2.0으로 DQN 바닥부터 짜보기\n- Stable- baseline Library로 DQN 사용하기\n- Tensorflow 2.0으로 REINFORCE 바닥부터 짜보기\n- Stalbe-baseline Library로 PPO 사용하기",
      "target_audience": [
        "강화학습을 제대로 배우고 싶은 분들",
        "기술 영역을 넓히고 싶은 개발자",
        "관련 IT 전공생 또는 커리어 전환을 꿈꾸는 분들",
        "AI 대학원에 진학 예정인 분들"
      ]
    },
    {
      "title": "Python 機器學習全攻略：從理論到應用",
      "url": "https://www.udemy.com/course/wilson-machine-learning/",
      "bio": "近40小時的全攻略課程，帶你掌握資料科學套件（NumPy、Pandas、Matplotlib、Seaborn），由淺入深學習「機器學習」的數學原理、理論與程式應用，全面涵蓋監督式學習（回歸、分類）與非監督式學習（降維、聚類）等 AI 模型！",
      "objectives": [
        "了解監督式學習（Supervised Learning）、非監督式學習（Unsupervised Learning）與強化學習（Reinforcement）的基本概念",
        "了解人工智能（AI）領域的分類、研究領域，並且熟悉機器學習的常見演算法",
        "使用監督式學習中的迴歸任務（Regression）與分類任務（Classification）的常見機器學習模型",
        "熟悉 Numpy Array 的建立、索引、切片與運算，以及能夠使用 Numpy 進行向量化運算以提升效能",
        "熟悉 DataFrame 與 Series 的常見操作",
        "能夠使用 Matplotlib、Seaborn 與 Plotly 等工具繪製各類圖表（長條圖、折線圖、散佈圖等）",
        "理解資料視覺化在資料分析中的重要性",
        "能夠進行資料探索性分析（EDA），發現資料中的趨勢與異常值",
        "熟悉資料清理、轉換、特徵選擇等前處理技巧",
        "能夠透過 Python 產生統計數據並解讀其意義",
        "理解梯度下降演算法的原理與實作",
        "能夠評估模型的效能，包含迴歸任務與分類任務的評估指標(Evaluation metrics)",
        "能夠辨識並解釋過擬合與欠擬合的現象",
        "了解 Bias-Variance Tradeoff",
        "理解正則化（L1、L2）對模型的影響",
        "能夠運用交叉驗證（Cross Validation）與超參數調整（Hyperparameter Tuning）提升模型效能",
        "熟悉特徵工程（Feature Engineering）與特徵縮放（Feature Scaling）的方法",
        "深度了解邏輯迴歸（Logistic Regression）的數學邏輯、模型優化方法",
        "了解 K Nearest Neighbors（KNN）模型，並且透過 KD Tree 優化演算法",
        "對文字資料使用 Naive Bayes Classifier 實行自然語言處理、文本分類",
        "理解混淆矩陣、Accuracy、Recall、Precision、F1 分數等分類評估指標",
        "理解集成學習（Bagging、Boosting、Stacking、Voting）的原理",
        "了解決策樹（Decision Tree）與隨機森林（Random Forest）的生成與優化過程",
        "比較與使用 KMeans, Hierarchical Clustering 與 DBSCAN 演算法",
        "理解主成分分析（PCA）等降維技術的應用與優缺點分析",
        "能夠說明深度學習的基本概念與常見架構",
        "理解機器學習倫理議題（如偏見、可解釋性）",
        "理解大型語言模型（如 ChatGPT）的基本架構與原理",
        "能夠根據資料特性與問題需求選擇合適的機器學習模型"
      ],
      "course_content": {
        "Chapter 0 - 機器學習入門（Intro to Machine Learning）": [
          "機器學習課程架構",
          "如何最好的使用這堂課",
          "課前準備",
          "人工智能發展歷史",
          "Notion and Google Colab",
          "課程 PPT 下載位置"
        ],
        "Chapter 1 - Numpy": [
          "Numpy 概念介紹",
          "Broadcasting Rule 解說",
          "Reshaping and Combining 規則",
          "Linear Algebra and Statistics 函式支援",
          "MSE 解答與 Lab 1 「向量化」介紹",
          "Lab 1 「向量化」解答"
        ],
        "Chapter 2 - Pandas": [
          "Series and DataFrame",
          "Subsetting DataFrame",
          "其他常見函式介紹"
        ],
        "Chapter 3 - 資料可視化（Data Visualization）": [
          "資料可視化",
          "Matplotlib 範例 1",
          "Matplotlib 範例 2",
          "Matplotlib 其他內容補充",
          "Seaborn 範例",
          "Seaborn 其他內容補充",
          "plotly 範例"
        ],
        "Chapter 4 - 資料處理與探索式資料分析（Data Wrangling and EDA）": [
          "Data Wrangling",
          "探索式資料分析（Exploratory Data Analysis）",
          "Penguins 帕爾默企鵝資料集示範",
          "Lab 4 注意事項",
          "Lab 4 「Netflix 資料集」介紹與示範 1",
          "Lab 4 「Netflix 資料集」介紹與示範 2"
        ],
        "Chapter 5 - 監督式學習 - 迴歸任務（Supervised Learning - Regression）": [
          "機器學習概念介紹",
          "機器學習的三大類型",
          "線性迴歸模型（Linear Regression）",
          "梯度下降演算法解說",
          "梯度下降程式碼實作",
          "梯度下降的特性討論",
          "Performance Metric",
          "房價預測的模型訓練",
          "多項式迴歸（Polynomial Regression）",
          "Delta Rule for Polynomial Regression",
          "Polynomial Regression 程式碼",
          "Lab 5 「共享單車數據」介紹與解答"
        ],
        "Chapter 6 - 模型表現評估（Model Performance）": [
          "Overfitting and Underfitting",
          "Train Test Split",
          "模型訓練的問題處理方法",
          "L1, L2 正則化介紹",
          "L1, L2 特性分析",
          "Bias-Variance Tradeoff",
          "交叉驗證與超參數調整",
          "Feature Scaling",
          "特徵選擇與 Art Not Science",
          "Lab 6「BMI 健康數據分析」介紹與第一部分解答",
          "Lab 6「BMI 健康數據分析」 第二部分解答"
        ],
        "Chapter 7 - 監督式學習 - 分類任務（Supervised Learning - Classification）": [
          "分類任務介紹",
          "Accuracy, Recall, Recision and F1 Score",
          "ROC-AUC",
          "分類問題的 Loss Function",
          "One-Hot Encoding and Class Imbalance",
          "Logistic Regression 概念介紹",
          "Logistic Regression Delta Rule",
          "Logistic Regression 注意事項",
          "台灣信用卡資料集示範 第一部分",
          "台灣信用卡資料集示範 第二部分",
          "KNN 演算法講解",
          "KNN 台灣信用卡資料集示範",
          "KNN 優缺點與維度詛咒（Curse of Dimensionality）",
          "KNN 模型的優化方法 - KD Tree",
          "Naive Bayes Classifier 演算法介紹",
          "Generative 與 Discriminative Model",
          "Naive Bayes 垃圾訊息檢測與分類 第一部分",
          "Naive Bayes 垃圾訊息檢測與分類 第二部分",
          "Naive Bayes 台灣信用卡資料預測",
          "Suport Vector Machine 演算法",
          "Kernel Methods",
          "SVM 台灣信用卡資料集示範",
          "Multi-class 分類問題",
          "帕爾默企鵝資料集範例",
          "Lab 7「中文真假新聞分類器」示範第一部分",
          "Lab 7「中文真假新聞分類器」示範第二部分"
        ],
        "Chapter 8 - 樹狀模型與集成學習（Tree Based Models and Ensemble）": [
          "Decision Tree 決策樹介紹",
          "Regression Tree",
          "Entropy 與 Information Gain",
          "Gini Impurity",
          "Decision Tree 特性解說",
          "Decision Tree 台灣信用卡資料集示範",
          "Random Forest 概念講解",
          "Out Of Bag Errors",
          "Random Forest 台灣信用卡資料集示範",
          "Ensemble Learning 集成學習",
          "AdaBoost 演算法",
          "Gradient Boosting 演算法",
          "Boosting 台灣信用卡資料集範例",
          "XGBoost 演算法介紹",
          "Lab 8 「鮑魚年齡預測」資料集示範"
        ],
        "Chapter 9 - 無監督學習（Unsupervised Learning）": [
          "KMeans Clustering 介紹",
          "質心初始值與 Elbow Method 討論",
          "Silhouette Score",
          "Hierarchical Agglomerative Clustering 演算法介紹",
          "HAC 優缺點與程式碼示範",
          "DBSCAN 算法解釋",
          "DBSCAN 問題與 Clustering 演算法差異比較",
          "PCA 降維原理說明",
          "PCA 演算法實作範例",
          "「乳腺癌資料集」範例與 PCA 優缺點討論",
          "Lab 9「百貨公司消費者」客群分析"
        ]
      },
      "requirements": [
        "對 Python 語法有基本的認識，並且對習資料科學、人工智能與機器學習領域有興趣"
      ],
      "description": "你對資料科學、人工智能、機器學習有興趣嗎？未來的世界很可能將由 AI 技術驅動，無論是資訊、醫療、金融、零售還是日常生活，人工智慧都將扮演關鍵角色。如果你對資料科學、人工智能和機器學習等領域有興趣，卻不知道從哪裡開始，本課程將是你進入 AI 領域的最佳起點！\n學習機器學習不僅需要動手寫程式，更需要紮實的數學基礎。因此，本課程特別強調「理論與實作並重」：你不僅會學到每個演算法背後的數學原理，還會學會如何用 Python 及主流資料科學套件將這些理論落實到實際應用中。無論你是理論數學派還是實戰應用派，都能在這門課找到屬於自己的學習重點！\n這堂課程的內容會循序漸進，涵蓋以下主題：\n1. 從零開始，熟悉和學習以下常見的資料科學套件，並且從統計分析、資料可視化到探索式資料分析（Exploratory Data Analysis），全面理解資料背後的故事。運用統計方法發掘資料中的關鍵特徵，並透過各種視覺化工具將複雜的數據轉化為直觀易懂的圖像，快速掌握資料的分布、趨勢與潛在關係。\nNumPy\nPandas\nMatplotlib\nSeaborn\nPlotly\n2. 監督式學習：詳細介紹回歸與分類模型，理解以下模型的數學推導、損失函數設計方法、模型訓練與優化流程、模型差異比較、評估指標，並學會如何透過超參數調整來選擇最佳模型：\n線性迴歸（Linear Regression）\n多項式迴歸（Polynomial Regression）\n邏輯迴歸（Logistic Regression）\nK-近鄰演算法（K Nearest Neighbors）\nNaive Bayes Classifier\n支援向量機（Support Vector Machine）\n決策樹（Decision Tree）\n隨機森林（Random Forest）\n自適應增強（AdaBoost）\n梯度提升技術（Gradient Boosting）\nXGBoost\n3. 無監督學習：了解以下每種無監督學習的模型運作原理、數學推導、適用場景與限制，並透過實際案例操作，讓你掌握如何在不同資料型態下選擇合適的演算法。\nKMeans Clustering\nHierarchical Agglomerative Clustering\nDBSCAN\n主成份分析（Principal Component Analysis）\n4. 了解機器學習中的其他重要概念，包含：\n梯度下降演算法（Gradient Descent Algorithm）\n反向傳播演算法（Back Propagation Algorithm）\nOverfitting 與 Underfitting\nBias-Variance Tradeoff\n模型訓練的問題處理方法\nL1, L2 正則化規則與特性分析\n交叉驗證（Cross Validation）與超參數調整（Hyperparameter Tuning）\n特徵選擇（Feature Selection）與特徵縮放（Feature Scaling）\nROC-AUC 原理\n交叉熵（Cross Entropy）完整公式推導\nAccuracy, Recall, Recision and F1 Score 等模型分數差異比較\n為了學以致用、強化實戰能力，此課程中特別設計了 7 個 Labs 實作練習。每個 Lab 都以真實世界的資料集為主題，讓你有機會將課堂所學應用到實際問題中，從資料清理、特徵工程到模型訓練與評估，全面提升你的資料思維與解決問題的能力。這些 Labs 不僅是課程範例的一部分，更是額外的 bonus 練習，讓實作過程中累積實戰經驗。\n無論你是剛踏入 AI 領域的新手，還是希望補強基礎的進階學習者，此課程都能帶給你紮實的知識與技能。現在就加入此課程，和來自各地的同學一起學習、交流，開啟屬於你的 AI 人工智慧、機器學習之旅吧！",
      "target_audience": [
        "對 Python 有基本認識，希望繼續學習其應用領域的人",
        "對「資料科學」有興趣的初學者，希望從零開始建立基礎知識",
        "對「人工智能」技術有興趣，希望可以開始學習者",
        "希望了解「機器學習」的基本原理與應用場景者",
        "對資料視覺化、資料處理、機器學習模型等主題有興趣的學習者",
        "目前正在主修資訊工程、統計、數學、電機、商管等相關科系，想要學習機器學習理論與應用的人",
        "曾學過機器學習、深度學習，但希望可以更鞏固知識或者更深入理解者"
      ]
    },
    {
      "title": "PENTAHO PDI e METABASE: ETL - gráficos e dashboards",
      "url": "https://www.udemy.com/course/pentaho-pdi-e-metabase-etl-graficos-e-dashboards/",
      "bio": "Construção de ETLs para seus processos de carga e geração de dashboards para tomada de decisão",
      "objectives": [
        "PENTAHO PDI: O que é o Pentaho PDI",
        "PENTAHO PDI: Entendendo sobre fluxos de trabalho e pipelines",
        "PENTAHO PDI: Entendendo sobre projetos e ambientes",
        "PENTAHO PDI: Instalando o Pentaho PDI",
        "PENTAHO PDI: Criando pipelines com arquivos texto",
        "PENTAHO PDI: Realizando tratamento de dados para entendimento do processo de engenharia de dados",
        "PENTAHO PDI: O que são transformações, Jobs e ações dentro de um pipeline",
        "PENTAHO PDI: Construindo um workflow com Jobs, orquestrador da sequência das operações",
        "PENTAHO PDI: Entendendo os menus principais e o seu GUI e seus componentes",
        "PENTAHO PD: Comp. pipelines: Sort, Select value, CSV file input, Value mapper, Filter rows, Dummy, Unique rows, Merge Join, Text File Output, Row Normaliser",
        "PENTAHO PDI: Entendendo como podem ser depurados os dados via output, logs",
        "PENTAHO PDI: Number Range, Concat Field, String Operations, Replace in String, IF Field Value is Null, Split Fields, CSV File Input, Mail, File Exist",
        "PENTAHO PDI: Leitura de dados em uma API: Rest Client, JSON Input, JSON Output",
        "PENTAHO PDI: Construindo Workflow com execução de pipelines",
        "PENTAHO PDI: Entendo o uso de variáveis globais no PENTAHO PDI",
        "PENTAHO PDI: Automatização de pipeline ou workflow",
        "PENTAHO PDI: Construindo pipelines em banco de dados Postgresql: Table Input, Table Output, Configurando conexão",
        "PENTAHO PDI: Instalação de banco de dados Postgresql, uso do PGAdmin",
        "PENTAHO PDI: Automatização de JOBs e Transformações com o Kitchen e Pan",
        "PENTAHO PDI: Construção do projeto de dados a sua escolha e correção com o uso do Pentaho PDI",
        "METABASE: É uma ferramenta simples para construção de suas análises",
        "METABASE: Permite a leitura de tabelas de banco de dados",
        "METABASE: Uso de SQL nativo",
        "METABASE: Permite que você construa painéis com gráficos e sua storytelling",
        "METABASE: Possui uma interface intuitiva e simples utilizando objetos em menus",
        "METABASE: É uma ferramenta que utiliza os dados para responder as suas principais perguntas",
        "METABASE: Utiliza tratamento em dados e ajustes como seleção de conteúdo e preparação de consultas personalizadas",
        "METABASE: Componentes gráficos diversos que facilitam a construção visual de suas respostas: gráfico de barras, gráfico de linha, gráfico de área, KPI, etc",
        "METABASE: Uma ferramenta de inteligência de negócios (BI) que permite a você a busca por respostas em suas mais diversas perguntas",
        "METABASE: Permite a construções de coleções de dados, armazenando todas as suas análises",
        "METABASE: Poderosa função RAIO-X que permite obter insights automáticos e explorações de seus dados",
        "METABASE: Construção de conexões externas, bancos externos",
        "METABASE: Apostila e material de apoio próprio passo a passo",
        "METABASE: Construção de filtros, colunas customizadas, mapas",
        "METABASE: Trabalhando e explicando o METABASE ADMIN"
      ],
      "course_content": {
        "PENTAHO PDI: ETL-Integração e Ingestão de dados": [
          "Apresentação sobre o Pentaho PDI",
          "INFORMAÇÕES IMPORTANTES - Leia antes de começar o curso",
          "Introdução ao Pentaho PDI",
          "Instalação do JAVA",
          "Site de instalação do PDI",
          "Instalação do Pentaho PDI",
          "Entendo como funciona a área de trabalho",
          "Pipeline de Tratamento: arquivo vinhos",
          "Pipeline de tratamento: filtragem e gravação de arquivo ajustado- arquivo vinhos",
          "Pipeline de tratamento: seleção de atributos - arquivos vinhos",
          "Pipeline de tratamento: ordenação, agrupamento - arquivos vinhos",
          "Aula extra - 01 - Leitura de arquivo sem cabeçalho e sem delimitador",
          "Pipeline Merge dos dados: leitura arquivos de entrada e sort dados - 4 arquivos",
          "Pipeline Merge dos dados: componente merge e sort dados - 4 arquivos",
          "Pipeline Merge dos dados: seleção de campos e gravação arquivo - 4 arquivos",
          "Aula extra - 02 - Tratamento e leitura de arquivo colunar",
          "Pipeline Tratamento de dados: arquivo cliente veículos e replace dados",
          "Pipeline Tratamento de dados: operação string, categorias e componente IF null",
          "Pipeline Tratamento de dados: componentes cut e split fields",
          "Pipeline Tratamento de dados: componente number range e concat fileds",
          "Utilizando Debug do PDI dentro de um pipeline",
          "Tratando dados em um Pipeline lendo WebService",
          "Construindo JOB - encadeamento de pipelines",
          "Instalação do Postgres",
          "Carregado dados tabela AUTOR e gerando novos dados tabela NOVO_TB_AUTOR",
          "Aula extra - 03 - Criando JOB para movimentação de arquivos e pastas",
          "Automatizando Jobs e Transformações - Kitchen e Pan",
          "Aula Final - Construa o seu projeto de dados",
          "Vamos responder ao nosso Quiz?"
        ],
        "METABASE - Análise e conexão com os seus dados": [
          "Apresentação do treinamento de METABASE",
          "Introdução ao METABASE e sua arquitetura pra instalação",
          "Instalação do METABASE",
          "Instalação do JAVA",
          "Entendendo sobre Collection, apostila e primeiros gráficos",
          "Construindo uma coleção",
          "Trabalhando com consulta SQL, escala semafórica e tabelas",
          "Inclusão de insights no METABASE",
          "Construção automática de insights - RAIO-X",
          "Construindo filtros",
          "Construindo filtros inteligentes - uso de SQL",
          "Criando colunas customizadas",
          "Entendo a administração do METABASE",
          "Construção de mapas temáticos",
          "Entendo como conectar banco de dados externos",
          "Instalação do Postgres para análises",
          "Linkando dados do Postgres e trabalhando no METABASE",
          "Export dos dados do METABASE",
          "Administração de usuários - criando novos acessos",
          "Avaliação final"
        ]
      },
      "requirements": [
        "Importante ter conhecimento sobre banco de dados, arquivos de dados",
        "Importante que você conheça lógica de programação",
        "Conhecimento elementar de SQL"
      ],
      "description": "Unimos em um mesmo curso o que vem sendo utilizado nas organizações hoje em dia, venha aprender sobre PENTAHO PDI e METABASE.\nO Pentaho Data Integration (PDI) e o Metabase são duas ferramentas importantes para quem trabalha com análise e gerenciamento de dados. Ambas possuem funcionalidades distintas, mas são complementares e podem ser estudadas em conjunto para potencializar a gestão de informações.\nO PDI é uma ferramenta de ETL (Extract, Transform and Load) que permite extrair dados de diversas fontes, transformá-los em formatos compatíveis e carregá-los em um data warehouse ou data lake. Com o PDI, é possível criar pipelines de dados personalizados e automatizados, facilitando a integração de dados de diferentes sistemas e fontes. Entre as vantagens do PDI, estão a sua flexibilidade, sua capacidade de lidar com grandes volumes de dados e sua integração com outras ferramentas do ecossistema Pentaho.\nJá o Metabase é uma ferramenta de análise de dados que permite criar dashboards e relatórios interativos a partir de diferentes fontes de dados. Com o Metabase, é possível visualizar e explorar dados de forma intuitiva, sem a necessidade de conhecimentos avançados em programação ou bancos de dados. Entre as vantagens do Metabase, estão a sua facilidade de uso, a sua capacidade de personalização e a sua integração com diversas fontes de dados.\nSe você é um profissional que trabalha com análise e gestão de dados, estudar o PDI e o Metabase pode ser uma ótima opção para aprimorar suas habilidades e se destacar no mercado. Ao aprender a usar essas ferramentas em conjunto, você será capaz de integrar e explorar dados de forma mais eficiente e criar soluções de análise de dados mais robustas e personalizadas.\nVenha então e comece hoje a fazer o curso de Pentaho e Metabase para aprimorar seus conhecimentos e expandir suas possibilidades de atuação na área de dados.",
      "target_audience": [
        "Estudantes e profissionais de computação, Informática, estatística, data science, analista de dados, engenheiro de dados",
        "Pessoas interessadas em aprender os conceitos sobre ferramentas de ingestão de dados, ou que gostariam adentrar na área de engenharia de dados",
        "Profissionais que, de alguma forma, utilizam dados no seu dia a dia"
      ]
    },
    {
      "title": "【한글자막】 Tensorflow 2와 Keras를 이용한 Deep Learning 부트캠프",
      "url": "https://www.udemy.com/course/best-tensorflow-2-keras-deep-learning/",
      "bio": "Google의 최신 Tensorflow 2 라이브러리와 Keras를 이용해 딥 러닝을 위하여 Python을 활용하는 방법을 학습 | NumPy, Pandas 포함",
      "objectives": [
        "딥 러닝을 위해 TensorFlow 2.0을 사용하는 방법",
        "Keras API를 활용하여 Tensorflow 2에서 실행하는 모델을 신속히 구축하는 방법",
        "컨볼루션 신경망(CNN)을 이용한 이미지 분류 수행하는 방법",
        "의료 이미지에 딥 러닝을 사용하는 방법",
        "순환 신경망(RNN)을 이용해서 시계열 데이터를 예측하는 방법",
        "생성적 대립 신경망(GAN)을 사용해서 이미지를 생성하는 방법",
        "스타일 전환을 위해 딥 러닝을 사용하는 방법",
        "RNN 및 자연어 처리를 사용하여 텍스트를 생성하는 방법",
        "API를 통한 Tensorflow 모델을 제공하는 방법",
        "가속화된 딥 러닝을 위해 GPU를 사용하는 방법"
      ],
      "course_content": {
        "강의 개요, 설치 및 환경 설정": [
          "환영합니다",
          "강의 개요",
          "강의 환경 설정과 설치",
          "FAQ - 자주 묻는 질문들"
        ],
        "강의 개요 확인!": [
          "퀴즈 1: 강의 개요 영상을 시청하세요"
        ],
        "NumPy 특강": [
          "NumPy 입문",
          "NumPy 배열하기",
          "NumPy 인덱스 선택",
          "NumPy 연산하기",
          "NumPy 연습 문제",
          "NumPy 연습 문제 - 풀이법"
        ],
        "Pandas 특강": [
          "Pandas 입문",
          "Pandas 시리즈",
          "Pandas 데이터 프레임 - 파트 1",
          "Pandas 데이터 프레임 - 파트 2",
          "Pandas 누락 데이터",
          "그룹화 연산",
          "Pandas 연산",
          "데이터 입력과 출력",
          "Pandas 연습 문제",
          "Pandas 연습 문제 - 풀이법"
        ],
        "시각화 특강": [
          "Python 시각화 입문",
          "Matplotlib 기초",
          "Seaborn 기초",
          "데이터 시각화 연습 문제",
          "데이터 시각화 연습 문제 - 풀이법"
        ],
        "머신 러닝 개념 개요": [
          "머신 러닝이란?",
          "지도 학습 개요",
          "과적합",
          "성능 평가하기 - 분류 오류 척도",
          "성능 평가하기 - 회귀 오류 척도",
          "비지도 학습"
        ],
        "기초 인공 신경망 - ANN": [
          "인공 신경망(ANN) 섹션 입문",
          "퍼셉트론 모델",
          "신경망",
          "활성화 함수",
          "다중 클래스 분류 고려 사항",
          "비용 함수와 경사 하강법",
          "역전파",
          "TensorFlow와 Keras의 차이점",
          "Keras 기본 구문 - 파트 1 - 데이터 준비",
          "Keras 기본 구문 - 파트 2 - 모델 생성 및 학습하기",
          "Keras 기본 구문 - 파트 3 - 모델 평가",
          "Keras 회귀 코드 - 탐색적 데이터 분석",
          "Keras 회귀 코드 - 탐색적 데이터 분석 - 이어서",
          "Keras 회귀 코드 - 데이터 전처리와 모델 생성하기",
          "Keras 회귀 코드 - 모델 평가와 예측",
          "Keras 분류 코드 - EDA와 전처리",
          "Keras 분류 - 과적합과 평가 문제 해결하기",
          "TensorFlow 2.0 Keras 프로젝트 옵션 개요",
          "Tensorflow 2.0 Keras 프로젝트 노트북 개요",
          "Keras 프로젝트 풀이법 - 탐색적 데이터 분석 설명",
          "Keras 프로젝트 풀이법 - 누락데이터 처리하기",
          "Keras 프로젝트 풀이법 - 누락데이터 처리하기 - 파트 2",
          "Keras 프로젝트 풀이법 - 범주형 데이터",
          "Keras 프로젝트 풀이법 - 데이터 전처리하기",
          "Keras 프로젝트 풀이법 - 모델 생성 및 학습하기",
          "Keras 프로젝트 풀이법 - 모델 평가",
          "텐서보드"
        ],
        "컨볼루션 신경망 - CNN": [
          "컨볼루션 신경망(CNN) 섹션 개요",
          "이미지 필터와 커널",
          "컨볼루션 레이어",
          "풀링 레이어",
          "MNIST 데이터 세트 개요",
          "MNIST의 컨볼루션 신경망(CNN) - 파트 1 - 데이터",
          "MNIST의 컨볼루션 신경망(CNN) - 파트 2 - 모델 생성 및 학습하기",
          "MNIST의 컨볼루션 신경망(CNN) - 파트 3 - 모델 평가",
          "CIFAR - 10의 컨볼루션 신경망(CNN) - 파트 1 - 데이터",
          "CIFAR - 10의 컨볼루션 신경망(CNN) - 파트 2 - 모델 평가하기",
          "실제 이미지 강의를 위한 데이터 세트 다운로드하기",
          "실제 이미지 파일의 컨볼루션 신경망(CNN) - 파트 1 - 데이터 읽기",
          "실제 이미지 파일의 컨볼루션 신경망(CNN) - 파트 2 - 데이터 처리하기",
          "실제 이미지 파일의 컨볼루션 신경망(CNN) - 파트 3 - 모델 생성하기",
          "실제 이미지 파일의 컨볼루션 신경망(CNN) - 파트 4 - 모델 평가하기",
          "컨볼루션 신경망(CNN) 연습 문제 개요",
          "컨볼루션 신경망(CNN) 연습 문제 풀이법"
        ],
        "순환 신경망 - RNN": [
          "순환 신경망(RNN) 섹션 개요",
          "순환 신경망(RNN) 기초 이론",
          "기울기 소실",
          "LSTM과 GRU",
          "순환 신경망(RNN) 배치",
          "사인파에서의 순환 신경망(RNN) - 데이터",
          "사인파에서의 순환 신경망(RNN) - 배치 생성자",
          "사인파에서의 순환 신경망(RNN) - 모델 생성하기",
          "사인파에서의 순환 신경망(RNN) - LSTM과 예측하기",
          "시계열에서의 순환 신경망(RNN) - 파트 1",
          "시계열에서의 순환 신경망(RNN) - 파트 2",
          "순환 신경망(RNN) 연습 문제",
          "순환 신경망(RNN) 연습 문제 - 풀이법",
          "보너스 - 다변량 시계열 - 순환 신경망(RNN)과 LSTM"
        ],
        "자연어 처리": [
          "자연어 처리 섹션 입문하기",
          "자연어 처리 - 파트 1 - 데이터",
          "자연어 처리 - 파트 2 - 텍스트 처리하기",
          "자연어 처리 - 파트 3 - 배치 생성하기",
          "자연어 처리 - 파트 4 - 모델 생성하기",
          "자연어 처리 - 파트 5 - 모델 학습하기",
          "자연어 처리 - 파트 6 - 텍스트 생성하기"
        ]
      },
      "requirements": [
        "Python으로 코딩을 할 수 있는 능력",
        "미분과 같은 기본적인 수학 개념"
      ],
      "description": "Tensorflow와 Keras 집중 강의!\nDeep learning 인공 신경망 생성 방법 학습!\nJupyter Notebook 가이드와 참조하기 쉬운 슬라이드 함께 제공 및 연습 문제 포함!\n\n\nTensorflow 2와 Keras를 이용한 Deep Learning 부트캠프 과정을 선택해야 하는 이유\n본 코스에서는 Google의 최신 TensorFlow 2 프레임워크를 사용하여 Deep Learning을 위한 인공 신경망 생성 방법을 배울 수 있습니다. 본 코스는 Google TensorFlow 2 프레임워크의 복잡한 특징을 이해하기 쉽게 안내해드리는 것을 목표로 합니다.\n\n\nTensorFlow의 최신 업데이트 사항을 이해하고 Keras API (TensorFlow 2.0 공식 API)를 활용하여 모델을 빠르고 쉽게 구축하는 과정을 집중적으로 살펴볼 것입니다. 본 코스에서는 미래 주택 가격을 예측하거나, 의료 이미지를 분류하고, 미래 판매 데이터를 예측하며, 완전히 새로운 인공 텍스트를 생성하는 등 다양한 모델을 구축할 것입니다.\n\n\n본 강의는 이론과 실제 구현이 균형을 이룰 수 있도록 설계되었으며, 코드를 위한 완벽 Jupyter Notebook 가이드와 참조하기 쉬운 슬라이드 및 노트를 함께 제공합니다. 새로 배운 기술을 시험해 볼 수 있는 많은 연습 문제도 포함되어 있습니다!\n\n\nTensorflow 2와 Keras를 이용한 Deep Learning 부트캠프 과정은 이렇게 진행됩니다\nNumPy 특강\nPandas 데이터 분석 특강\n데이터 시각화 특강\n신경망 기초\nTensorFlow 기초\nKeras 구문법 기초\n인공 신경망\n밀집 연결망\n컨볼루션 신경망\n순환 신경망\n오토인코더\nGAN - 생산적 대립 신경망\n프로덕션에 TensorFlow 배포\n그 외 다수\n\n\n베스트 강사 Jose Portilla의 한마디!\n한국 수강생 여러분들 안녕하세요?\n\"Tensorflow 2와 Keras를 이용한 Deep Learning 부트캠프\"에 오신 것을 환영합니다!\n\n\nMachine Learning 을 위한 사용자 친화적 API 표준인 Keras는 모델을 구축하고 교육함에 있어서 중심적인 고급 API가 됩니다. Keras API를 사용하면 TensorFlow 2를 쉽게 시작할 수 있습니다!\n무엇보다 Keras가 일부 모델 구축 API (순차, 함수형, 하위 클래스)를 제공하여 여러분의 프로젝트에 적합한 추상화 수준을 선택할 수도 있습니다. TensorFlow의 구현은 즉각적인 반복과 직관적인 디버깅을 위한 신속한 실행을 포함하여 스케일링이 가능한 입력 파이프라인 구축을 위한 tf 데이터 등 향상된 기능을 포함하고 있습니다.\n\n\nTensorFlow 2를 사용하면 코드 개념과, 모델에서 발행으로까지 이어지는 새로운 아이디어를 쉽게 얻을 수 있습니다. TensorFlow 2.0는 속도나 성능의 저하 없이 최신 모델의 정의와 훈련이 가능한 많은 특성을 통합하기 때문이지요.\n\n\nAirbnb, Ebay, Dropbox, Snapchat, Twitter, Uber, SAP, Qualcomm, IBM, Intel, 여기에 당연하게도 Google을 포함한 전 세계 유수의 기업에서 TensorFlow를 사용하고 있습니다!\n\n\n\n\n바로 오늘 Deep Learning 전문가로 거듭나세요!\n\n\n\n\nP.S. 강의를 들으시고 강의와 관련하여 궁금하신 점은 무엇이든 Q&A에 남기실 수 있지만, 꼭 영어로 남겨주세요. 그래야 답변을 드릴 수 있습니다. :)\n\n\n그럼 강의에서 뵙겠습니다,\n감사합니다.\n\n\n- Jose Portilla",
      "target_audience": [
        "딥 러닝 및 인공 지능을 위한 TensorFlow 2 학습에 관심이 있는 Python 개발자"
      ]
    },
    {
      "title": "[ES] Curso de Certificación Profesional en Ingeniería de IA",
      "url": "https://www.udemy.com/course/es-curso-de-certificacion-profesional-en-ingenieria-de-ia/",
      "bio": "Domina el Aprendizaje Profundo, Transformers, MLOps y el Desarrollo de Agentes de IA con Proyectos del Mundo Real",
      "objectives": [
        "Ajusta y optimiza modelos de aprendizaje automático utilizando técnicas avanzadas",
        "Construye y entrena CNNs para tareas de clasificación de imágenes y visión por computadora",
        "Desarrolla RNNs, LSTMs y GRUs para modelado de series temporales y secuencias",
        "Comprende e implementa transformadores y mecanismos de atención",
        "Aplica aprendizaje por transferencia para afinar modelos preentrenados potentes",
        "Diseña y analiza agentes de IA para la toma de decisiones autónoma",
        "Utiliza TensorFlow y PyTorch para proyectos de aprendizaje profundo",
        "Utiliza TensorFlow y PyTorch para proyectos de aprendizaje profundo"
      ],
      "course_content": {
        "Introducción al Curso y al Instructor": [
          "Qué aprenderás en el Curso de Certificación Profesional en Ingeniería de IA"
        ],
        "Ajuste y Optimización de Modelos": [
          "Día 1: Introducción al Ajuste de Hiperparámetros",
          "Día 2: Grid Search y Random Search",
          "Día 3: Ajuste Avanzado de Hiperparámetros con Optimización Bayesiana",
          "Día 4: Técnicas de Regularización para la Optimización de Modelos",
          "Día 5: Validación Cruzada y Técnicas de Evaluación de Modelos",
          "Día 6: Ajuste Auto de Hiperparámetros con GridSearchCV y RandomizedSearchCV",
          "Día 7: Proyecto de Optimización – Construcción y Ajuste de un Modelo Final",
          "Recursos para proyectos en formación"
        ],
        "Redes Neuronales Convolucionales (CNNs)": [
          "Día 1: Introducción a las Redes Neuronales Convolucionales",
          "Día 2: Capas Convolucionales y Filtros",
          "Día 3: Capas de Pooling y Reducción de Dimensionalidad",
          "Día 4: Construcción de Arquitecturas CNN con Keras y TensorFlow",
          "Día 5: Construcción de Arquitecturas CNN con PyTorch",
          "Día 6: Regularización y Aumento de Datos para CNNs",
          "Día 7: Proyecto CNN – Clasificación de Imágenes en Fashion MNIST o CIFAR-10",
          "Recursos para proyectos en formación"
        ],
        "Redes Neuronales Recurrentes (RNNs) y Modelado de Secuencias": [
          "Día 1: Introducción al Modelado de Secuencias y RNNs",
          "Día 2: Comprensión de la Arquitectura RNN y Retropropagación en el Tiempo (BPTT)",
          "Día 3: Redes de Memoria a Largo Plazo (LSTM)",
          "Día 4: Unidades Recurrentes Gated (GRUs)",
          "Día 5: Preprocesamiento de Texto y Word Embeddings para RNNs",
          "Día 6: Modelos Secuencia a Secuencia y Aplicaciones",
          "Día 7: Proyecto RNN – Generación de Texto o Análisis de Sentimientos",
          "Recursos para proyectos en formación"
        ],
        "Transformers y Mecanismos de Atención": [
          "Día 1: Introducción a los Mecanismos de Atención",
          "Día 2: Introducción a la Arquitectura Transformers",
          "Día 3: Auto-atención y Atención Multi-Cabeza en Transformers",
          "Día 4: Codificación Posicional y Redes Feed-Forward",
          "Día 5: Práctica con Transformers Preentrenados – BERT y GPT",
          "Día 6: Transformers Avanzados – Variantes de BERT y GPT-3",
          "Día 7: Proyecto Transformer – Resumen de Texto o Traducción",
          "Recursos para proyectos en formación"
        ],
        "Aprendizaje por Transferencia y Fine-Tuning": [
          "Día 1: Introducción al Aprendizaje por Transferencia",
          "Día 2: Transferencia de Aprendizaje en Visión por Computador",
          "Día 3: Técnicas de Fine-Tuning en Visión por Computador",
          "Día 4: Transferencia de Aprendizaje en PLN",
          "Día 5: Técnicas de Fine-Tuning en PLN",
          "Día 6: Adaptación de Dominio y Desafíos del Aprendizaje Transferido",
          "Día 7: Proyecto de Transfer Learning – Fine-Tuning para una Tarea Personalizada",
          "Recursos para proyectos en formación"
        ],
        "Agentes de IA: Una Visión General Integral": [
          "Práctica con AutoGen | IBM Bee | LangGraph | CrewAI | AutoGPT",
          "Práctica con AutoGen",
          "Práctica con el Framework IBM Bee",
          "Práctica con LangGraph",
          "Práctica con CrewAI",
          "Práctica con AutoGPT"
        ],
        "Introducción y Práctica de MLOps": [
          "Introducción a las Sesiones de MLOps",
          "Visión General de MLOps y su Importancia",
          "Evolución de las Operaciones de Machine Learning",
          "Conceptos Clave en MLOps: Versionado, Automatización y Monitoreo",
          "MLOps vs. DevOps: Similitudes y Diferencias",
          "Práctica: Configurar Estructura Básica MLOps (Git, Docker, Pipeline)",
          "Introducción a la Sección de Ciencia de Datos a Producción",
          "Descripción del Flujo de Trabajo ML: Desde Preparación de Datos hasta Despliegue",
          "Experimentación vs. Producción",
          "Desafíos en el Despliegue de Modelos de ML",
          "Práctica: Construcción de un Pipeline de ML de Extremo a Extremo",
          "Introducción a la Infraestructura para MLOps",
          "Introducción a Plataformas en la Nube (AWS, GCP, Azure)",
          "Contenerización con Docker",
          "Kubernetes para la Orquestación de Cargas de Trabajo de ML",
          "Configuración de Entornos MLOps Locales",
          "Práctica: Contenerizar Modelo ML y Desplegarlo Local con Kubernetes"
        ],
        "Examen Final y Felicitaciones": [
          "Examen Final",
          "Felicitaciones y los Mejores Deseos"
        ]
      },
      "requirements": [
        "Haber completado un curso de nivel principiante o intermedio en IA o aprendizaje automático (o poseer conocimientos equivalentes)",
        "Sólido entendimiento de programación en Python, incluyendo experiencia con funciones, clases y bibliotecas como NumPy y Pandas",
        "Sólido entendimiento de programación en Python, incluyendo experiencia con funciones, clases y bibliotecas como NumPy y Pandas",
        "Familiaridad con los fundamentos del aprendizaje profundo, incluidas las redes neuronales y arquitecturas básicas de modelos",
        "Experiencia previa con herramientas como Jupyter Notebook, TensorFlow o PyTorch",
        "Conocimientos prácticos de matemáticas para IA, incluyendo álgebra lineal, probabilidad y cálculo",
        "Una computadora (Windows, macOS o Linux) con acceso confiable a internet y capacidad para instalar herramientas de desarrollo",
        "Disposición para explorar sistemas complejos de nivel de producción e invertir tiempo en codificación práctica, experimentación de modelos y flujos de despliegue"
      ],
      "description": "Adéntrate en el mundo de la ingeniería avanzada de IA con el Curso de Certificación Profesional en Ingeniería de IA — tu guía completa para dominar el aprendizaje profundo, la optimización de modelos, las arquitecturas de transformers, los agentes de IA y MLOps. Este programa de nivel experto está diseñado para quienes están listos para pasar de la teoría a la producción, construyendo sistemas de IA de vanguardia con herramientas y marcos del mundo real.\nComenzarás con Ajuste y Optimización de Modelos, donde aprenderás a afinar hiperparámetros usando Grid Search, Random Search y Optimización Bayesiana. Descubre el impacto de la regularización, la validación cruzada y los flujos automatizados de ajuste: claves para mejorar la precisión y eficiencia de tus modelos de ML.\nLuego te sumergirás en las Redes Neuronales Convolucionales (CNNs), la base de la visión por computadora. Aprenderás a construir CNNs desde cero, conocerás las capas convolucionales, de agrupamiento (pooling) y dropout, y las aplicarás a la clasificación de imágenes, detección de objetos y más, utilizando TensorFlow y PyTorch.\nDe las imágenes a las secuencias: el módulo de Redes Neuronales Recurrentes (RNNs) y Modelado Secuencial cubre los principios fundamentales del análisis de datos temporales. Aprende a modelar series temporales, texto y voz usando RNNs, LSTMs y GRUs, incluyendo cómo abordar los gradientes que desaparecen y las dependencias a largo plazo.\nDespués, prepárate para explorar la joya de la corona de la IA moderna: Transformers y Mecanismos de Atención. Aprende cómo la autoatención, la atención multi-cabeza y la codificación posicional alimentan modelos como BERT, GPT y T5. Construirás transformers desde cero y aplicarás arquitecturas preentrenadas para resolver problemas del mundo real.\nTambién dominarás el Aprendizaje por Transferencia y Fine-Tuning, una de las habilidades más prácticas para los ingenieros de IA actuales. Aprende a utilizar modelos preentrenados y adaptarlos a tareas específicas usando estrategias de extracción de características y ajuste fino, ahorrando tiempo de cómputo y datos.\nEl curso incluye además una visión profunda de los Agentes de IA: Una Visión General Completa. Explorarás la arquitectura de agentes autónomos, incluidos agentes reactivos, basados en objetivos y sistemas multiagente. Verás cómo los agentes de IA se utilizan en la toma de decisiones en tiempo real, IA para videojuegos, asistentes personales y simulaciones basadas en agentes.\nFinalmente, reúne todo con la Introducción y Práctica de MLOps. Descubre cómo desplegar, monitorear y mantener modelos en producción usando herramientas como Docker, MLflow, Kubeflow y pipelines de CI/CD. Aprende sobre versionado de modelos, reproducibilidad y escalabilidad — habilidades clave para cualquier ingeniero de IA moderno.\nAl finalizar este curso, podrás:\nAfinar y optimizar modelos de aprendizaje profundo para producción\nConstruir arquitecturas basadas en CNNs, RNNs y Transformers\nUsar aprendizaje por transferencia para adaptar modelos potentes a nuevos dominios\nComprender y diseñar agentes de IA para entornos reales\nAplicar buenas prácticas de MLOps para despliegues de IA escalables\nYa sea que aspiras a convertirte en Ingeniero de Machine Learning, Investigador en IA o Arquitecto Principal de IA, este es el curso definitivo para pasar de practicante capacitado a profesional en IA.\nÚnete hoy y obtén tu Certificado Profesional en Ingeniería de IA — el estándar de oro en formación avanzada en inteligencia artificial.",
      "target_audience": [
        "Ingenieros de IA y practicantes de aprendizaje automático que buscan profundizar su experiencia en ajuste de modelos, aprendizaje profundo y despliegue",
        "Científicos de datos que desean especializarse en arquitecturas de aprendizaje profundo y sistemas de IA en tiempo real",
        "Ingenieros de software que buscan integrar capacidades de IA en aplicaciones full-stack usando TensorFlow y PyTorch",
        "Estudiantes de posgrado o investigadores académicos que hacen la transición a roles de IA a nivel industrial",
        "Profesionales tecnológicos que desean dominar Transformers, MLOps y marcos de agentes de IA para resolver problemas empresariales complejos",
        "Cualquier persona que ya haya completado un curso introductorio de IA o aprendizaje automático y desee construir, ajustar y desplegar con confianza modelos de IA de vanguardia"
      ]
    },
    {
      "title": "Машинное обучение: нейросети и глубокое обучение на Python",
      "url": "https://www.udemy.com/course/ittensive-python-machine-learning-neural/",
      "bio": "Выигрываем соревнование Kaggle по классификации/сегментации изображений со сверточными и остаточными нейросетями",
      "objectives": [
        "Распознавание формы облаков по фотографии",
        "Оценка F1 и критерий сходства Дайса",
        "Многослойный перцептрон",
        "Сверточные нейронные сети",
        "Функции активации, регуляризаторы и оптимизаторы",
        "Нормализация, отсев и дополнение изображений",
        "LeNet, AlexNet и GoogLeNet, Inception",
        "VGG, ResNet и DenseNet",
        "MobileNet, FPN, Unet, PSPNet",
        "Ансамбли нейросетей"
      ],
      "course_content": {
        "Раздел 1. Процесс машинного обучения": [
          "Задачи машинного обучения",
          "Задачи машинного обучения",
          "Модель и процесс машинного обучения",
          "Что такое ETL",
          "Процесс машинного обучения",
          "Что такое EDA",
          "Подготовка данных",
          "Подготовка данных",
          "Разбиение выборки",
          "Оптимизация гиперпараметров",
          "Недообучение и переобучение",
          "Обучение модели",
          "Смещение, разброс и ошибка данных",
          "Использование HDF"
        ],
        "Метрики и модели": [
          "Метод максимального правдоподобия",
          "Метод наименьших квадратов",
          "Метод наименьших квадратов",
          "Аппроксимация пропусков в данных",
          "Аппроксимация данных",
          "Среднеквадратичная ошибка",
          "Метрики и расстояния",
          "Метрики и расстояния",
          "Линейная регрессия и L1/L2-регуляризация",
          "Линейная регрессия",
          "BIC и AIC",
          "Логистическая регрессия",
          "Линейные модели"
        ],
        "Искусственные нейронные сети": [
          "Искусственные нейронные сети",
          "Нейросети",
          "Слои в нейросетях",
          "Нейрон смещения",
          "Функции активации",
          "Функции активации",
          "Обратное распространение ошибки",
          "Многослойный перцептрон",
          "Многослойный перцептрон",
          "Ландшафт функции потерь"
        ],
        "Раздел 2. Практикум: Распознавание формы облаков": [
          "Предсказание формы облаков",
          "Предобработка изображений",
          "Опорные векторы и коэффициент сходства",
          "Двухслойный перцептрон",
          "Многослойный перцептрон"
        ],
        "Обучение нейросети": [
          "Эпохи, пакеты, итерации",
          "Пакетное обучение",
          "Оптимизация нейросетей по Нестерову",
          "Адаптивная оптимизация нейросетей",
          "RMSprop, adadelta, adam",
          "Оптимизация нейросетей",
          "Оптимизация нейросетей",
          "Пакетная нормализация",
          "Регуляризация обучения нейросетей",
          "Методы инициализации весов",
          "Дополнение данных",
          "Обучение нейросети",
          "Свертка и подвыборка",
          "Сверточные нейросети"
        ],
        "Практикум: Сверточные нейросети": [
          "Свертка и подвыборка",
          "Активация и оптимизаторы",
          "Нормализация и переобучение",
          "Дополнение изображений",
          "Сверточная нейросеть"
        ],
        "Ансамбли машинного обучения": [
          "Ансамблевые модели",
          "Ансамбль стекинга"
        ],
        "Архитектуры сверточных нейросетей": [
          "LeNet",
          "AlexNet",
          "VGG",
          "GoogLeNet",
          "Inception",
          "ResNet",
          "ResNeXt",
          "SE-ResNet",
          "EfficientNet",
          "DenseNet",
          "MobileNet"
        ],
        "Практикум: Архитектуры нейронных сетей": [
          "LeNet, CaffeNet и AlexNet",
          "VGG16 и VGG19",
          "GoogLeNet и Inception-BN",
          "Inception V3 и V4",
          "ResNet34, 50, 101, 152",
          "Архитектура нейросети",
          "Ансамбль нейросетей"
        ],
        "Практикум: нейросети для сегментации": [
          "MobileNet для областей",
          "U-Net",
          "FPN",
          "Классификация облаков"
        ]
      },
      "requirements": [
        "Основы математической статистики",
        "Основы машинного обучения",
        "Продвинутый Python"
      ],
      "description": "Мы разберем сегментацию и классификацию изображений облаков с помощью сверточных, пирамидальных, остаточных и полносвязных нейронных сетей в соревновании на Kaggle вплоть до формирования конечного результата.\nКурс разбит на 2 части. В первой части мы последовательно пройдем все этапы работы с данными: от видов задач и их постановки до работы с моделями машинного обучения для минимизации предсказательной ошибки. Дополнительно рассмотрим фундаментальные основы построения моделей машинного обучения, базовые метрики и наиболее простые модели - линейную и логистическую регрессии.\nВо второй части разберем на практических примерах:\nПроведение исследовательского анализа данных для поиска зависимостей: EDA.\nМетрики точности: оценка F1 и коэффициент Дайса.\nОчистка данных и обработка изображений.\nЗагрузка и сохранение моделей и данных в HDF5.\nДвухслойный и многослойный перцептрон.\nНейросети со сверточными слоями и слоями подвыборки.\nФункции активации, инициализация и оптимизаторы нейросетей.\nПреобразование и дополнение (аугментация) бинарных данных.\nLeNet, AlexNet, GoogLeNet.\nVGG, Inception, ResNet, DenseNet.\nСегментация изображений с MobileNet, Unet, PSPNet и FPN.\nАнсамбль нейросетей.\nВыгрузка результата для соревнования на Kaggle.",
      "target_audience": [
        "Аналитики Python, изучающие машинное обучение",
        "Программисты больших данных",
        "Исследователи больших данных"
      ]
    },
    {
      "title": "分布式事务：SEATA开发实践",
      "url": "https://www.udemy.com/course/seata-jr/",
      "bio": "实战Alibaba Seata，掌握AT模式、TCC模式架构与开发",
      "objectives": [
        "理解Seata AT模式、TCC模式、Saga模式的原理与区别",
        "学习并掌握如何运用Seata AT模式实现网络架构",
        "学习并掌握SeataTCC模式逻辑与组件选型",
        "通过实战掌握不同模式中TC搭建、RM端开发、TM端开发，解析模式执行的流程",
        "深入理解分布式事务，系统实现事务模式开发"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "Alibaba Seata": [
          "Alibaba Seata"
        ],
        "Seata AT模式原理": [
          "Seata AT模式原理"
        ],
        "Seata AT模式开发": [
          "搭建TC（Seata-Server）",
          "RM端开发",
          "TM端开发"
        ],
        "AT模式执行解析": [
          "AT模式执行解析"
        ],
        "AT模式更多开发细节": [
          "AT模式更多开发细节"
        ],
        "Seata TCC模式原理": [
          "Seata TCC模式原理"
        ],
        "Seata TCC模式开发": [
          "开发准备与运行环境",
          "系统架构",
          "RM端-订单服务开发",
          "RM端-库存服务开发"
        ],
        "Seata Saga模式原理": [
          "Seata Saga模式原理"
        ],
        "总结：Seata三种模式对比": [
          "总结：Seata三种模式对比"
        ]
      },
      "requirements": [
        "有后端开发基础，了解分布式架构"
      ],
      "description": "随着业务规模不断扩大，单体系统逐渐无法满足业务的需求，因此分布式事务相关规范和框架应运而生，并逐渐成为大型互联网平台首选。Seata是一款阿里巴巴团队开源的分布式事务解决方案，具备部署简单、学习成本低、对业务无入侵、应用方式灵活、可拓展性高等优点，因此得到行业从业人员的广泛关注，便于实践事务模式开发。本节课将从Alibaba Seata解决方案入手，深入讲解Seata的AT事务模式及TCC事务模式，并通过实操演示掌握RM端TM端开发，系统梳理完整的模型搭建，综合提升对分布式事务的理解，实现从理论到熟练应用的跨越。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。\n未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "架构师",
        "后端开发工程师",
        "Java工程师"
      ]
    },
    {
      "title": "Deep Reinforcement Learning - einfach verstehen",
      "url": "https://www.udemy.com/course/deep-reinforcement-learning-einfach-verstehen/",
      "bio": "Vom autonomen Fahren zum besten Go-Spieler der Welt",
      "objectives": [
        "Wissen, wie autonomes Fahren funktioniert.",
        "Wissen, mit welchem Algorithmus der beste Go-Spieler der Welt geschlagen wurde.",
        "Verstehen, warum Deep Reinforcement Learning so ein großes Potential hat."
      ],
      "course_content": {},
      "requirements": [
        "Du solltest Interesse an Machine Learning und künstlicher Intelligenz haben.",
        "Es wird kein konkretes Vorwissen benötigt."
      ],
      "description": "Deep Reinforcement Learning verbindet Deep Learning mit Reinforcement Learning - zwei Teilgebiete der künstlichen Intelligenz.\nDie Anwendung dieser Technik auf das komplexe Spiel Go hat einen Algorithmus hervorgebracht, der ohne Daten und ohne programmierte Spiel-Kenntnisse sich so gut selbst trainiert hat, dass er nach kurzer Zeit den besten Go-Spieler der Welt geschlagen hat.\nIn diesem Kurs gehen wir diesem Phänomen Schritt-für-Schritt auf den Grund.\nZunächst klären wir die Grundlagen der Künstlichen Intelligenz. Was steckt hinter Machine Learning, Reinforcement Learning, Neuronalen Netzen und Deep Learning?\nDann werden wir autonomes Fahren als Anwendung von Reinforcement Learning kennen lernen. Danach führen wir Reinforcement Learning und Deep Learning zusammen und analysieren den Algorithmus, der den besten Go-Spieler der Welt geschlagen hat, im Detail.\nMan stelle sich vor, welche weiteren Probleme mit einem solch mächtigen Algorithmus noch gelöst werden könnten...\nUnd das Beste: Man versteht nach diesem Kurs einfach wie es funktioniert!",
      "target_audience": [
        "Der Kurs eignet sich für jeden, der einfach verstehen möchte, wie Deep Reinforcement Learning funktioniert und sich fragt, warum ein Computer den besten Go-Spieler der Welt schlagen kann."
      ]
    },
    {
      "title": "Data Analysis and Visualization using Python in Hindi",
      "url": "https://www.udemy.com/course/data-analysis-and-visualization-using-python/",
      "bio": "Learn how to analyze, visualize and present data using Python in Hindi. Learn a real life data analysis project.",
      "objectives": [
        "Project on data analysis.",
        "Data visualization using various plots like line plot, scatter plot, bar plot, pie plot, suplots etc and multiple plots in same graphs and many things",
        "Pandas (Series, DataFrames creation, and operations on them in deep) and working on Datasets.",
        "Working with multidimensional array",
        "Working with mathematical operations and various Numpy functions",
        "Installation of Jupyter Notebook using anaconda",
        "Indexing and slicing in deep using both Numpy and Pandas (i.e. loc and iloc)",
        "Axis in 2D and 3D array"
      ],
      "course_content": {
        "Numpy": [
          "Introduction about my course",
          "Introduction to Numpy (1)",
          "Installation of Jupyter notebook with anaconda (2)",
          "Multidimensional array (03)",
          "Accessing array elements (4)",
          "Axis in 2D array (5)",
          "Axis in 3D array (6)",
          "Swapaxes (7)",
          "Numpy Quiz 01",
          "Mathematical operations (8)",
          "Various Numpy functions (9)",
          "Indexing and slicing (10)",
          "Numpy array vs simple list (11)",
          "Numpy Quiz 02",
          "Solved exercise Problems (12)",
          "Numpy Basic Problems"
        ],
        "Pandas": [
          "Introduction to Pandas (1)",
          "Series (2)",
          "Querying Series (3)",
          "DataFrame (4)",
          "Accessing elements (5)",
          "Reading data from File (6)",
          "Querying DataFrame (7)",
          "Indexing DataFrame (8)",
          "Missing value (9)"
        ],
        "Matplotlib": [
          "Introduction to Matplotlib (01)",
          "Adding color to line (02)",
          "Marker (03)",
          "Marker Face color (04)",
          "Changing line style (05)",
          "Settings X and Y-axis (06)",
          "Scatter Plot (7)",
          "Subplots (8)",
          "Pie Plot (9)"
        ],
        "Data Analysis Project": [
          "Data Analysis Project",
          "Various Joins"
        ]
      },
      "requirements": [
        "Only a passion to build a successful Data Science career"
      ],
      "description": "In this course, you'll get very well knowledge of Numpy, Pandas, and Matplotlib with a project. You will learn all the essential things which are needed in data science and data analysis.\n\n\nBy the end of this course you will learn:\n1.NUMPY\nWhat is Numpy and how to use it?\nYou'll learn how to download install Anaconda.\nLearn about 1D, 2D, 3D arrays, how to create them, accessing them, changing them.\nLearn how Numpy array is better than a simple List with code.\nLearn axis in 2D array and 3D array which is too confusing to understand.\nLearn various Mathematical operations that you can perform on Numpy arrays like Addition, Subtraction, Multiplication, Division, Power, sin, cos, tan, Natural log, log base2, log base 10, etc.\nLearn Various Numpy functions like vertical stacking, horizontal stacking, mean, sum, variance, standard deviation.\nLearn Indexing and Slicing.\nWe'll do an exercise in which we learn to solve different Numpy related questions.\n2. Pandas\nWhat is Pandas and how it is useful in data analysis?\nLearn about the Series Data Structure, create them with a tuple, list, and dictionary.\nQuerying a Series\nLearn Indexing and Slicing using loc and iloc in 1D, 2D, and 3D arrays.\nLearn the DataFrame Data Structure, create them, analyze them, accessing them, etc\nLearn Reading data from files.\nLearn Indexing DataFrames.\nLearn to handle Missing Values\n3. Matplotlib\nLearn what is Matplotlib, why, and how to use it.\nLearn the Line plot and all operation on that plot like adding and changing the style of markers, legend, shape, face color, etc.\nSetting x and y-axis and use your data on the x and y-axis.\nLearn Subplots.\nLearn Pie Plot.\nLearn the Scatter Plot.\nLearn Bar plot\n4. Data Analysis Project\nIn this project, you'll be able to learn:\nhow to handle new data.\nhow to read datasets.\nhow to merge two datasets.\nRemoving unnecessary rows and columns.\nArrange dataset according to your need.\nPlot the datasets.\nBarplot with subplots.\nBarplot with multiple plots in a single diagram.\nETC.\nWith Python code notebooks, you will be excellently prepared for a future in data science.",
      "target_audience": [
        "Python developers curious about data science",
        "Python developers doing data analysis and visualization",
        "Students in college wanting to start a career in Data Science",
        "Junior Data Scientists aiming to boost their career prospects"
      ]
    },
    {
      "title": "실무에서 바로 쓰는 SQL",
      "url": "https://www.udemy.com/course/sql-rtzs/",
      "bio": "정보문화사에서 출간된 SQL200제 책의 저자 직강 강의. 그동안 수많은 데이터 분석가들이 유연수 강사의 SQL 강의로 데이터 검색 기술에 날개를 달았습니다. 수료생 추천 1위 . 2천명이 선택했던 그 과정",
      "objectives": [
        "수강생 만족도 4.9/5.0 의 명강의를 온라인으로 편안하게 시청하세요!!",
        "정보문화사의 SQL200제 저자의 직강 !",
        "국내의 수많은 데이터 분석가들이 유연수 강사의 SQL강의를 수강했습니다.",
        "SQL로 원하는 데이터에서 인사이트를 찾아내세요!"
      ],
      "course_content": {
        "SQL Basic": [
          "01 오라클 21c 설치 및 셋팅, Sqldeveloper 접속",
          "02 SQL - 테이블 데이터 출력방법",
          "03 SQL - 산술연산자, 비교연산자",
          "04 SQL - 특정 조건으로 데이터 출력하기",
          "05 SQL - 날짜 데이터와 데이터 유형 변환하기",
          "06 SQL - 조건문과 통계값 구해보기",
          "07 SQL - 데이터 분석 함수1",
          "08 SQL - ROW와 COLUMN 변환하기",
          "09 SQL - 데이터 분석 함수2",
          "10 SQL - 테이블 JOIN 1",
          "11 SQL - 테이블 JOIN 2",
          "12 SQL - 집합 연산자와 서브쿼리 사용하기 1",
          "13 SQL - 서브 쿼리 사용하기 2",
          "14 SQL - DDL 학습하기 1",
          "15 SQL - DDL 학습하기 2",
          "16 SQL - 서브 쿼리 사용하기 3",
          "17 SQL - 계층형 질의문과 테이블 생성하기",
          "18 SQL - VIEW와 INDEX, SEQUENCE",
          "19 SQL - FLASHBACK 학습하기 1 (QUERY, TABLE, DROP)",
          "20 SQL - FLASHBACK 학습하기 2 (VERSION QUERY, TRANSACTION)",
          "21 SQL - 데이터 품질 높여보기1 (PRIMARY KEY, UNIQUE)",
          "22 SQL - 데이터 품질 높여보기 2"
        ],
        "SQL 알고리즘 & 데이터 분석": [
          "23 SQL - SQL로 알고리즘 구현하기 1",
          "24 SQL - SQL로 알고리즘 구현하기 2",
          "25 SQL - SQL로 알고리즘 구현하기 3",
          "26 SQL - SQL로 빅데이터 분석하기"
        ]
      },
      "requirements": [
        "데이터베이스 소프트웨어를 설치할 컴퓨터만 있으면 됩니다."
      ],
      "description": "정보문화사에서 출간된 SQL200제 책의 저자 직강 강의 !\n그동안 수많은 데이터 분석가들이 유연수 강사의 SQL 강의로 데이터 검색 기술에 날개를 달았습니다.\n수료생 추천 1위 ! 2천명이 선택했던 그 과정 !  SQL 강의가 유데미에서 개설 되었습니다.\n\n\n강의 목차\nPART 1 〈입문〉 SQL 첫발 내딛기\n001 테이블에서 특정 열(COLUMN) 선택하기\n002 테이블에서 모든 열(COLUMN) 출력하기\n003 컬럼 별칭을 사용하여 출력되는 컬럼명 변경하기\n004 연결 연산자 사용하기(||)\n005 중복된 데이터를 제거해서 출력하기(DISTINCT)\n006 데이터를 정렬해서 출력하기(ORDER BY)\n007 WHERE절 배우기 ①(숫자 데이터 검색)\n008 WHERE절 배우기 ②(문자와 날짜 검색)\n009 산술 연산자 배우기(*, /, +, -)\n010 비교 연산자 배우기 ①(〉, 〈, 〉=, 〈=, =, !=, 〈〉, ^=)\n011 비교 연산자 배우기 ②(BETWEEN AND)\n012 비교 연산자 배우기 ③(LIKE)\n013 비교 연산자 배우기 ④(IS NULL)\n014 비교 연산자 배우기 ⑤(IN)\n015 논리 연산자 배우기(AND, OR, NOT)\n\nPART 2 〈초급〉 SQL 기초 다지기\n016 대소문자 변환 함수 배우기(UPPER, LOWER, INITCAP)\n017 문자에서 특정 철자 추출하기(SUBSTR)\n018 문자열의 길이를 출력하기(LENGTH)\n019 문자에서 특정 철자의 위치 출력하기(INSTR)\n020 특정 철자를 다른 철자로 변경하기(REPLACE)\n021 특정 철자를 N개 만큼 채우기(LPAD, RPAD)\n022 특정 철자 잘라내기(TRIM, RTRIM, LTRIM)\n023 반올림해서 출력하기(ROUND)\n024 숫자를 버리고 출력하기(TRUNC)\n025 나눈 나머지 값 출력하기(MOD)\n026 날짜 간 개월 수 출력하기(MONTHS_BETWEN)\n027 개월 수 더한 날짜 출력하기(ADD_MONTHS)\n028 특정 날짜 뒤에 오는 요일 날짜 출력하기(NEXT_DAY)\n029 특정 날짜가 있는 달의 마지막 날짜 출력하기(LAST_DAY)\n030 문자형으로 데이터 유형 변환하기(TO_CHAR)\n031 날짜형으로 데이터 유형 변환하기(TO_DATE)\n032 암시적 형 변환 이해하기\n033 NULL 값 대신 다른 데이터 출력하기(NVL, NVL2)\n034 IF문을 SQL로 구현하기 ①(DECODE)\n035 IF문을 SQL로 구현하기 ②(CASE)\n036 최대값 출력하기(MAX)\n037 최소값 출력하기(MIN)\n038 평균값 출력하기(AVG)\n039 토탈값 출력하기(SUM)\n040 건수 출력하기(COUNT)\n041 데이터 분석 함수로 순위 출력하기 ①(RANK)\n042 데이터 분석 함수로 순위 출력하기 ②(DENSE_RANK)\n043 데이터 분석 함수로 등급 출력하기(NTILE)\n044 데이터 분석 함수로 순위의 비율 출력하기(CUME_DIST)\n045 데이터 분석 함수로 데이터를 가로로 출력하기(LISTAGG)\n046 데이터 분석 함수로 바로 전 행과 다음 행 출력하기(LAG, LEAD)\n047 COLUMN을 ROW로 출력하기 ①(SUM+DECODE)\n048 COLUMN을 ROW로 출력하기 ②(PIVOT)\n049 ROW를 COLUMN으로 출력하기(UNPIVOT)\n050 데이터 분석 함수로 누적 데이터 출력하기(SUM OVER)\n051 데이터 분석 함수로 비율 출력하기(RATIO_TO_REPORT)\n052 데이터 분석 함수로 집계 결과 출력하기 ①(ROLLUP)\n053 데이터 분석 함수로 집계 결과 출력하기 ②(CUBE)\n054 데이터 분석 함수로 집계 결과 출력하기 ③(GROUPING SETS)\n055 데이터 분석 함수로 출력 결과 넘버링 하기(ROW_NUMBER)\n\nPART 3 〈중급〉 SQL 실력 다지기\n056 출력되는 행 제한하기 ①(ROWNUM)\n057 출력되는 행 제한하기 ②(Simple TOP-n Queries)\n058 여러 테이블의 데이터를 조인해서 출력하기 ①(EQUI JOIN)\n059 여러 테이블의 데이터를 조인해서 출력하기 ②(NON EQUI JOIN)\n060 여러 테이블의 데이터를 조인해서 출력하기 ③(OUTER JOIN)\n061 여러 테이블의 데이터를 조인해서 출력하기 ④(SELF JOIN)\n062 여러 테이블의 데이터를 조인해서 출력하기 ⑤(ON절)\n063 여러 테이블의 데이터를 조인해서 출력하기 ⑤(USING절)\n064 여러 테이블의 데이터를 조인해서 출력하기 ⑥(NATURAL JOIN)\n065 여러 테이블의 데이터를 조인해서 출력하기 ⑦(LEFT/RIGHT OUTER JOIN)\n066 여러 테이블의 데이터를 조인해서 출력하기 ⑧(FULL OUTER JOIN)\n067 집합 연산자로 데이터를 위아래로 연결하기 ①(UNION ALL)\n068 집합 연산자로 데이터를 위아래로 연결하기 ②(UNION)\n069 집합 연산자로 데이터의 교집합을 출력하기(INTERSECT)\n070 집합 연산자로 데이터의 차이를 출력하기(MINUS)\n071 서브 쿼리 사용하기 ①(단일행 서브쿼리)\n072 서브 쿼리 사용하기 ②(다중 행 서브쿼리)\n073 서브 쿼리 사용하기 ③(NOT IN)\n074 서브 쿼리 사용하기 ④(EXISTS와 NOT EXISTS)\n075 서브 쿼리 사용하기 ⑤(HAVING절의 서브 쿼리)\n076 서브 쿼리 사용하기 ⑥(FROM절의 서브 쿼리)\n077 서브 쿼리 사용하기 ⑦(SELECT절의 서브 쿼리)\n078 데이터 입력하기(INSERT)\n079 데이터 수정하기(UPDATE)\n080 데이터 삭제하기(DELETE, TRUNCATE, DROP)\n081 데이터 저장 및 취소하기(COMMIT, ROLLBACK)\n082 데이터 입력, 수정, 삭제 한번에 하기(MERGE)\n083 락(LOCK) 이해하기\n084 SELECT FOR UPDATE절 이해하기\n085 서브 쿼리를 사용하여 데이터 입력하기\n086 서브 쿼리를 사용하여 데이터 수정하기\n087 서브 쿼리를 사용하여 데이터 삭제하기\n088 서브 쿼리를 사용하여 데이터 합치기\n089 계층형 질의문으로 서열을 주고 데이터 출력하기 ①\n090 계층형 질의문으로 서열을 주고 데이터 출력하기 ②\n091 계층형 질의문으로 서열을 주고 데이터 출력하기 ③\n092 계층형 질의문으로 서열을 주고 데이터 출력하기 ④\n093 일반 테이블 생성하기(CREATE TABLE)\n094 임시 테이블 생성하기(CREATE TEMPORAY TABLE)\n095 복잡한 쿼리를 단순하게 하기 ①(VIEW)\n096 복잡한 쿼리를 단순하게 하기 ②(VIEW)\n097 데이터 검색 속도를 높이기(INDEX)\n098 절대로 중복되지 않는 번호 만들기(SEQUENE)\n099 실수로 지운 데이터 복구하기 ①(FLASHBACK QUERY)\n100 실수로 지운 데이터 복구하기 ②(FLASHBACK TABLE)\n101 실수로 지운 데이터 복구하기 ③(FLASHBACK DROP)\n102 실수로 지운 데이터 복구하기 ④(FLASHBACK VERSION QUERY)\n103 실수로 지운 데이터 복구하기 ⑤(FLASHBACK TRANSACTION QUERY)\n104 데이터의 품질 높이기 ①(PRIMARY KEY)\n105 데이터의 품질 높이기 ②(UNIQUE)\n106 데이터의 품질 높이기 ③(NOT NULL)\n107 데이터의 품질 높이기 ④(CHECK)\n108 데이터의 품질 높이기 ⑤(FOREIGN KEY)\n109 WITH절 사용하기 ①(WITH ~ AS)\n110 WITH절 사용하기 ②(SUBQUERY FACTORING)\n111 SQL로 알고리즘 문제 풀기 ①(구구단 2단 출력)\n112 SQL로 알고리즘 문제 풀기 ②(구구단 1단 ~ 9단 출력)\n113 SQL로 알고리즘 문제 풀기 ③(직각삼각형 출력)\n114 SQL로 알고리즘 문제 풀기 ④(삼각형 출력)\n115 SQL로 알고리즘 문제 풀기 ⑤(마름모 출력)\n116 SQL로 알고리즘 문제 풀기 ⑥(사각형 출력)\n117 SQL로 알고리즘 문제 풀기 ⑦(1부터 10까지 숫자의 합)\n118 SQL로 알고리즘 문제 풀기 ⑧(1부터 10까지 숫자의 곱)\n119 SQL로 알고리즘 문제 풀기 ⑨(1부터 10까지 짝수만 출력)\n120 SQL로 알고리즘 문제 풀기 ⑩(1부터 10까지 소수만 출력)\n121 SQL로 알고리즘 문제 풀기 ⑪(최대 공약수)\n122 SQL로 알고리즘 문제 풀기 ⑫(최소 공배수)\n123 SQL로 알고리즘 문제 풀기 ⑬(피타고라스의 정리)\n124 SQL로 알고리즘 문제 풀기 ⑭(몬테카를로 알고리즘)\n125 SQL로 알고리즘 문제 풀기 ⑮(오일러 상수 자연상수 구하기)\n\nPART 4 〈활용〉 SQL 응용 다지기\n126 엑셀 데이터를 DB에 로드하는 방법\n127 스티브 잡스 연설문에서 가장 많이 나오는 단어는 무엇인가?\n128 스티브 잡스 연설문에는 긍정 단어가 많은가 부정 단어가 많은가?\n129 절도가 많이 발생하는 요일은 언제인가?\n130 우리나라에서 대학 등록금이 가장 높은 학교는 어디인가?\n131 서울시 물가 중 가장 비싼 품목과 가격은 무엇인가?\n132 살인이 가장 많이 발생하는 장소는 어디인가?\n133 가정불화로 생기는 가장 큰 범죄 유형은 무엇인가?\n134 방화 사건의 가장 큰 원인은 무엇인가?\n135 전국에서 교통사고가 제일 많이 발생하는 지역은 어디인가?\n136 치킨집 폐업이 가장 많았던 연도가 언제인가?\n137 세계에서 근무 시간이 가장 긴 나라는 어디인가?\n138 남자와 여자가 각각 많이 걸리는 암은 무엇인가?",
      "target_audience": [
        "데이터 과학"
      ]
    },
    {
      "title": "如何运用开源商用大模型Llama（中文课程）",
      "url": "https://www.udemy.com/course/soulai-llama2/",
      "bio": "掌握AI新技术，云端配置、模型部署、实际应用全面解析",
      "objectives": [
        "理解并应用Llama2模型：学员能理解Llama2模型的基本构成，知道它的进化历程以及可能的应用场景。学员也将能够明白Llama2模型在AI领域的重要性和作用。",
        "熟练操作AWS和EC2环境设置：学员将会掌握如何在AWS上进行账户设置，选择合适的算力，申请所需资源，并能熟练地在云端进行设置。",
        "掌握Llama2模型的设置、安装和测试：学员将能够熟练地申请和设置Llama2模型，了解如何在Github上进行设置，安装模型，以及如何进行模型测试。",
        "运用Llama2模型进行实际应用部署：学员将会掌握如何运用AWS的各项服务进行Web App部署，并掌握Llama 2的八大应用实例。"
      ],
      "course_content": {
        "课程介绍": [
          "课程简介",
          "讲师介绍",
          "课程大纲",
          "How老师的话：未来已来，只是分布不均"
        ],
        "Llama 2介绍": [
          "模型简介",
          "模型进化",
          "模型应用",
          "How老师的话：借鉴过去，塑造未来",
          "How老师课程群"
        ],
        "AWS算力配置": [
          "章节简介",
          "账户设置",
          "算力选择",
          "资源申请",
          "云端设置",
          "How老师的话：准备工作是成功的一半"
        ],
        "EC2环境设置": [
          "章节简介",
          "基本环境",
          "硬件环境",
          "模型环境",
          "How老师的话：EC2环境的重要性"
        ],
        "Llama 2模型设置": [
          "章节简介",
          "模型申请",
          "Github设置",
          "模型安装",
          "模型测试",
          "How老师的话：理论与实践"
        ],
        "Llama 2模型部署": [
          "章节介绍",
          "FastAPI部署",
          "Lambda Function部署",
          "API Gateway部署",
          "Web App部署",
          "How老师的话：AI解决问题，创造价值"
        ],
        "Llama 2模型应用": [
          "章节介绍",
          "应用实例一：QA问答",
          "应用实例二：文本生成",
          "应用实例三：内容翻译",
          "应用实例四：文章总结",
          "应用实例五：解决方案",
          "应用实例六：休闲聊天",
          "应用实例七：创意内容",
          "应用实例八：定义解释",
          "How老师的话：像艺术家一样创造，像科学家一样深挖"
        ],
        "课程总结": [
          "课程总结",
          "行业展望",
          "How老师的话：站在Llama 2的肩膀上"
        ]
      },
      "requirements": [
        "我们坚信学习的热情和决心是最重要的。即使你在以下这些方面的经验不足，只要有决心，也可以通过本课程系统地学习和掌握这些技能。我们会在课程中提供足够的指导和帮助，让你可以顺利完成学习任务。",
        "编程基础：理想情况下，学员应具备Python或者Javascript语言的基础编程能力。本课程涉及到的模型设置、环境配置和模型应用都需要一定的编程基础。如果你已经有Python或者其他编程语言的经验，将更能够深入理解和应用课程内容。",
        "基本的机器学习理论知识：理解和应用Llama2模型，需要一定的机器学习理论知识。例如，你应该了解什么是深度学习，了解基础的模型架构等。",
        "AWS的基础知识：如果你已经对AWS云服务有基础的了解和使用经验，那么在配置和应用AWS算力方面会更得心应手。",
        "基础的硬件和软件设备：本课程需要学员使用电脑进行操作，需要稳定的网络环境，并且需要安装一些基础的开发工具，例如Python解释器和代码编辑器。"
      ],
      "description": "\"如何运用开源商用大模型Llama\"课程将带领你全面深入了解和实践使用AI新兴模型Llama。我们首先会介绍模型的基础知识，解析其模型结构与工作原理。接下来，我们将跳出理论框架，进入实战环节。你将学习如何在云端，特别是在AWS环境中，进行模型的部署和运行。我们会详细介绍如何配置AWS的算力，如何在EC2环境中进行模型的设置和测试，直到部署成为一个可以实际运行的服务。最后，我们会介绍模型的实际应用，将模型与实际业务进行对接，提供有效解决方案。你将看到模型如何在问答、文本生成、内容翻译、文章总结等多种场景中进行应用。这不仅是一门技术课程，更是一门实战课程。我们期待你能在此过程中，既能提升自身的AI技术水平，又能对如何将AI技术应用于实际问题有更深入的理解。",
      "target_audience": [
        "本课程会以全方位、深入浅出的方式，帮助你了解、学习并掌握如何运用开源商用大模型Llama2，期待你的加入！",
        "AI爱好者和学者：如果你对人工智能、深度学习模型有浓厚的兴趣，希望进一步了解和掌握最新的AI技术和开源模型，那么你会从本课程中获得大量的有价值信息和知识。",
        "创业者和企业决策者：如果你正在运营或者计划创立一个与AI技术相关的企业，希望找到可以提升企业效率、降低成本的解决方案，那么本课程会为你提供一些实践的操作和理念。",
        "软件开发者和工程师：如果你是一位软件开发者，希望扩展你的技能集，将AI技术和云计算能力纳入你的技术栈，那么本课程将为你提供实际的项目操作经验和知识。",
        "学生和职业发展者：如果你是在校学生，或者希望通过学习最新的AI技术来提升职业竞争力，那么本课程将为你打开新的视野，让你了解AI行业的最新动态和应用。",
        "热爱学习的所有人：无论你的背景是什么，只要你对AI和开源模型感兴趣，都欢迎你参与我们的课程。我们相信，只要有热爱学习的心，没有什么是无法达成的。"
      ]
    },
    {
      "title": "Python/PuLPで解く初めての数理最適化（Google Colaboratoryで実践）",
      "url": "https://www.udemy.com/course/python-pulp-colab/",
      "bio": "Google ColaboratoryとPuLPライブラリを使って数理最適化にチャレンジしましょう！巡回セールスマン問題やナップサック問題、シフト最適化といった有名な問題に実際に取り組み、理解を深めます。",
      "objectives": [
        "数理最適化の活用事例",
        "数理最適化の基本概念",
        "PythonライブラリPuLPの使い方",
        "線形計画問題の解き方",
        "巡回セールスマン問題",
        "ナップサック問題"
      ],
      "course_content": {
        "コース紹介": [
          "本コースの紹介",
          "コース準備レクチャー"
        ],
        "1. 数理最適化とは": [
          "数理最適化のイメージ",
          "数理最適化における定式化",
          "数理最適化を実務に適用する場合の流れ",
          "数理最適化のテーマと種類"
        ],
        "2. PuLPを使用した数理最適化": [
          "PuLPとは",
          "参考コードのダウンロード",
          "2.1 簡単な線形最適化の問題",
          "2.1 PuLPによる簡単な線形最適化問題の実践①",
          "2.1 PULPによる簡単な線形最適化問題の実践②",
          "2.1 線形最適化問題の具体例",
          "2.1 PuLPによる線形最適化問題の具体例の実践",
          "2.2 ナップサック問題",
          "2.2 PuLPによるナップサック問題の実践",
          "2.3 巡回セールスマン問題①",
          "2.3 巡回セールスマン問題②",
          "2.3 巡回セールスマン問題③",
          "2.3 PuLPによる巡回セールスマン問題の実践①",
          "2.3 PuLPによる巡回セールスマン問題の実践②",
          "2.4 シフト最適化問題①",
          "2.4 シフト最適化問題②",
          "2.4 PuLPによるシフト最適化問題の実践①",
          "2.4 PuLPによるシフト最適化問題の実践②"
        ],
        "終わりに": [
          "実務で数理最適化を使う場合の注意点"
        ]
      },
      "requirements": [
        "前提条件なし"
      ],
      "description": "数理最適化は、様々な現実世界の問題に対して最適解を見つけるための手法です。\n本コースでは、簡単な線形計画問題から始め、ナップサック問題、巡回セールスマン問題、そしてシフト最適化など、実践的な課題に焦点を当て、その基礎についてPythonのPuLPライブラリを使用して学んでいきます。\n\n\nコース内容\n簡単な線形計画問題\n数理最適化の基礎として、まず初めに線形計画問題の基礎を学びます。これにより、PuLPを使用した最適化問題へのアプローチを理解します。\nナップサック問題\n現実のリソース配分問題を最適化するためにナップサック問題に取り組みます。\n巡回セールスマン問題\n最適な経路を見つけるための巡回セールスマン問題に挑戦します。\nシフト最適化\n効率的で制約条件を満たすシフトスケジュールを作成します。\n\n\n受講対象者\nPythonに興味があり、数理最適化の基本を学びたい方\nビジネスやエンジニアリングの領域で最適化問題に対応したい方\n新しいことを学びたいデータサイエンティストやデータアナリストの方\n\n\n前提知識\nPythonの基本的な知識があれば理解が容易ですが、プログラミング初心者でも問題ありません。\n\n\nこのコースを受講することで、PythonとPuLPを使用して数理最適化問題にアプローチし、解決するスキルの基礎が身につくはずです。\n現実のビジネス課題に対して、最適化手法を用いてデータドリブンな意思決定に貢献できるようになっていきましょう！",
      "target_audience": [
        "数理最適化の勉強をしたい方",
        "仕事で数理最適化を使う必要が出てきた方",
        "Pythonで数理最適化を動かして見たい方"
      ]
    },
    {
      "title": "Alteryx TRIFACTA e NIFI: streaming e preparação de dados",
      "url": "https://www.udemy.com/course/alteryx-trifacta-e-nifi-streaming-e-preparacao-de-dados/",
      "bio": "Trabalhando com fluxo de dados e construindo pipelines de ajustes, qualidade de dados",
      "objectives": [
        "Preparação de dados aberta que pode se conectar a diversas fontes de dados",
        "Integração em todas as principais plataformas de dados em nuvem",
        "Decida entre ETL ou ELT, ou uma combinação ideal dos dois com base no desempenho",
        "Suporte para todas as principais nuvens, Google, AWS, Azure e on-premise",
        "Interface intuitiva e simples utilização de objetos de dados",
        "Perfilização de dados, ajudando na identificação de outliers",
        "Tratamento de dados, criação de novos campos, dentre outras tarefas",
        "Eliminação de dados nulos, inconsistências, criação de novos campos",
        "Exploração e avaliação de conteúdo e de qualidade de qualquer conjunto de dados",
        "Engenharia de dados com low-code, visual, direto na nuvem",
        "Construção, implantação e automatização de pipelines de dados",
        "Criação de flow de dados, que permite ao analista encadear suas ações de tratamento",
        "Action com os dados: Columns, Rename, Sort, Calculate, Group By, Filter Rows, Replace",
        "Action com os dados: Split, Create formula, dentre outros",
        "Exportação dos resultados automatizados",
        "Entendo sobre Apache Nifi, uma plataforma de ingestão de dados",
        "Entendo sobre o gerenciamento e a automatização do fluxo de dados",
        "Entendendo sobre coleta de dados, transmissão de dados, armazenamento de dados",
        "Ecossistema NiFi: Repositórios, controle de fluxo, máquina JVM, extensões",
        "O que é Flow File",
        "O que é um Processor",
        "O que é um Fluxo de Dados",
        "O que é uma conexão",
        "O que é um grupo de processor",
        "Aprendendo sobre a barra de componentes",
        "Aprendendo sobre a barra de controle navegação",
        "Aprendendo sobre a barra de operação",
        "Desenvolvimento de diversos fluxos de dados",
        "Extensões: Putfile, Getfile, ExtractText, SplitText",
        "Extensões: EvaluateXpath, UpdateAttribute, GenerateFlowFile",
        "Extensões: RouteOnAttribute, SplitXML, LogMessage, LogAttibute",
        "Extensões: MergeContent, ReplaceText, CompressContent",
        "Uso de Input Port, Funil e Process Group"
      ],
      "course_content": {
        "Alteryx TRIFACTA - Preparação de dados - explore e qualidade": [
          "O que é o Alteryx TRIFACTA",
          "Apresentação da Ferramenta e arquitetura",
          "INFORMAÇÕES IMPORTANTES! - Leia antes de começar o curso",
          "Carregando os dados e fazendo Explorer dos dados",
          "Action Substituir - Tratamento de dados - parte 01",
          "Action Rename e Sort - Tratamento de dados - parte 02",
          "Action Move e Hide - Tratamento de dados - parte 03",
          "Action Format - Tratamento de dados - parte 04",
          "Action Calculate - Tratamento de dados - parte 05",
          "Action Create Columns From Examples - Tratamento de dados - parte 06",
          "Action Extract - Tratamento de dados - parte 07",
          "Action Split Column - Tratamento de dados - parte 08",
          "Action Merge - Tratamento de dados - parte 09",
          "Action Group By - Tratamento de dados - parte 10",
          "Action Filtro - Tratamento de dados - parte 11",
          "Aplicação de Funções de Controle",
          "Aplicação de Funções Especiais",
          "Executando o Fluxo de dados e gerando arquivo de saída",
          "Gerando o agendamento (Schedule) do Fluxo de Dados",
          "Aula Final - Atividade para entrega"
        ],
        "NiFi : Construindo fluxo de dados em projetos de dados": [
          "Entendo sobre o Nifi",
          "Preparando o ambiente para instalação",
          "Instalação: Java JRE",
          "Instalação: NiFi",
          "Fluxo de Dados: Carga 1 - Gerando o primeiro fluxo de dados",
          "Fluxo de Dados: Carga 2 - Salvando arquivos gerados em tempo real",
          "Fluxo de Dados: Carga 3 - Movendo arquivo entre pastas",
          "Fluxo de Dados: Carga 4 - Compactando arquivos",
          "Fluxo de Dados: Carga 5 - Filtrando um conjunto de dados",
          "Fluxo de Dados: Carga 6 - Alterando conteúdo de dados",
          "Fluxo de Dados: Carga 7 - Extraindo dados XML",
          "Fluxo de Dados: Carga 8 - Interligação de fluxo de dados",
          "Aula Final - Atividade para entrega",
          "Responda a nossa pergunta"
        ]
      },
      "requirements": [
        "Necessário entendimento básico sobre banco de dados",
        "Importante que já tenha tido o contato com alguma linguagem de programação"
      ],
      "description": "Este é o tipo de treinamento que vai fazer você mudar a sua concepção sobre manipulação, tratamento e construção de cargas que envolvem dados, dados estruturados ou não estruturados. Trabalharemos com duas das grandes ferramentas de mercado que permitem manipular o dado até a exaustão, estamos falando do Alteryx TRIFACTA e do Apache Nifi.\nO Nifi é uma plataforma de ingestão de dados, criada para processar e distribuir dados entre diferentes sistemas, sua principal funcionalidade é o gerenciamento e a automatização do fluxo de dados entre os sistemas, para execução destes fluxos de dados é fornecida uma interface WEB . É a programação baseada em fluxos.\nEste é um curso de introdução, trabalharemos com modelos simples e típicos da ferramenta Nifi.\nÉ possível realizar: coleta de dados, transmissão de dados, armazenamento de dados, dentre outras tarefas.\nNiFi é um programa java que é executado dentro de uma JVM em um servidor, aproveita o conceito de extração, transformação e carga. A plataforma foi construída para que você possa ter liberdade para determinar como deseja construir seus fluxos de dados independente de tecnologia e linguagem, ele possui programas conhecidos como Processadores que realizam estas atividades diversas.\nÉ uma das principais plataformas criadas atualmente para manipulação de grandes bases de dados sendo estas em bancos convencionais ou No Sql.\nJá o Alteryx TRIFACTA, é uma ferramenta 100% na nuvem, low-code, totalmente prática e com grande destaque no mercado. Ela é uma plataforma em nuvem aberta e interativa, que permite a capacitação de engenheiros de dados e analistas a interpretar, preparar e criar pipelines de dados para acelerar suas análises.\nA sua principal tarefa é ler uma base de dados, identificar os principais pontos de ajustes nos dados, permitir que sejam construídas transformações nos dados e executar um pipeline de dados (fluxo de dados) gerando as informações ajustadas em qualquer fonte de dados, tudo 100% visual.\nCom isso, você comandará a governança de dados nos seus dados, pois identificará outliers, inconsistências, ausências de informações, identificações de padrões, dentre outras tarefas.\n\n\nAs principais características do Alteryx TRIFACTA são:\nExplore e avalie o conteúdo e a qualidade de qualquer conjunto de dados.\nAcelere e acompanhe transformações de dados de forma visual.\nConstrua, implante e automatize pipelines de dados.\nUtilize os fluxos de dados para definir TODAS as suas necessidades em tratamento de dados e governança de dados\nEntão comece hoje mesmo e aprenda em um treinamento que vai lhe trazer um conhecimento aprofundado na manipulação de dados.",
      "target_audience": [
        "Profissionais de TI",
        "Profissionais que querem trabalham na área de Engenharia de dados, Análise de dados, Ciência de Dados, Business Intelligence",
        "Pessoas interessadas em aprender os conceitos sobre NiFi, ou que gostariam adentrar na área de engenharia de dados"
      ]
    },
    {
      "title": "Apache Spark mit Databricks - Crashkurs",
      "url": "https://www.udemy.com/course/apache-spark-mit-databricks-crashkurs/",
      "bio": "Lerne, wie Du ETL Aufgaben mit Spark und Databricks schnell und einfach lösen kannst.",
      "objectives": [
        "Verstehe die Architektur von Apache Spark und lerne die Grundlagen zur Apache Spark Programmierung.",
        "Lerne, was Databricks ist und wie du damit Data Science/Engineering Aufgaben schnell und einfach lösen kannst.",
        "Lerne die Spark DataFrames API praktisch anzuwenden."
      ],
      "course_content": {
        "Einleitung": [
          "Einleitung"
        ],
        "Databricks Einführung": [
          "Databricks Einführung",
          "Databricks Community Edition [Einrichtung]",
          "Erste Schritte mit Databricks",
          "Bewertung"
        ],
        "Apache Spark Einführung": [
          "Einführung in Spark",
          "Spark Session",
          "Spark RDD",
          "Erstellung von RDDs [Übung]",
          "Architektur einer Spark Applikation",
          "Spark RDD Operationen",
          "Spark RDD Operationen [Übung]",
          "DAG"
        ],
        "Apache Spark mit Databricks": [
          "Spark mit Databricks Kapitelüberblick",
          "Spark DataFrames",
          "Spark DataFrames [Übung]",
          "Databricks Dateisystem",
          "Databricks Dateisystem [Übung]",
          "Databricks Magic-Befehle",
          "Übungsaufgabe - DataFrame Fussball Bundesliga [Aufgabenstellung]",
          "Übungsaufgabe - DataFrame Fussball Bundesliga Teil 1 [Lösung]",
          "Übungsaufgabe - DataFrame Fussball Bundesliga Teil 2 [Lösung]"
        ],
        "Abschluss": [
          "Abschluss"
        ]
      },
      "requirements": [
        "Grundkenntnisse in Python und SQL"
      ],
      "description": "Databricks wurde von den Apache Spark Schöpfern gegründet. Databricks stellt eine webbasierte Plattform für Datenanalysen mit Apache Spark bereit, die Data Scientists, Data Engineers, Machine-Learning Engineers und Data Analysts zusammenbringt und ist mit der AWS oder Azure Cloud integrierbar.\nDurch die zusätzlichen Features, die Databricks mitbringt sind produktive und skalierbare Data Science und Data Engineering möglich. Die Features sind eine optimierte Performance auf Apache Spark, zuverlässige und leistungsstarke Data Lakes mit Delta Lake, Interaktive Data Science und Zusammenarbeit zwischen den unterschiedlichen Beteiligten wie Data Scientists, Data Engineers, Machine-Learning Engineers usw..\nMit Databricks sind Jobs und Workflows in Produktivumgebung möglich, für die Anforderungen hinsichtlich Unternemenssicherheit ist durch End-to-End Datenschutz und Compliance gesorgt, bekannte und geläufige Tools sind ohne Probleme integrierbar und es wird ein Experten-Support durch die Apache Spark Schöpfer gegeben.\n\n\nNach erfolgreichem Abschluss des Kurses wirst du in der Lage sein mit Databricks interaktive Data Science und Data Engineering zu betreiben. Du bekommst eine Einführung in die Grundlagen von Apache Spark und Databricks. Du lernst mit Spark DataFrames umzugehen sowie Aufgaben mit der kostenlosen Community Edition schnell und einfach zu lösen.\n\n\nWas erwartet dich in dem Kurs?\nDu bekommst über 20 theoretische und praktische Lektionen sowie Übungsaufgaben – was mehr als 3 Stunden Videomaterial umfasst.\nDich erwartet ein Abschlussprojekt mit echten Daten und Databricks-Notebooks sowohl mit Vorlage als auch Lösung, die du herunterladen kannst.\nDu erhältst Zugang zum Online Q&A Forum, wo entweder andere Kursteilnehmer oder ich deine Fragen beantworten werden.\nUnd schließlich erhältst du auch ein Zertifikat bei erfolgreichem Kursabschluss, das sich gut in deinem Lebenslauf macht.\n\n\nAchtung!\nDieser Kurs überschneidet sich mit meinem Kurs \"Spark mit Databricks in AWS für Data Science/Engineering\", der die Inhalte diesen Kurses und noch umfangreiche weitere Lektionen für Databricks in der AWS Cloud enthält.\n\n\n30 Tage Geld-zurück-Garantie!\nWenn du mit dem Kurs schließlich nicht zufrieden bist, kannst du ihn gerne ohne Probleme innerhalb von 30 Tagen zurückgeben und du bekommst dein Geld wieder.\n\n\nFür wen ist dieser Kurs eher nicht geeignet?\nAlle, die PySpark, Databricks oder AWS von A-Z lernen möchten – also alle Aspekte von hinten bis vorne wissen möchten – werden in diesem Kurs höchstwahrscheinlich nicht auf ihre Kosten kommen.\nDer Ansatz, der hier genommen wird ist, dass wir notwendige Spark, Databricks und AWS Features lernen werden, um schnell und mit praktischem Fokus Mehrwert durch Projekte im Data Engineering und Data Science Bereich zu generieren.",
      "target_audience": [
        "Jeder, der Apache Spark mit Databricks lernen möchte",
        "Data Engineers, Data Scientists, Machine-Learning Engineers mit und ohne Vorkenntnisses in Apache Spark",
        "Informatiker, Python-Entwickler, DWH-Entwickler mit geringen Vorkenntnisses in Apache Spark und Interesse an Data Engineering/Data Science"
      ]
    },
    {
      "title": "Python | Yeni Başlayanlar İçin Hızlı Ve Pratik Öğrenme |2024",
      "url": "https://www.udemy.com/course/python-yeni-baslayanlar-icin-hzl-ve-pratik-ogrenme-2024/",
      "bio": "Jupyter Notebooks'ta interaktif derslerle Python öğrenin. Her bölüm için Lab dosyalarıyla pratik yaparak hızla ilerleyin",
      "objectives": [
        "Veri Bilimi ve Yazılım Geliştirme için en popüler programlama dili olan Python'u öğrenin.",
        "Python programlama mantığını uygulamak Değişkenler, Veri Yapıları, Dallanma, Döngüler, Fonksiyonlar, Nesneler ve Sınıflar.",
        "Pandas & Numpy gibi Python kütüphanelerini kullanma ve Jupyter Notebooks kullanarak kod geliştirme konusunda yeterlilik gösterme.",
        "API'leri ve Beautiful Soup gibi Python kütüphanelerini kullanarak verilere erişin ve web'de kazıyın."
      ],
      "course_content": {
        "Kurs Hakkında": [
          "Kurs Tanıtımı",
          "Kursa Genel Bakış"
        ],
        "Python ve Jupyter ile Başlarken": [
          "Python ve Jupyter Kurulum",
          "Jupyter'e Giriş",
          "Jupyter ile Başlarken",
          "İlk Programınızı Yazın"
        ],
        "Veri Türleri": [
          "Veri Türleri",
          "Alıştırma Sınavı : Türler",
          "İfadeler ve Değişkenler",
          "Alıştırma Sınavı: İfadeler ve Değişkenler",
          "Dize İşlemleri"
        ],
        "Python Veri Yapıları": [
          "Listeler",
          "Demetler",
          "Alıştırma Sınavı : Listeler ve Demetler",
          "Sözlükler",
          "Alıştırma Sınavı : Sözlükler",
          "Kümeler",
          "Alıştırma Sınavı : Kümeler"
        ],
        "Python Programlamanın Temelleri": [
          "Koşullar",
          "Alıştırma Sınavı: Koşullar",
          "Döngüler",
          "Alıştırma Sınavı: Döngüler",
          "Fonksiyonlar 1",
          "Fonksiyonlar 2",
          "Fonksiyonlar 3",
          "Alıştırma Sınavı : Fonksiyonlar",
          "İstisnai Durumlar (Try & Except)",
          "Nesneler ve Sınıflar"
        ],
        "Python'da Verilerle Çalışma": [
          "Open ile Dosyaları Okuma",
          "Open ile Dosya Yazma",
          "Pandas: Verilerle Çalışma ve Kaydetme",
          "Tek Boyutlu Numpy",
          "İki Boyutlu Numpy"
        ],
        "API'ler ve Veri Toplama": [
          "API'ye Giriş",
          "REST API'ler",
          "Web Kazıma ve Dosyalarla Çalışma"
        ]
      },
      "requirements": [
        "Programlama deneyimi gerekmez. Bilmeniz gereken her şeyi öğreneceksiniz"
      ],
      "description": "Yeni başlayanlar için uygun olan bu kendi hızınızda ilerleyen kursla Python öğrenmeye başlayın. Python, programlama ve veri bilimi dünyasında en popüler dillerden biridir ve Python'ı kullanma becerisine sahip bireylere olan talep hiç bu kadar yüksek olmamıştı.\n\n\nPython'a giriş niteliğindeki bu kurs, sizi birkaç saat içinde Python'da sıfırdan programlamaya taşıyacak - önceden programlama deneyimi gerektirmez!\n\n\nPython'un temellerini öğrenecek ve farklı veri türleri hakkında bilgi edineceksiniz. Liste ve Tuple gibi Python veri yapılarına, koşullar ve dallanma gibi mantık kavramlarına aşina olacaksınız. Python kütüphaneleri olan Pandas, Numpy ve Beautiful Soup kullanarak veri işleme becerilerinizi geliştireceksiniz. Ayrıca API'ler ile veri toplama ve web kazıma gibi ileri seviye görevleri gerçekleştirmek için Python'u nasıl kullanacağınızı öğreneceksiniz. Jupyter Notebooks kullanarak uygulamalı laboratuvarlar aracılığıyla öğrendiklerinizi pratik edecek ve pekiştireceksiniz.\nBu tamamen uygulamalı ve interaktif eğitimin sonunda, Python kullanarak temel programlar oluşturabilir, verilerle çalışabilir ve gerçek dünyadaki görevleri otomatikleştirebilir hale geleceksiniz. Bu kurs, özellikle Veri Bilimi, Veri Analitiği, Yazılım Geliştirme, Veri Mühendisliği, Yapay Zeka ve DevOps alanlarında kariyer yapmayı düşünenler için mükemmel bir başlangıç noktasıdır.\nEğitim boyunca karşılaşabileceğiniz zorlukların üstesinden gelmek için gereken bilgi ve araçları sağlamayı amaçlıyoruz. Bu kurs, teknoloji alanındaki yeni ve heyecan verici kariyer fırsatlarını keşfetmek isteyen herkes için idealdir.\nÖğrenimi pekiştirmek için sürekli ders sonlarında sorular ve Jupyter notebook içerisindeki Task ve Quizler ile öğrendiğiniz bilgileri pekiştirmenizi ve geliştirmenizi sağlıyoruz.",
      "target_audience": [
        "Veri bilimine meraklı, başlangıç düzeyindeki Python geliştiricileri"
      ]
    },
    {
      "title": "Python-深度学习-物体检测实战",
      "url": "https://www.udemy.com/course/maskrcnn/",
      "bio": "MaskRcnn实战",
      "objectives": [
        "物体检测经典算法MaskRcnn",
        "物体检测框架源码解读",
        "物体检测算法各核心模块详解",
        "熟练使用MaskRcnn进行实际任务",
        "基于labelme工具进行数据集构建",
        "熟练应用物体检测框架到自己的实际项目中"
      ],
      "course_content": {
        "物体检测框架-MaskRcnn项目介绍与配置": [
          "课程简介",
          "Mask-Rcnn开源项目简介",
          "开源项目数据集",
          "参数配置",
          "课程数据代码下载（谷歌网盘）"
        ],
        "MaskRcnn网络框架源码详解": [
          "FPN层特征提取原理解读",
          "FPN网络架构实现解读",
          "生成框比例设置",
          "基于不同尺度特征图生成所有框",
          "RPN层的作用与实现解读",
          "候选框过滤方法",
          "Proposal层实现方法",
          "DetectionTarget层的作用",
          "正负样本选择与标签定义",
          "RoiPooling层的作用与目的",
          "RorAlign操作的效果",
          "整体框架回顾"
        ],
        "于MASK-RCNN框架训练自己的数据与任务": [
          "Labelme工具安装",
          "使用labelme进行数据与标签标注",
          "完成训练数据准备工作",
          "maskrcnn源码修改方法",
          "基于标注数据训练所需任务",
          "测试与展示模块"
        ],
        "练手小项目-人体姿态识别demo": [
          "COCO数据集与人体姿态识别简介",
          "网络架构概述",
          "流程与结果演示"
        ],
        "必备基础-迁移学习与Resnet网络架构": [
          "迁移学习的目标",
          "迁移学习策略",
          "Resnet原理",
          "Resnet网络细节",
          "Resnet基本处理操作",
          "shortcut模块",
          "加载训练好的权重",
          "迁移学习效果对比"
        ],
        "必备基础-物体检测FasterRcnn系列": [
          "物体检测经典算法概述",
          "经典检测方法",
          "faster-rcnn概述",
          "论文整体概述",
          "RPN网络结构",
          "损失函数定义",
          "网络细节"
        ]
      },
      "requirements": [
        "熟悉Python",
        "熟悉深度学习中的CNN网络"
      ],
      "description": "计算机视觉-物体检测-通用解决框架Mask-Rcnn实战课程旨在帮助同学们快速掌握物体检测领域当下主流解决方案与网络框架构建原理，基于开源项目解读其应用领域与使用方法。通过debug方式，详细解读项目中每一模块核心源码，从代码角度理解网络实现方法与建模流程。为了方便同学们能将项目应用到自己的数据与任务中，实例演示如何针对自己的数据集制作标签与代码调整方法，全程实战操作，通俗讲解其中复杂的网络架构。",
      "target_audience": [
        "人工智能方向的同学们",
        "深度学习领域的同学们"
      ]
    },
    {
      "title": "비트코인 암호화폐 자동매매 코인봇 만들기 Part 1 - 무위험 전략 학습하기",
      "url": "https://www.udemy.com/course/bitcoin-crypto-trading-bot-part-1/",
      "bio": "암호화폐 자동매매 코인봇 제작의 A-to-Z (업비트, 빗썸)",
      "objectives": [
        "암호화폐 자동 매매 코인봇 만들기 (거래소 API 사용법 기초부터 AWS를 이용한 배포까지)",
        "기초 투자 전략 (변동성 돌파 전략 등)",
        "암호화폐 시장의 구조적 특성을 이용한 연 10% 내외의 무위험 수익을 얻을 수 있는 투자전략 (김치 프리미엄 이용 등)",
        "투자 전략 개발 방법론"
      ],
      "course_content": {
        "강의 소개": [
          "강의 소개"
        ],
        "강의에서 구현하는 소스코드 다운로드": [
          "강의에서 구현하는 소스코드 다운로드"
        ],
        "암호화폐 자동매매 코인봇 구현을 위한 기초지식": [
          "암호화폐 자동 매매 시스템 구현을 위한 Python 기초 - f-string, 딕셔너리(dict), if문",
          "f-string, 딕셔너리(dict), if문 구현",
          "REST(Representational State Transfer) API 기초"
        ],
        "빗썸 API": [
          "빗썸, 업비트 거래소 소개",
          "빗썸 API 활성화",
          "빗썸 API 샘플 코드 다운로드 및 실행"
        ],
        "빗썸 API - Public API": [
          "빗썸 Public API 소개",
          "GET, POST 메소드, 타임스탬프(Timestamp), 빗썸 Public API - Ticker",
          "빗썸 Public API - Transaction History",
          "빗썸 Public API – Assets Status",
          "빗썸 Public API - BTCI (빗썸지수)",
          "빗썸 API 에러 코드(Error Code)"
        ],
        "빗썸 API - 매수, 매도 주문 구현": [
          "호가창(Orderbook)의 구조",
          "빗썸 Public API - Orderbook",
          "지정가 주문과 시장가 주문",
          "빗썸 지정가 매수 API, 시장가 매수 주문 API",
          "빗썸 지정가 매수 API, 시장가 매수 주문 API 구현",
          "빗썸 지정가 매도 API, 시장가 매도 주문 API",
          "빗썸 지정가 매수 API, 시장가 매도 주문 API 구현"
        ],
        "빗썸 API - 잔고확인, 주문확인, 취소주문, 거래 체결내역 조회": [
          "빗썸 보유자산 확인 API",
          "빗썸 보유자산 확인 API 구현",
          "빗썸 주문내역 확인 API, 주문취소 API",
          "빗썸 주문내역 확인 API, 주문취소 API 구현",
          "빗썸 거래 체결내역 조회 API",
          "빗썸 거래 체결내역 조회 API 구현"
        ],
        "빗썸 웹소켓(Websocket) API": [
          "웹소켓(WebSocket) 기초, 웹소켓(Websocket) API의 필요성",
          "빗썸 웹소켓(Websocket) API - 현재가(ticker) 받아오기",
          "빗썸 웹소켓(Websocket) API - 현재가(ticker) 받아오기 구현"
        ],
        "종합실습1 - 빗썸 기초 자동매매 코인봇 만들어보기": [
          "종합실습1 - 빗썸 기초 자동매매 코인봇 만들어보기"
        ],
        "업비트(UPbit) API": [
          "업비트 API 활성화"
        ]
      },
      "requirements": [
        "기초적인 Python 사용경험",
        "최소 1번이상의 암호화폐 매매경험 (빗썸, 업비트 등의 거래소를 이용)"
      ],
      "description": "암호화폐 자동매매 코인봇을 만들고 운영하는데 필요한 A부터 Z까지의 모든 과정을 학습합니다. (거래소 API 사용법 기초, AWS를 이용한 배포) 또한 암호화폐 시장의 구조적 특성을 이용한 연 10% 내외의 무위험 수익을 얻을 수 있는 투자전략(김치 프리미엄 이용 등)을 학습할 수 있습니다.\n\n\n암호화폐 자동매매 코인봇 제작의 비결,\n파이썬(Python)으로 학습해보세요!\n\n\n널뛰는 암호화폐 투자 시장,\n똑똑한 전략을 찾고 있다면?\n거래소 API 사용법 기초부터 AWS를 이용한 배포까지 코인봇 제작의 A to Z를 학습합니다.\n연 10% 내외의 무위험 수익을 얻을 수 있는 투자 전략을 세워보세요! (예: 김치 프리미엄 이용하기)\n\n\n이런 분들께 추천드려요!\n24시간 운영되는 암호화폐 자동매매 코인봇을 만들어보고 싶으신 분\n거래소 API 사용법 기초부터 AWS 배포까지 코인봇 제작의 전 과정을 학습하고 싶은 분\n암호화폐 시장의 구조적 특성을 이용한, 연 10% 내외의 무위험 수익을 얻는 투자전략을 학습하고 싶은 분\n나만의 투자전략을 만드는 방법론과 노하우에 대해 학습하고 싶은 분",
      "target_audience": [
        "암호화폐 자동 매매 코인봇 만들기 (거래소 API 사용법 기초부터 AWS를 이용한 배포까지)",
        "기초 투자 전략 (변동성 돌파 전략 등)",
        "암호화폐 시장의 구조적 특성을 이용한 연 10% 내외의 무위험 수익을 얻을 수 있는 투자전략 (김치 프리미엄 이용 등)",
        "투자 전략 개발 방법론"
      ]
    },
    {
      "title": "Курс по нейросети \"Stable Diffusion\"",
      "url": "https://www.udemy.com/course/stable-diffusion/",
      "bio": "\"На этом курсе ты научишься использовать профессиональную нейросеть \"Stable Diffusion\" + \"FramePack\"",
      "objectives": [
        "Научишься использовать профессиональную нейросеть \"Stable Diffusion\", которая является универсальным Digital Art инструментом для многих профессий.",
        "Рисовать и изменять мельчайшие детали с помощью нейро-кистей! А не только генерировать изображения по промту!",
        "Вы научитесь создавать невероятно реалистичные портреты и делать невероятную обработку фотографий и других изображений до уровня 4К и 8К.",
        "Создавать видео с помощью ИИ, вдыхая жизнь в ваши изображения!",
        "Выйдите на новый уровень своих возможностей вместе с нейросетью \"Stable Diffusion\"!",
        "Сможете уверенно брать заказы на фриланс и гордиться своей работой, а ваш заказчик скажет \"Wow!\""
      ],
      "course_content": {
        "Курс по нейросети \"Stable Diffusion\"": [
          "Введение в курс SD",
          "Материалы (Скачай их себе сразу)",
          "Урок 1 Подробная установка Stable Diffusion",
          "+Дополнительный вариант установки через Google",
          "Урок 2 Обзор моделей и стандартных функций пробная работа",
          "Урок 3 Обзор моделей и стандартных функций пробная работа",
          "Урок 4 Обзор Разширений Controlnet Posex",
          "Урок 5 Создаем модель и обозреваем дополнительные функции Stable Diffusion"
        ],
        "FramePack AI Генерация Видео": [
          "Материалы (Скачайте их себе сразу)",
          "Как установить FramePack?",
          "Создаем промты для видео во FramePack",
          "Создаем видео в FramePack"
        ]
      },
      "requirements": [
        "Этот курс для абсолютных новичков в нейросетях и Stable Diffusion, по этому вам не нужно обладать какими либо знаниями для прохождения данного курса! Вместе, вы и я, пройдем путь - от установки программы Figma до уровня UI дизайнера, который уверенно может брать заказы на фриланс и гордиться своей работой! Мы начнем с самых основ и вместе, шаг за шагом изучим все необходимые инструменты Stable Diffusion, так, что в конце курса, вы будете чувствовать себя профессионалом!",
        "Нужен только PC и интернет + желание учиться и творить."
      ],
      "description": "Добро пожаловать в захватывающий мир цифрового творчества с курсом \"Stable Diffusion\"! Этот курс откроет перед вами двери в будущее, где искусственный интеллект становится вашим главным инструментом для реализации самых смелых идей. Если вы графический дизайнер, иллюстратор, маркетолог, контент-мейкер, фотограф, 3D-художник или веб-дизайнер, этот курс поможет вам освоить передовые технологии и вывести ваше творчество на новый уровень.\nО курсе\nКурс \"Stable Diffusion\" — это практическое обучение, созданное для творческих профессионалов, стремящихся быть в авангарде цифровых трендов. Вы научитесь использовать мощную нейросеть Stable Diffusion, чтобы создавать уникальные иллюстрации, анимации, 3D-модели и даже видео. Никаких навыков программирования не требуется — мы сделаем процесс простым и доступным!\nЭтот курс подойдет как новичкам, так и опытным профессионалам, желающим расширить свои возможности и интегрировать ИИ в свои проекты. Вы не только освоите инструмент, но и узнаете, как применять его для создания конкурентоспособных работ, которые выделят вас на рынке.\nЧто вы получите?\nПрактические навыки работы с нейросетью Stable Diffusion на вашем компьютере.\nОсвоение FramePack — генератора видео и анимаций на базе ИИ.\n10 подробных видеоуроков с пошаговыми инструкциями: от установки программ до создания профессиональных работ.\nЭксклюзивные материалы: промты, модели, расширения (ControlNet, Poses, MidJourney) и инструкции по их установке и использованию.\nПерсональные консультации с автором курса для решения ваших вопросов.\nДоступ к сообществу единомышленников, где вы сможете делиться опытом и вдохновляться.\nПочему стоит выбрать этот курс?\nУниверсальность: Подходит для множества профессий — от дизайна до маркетинга.\nПростота: Освойте нейросеть без технического бэкграунда.\nПрактика: Реальные проекты, которые вы сможете добавить в портфолио.\nТренды: Будьте впереди, используя ИИ — главный инструмент будущего.\nКому подойдет курс?\nГрафическим дизайнерам, желающим создавать уникальные визуалы.\nИллюстраторам, стремящимся ускорить и разнообразить творческий процесс.\nМаркетологам, ищущим креативные решения для кампаний.\nФотографам, которые хотят экспериментировать с новыми стилями.\n3D-художникам, готовым интегрировать ИИ в моделирование.\nВеб-дизайнерам, создающим современные и инновационные сайты.\nПрисоединяйтесь к цифровой революции!\nС курсом \"Stable Diffusion\" вы не просто учитесь — вы создаете будущее. Дайте волю своему воображению, освойте нейросети и станьте частью главного тренда цифровой эпохи. Запишитесь сегодня и начните создавать проекты, которые вдохновляют!\nСистемные требования нейросети Stable Diffusion к вашему PC или ноутбуку:\nЕсли у вас нет GPU (Видеокарты), то используйте дополнительный урок по запуску нейросети без видеокарты через Google Collab.\nРекомендуется для комфортного обучения и дальнейшего творчества:\nВидеокарты (GPU) от 2018 года выпуска и новее, серии Nvidia RTX 2000, RTX 3000, RTX 4000, RTX 5000.\nИли AMD Radeon серии RX 5000 XT, RX 6000, RX 7000.",
      "target_audience": [
        "Курс способен вывести на новый уровень и повысить квалификацию графических дизайнеров, иллюстраторов, маркетологов, фотографов, 3D-художников и веб-дизайнеров, которые хотят быть в центре цифровых трендов.",
        "Это практическое обучение создано для всех творческих личностей, как новичков, так и для опытных пользователей, которые хотят начать применять нейросети в своих сферах."
      ]
    },
    {
      "title": "파이썬 머신러닝 Starter 강의 (초급)",
      "url": "https://www.udemy.com/course/stroke-starter/",
      "bio": "플로우 챠트로 설명하고 오렌지3로 보강한 파이썬 머신러닝",
      "objectives": [
        "파이썬 머신러닝 실행 가능",
        "Orange3를 통한 메뉴 클릭 버전의 머신러닝 수행",
        "분류(classification) 문제에 대한 4가지 머신러닝 모델 실행",
        "머신러닝 및 딥러닝 개요 학습"
      ],
      "course_content": {
        "파이썬 머신러닝 Starter 강의 Part 1 - 파이썬 머신러닝 개요 및 데이터셋 소개": [
          "파이썬 머신러닝 Starter 강의 Part 1 - 머신러닝 개념 및 파이썬 맛보기"
        ],
        "파이썬 머신러닝 Starter 강의 Part 2 - 머신러닝 초반 3단계 설명 및 오렌지3 맛보기": [
          "파이썬 머신러닝 Starter 강의 Part 2 - 머신러닝 초반 3단계 실행 그리고 오렌지3 실행"
        ],
        "파이썬 머신러닝 Starter 강의 Part 3 - 데이터 처리": [
          "파이썬 머신러닝 Starter 강의 Part 3 - 데이터 처리"
        ],
        "파이썬 머신러닝 Starter 강의 Part 4 - 탐색적 자료분석 및 시각화": [
          "파이썬 머신러닝 Starter 강의 Part 4 - 탐색적 자료 분석 및 시각화"
        ],
        "파이썬 머신러닝 Starter 강의 Part 5 - 탐색적 자료 분석 및 시각화 계속": [
          "파이썬 머신러닝 Starter 강의 Part 5 - 탐색적 자료 분석 및 시각화 계속"
        ],
        "파이썬 머신러닝 Starter 강의 Part 6 - 머신러닝 모델 구축 및 실행 Beginning": [
          "파이썬 머신러닝 Starter 강의 Part 6 - 머신러닝 모델 구축 및 실행 Beginning"
        ],
        "파이썬 머신러닝 Starter 강의 Part 7 - 결정 트리 모델 실행": [
          "파이썬 머신러닝 Starter 강의 Part 7 - 결정 트리 모델 실행"
        ],
        "파이썬 머신러닝 Starter 강의 Part 8 - 로지스틱 회귀 모델 실행": [
          "파이썬 머신러닝 Starter 강의 Part 8 - 로지스틱 회귀 모델 실행"
        ],
        "파이썬 머신러닝 Starter 강의 Part 9 - 로지스틱 회귀 모델 실행 계속": [
          "파이썬 머신러닝 Starter 강의 Part 9 - 로지스틱 회귀 모델 실행 계속"
        ],
        "파이썬 머신러닝 Starter 강의 Part 10 - 사이킷런 신경망 모델 실행": [
          "파이썬 머신러닝 Starter 강의 Part 10 - 사이킷런 신경망 모델 실행"
        ]
      },
      "requirements": [
        "별도의 선행 지식 없이 처음부터 초보자 레벨부터 강의 수강 가능"
      ],
      "description": "5110행, 12개의 변수를 갖는 캐글 데이터셋을 가지고 Stoke(뇌졸증) 환자를 분류하는 기초적인 머신러닝 모델 4개를 파이썬 및 Orange3로 실행합니다. 결정 트리, 로지스틱 회귀 , 신경망, K 최근접 이웃 모델 등이 그것입니다. 머신러닝 모델을 파이썬으로 코딩하면 작업이 길어짐에 따라 중간에 방향을 잃고 헤매기 쉽습니다. 이에 내비게이션 장치로서 플로우 챠트(흐름도)를 추가하여 매 단계마다 수강하시는 분들의 쉬운 이해를 돕습니다.\n아울러 파이썬 코딩으로 상세한 머신러닝 과정을 처음부터 끝까지 일괄적으로 수행함으로써 프로젝트별로 여러 머신러닝 모델을 실행할 수 있는 능력을 키워 드립니다. 기존 책자들은 프로젝트내에서 다수의 머신러닝 모델을 일괄적으로 처리하는 과정이 부족합니다. 이 강의에서는 이를 보완하여 머신러닝 프로젝트를 처음부터 끝까지 수행할 수 있게 보완해 드립니다.\n또한 책에 없는 내용인 Orange3를 통해 머신러닝 모델을 매우 쉽게 실행할 수 있는 방법을 강의에 추가하였습니다. Orange3는 오픈 소스 머신러닝 그래픽 버전 툴로서 메뉴를 클릭하여 드래그하여 간단하게 머신러닝 모델을 실행할 수 있습니다. 매우 직관적이고 사용자 친화적인 툴입니다. 머신러닝 초보자가 파이썬 코딩과 함께 Orange3를 함께 사용하면 학습효과를 배가할 수 있습니다.\n강의는 12편으로 구성되어 있습니다. 1~2편에서는 파이썬 및 머신러닝의 기본 개념 그리고 이 강의에서 다루는 데이터셋에 대해 설명합니다. 2편 중간부터는 그래픽으로 쉽게 머신러닝을 다루는 Orange3 사용 예를 집중적으로 다루었습니다. 3편부터 11편까지는 먼저 파이썬으로 머신러닝을 구현하고, 그 후 동일 과정을 Orange3로 구현하여 독자들의 학습을 돕습니다. 마지막으로 부록 1에서는 신경망 및 딥러닝 이론을 소개합니다. 특히 최근 가장 앞선 딥러닝 모델 중 하나인 허깅페이스 라이브러리 기반의 자동번역을 소개합니다.",
      "target_audience": [
        "머신러닝을 배우고자 하나 파이썬 코딩을 어려워 하시는 분들이 처음부터 차근차근 쉽게 배울 수 있습니다.",
        "파이선 머신러닝을 그래픽 버전인 Orange3로도 병행해서 보여드립니다.",
        "데이터 분석에 관심이 많은 초보 파이썬 학습자"
      ]
    },
    {
      "title": "Deep Learning In Arabic التعلم العميق",
      "url": "https://www.udemy.com/course/deep-learning-in-arabic-g/",
      "bio": "Deep Learning and Neural Network with Python التعلم العميق والشبكات العصبية باستعمال لغة بايثون",
      "objectives": [
        "مدخل إلى التعلم العميق | Introduction to Deep Learning",
        "الرياضيات التطبيقية في التعلم العميق | Applied Mathematics for Deep Learning",
        "الشبكة العصبية ANN | Artificial Neural Network",
        "الشبكة العصبية الالتفافية CNN | Convolutional Neural Network",
        "الشبكة العصبية التكرارية RNN | Recurrent Neural Network",
        "الشبكة العصبية LSTM | Long Short Terme Memory",
        "الشبكة العصبية GRU | Gated Recurrent Unit",
        "الشبكة العصبية GAN | Generative Adversarial Network",
        "مشروع تطبيق التعلم العميق في مجال الصحة باستعمال بايثون | Python Deep Learning Project for Heathcare",
        "مشروع تطبيق التعلم العميق في مجال التعليم باستعمال بايثون | Python Deep Learning Project for Education",
        "مشروع تطبيق التعلم العميق في القطاع البنكي والمالي باستعمال بايثون | Python Deep Learning Project for Bank and Finance"
      ],
      "course_content": {},
      "requirements": [
        "أساسيات لغة البرمجة بايثون | Python Programming Language Basics",
        "أساسيات التعلم الآلي | Machine Learning Basics"
      ],
      "description": "كورس التعلم العميق  إسهام لإثراء المحتوى العربي في هذا المجال، وإضافة نوعية لتمكين كل الراغبين من مختلف المستويات من فهم واستيعاب التعلم العميق وآلياته، من خلال مدخل عام للتعريف بالتعلم العميق وعلوم البيانات والتعلم الآلي والتمييز بينها والتطرق إلى الرياضيات التطبيقية اللازمة لفهم خوارزميات مختلف الشبكات العصبية المستعملة في التعلم العميق، مثل مفهوم الانتشار الأمامي والانتشار العكسي والنزول الاشتقاقي.\nوسيتم التطرق في هذا الكورس بالتفصيل إلى الشبكات العصبية الآتية :\nالشبكة العصبية\nالشبكة العصبية الالتفافية\nالشبكة العصبية التكرارية\nالشبكة العصبية\nالشبكة العصبية\nالشبكة العصبية\nكما سيتم الاشتغال على مشاريع تطبيقية في مجال الصحة والتعليم والقطاع البنكي والمالي.\nDeep Learning is an advanced concept in Artificial intelligence. The functioning of Deep Learning is based on Artificial Neural Networks with representation learning. The learning can be Supervised, Semi-Supervised, or Unsupervised Learning.\nDeep learning is a subset of machine learning that is used to mimic the human brain in processing data, recognizing speech, translating languages, and making decisions. Deep learning is also used in self-driving cars, news aggregation and fraud news detection, virtual assistants, entertainment, healthcare...\nIn this course you will learn deep learning fundamentals and mathematics behind it, like Forward Propagation, Backward Propagation and Gradient Descent. Then you will learn  different neural networks like :\nArtificial Neural Network ANN\nConvolutional Neural Network CNN\nRecurrent Neural Network RNN\nLong-short Term Memory LSTM\nGated Recurrent Unit GRU\nGenerative Adversarial Network GAN\nYou'll practice what you learned through real projects :\nHealthcare\nEducation\nBank and Finance.",
      "target_audience": [
        "كل من يرغب في فهم التعلم العميق وكيفية اشتغاله من خلال توظيف بعض الشبكات العصبية"
      ]
    },
    {
      "title": "Машинное обучение с подкреплением на Python",
      "url": "https://www.udemy.com/course/ittensive-python-reinforcement-learning/",
      "bio": "Изучим взаимодействие среды и агенты, Q-обучение, Монте-Карло и Deep Q-Network",
      "objectives": [
        "Машинное обучение с подкреплением",
        "Среда, агент, действия и вознаграждения",
        "Случайная стратегия и методы Монте-Карло",
        "Уравнение Беллмана",
        "Q-таблица и Q-обучение",
        "Эпсилон-жадные стратегии",
        "UCB и стратегия Томпсона",
        "Deep Q-Network"
      ],
      "course_content": {
        "Введение": [
          "Приветствие",
          "Задачи машинного обучения"
        ],
        "Основные определения": [
          "Задачи обучения с подкреплением",
          "Метрики обучения с подкреплением",
          "Компромисс эксплуатации и разведки",
          "Метод Монте-Карло",
          "Цепь Маркова",
          "Принцип оптимальности Беллмана",
          "Q-learning",
          "Эпсилон-жадный алгоритм"
        ],
        "Практикум: крестики-нолики": [
          "Среда и агент",
          "Q-состояния",
          "Случайный агент",
          "Q-learning",
          "Сбалансированный Q-learning",
          "Эпсилон-жадный Q-learning",
          "Монте-Карло",
          "Адаптивный эпсилон-жадный Q-learning"
        ],
        "Проект: выигрышная стратегия": [
          "Softmax-стратегия",
          "Стратегия преследования",
          "\"Умный\" агент"
        ],
        "Случайная среда и DQN": [
          "UCB-стратегия",
          "Стратегия Томпсона",
          "DQN",
          "Обучение DQN"
        ],
        "Практикум: балансировка тележки": [
          "CartPole Gym",
          "Дискретизация среды",
          "ε-жадная стратегия",
          "Оптимизированная ε-жадная стратегия",
          "Стратегия Томпсона",
          "DQN"
        ],
        "Проект: вечная балансировка": [
          "Единая и разделенная стратегии",
          "Оптимизация DQN"
        ],
        "Стратегии Монте-Карло": [
          "Монте-Карло для оценки стратегий",
          "Монте-Карло первого прохода"
        ],
        "Практикум: Монте-Карло": [
          "Игра в блекджек",
          "Политика Монте-Карло",
          "Оптимальная политика Монте-Карло",
          "Разделенная стратегия",
          "Дисконтированное обновление"
        ],
        "Проект: игра против казино": [
          "Игра против казино",
          "* Дополнительные правила блекджека"
        ]
      },
      "requirements": [
        "Основы машинного обучения",
        "Продвинутый Python",
        "Основы математической статистики"
      ],
      "description": "Внимание: для доступа к курсам ITtensive на Udemy напишите, пожалуйста, на support@ittensive.com с названием курса или группы курсов, которые хотите пройти.\n\n\nЭто завершающий курс программы Машинное обучение от ITtensive., В этом курсе мы разберем 3 задачи обучения с подкреплением:\n1. Игра в крестики-нолики на доске 3x3. Запрограммируем среду, условия выигрыша и обучим простых и не очень агентов (игроков) сводить игру к ничье. На примере базовых стратегий рассмотрим работу уравнения Беллмана, Q-обучения и обучения с преследованием.\nИспользуя различные стратегии, включая эпсилон жадные и оптимизированные эпсилон жадные, сравним их эффективность при игре друг с другом.\nПроект: разработка собственного выигрышного агента для игры в крестики-нолики.\n2. Задача балансировка тележки в физическом окружении. Используем CartPole AI Gym и научимся балансировать тележку в зависимости от данных сенсоров. Изучим принципы построения нейросети обучения с подкреплением (DQN = Deep Q-Network) и используем ее для ускорения и стабилизации процесса обучения.\nСравним обучение агента на случайных процессах, на изучении распределения состояний среды (априорные и постериорные вероятности), на эмуляции кратковременной и долговременной памяти агента, разберем проблемы обучения и оптимизации полносвязной нейросети.\nПроект: разработка оптимизированной DQN для балансировки тележки.\n3. Игра в блекджек (21 очко). Используем окружение AI Gym для расчета оптимальных ходов при игре в Блекджек. Используем методы Монте-Карло, включая одиночные и множественные касания, единую и разделенную политики, а также оптимизацию исследовательских стартов.\nВизуализируем оптимальную политику поведения агента через изоповерхности в пространстве состояний среды.\nПроект: расчет оптимальной стратегии игры в блекджек.\nТеория по курсу включает:\nЗадачи машинного обучения и обучения с подкреплением\nМетрики обучения с подкреплением\nПроблема баланса эксплуатации и разведки\nЦепь случайных процессов Маркова\nПринцип и уравнение Беллмана\nМетоды Монте-Карло\nQ-таблица и Q-обучение\nЭпсилон-жадная стратегии\nЭпсилон-жадная стратегия с убыванием\nUCB-стратегия\nСтратегия Томпсона\nСоздание и обучение Deep Q-Network\nКратковременная и долговременная память\nЕдиная и разделенная политики",
      "target_audience": [
        "Аналитики Python, изучающие машинное обучение",
        "Программисты больших данных",
        "Исследователи больших данных",
        "Разработчики автономных систем"
      ]
    },
    {
      "title": null,
      "url": "https://www.udemy.com/course/automatizazione-web-con-python/",
      "bio": null,
      "objectives": [],
      "course_content": {},
      "requirements": [],
      "description": null,
      "target_audience": []
    },
    {
      "title": "Data Mining without coding ~ Visual Programing ~ OrangeDM",
      "url": "https://www.udemy.com/course/vpt-orange3/",
      "bio": "視覺化程式設計工具(Visual Programming Tools)提供開發人員和資料科學家各式各樣的模組，讓資料處理、模型建置、訓練及部署機器學習模型更加快速且視覺化。",
      "objectives": [
        "資料探勘 Data Mining",
        "機器學習 Machine Learning",
        "圖像分析 Image Analysis",
        "文字探勘 Text Mining",
        "ChatGPT KPI"
      ],
      "course_content": {
        "Orange 安裝與簡介": [
          "模組安裝 Orange Installation",
          "基本操作 Basic Function of Orange",
          "資料可視化 Data Visualization I",
          "資料可視化 Data Visualization II"
        ],
        "機器學習 Machine Learning": [
          "Data Pipeline",
          "分類模型 Classification Model",
          "迴歸模型 Regression Model",
          "模型可解釋性 Explain Model"
        ],
        "非監督式學習 Unsupervised Learning": [
          "分群模型 Clustering Model",
          "主成份分析 Principle Component Analysis (PCA)"
        ],
        "影像分析 Image Analysis": [
          "Image Grid & Neighbors",
          "Image Embedding & Training Image Model"
        ],
        "文字探勘 Text Mining": [
          "Preprocess Text, Bag of Words, Text Model",
          "Document Embedding & Document Model"
        ],
        "ChatGPT in OragneDM": [
          "ChatGPT Constructor",
          "ChatGPT Summarize"
        ]
      },
      "requirements": [
        "無需特別背景，文法商也可以學習資料分析 can learn data analysis without special background"
      ],
      "description": "學習到資料探勘技術(Data Mining)與機器學習(Machine Learning)方法，包括預測/分類/分群等\n建立自己的數據分析流程(Data Pipeline)，並使用這些數據建構所需要的機器學習模型\n視覺化編程(Visual Programming)使您可以專注於探索性數據而不是編碼工作\n透過延伸模組，可以進行ChatGPT, 文字探勘(Text Mining)與圖像分類(Image Classification)等進階分析",
      "target_audience": [
        "對數據分析、機器學習感興趣者 Interested in data analysis and machine learning"
      ]
    },
    {
      "title": "Résoudre ses problèmes d'optimisation avec le solveur Excel",
      "url": "https://www.udemy.com/course/resoudre-ses-problemes-doptimisation-avec-le-solveur-excel/",
      "bio": "Résolution de problèmes d'optimisation avec le Solveur Excel (Excel Solver) à travers divers exemples concrets",
      "objectives": [
        "Installation du solveur Excel",
        "Problématique, définitions et méthodologie",
        "Modélisation de problèmes complexes sous Excel",
        "Définition des contraintes d'un problème d'optimisation",
        "Paramétrage et utilisation du solveur Excel",
        "Interprétation des résultats obtenus à l'aide du solveur"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Structure du cours",
          "Installer le Solveur Excel"
        ],
        "\"Optimisation\", \"Modélisation\" : de quoi parle-t-on précisément ?": [
          "Un problème d'optimisation, qu'est-ce que c'est ?",
          "La modélisation, qu'est ce que c'est ?",
          "Modéliser un problème d'optimisation : mode d'emploi"
        ],
        "Exemple basique d'optimisation": [
          "Présentation du problème",
          "Présentation des données du problème",
          "Modélisation du problème sur Excel",
          "Résolution du problème avec le solveur Excel",
          "Le chocolatier"
        ],
        "Exemple d'un problème de production - Maximiser les bénéfices": [
          "Présentation d'un problème de production",
          "Présentation des données du problème",
          "Modélisation du problème de production",
          "Résolution du problème de production",
          "Paramétrer le solveur pour trouver la solution optimale",
          "Pour aller plus loin face à l'interprétation des résultats"
        ],
        "Les différentes méthodes de résolution des problèmes": [
          "Algorithmes de résolution et linéarité",
          "Linéaire ou pas ?"
        ],
        "Exemple d'un problème de transport - Minimiser les coûts de livraison": [
          "Présentation d'un problème de transport",
          "Présentation des données du problème de transport",
          "Modélisation du problème de transport",
          "Petite pause ludique : Pouvez-vous rivaliser avec le solveur ?",
          "Résolution du problème de transport"
        ],
        "Le voyageur de commerce (TSP)": [
          "Présentation du voyageur de commerce",
          "Présentation des données du problème",
          "Première modélisation du problème",
          "Deuxième modélisation du problème",
          "Troisième modélisation du problème",
          "Résolution du voyageur de commerce avec le solveur Excel"
        ],
        "Gestion de projet - Project Crashing": [
          "Présentation du problème de Project Crashing",
          "Présentation des données du problème",
          "Modélisation du problème de Project Crashing",
          "Résolution du problème : réduire au mieux le problème"
        ],
        "A vous de jouer ! Résolvez un problème d'optimisation": [
          "Le container sécurisé"
        ],
        "Open Solveur : en complément du solveur Excel": [
          "Les limites du solveur Excel",
          "Installer Open Solveur",
          "Utiliser Open Solveur"
        ]
      },
      "requirements": [
        "Avoir des connaissances de base sur l'utilisation d'Excel"
      ],
      "description": "Comment minimiser mes charges ?\nComment organiser la livraison de mes clients ?\nComment mieux gérer mes ressources ?\nQuelles quantités faut-il produire pour maximiser mes bénéfices ?\nComment organiser mon stock pour maximiser ma capacité de stockage ?\nDans quel ordre faut-il produire pour minimiser les coûts ?\n…\nVoilà le genre de casse-têtes auxquels vous devez faire face régulièrement que vous soyez chef d'entreprise, manager, cadre, indépendant, fonctionnaire, chef de projet, manager, artisan, enseignant…\nDans cette formation vous allez apprendre comment donner les meilleures réponses possibles, de manière chiffrée, à ces questions en quelques minutes. Cette opération se fait avec un outil informatique qu'on appel un solveur.\nIl en existe plusieurs disponibles sur le marché : parfois gratuits, parfois payants et très onéreux. Mais il y en a un, peu utilisé, auquel vous avez déjà accès : le solveur Excel. Et ça tombe bien, c'est certainement le plus accessible et le plus simple d'utilisation qui existe.\nConcrètement il s'agit d'un outil intégré à Excel qui va modifier les valeurs de certaines cellules automatiquement pour optimiser les problèmes que vous pourriez vous poser.\n\n\nDans cette formation, nous allons nous concentrer sur plusieurs problèmes concrets : nous allons voir comment les modéliser dans Excel et comment utiliser le solveur pour les résoudre.\nCes problèmes sont tels qu'il est quasiment impossible de les résoudre à la main dans un temps raisonnable (pour certains, il vous faudrait plusieurs vies).\nIl s'agit d'une formation courte, qui se veut efficace et surtout très pragmatique.\nJe vous dis donc à très vite dans cette formation.",
      "target_audience": [
        "Chefs d'entreprises",
        "Chefs de projets",
        "Managers",
        "Artisans",
        "Développeurs voulant faire un premier pas dans le domaine de l'optimisation",
        "Toute personne intéressée par la science des données"
      ]
    },
    {
      "title": "350+ Pytań: Data Scientist + GenAI - Testy rekturacyjne",
      "url": "https://www.udemy.com/course/pytania-rozmowa-kwalifikacyjna-interview-data-scientist/",
      "bio": "Kompletny zestaw pytań do rekrutacji na stanowisko Data Scientist + GenAI z wyjaśnieniami!",
      "objectives": [],
      "course_content": {
        "Practice Tests": []
      },
      "requirements": [],
      "description": "Ten kurs to kompleksowy zestaw pytań testowych z wyjaśnieniami, stworzony z myślą o osobach przygotowujących się do rozmowy kwalifikacyjnej na stanowisko Data Scientist lub ML Engineer, z uwzględnieniem najnowszych trendów takich jak Generative AI i prompt engineering.\nZawiera 350 starannie opracowanych pytań, podzielonych na 6 modułów tematycznych:\nPodstawy analizy danych i statystyki – 60 pytań o rozkłady, estymację, testy statystyczne i analizę danych\nUczenie maszynowe i modele predykcyjne – 60 pytań z regresji, klasyfikacji, metryk, modelowania i ewaluacji\nProgramowanie w Python i narzędzia data science – 60 pytań z kodu, bibliotek (Pandas, NumPy, Scikit-learn), optymalizacji\nData science w środowisku produkcyjnym – 60 pytań o CI/CD, pipeline'y, monitorowanie, wersjonowanie i MLOps\nGeneratywna sztuczna inteligencja (GenAI) – 50 pytań o LLM-y, modele generatywne, ich architekturę i zastosowania\nPrompt engineering i dostrajanie modeli – 60 pytań o techniki tworzenia promptów, fine-tuning i strategie optymalizacji\nKażde pytanie posiada szczegółowe wyjaśnienie poprawnej odpowiedzi oraz analizę błędnych opcji – dzięki czemu uczysz się nie tylko trafiać w poprawną odpowiedź, ale też rozumieć dlaczego inne odpowiedzi są niepoprawne.\nTen kurs to idealne narzędzie dla osób przygotowujących się do rekrutacji w branży data science, chcących sprawdzić swoją wiedzę i zrozumienie praktycznych zagadnień na poziomie profesjonalnym. Kurs zawiera pytania aktualne i dopasowane do realiów współczesnych rozmów kwalifikacyjnych w firmach technologicznych.\n\n\nGenerative AI – przyszłość sztucznej inteligencji!\nGenerative AI (GenAI), czyli generatywna sztuczna inteligencja, to nowoczesna dziedzina sztucznej inteligencji, która umożliwia maszynom tworzenie nowych treści – tekstów, obrazów, kodu czy dźwięku – na podstawie dostarczonych danych. Dzięki modelom takim jak GPT, Gemini, DALL·E czy Codex, GenAI zmienia sposób, w jaki pracujemy, uczymy się i komunikujemy z technologią. W ramach kursu sprawdzisz swoją wiedzę na temat kluczowych pojęć, architektury modeli generatywnych oraz dobrych praktyk ich wykorzystania.\n\n\nPrompt Engineering – sztuka skutecznej komunikacji z AI!\nPrompt engineering to technika formułowania precyzyjnych zapytań (promptów), które pozwalają modelom językowym – takim jak ChatGPT czy Claude – generować dokładne, trafne i wartościowe odpowiedzi. To nie tylko umiejętność pisania komend, ale też zrozumienie, jak modele przetwarzają kontekst, jakie mają ograniczenia oraz jak optymalizować interakcję, by osiągnąć zamierzony efekt. W module poświęconym prompt engineeringowi sprawdzisz swoją wiedzę na temat zasad projektowania efektywnych promptów, różnic między podejściami zero-shot, one-shot i few-shot, a także technik dostrajania zachowania modeli za pomocą parametrów i przykładów kontekstowych.\n\n\nCzy mogę przystąpić do testu praktycznego więcej niż jeden raz?\nTak, do każdego testu praktycznego można podejść wielokrotnie. Po ukończeniu testu zostanie zaprezentowany Twój wynik końcowy wraz z analizą poprawnych i błędnych odpowiedzi. Przy każdym kolejnym podejściu kolejność pytań oraz odpowiedzi jest losowa, co pozwala na bardziej wszechstronne przygotowanie.\n\n\nCzy obowiązuje limit czasowy dla testów praktycznych?\nTak, każdy test posiada zdefiniowany limit czasowy, który został ustalony w celu odzwierciedlenia warunków zbliżonych do rzeczywistych sytuacji rekrutacyjnych.\n\n\nJaki próg punktowy oznacza zaliczenie testu?\nAby uzyskać pozytywny wynik, należy osiągnąć minimum 80% poprawnych odpowiedzi.\n\n\nCzy pytania mają jedną poprawną odpowiedź?\nW celu realistycznego odwzorowania warunków rozmowy kwalifikacyjnej, testy zawierają zarówno pytania jednokrotnego, jak i wielokrotnego wyboru. Taki układ zwiększa poziom trudności i lepiej przygotowuje do rzeczywistej selekcji rekrutacyjnej.\n\n\nCzy mogę przeanalizować swoje odpowiedzi po zakończeniu testu?\nTak, po zakończeniu testu masz możliwość przeglądu wszystkich udzielonych odpowiedzi wraz z informacją, które z nich były poprawne, a które nie.\n\n\nNie czekaj — rozpocznij przygotowania już dziś i zwiększ swoje szanse na sukces podczas rozmowy kwalifikacyjnej!",
      "target_audience": [
        "Studenci kierunków technicznych i analitycznych",
        "Junior Data Scientists i Analitycy Danych",
        "Osoby ubiegające się o staże lub praktyki w obszarze analizy danych i ML",
        "Osoby uczące się samodzielnie (self-taught)",
        "Specjaliści z innych branż planujący przebranżowienie się",
        "Osoby przygotowujące się do rozmów w języku angielskim (English-friendly)",
        "Rekruterzy techniczni"
      ]
    },
    {
      "title": "【初心者向け】機械学習モデル構築で重要な特徴量エンジニアリングのテクニックをPythonを使って学んでいこう！",
      "url": "https://www.udemy.com/course/ml-feature-engineering/",
      "bio": "特徴量エンジニアリングの基本と実践をPythonで学ぼう！外れ値・欠損値・不均衡データ・日付データへの対応、様々なエンコーディング方法、クラスター分析を使った特徴量作成、ログ変換など様々なテクニックを学び、実践課題を通して実力をつけていこう",
      "objectives": [
        "特徴量エンジニアリングとは何か",
        "外れ値の処理方法",
        "欠損値の処理方法",
        "不均衡データへの対処方法",
        "既存の特徴量を組み合わせた新たな特徴量の作成",
        "カテゴリ変数の様々なエンコーディング方法（One-Hot, Label Encodingなど）",
        "日付データの処理やサイクルエンコーディング",
        "クラスター分析を使った新たな特徴量の作成",
        "数値変数のスケーリング（ログ変換）",
        "タイタニックデータを使った実践的な特徴量エンジニアリングと精度向上アプローチ"
      ],
      "course_content": {
        "紹介": [
          "紹介"
        ],
        "Pythonの使い方": [
          "Pythonを学ぼう！",
          "Pythonの実行環境",
          "Google Colabの使い方",
          "演算子の種類を学ぼう！",
          "Pythonの変数と型を学ぼう！",
          "list型(リスト)を学ぼう！",
          "dict型(辞書)を学ぼう！",
          "print関数を学ぼう！",
          "文字列の操作方法を学ぼう！",
          "条件分岐のif文を学ぼう！",
          "繰り返し処理のfor文を学ぼう！",
          "関数の作り方と使い方を学ぼう！",
          "Pandasの使い方を学ぼう！",
          "Numpyについて学ぼう！",
          "Matplotlibの使い方を学ぼう！",
          "Searbornについて学ぼう！",
          "Python構文の復習"
        ],
        "特徴量エンジニアリングの基礎": [
          "特徴量エンジニアリングをはじめよう！",
          "特徴量エンジニアリングについてアニメーションで概要を掴もう！",
          "今回使うデータの確認をしていこう！",
          "外れ値についてアニメーションで概要を掴もう！",
          "外れ値の処理をしてみよう！",
          "欠損値の処理をしてみよう！",
          "不均衡データの処理をしてみよう！",
          "既存の特徴量の組み合わせから新たな特徴量を作ろう！",
          "Lable EncodingとOne Hot Encodingを実装してみよう！",
          "Frequency EncodingとTarget Encodingを実装してみよう！",
          "日付に対する処理を見ていこう！",
          "日付をSin・Cos変換する処理を見ていこう！",
          "クラスター分析を使って特徴量を追加する方法を見ていこう！",
          "クラスター分析についてアニメーションで理解しよう！",
          "ログ変換をやってみよう！"
        ],
        "特徴量エンジニアリング実践！": [
          "タイタニックのデータの概要を掴もう！",
          "欠損値補完などの前処理を実行しよう！",
          "ラベルエンコーディングを実行しよう！",
          "モデル構築をして予測精度を出力していこう！",
          "5000回ループを回して予測精度の信頼性を高めよう！",
          "Label EncodingをOne-Hot Encodingに変えて精度を確かめよう！",
          "家族の人数などの新たな特徴量を追加して精度を確かめよう！",
          "色々な特徴量追加や数値変換を試して精度を確かめよう！",
          "ボーナスレクチャー"
        ]
      },
      "requirements": [
        "Pythonの基礎から学びますのでプログラミングの知識は特に必要ありません"
      ],
      "description": "このコースは、機械学習において最も重要なステップの一つである「特徴量エンジニアリング」を、初心者でも理解できるよう丁寧に解説していきます。\n\n\n特徴量エンジニアリングには様々なテクニックがありますが、その中でもよく使うものをピックアップして解説していきます。\n・外れ値の処理方法\n・欠損値の処理方法\n・不均衡データへの対処方法\n・既存の特徴量を組み合わせた新たな特徴量の作成\n・カテゴリ変数の様々なエンコーディング方法（One-Hot, Label Encodingなど）\n・日付データの処理やサイクルエンコーディング\n・クラスター分析を使った新たな特徴量の作成\n・数値変数のスケーリング（ログ変換）\nなど、実務でも役立つテクニックを順を追って学んでいきます。\n\n\nまた、それらを学んだ後にKaggleというデータ分析コンペで最初のトライアルとしてよく使われるタイタニックのデータを使って、実際に手を動かしながら特徴量を工夫してモデル精度を上げるプロセスを体験していただきます\n\n\n「モデルを作ったけど精度が上がらない」「もっと実践的な知識をつけたい」そんな方にぴったりの内容です。\nぜひ特徴量エンジニアリングをマスターして市場価値の高い人材になりましょう！",
      "target_audience": [
        "機械学習やデータサイエンスを学び始めた初心者〜中級者",
        "モデルの精度をなかなか上げられず悩んでいる方",
        "実務で使える特徴量エンジニアリングを習得したい方",
        "タイタニックコンペを通して実践力をつけたいKaggle初心者"
      ]
    },
    {
      "title": "【画像処理マスターコース】Pythonによる基本的な画像処理からセグメンテーションまで",
      "url": "https://www.udemy.com/course/python-image-ai/",
      "bio": "Pythonで学ぶ画像処理の基礎から、画像データの特徴量エンジニアリングと機械学習、そして最新のセグメンテーション技術SAMまでを体系的に習得する完全ガイドです",
      "objectives": [
        "Pythonによる画像データの扱い",
        "古典的な画像処理の手法（エッジ処理、フィルタ、二値化、テンプレートマッチングなど）",
        "機械学習・深層学習の基本",
        "Vision Transformerの基本",
        "SAMによる教師なしセグメンテーション"
      ],
      "course_content": {
        "コース紹介": [
          "コース紹介",
          "サンプルコードとデータのダウンロード",
          "コース準備レクチャー"
        ],
        "イントロダクション": [
          "画像処理について",
          "画像データの扱い",
          "Numpyの演習①",
          "Numpyの演習②",
          "Numpyの演習③"
        ],
        "古典的な画像処理手法": [
          "古典的な画像処理手法",
          "フィルタとノイズ除去",
          "（演習）データ読み込み",
          "（演習）フィルタとノイズ除去",
          "エッジ検出",
          "（演習）エッジ検出",
          "幾何変換",
          "（演習）幾何変換",
          "ヒストグラムとコントラスト調整",
          "（演習）ヒストグラムとコントラスト調整",
          "閾値と二値化",
          "（演習）閾値と二値化",
          "テンプレートマッチング",
          "（演習）テンプレートマッチング"
        ],
        "（振り返り）機械学習の基礎": [
          "二値分類",
          "教師あり・教師なし学習",
          "機械学習の流れ",
          "データの取得",
          "データの加工",
          "目的変数と説明変数",
          "モデリング",
          "ランダムフォレスト",
          "評価指標",
          "混同行列"
        ],
        "機械学習のための特徴量エンジニアリング": [
          "画像から作る特徴量",
          "色特徴量",
          "（演習）環境と画像の準備",
          "（演習）色特徴量",
          "（演習）色特徴量の補足",
          "HOG特徴量",
          "（演習）HOG特徴量",
          "LBP特徴量（テクスチャ解析）",
          "（演習）LBP特徴量",
          "（演習）LBP特徴量の補足",
          "SIFT特徴量（特徴点検出）",
          "（演習）SIFT特徴量",
          "scikit-image",
          "prop_region",
          "opencv",
          "（演習）機械学習による分類モデル開発①",
          "（演習）機械学習による分類モデル開発②",
          "（演習）機械学習による分類モデル開発③",
          "タスクによってはディープラーニングが必要"
        ],
        "（振り返り）深層学習の基礎": [
          "ニューラルネットワークとは",
          "目的関数",
          "確率的勾配降下法",
          "ミニバッチ学習",
          "EpochとIteration",
          "逆誤差伝播法",
          "畳み込みニューラルネットワーク（CNN）",
          "Vision Transformerの基礎①",
          "Vision Transformerの基礎②"
        ],
        "セグメンテーション": [
          "物体検出について",
          "セグメンテーションについて",
          "U-netによる教師ありセグメンテーション",
          "U-netのサンプル",
          "SAM（Segment Anything Model）による教師なしセグメンテーション",
          "（演習）SAMによるセグメンテーション①",
          "（演習）SAMによるセグメンテーション②",
          "（演習）SAMによるセグメンテーション③",
          "（演習）SAMによるセグメンテーション④"
        ],
        "（参考）ChatGPTによる画像の扱い": [
          "ChatGPTで画像を扱う例",
          "（演習）ChatGPTでの画像の扱い"
        ],
        "終わりに": [
          "終わりに"
        ]
      },
      "requirements": [
        "基本的なPythonスキル"
      ],
      "description": "このコースは、Pythonを使った画像処理の基礎から、機械学習・ディープラーニングを活用した応用的な手法、そして最新のセグメンテーション技術までを幅広くカバーする完全ガイドです。\n「画像処理って何から始めればいいかわからない」「機械学習やディープラーニングは少し触ったけど、画像系の応用が難しい」といった方に最適のコースです。\n古典的なフィルタリングやエッジ検出といった基本から、特徴量エンジニアリングと機械学習、深層学習による画像分類、さらにVision TransformerやSAMによるセグメンテーションまで、段階的にステップアップできます。\n実際のコードを動かしながら、理論と実装をバランスよく学べる構成です。最終的には、実務レベルの画像処理プロジェクトに応用できるスキルが身につきます！\n\n\n【このコースで学べること】\n画像データの基礎知識とPythonでの基本操作\n古典的な画像処理手法（フィルタリング、エッジ検出など）\n機械学習・特徴量エンジニアリングによる画像分類\n畳み込みニューラルネットワーク（CNN）とVision Transformerの基礎\n教師あり・教師なしの画像セグメンテーション手法（U-net、SAM）\nOpenAI APIを活用した生成AIの応用方法\n\n【目次】\nイントロダクション\n画像データの基本\n古典的な画像処理\n機械学習の振り返り\n機械学習のための特徴量エンジニアリング\n特徴量を用いた機械学習\nディープラーニング・畳み込みニューラルネットワークの振り返り\nVision Transformerの振り返り\n教師ありセグメンテーション（U-net）\n教師なしセグメンテーション（SAM）\n生成AIの活用（OpenAI API）\n\n【対象となる方】\n画像処理やコンピュータビジョンに興味があるPythonユーザー\n機械学習やディープラーニングを画像処理に応用したい方\n最新のセグメンテーション技術や生成AIに触れてみたい方\n実務で画像解析やAIプロジェクトに取り組みたい方\n独学で学びきれなかった画像処理の理論と実装をしっかり学びたい方\n\n【必要なスキル】\nPythonの基本的な文法が理解できること（Pandasなどが使えればOK）\n機械学習やディープラーニングの基礎知識があるとよりスムーズに学べますが、振り返りながら進みます",
      "target_audience": [
        "画像データを扱う必要があるデータ分析者・初級Python開発者・マネージャー"
      ]
    },
    {
      "title": "데이터사이언스 훈련소 - DataFrame 다루기 및 시각화",
      "url": "https://www.udemy.com/course/dataframe/",
      "bio": "파이썬 패키지를 통해 데이터 분석과 친해지기!",
      "objectives": [
        "Dataframe에 대한 개념에 대해서 이해할 수 있다.",
        "Dataframe을 생성하고, 조회하고, 수정하기 등의 방법을 학습하여 활용할 수 있다.",
        "데이터 분석을 위한 기초 작업인 Dataframe을 Pandas 패키지를 활용하여 구성할 수 있다.",
        "시각화의 목적에 맞는 패키지를 활용하여 데이터 시각화를 할 수 있다."
      ],
      "course_content": {
        "강의 소개": [
          "01. 데이터 프레임 다루기"
        ],
        "파이썬 패키지 소개": [
          "02. 파이썬 패키지란",
          "03. 파이썬 패키지 종류",
          "04. 기본 패키지",
          "05. 데이터 분석을 위한 패키지",
          "06. 머신러닝 패키지",
          "07. 데이터 전처리 패키지",
          "08. 기타 패키지"
        ],
        "DataFrame의 개념 및 실습": [
          "09. Data Frame이란",
          "10. Data Frame을 배우는 이유",
          "11. Data Frame 실습 1",
          "12. Data Frame 실습 2",
          "13. Data Frame 실습 3"
        ],
        "Pandas 소개 및 실습": [
          "14. Pandas 실습"
        ],
        "Numpy 소개 및 실습": [
          "15. Numpy 실습 1",
          "16. Numpy 실습 2",
          "17. Numpy 실습 3"
        ],
        "Data Frame 과제 및 해설": [
          "18. [과제] Data Frame 과제",
          "19. [해설] Data Frame 과제 해설"
        ],
        "데이터 시각화란?": [
          "20. 데이터 시각화",
          "21. 데이터 시각화란",
          "22. 데이터 시각화의 절차"
        ],
        "도표 분석 방법": [
          "23. 도표 분석 방법 비교와 비율 분석",
          "24. 비교와 비율 분석 구성요소",
          "25. 도표 분석 방법 동향과 패턴 분석",
          "26. 도표 분석 방법 관계와 연관 분석",
          "27. 동향과 패턴 분석 구성요소"
        ],
        "데이터 시각화를 위한 Python 패키지": [
          "28. 데이터 시각화를 위한 파이썬 패키지",
          "29. 데이터 시각화 결과물"
        ],
        "패키지를 활용한 데이터 시각화 실습": [
          "30. 데이터 시각화 실습 1",
          "31. 데이터 시각화 실습 2",
          "32. [과제] 데이터 시각화 과제"
        ]
      },
      "requirements": [
        "파이썬 기초 과정(친절한 파이썬 스쿨 - 입학하기)에 대한 선수 학습이 필요합니다.",
        "구글 코랩을 이용하기 때문에 컴퓨터 사양에 대한 제약은 없습니다."
      ],
      "description": "DataFrame / Pandas, Numpy 패키지 / 데이터 시각화의 이론 뿐만이 아니라 실습도 함께 학습할 수 있습니다.\n\n\n\n\n◾ 파이썬 패키지의 종류를 정리하여 상황에 맞는 패키지가 무엇인지 확실하게 이해할 수 있습니다.\n\n\n기본, 데이터 분석, 머신러닝, 데이터 전처리, 기타 패키지에 대해서 정리할 수 있다.\n상황에 맞는 패키지 종류를 선택하여 사용할 수 있다.\n\n\n◾ DataFrame의 개념을 정리하여 파이썬 패키지를 어떻게 활용하면 되는지 이해 할 수 있습니다.\n\n\nDataFrame의 구조에 대해서 알 수 있다.\nDataFrame을 배워야 하는 이유에 대해서 정리할 수 있다.\nDataFrame을 생성 / 조회 / 수정하는 실습을 진행할 수 있다.\n\n\n◾ 데이터 시각화의 절차와 도표 분석 방법을 정리하여 직접 데이터 시각화를 진행해 볼 수 있습니다.\n\n\n데이터 시각화의 절차 과정에 대해서 정리 할 수 있다.\n도표 분석 방법 (비교와 비율 / 동향과 패턴 / 관계와 연관 분석)의 내용을 정리할 수 있다.\n데이터 시각화 패키지와 그래프 종류에 대해서 정리할 수 있다.\nMatplotlib 패키지를 이용하여 데이터 시각화 실습을 진행할 수 있다.\nSeaborn  패키지를 이용하여 데이터 시각화 실습을 진행할 수 있다.",
      "target_audience": [
        "파이썬 기초를 학습하신 분",
        "파이썬을 활용하여 데이터에 기반한 시각화 자료를 제작하고자 하는 분"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第21部 如何推理 Llama 大語言模型 上部",
      "url": "https://www.udemy.com/course/generative_ai_21/",
      "bio": "如何訓練 Transformer 模型實現英譯中",
      "objectives": [
        "LLaMA 2 模型架構與運作原理",
        "Rotary Positional Encoding (Rotary PE) 的原理與實現",
        "KV Cache 技術與自回歸推理優化",
        "Grouped Query Attention 的設計與實現",
        "RMSNorm 正規化技術"
      ],
      "course_content": {},
      "requirements": [
        "一台電腦"
      ],
      "description": "你是否對大語言模型的黑盒運作充滿好奇？你是否想親手打造屬於自己的高效 Transformer 模型？現在，一門全新的實戰課程正等著你來探索！\n從理論到實踐的全解析\n本課程將帶你從零開始，從模型整體架構到每一個細節組件的實現，都有專業講解。你將學會如何利用 PyTorch 搭建Transformer 模型，並深入掌握其核心技術：\nInput embedding\nPositional Encoding\nLayer Normalization\nFeed Forward Network\nAdd & Norm\nMulti-Head Attention\nCross Attention\nInitial Transformer\n如何處理翻譯功能的Tokenizer\n如何創建Bilingual Datasets\n如何創建 DataLoader\n如何實現數據驗證和評估\n深入淺出的代碼講解\n課程將逐步帶你走過每一行代碼，詳解 tensor 的維度變換、各層之間的連接（如 skip connection 和殘差結構）以及背後的數學推導。無論你是初學者還是有經驗的深度學習愛好者，都能在這裡找到屬於你的學習亮點！\n前沿技術與實戰案例結合\n不僅僅停留在理論講解，課程還融合了 Transformer 模型在實際應用中的各種優化技巧。你將看到如何利用預先計算的頻率參數，如何在推理階段借助 KV Cache 大幅減少不必要的運算，進而達到高性能的模型運行效果。",
      "target_audience": [
        "深度學習工程師與研究者",
        "自然語言處理 (NLP) 專家",
        "技術開發者與大學生",
        "AI 技術愛好者"
      ]
    },
    {
      "title": "빅데이터분석기사 완전정복 필기편 : Part.3. 빅데이터 모델링(2)",
      "url": "https://www.udemy.com/course/part3-2-j/",
      "bio": "빅데이터 모델링 배우기! 회귀 분석부터 분석 기법 예상 문제 풀이까지 함께 학습합시다.",
      "objectives": [
        "빅데이터 모델링",
        "분석기법 적용",
        "회귀 분석",
        "인공신경망",
        "의사결정나무 알고리즘",
        "서포트 벡터 머신",
        "군집 분석",
        "분석 기법 예상 문제 풀이"
      ],
      "course_content": {},
      "requirements": [
        "\"한번에 합격하겠다는 의지! 데이터 통계와 데이터 보안 관련 분야 지식이 있으면 좋습니다.\""
      ],
      "description": "안녕하세요, ITGO 입니다.\n\n\n본 강의는 빅데이터분석기사 완전정복 필기편 : Part.3. 빅데이터 모델링(2) 강의입니다.\n\n\n빅데이터 분석 기사는 국가 기술 자격증으로 필기와 실기 시험이 있습니다.\n\n\n본 강의는 빅데이터 분석 기사 자격증 취득을 위한 필기 시험 대비 강의입니다.\n\n\n빅데이터 분석 기사 필기 시험 핵심 개념부터 예상 문제 풀이까지 제대로 배워 한번에 합격합시다!\n\n\n\n\n누구를 위한 강의인가요?\n\n\n빅데이터 분석기사(필기) 자격증 취득을 원하시는 분들\n\n\n데이터 분석 직무로의 취업 및 이직을 준비하시는 분들\n\n\n데이터 분석에 대해 공부하고자 하시는 분들\n\n\n\n\n무엇을 배우나요?\n\n\n빅데이터 모델링\n\n\n분석기법 적용\n\n\n회귀 분석\n\n\n인공신경망\n\n\n의사결정나무 알고리즘\n\n\n서포트 벡터 머신\n\n\n군집 분석\n\n\n분석 기법 예상 문제 풀이\n\n\n\n\n빅데이터분석기사 완전정복 필기편 : Part.3. 빅데이터 모델링(2) 강의에 입문해봅시다~!\n\n\n\n\n그럼 강의에서 만나요 :)",
      "target_audience": [
        "빅데이터 분석기사(필기) 자격증 취득을 원하시는 분들",
        "데이터 분석 직무로의 취업 및 이직을 준비하시는 분들",
        "데이터 분석에 대해 공부하고자 하시는 분들"
      ]
    },
    {
      "title": "Geostatistiker - Räumliche Datenanalyse mit R",
      "url": "https://www.udemy.com/course/geostatistiker-raumliche-datenanalyse-mit-r/",
      "bio": "Lerne räumliche Muster in dein Daten zu analysieren, identifizieren und zu präsentieren.",
      "objectives": [
        "In diesem Kurs lernst du Grundprinzipien der räumlichen Statistik genauso kennen, wie der praktisch Einsatz dieser Methoden in R.",
        "Am Ende dieser Schulung wirst du in der Lage sein, das Skalenniveau deiner Daten zu benennen und geeignete Rechenverfahren auszuwählen.",
        "Du wirst in der Lage sein, statistische Grundprinzipien wie Wahrscheinlichkeitsverteilungen und statistische Hypothesentests zu verstehen und anzuwenden.",
        "Du wirst verstehen, was mit räumlicher Autokorrelation gemeint ist, wie man sie berechnet und wie man sie sich für statistische Verfahren zunutze machen kann.",
        "Du wirst räumliche Hotspot Analysen und weitere räumliche Clusteranalysen verstehen und durchführen können.",
        "Du wirst verschiedene räumliche Interpolationsverfahren kennen und wissen, wie man sie nutzt.",
        "Du wirst uni-, bi- und multivariate Regressionsanalysen beherrschen, sowie weitere Phänomene und Methoden, die damit zusammenhängen.",
        "Du wirst deine Regressionsanalysen auf ein neues Level heben, indem du mathematische Annahmen überprüfst, und den Raumbezug integrierst.",
        "Schließlich wirst du in der Lage sein, im weiteren Feld des Machine Learnings, geeignete Methoden zu benennen und für deine Daten auszuwählen."
      ],
      "course_content": {},
      "requirements": [
        "Du solltest sicher mit der Programmiersprache R umgehen können. Es empfiehlt sich den Kurs R-Analyst zu besuchen, auf den dieser Kurs aufbaut."
      ],
      "description": "Herzlich willkommen zur Geostatistiker Schulung! In dieser Schulung werden wir lernen räumliche Phänomene, Sachverhalte und Zusammenhänge zu analysieren und zu identifizieren, um sie letztendlich anschaulich zu präsentieren. Wir werden uns sowohl statistische Konzepte und Prinzipien, wie auch die praktische Anwendung in R angucken.\nWir werden uns als erstes Skalenniveaus und deskriptive Statistik angucken. Als nächstes lernen wir von Grund auf, wie Wahrscheinlichkeitsverteilungen zustande kommen und wie wir sie interpretieren. Wir werden in der Lage sein, zu erklären, wie statistische Hypothesentest funktionieren und wie das Ganze mit Wahrscheinlichkeitsverteilungen zusammenhängt. Als nächstes gucken wir uns Grundannahmen und den Raumbezug an. Hier werden wir mathematische Grundannahmen überprüfen, Korrelationsanalysen durchführen, und uns dann das Prinzip der räumlichen Autokorrelation näher angucken. Wir werden die räumliche Autokorrelation auch verwenden, um eine räumliche Hotspot Analyse durchzuführen.\nAls nächstes werden wir uns verschiedenste Clusteranalysen kennenlernen und verstehen, wie wir räumliche Häufungen hoher und niedriger Werte in unseren Daten identifizieren.\nAnschließend beschäftigen wir uns mit räumlichen interpolationsverfahren. Wir lernen verschiedene Methoden kennen, die es uns erlauben, punktförmige Daten in die Fläche zu interpolieren.\nSchließlich lernen wir Regressionsanalysen in 2 Teilen zu meistern. Zunächst beschäftigen wir uns mit uni-, bi- und multivariaten Regressionsverfahren und Phänomene und Methoden, die damit zusammenhängen. Im zweiten Teil gehen wir der Sache noch ein bisschen mehr auf den Grund. Wir gucken wir uns mathematischen Grundannahmen an, überprüfen diese und leiten dann gegebenenfalls die richtigen Schritte ein, um keine mathematischen Grundannahmen zu verletzen. In diesem Teil werden wir auch sehen, wie wir die räumliche Information in den Regressionsanalysen berücksichtigen können, um unsere Regressionsmodelle zu verbessern.\nZum Schluss gibt es dann noch einen kleinen Überblick über weitere Maschine Learning Techniken, womit wir dann die Schulung abschließen.",
      "target_audience": [
        "Dieser Kurs richtet sich an alle, die mit Daten zu tun haben, welche in irgendeiner Weise einen Raumbezug haben."
      ]
    },
    {
      "title": "L’intelligence Artificielle générative pour débutants",
      "url": "https://www.udemy.com/course/lintelligence-artificielle-generative-pour-debutants/",
      "bio": "Créez votre premier chatbot Intelligent avec l'IA Générative avec OpenAI, Langchain, streamlit et FAISS",
      "objectives": [
        "Vous allez comprendre les bases de l’intelligence artificielle",
        "Vous allez apprendre à concevoir des applications de chatbot",
        "Vous allez apprendre à utiliser des outils et technologies puissants",
        "Vous allez apprendre à utiliser OpenAI, Langchaine, Stremlit, FAISS etc."
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Aperçu historique et utilisations actuelles de l'IA générative",
          "Les modèles de langage (LLMs)"
        ],
        "Développement d'un projet de chatbot intelligent": [
          "Présentation du projet de chatbot",
          "Processus de conception d'un chatbot",
          "Environnement de développement et installation des bibliothèques nécessaires",
          "Étape 1: Création de l'interface utilisateur avec streamlit",
          "Etape 2: Extraction du texte",
          "Étape 3: Processus de division du texte en chunks",
          "Étape 4: Génération des embeddings",
          "Étape 5: Création de la base de données et stockage des embeddings",
          "Étape 6: Recherche de similarité avec FAISS",
          "Finalisation du développement du chatbot",
          "Optimisation et amélioration des performances du chatbot",
          "Conclusion et perspectives"
        ],
        "Installation de l'environnement de développement": [
          "Installation de Python sur macOs",
          "Installation de PyCharm sur macOs",
          "Installation de Python sur Windows",
          "Installation de PyCharm sur Windows"
        ]
      },
      "requirements": [
        "Les bases de Python"
      ],
      "description": "Êtes-vous prêt à plonger dans l’univers fascinant de l’intelligence artificielle (IA), même si vous n’avez aucune expérience préalable en IA ? Découvrez comment maîtriser les bases de l’IA et créez des applications concrètes, comme des chatbots, que vous pourrez monétiser.\nDans cette formation accessible et pratique, vous apprendrez à concevoir vos propres solutions basées sur l’intelligence artificielle. Que vous soyez étudiant, entrepreneur ou professionnel cherchant à élargir vos compétences, cette formation est l’occasion parfaite de transformer une technologie complexe en une opportunité professionnelle.\nCe que vous allez apprendre :\nComprendre les fondamentaux de l’IA : Découvrez les concepts clés de manière simple et intuitive, sans jargon compliqué.\nCréer vos propres chatbots intelligents : Développez des applications capables de répondre à des questions, d’interagir avec des utilisateurs et de traiter des informations.\nMaîtriser des outils puissants : Travaillez avec des technologies comme OpenAI, LangChain, FAISS et Streamlit pour concevoir des chatbots performants et professionnels.\nPourquoi suivre cette formation ?\nAccessible à tous : Aucun prérequis technique en IA. Seuls quelques notions de base en Python suffisent ! Les concepts sont expliqués étape par étape, et chaque module est conçu pour les débutants.\nApproche pratique : Vous développerez des compétences que vous pourrez appliquer immédiatement dans des projets réels.\nUne opportunité professionnelle : Le marché des chatbots est en pleine expansion, et les entreprises recherchent activement des solutions automatisées pour améliorer leur efficacité.\nTransformez vos compétences en revenus : Apprenez à exploiter l’IA générative en créant des solutions que vous pouvez proposer en freelance ou en vendant vos chatbots en ligne.\nÀ qui s’adresse cette formation ?\nAux débutants curieux de découvrir l’intelligence artificielle et ses applications.\nAux entrepreneurs qui souhaitent intégrer l’IA dans leurs projets ou automatiser leurs processus.\nAux freelances et professionnels cherchant à proposer de nouveaux services à leurs clients.\nÀ toute personne souhaitant explorer le potentiel de l’IA et se lancer dans un domaine innovant et rentable.\nÀ tous ceux qui souhaitent développer leurs compétences pour ne pas rester pas à la traine et être remplacé par l’intelligence artificielle.\nÀ la fin de cette formation, vous serez capable de :\nComprendre les bases de l'intelligence artificielle ;\nCréer des chatbots qui répondent aux questions des utilisateurs en se basant sur vos propres données.\nPourquoi cette formation est-elle unique ?\nPédagogie basée sur la pratique : Chaque concept est directement mis en pratique à travers la création d’un chatbot fonctionnel dès les premieres sections.\nSupport personnalisé : Je suis là pour répondre à vos questions et vous guider tout au long de votre apprentissage.\nContenu riche et actualisé : Vous apprendrez avec des outils modernes utilisés par les professionnels.\nLancez-vous maintenant !\nL’intelligence artificielle n’est plus réservée à une élite technologique. Avec cette formation, vous aurez toutes vos chances pour comprendre, créer et réussir. Inscrivez-vous dès aujourd’hui et commencez à apprendre comment construire vos propres solutions basées sur l’IA tout en explorant un nouveau monde d’opportunités professionnelles et financières.\nRejoignez-nous et transformez votre curiosité en compétences que vous pouvez les transformer en revenus !",
      "target_audience": [
        "Débutants"
      ]
    },
    {
      "title": "IA com Python SEM USAR BIBLIOTECAS: Algoritmos Genéticos",
      "url": "https://www.udemy.com/course/ia-com-python-sem-usar-bibliotecas-algoritmos-geneticos/",
      "bio": "Desenvolva do Zero sua própria biblioteca de Algoritmo Genético e utilize-a em seus próprios problemas de Otimização.",
      "objectives": [
        "Aprender sobre Computação Bio inspirada, área da Inteligência Artificial",
        "Criar do zero sua própria Biblioteca de Algoritmo Genético em Python",
        "Aplicar os Algoritmos Genéticos em problemas práticos de otimização",
        "Aprofundar os conhecimentos sobre o funcionamento interno dos Algoritmos Genéticos"
      ],
      "course_content": {},
      "requirements": [
        "Conhecimentos básicos de programação em Python"
      ],
      "description": "Nesse curso você aprenderá nos mínimos detalhes como um Algoritmo Genético funciona e desenvolverá sua própria biblioteca do zero com a linguagem de programação Python. Não fique refém de bibliotecas desenvolvidas por terceiros e nunca mais fique sem rumo sobre como funciona ou como utilizar as complexas bibliotecas criadas por outros desenvolvedores. Crie sua própria biblioteca de Inteligência Artificial, mais especificamente de Algoritmo Genético, e domine cada detalhe de sua utilização em seus próprios projetos. Com sua própria biblioteca o seu poder de criação será ilimitado.\n\n\nO professor desse curso é Engenheiro Eletrônico e Cientista da Computação com mais de vinte anos de experiência na indústria de tecnologia da informação e como docente no ensino técnico e superior, tendo atuado em grandes empresas multinacionais e em instituições de ensino superior de renome. Já participou de projetos de desenvolvimento de softwares para os mais diversos setores, em especial para a área de telecomunicações, financeira, médica e de entretenimento.\nrsos setores, em especial para a área de telecomunicações, financeira, médica e de entretenimento.\n\n\nCaso você esteja estudando ou trabalhando com Ciência de Dados, Inteligência Artificial, Aprendizado de Máquina ou assuntos relacionados, você extrairá muitos conhecimentos úteis nesse curso. Apresento passo a passo todos os detalhes e decisões de projeto durante o andamento das atividades, de modo que você saberá exatamente por que cada passo foi tomado.",
      "target_audience": [
        "Se você tem interesse na área de Inteligência Artificial aplicada a problemas de otimização e deseja conhecer em detalhes como funcionam os Algoritmos Genéticos, esse curso é para você."
      ]
    },
    {
      "title": "내손으로 자연어처리 with Python | 감정분류와 포털뉴스 검색 프로젝트",
      "url": "https://www.udemy.com/course/codingxnsnlp/",
      "bio": "사례를 통해 배우는 이론과 직접 내손으로 구현하는 NLP 프로젝트",
      "objectives": [
        "텍스트 데이터의 유사도를 표현하는 TF-IDF의 이론과 실습을 통한 이해를 배우게 됩니다.",
        "같은 단어의 수와 형태소 등 다양한 방법으로 유사도를 판단하는 접근을 배우게 됩니다.",
        "텍스트를 벡터화 한 후 각 문장 간의 유사도를 측정하는 딥러닝 기반 방법을 배우게 됩니다.",
        "자카드 유사도, 코사인 유사도, 유클리디언 유사도, 맨하탄 유사도 등",
        "영화 리뷰 데이터, RSS 포털 뉴스 검색기 등 실제 프로젝트를 통해 NLP 기본 기술을 배우게 됩니다."
      ],
      "course_content": {
        "자연어처리 시작": [
          "실습환경 세팅",
          "자주 등장하는 단어를 찾는 방법, TF-IDF",
          "유사도를 판단하는 방법, Similarity"
        ],
        "텍스트 데이터 이해하기": [
          "텍스트 데이터 이해하기",
          "영화 리뷰 데이터 실습으로 텍스트 데이터 이해하기",
          "긍정/부정 감정 분류하기 (데이터 준비)",
          "긍정/부정 감정 분류하기 (코드 실습)"
        ],
        "자연어처리로 알아보는 웹 검색 이해하기": [
          "포털뉴스 검색기 (이론 및 데이터 준비)",
          "포털뉴스 검색기 (코드 실습)"
        ]
      },
      "requirements": [
        "파이썬 프로그래밍 기초"
      ],
      "description": "초급자를 위해 준비한\nPython 언어로 배우는 자연어 처리 강의입니다.\n\n\n강의 한줄 소개\n- 인터넷 페이지, 다양한 문서 등 텍스트에서 유사도를 판단하고 분류하는 자연어처리 기술을 다루는 강의입니다.\n\n\n내손으로 체화하는 파이썬 실습\n- Google Colab으로 복잡한 설치 없이 간편하게 실습합니다.\n- 파이썬 개발환경에서 데이터를 가져오는 다양한 방법(zip 파일 읽기, 경로를 찾아 다운로드 받기 등)\n- 인공지능/데이터사이언스에서 자주 사용하는 패키지(scikit-learn, Keras, TF, numpy 등)을 경험합니다.\n- 영화 리뷰를 하나의 데이터로 긍정/부정을 분류하는 프로젝트를 구현합니다.\n- 포털뉴스에 자주 사용되는 RSS 데이터 중 원하는 기사를 검색하는 프로젝트를 구현합니다.\n\n\n핵심 내용\n자연어 기초 이론\n- 딥러닝 기반 자연어 처리 개념\n- 웹 문서에서 자연어 추출하기\n- 추천 및 검색 시스템의 기초\n- 워드 임베딩(Word- to Vector)\n\n\n텍스트의 이해\n- 파이썬 함수로 데이터 전처리하기\n- 문장 전처리 및 데이터 시각화\n\n\n감정분류기 설계\n- 데이터 전처리 구현, 실제 데이터로 구현하는 프로젝트\n\n\n신문기사 검색엔진\n- 검색엔진 개요와 검색 기본함수\n- 데이터 전처리\n- 신문기사 검색기 구현 및 응용",
      "target_audience": [
        "텍스트를 이용한 자연어 처리의 원리와 프로젝트를 경험해보고 싶은 학습자",
        "포털뉴스 검색기, 영화 리뷰 데이터 처리 등 데이터 분석의 기초를 경험해보고 싶은 학습자"
      ]
    },
    {
      "title": "KNIME Analytics Platform per Data Scientists, intermediate",
      "url": "https://www.udemy.com/course/knime-analytics-platform-per-data-scientists-intermediate/",
      "bio": "Implementare gli algoritmi di machine learning senza usare codice, conoscere le tecniche più avanzate.",
      "objectives": [
        "Conoscere aspetti avanzati di KNIME Analytics Platform",
        "Gestire e manipolare i valori di data e ora presenti nei dataset",
        "Costruire dei cicli per iterare rami di un Workflow in base ad una feature, o una sua porzione , un certo numero di volte, fino al verificarsi di una condizione",
        "Trasformare le feature di un dataset in variabili per modificare il comportamento di un Workflow o effettuare manipolazioni",
        "Conoscere tecniche avanzate di machine learning, come l'ensemble e le reti neurali profonde",
        "Approfondire i nodi per le gestione dei database",
        "Approfondire i nodi per la visualizzazione",
        "Inviare i dati elaborati verso Power BI",
        "Implementare una soluzione di RAG, con un LLM locale, usando KNIME Analytics Platform",
        "Esplorare KNIME Community Hub e Team Plan"
      ],
      "course_content": {
        "Introduzione": [
          "Introduzione al corso"
        ],
        "Flow Variables": [
          "Informazioni utili per il vostro feedback",
          "Dal dato alla variabile",
          "Path Data Type Flow Variable",
          "Value Selection Configuration Node",
          "Multiple Selection Configuration Node",
          "Esempio: aggiornare il nome di un file aggiungendo la data di esecuzione",
          "Configuration Nodes (parte 1)",
          "Configuration Nodes (parte 2)",
          "Configuration Nodes (parte 3)"
        ],
        "Gestione di Date & Time": [
          "Date&Time, introduzione",
          "String to Date&Time Node (parte 1)",
          "String to Date&Time Node (parte 2)",
          "Modify Time Zone Node",
          "Date&Time Difference Node",
          "Date&Time-based Row Filter Node",
          "String to Duration Node",
          "Date&Time Shift Node",
          "Modify Date Node",
          "Modify Time Node",
          "Extract Date&Time Fields Node",
          "Moving Average Node",
          "Moving Aggregation Node"
        ],
        "Controllo del flusso di un Workflow": [
          "Loops, Switches, Try-Catch",
          "Cos'è un Loop",
          "Group Loop Start Node",
          "Table Row to Variable Loop Start Node",
          "Chunk Loop Start Node",
          "List Files Node/Folder Node",
          "IF Switch Node",
          "Case Switch Start Node",
          "Utilizzo dei nodi End IF",
          "Gestione degli errori: Try-Catch",
          "Uscire da un'esecuzione: Breakpoint",
          "Streaming"
        ],
        "Manipolare i dati in un database": [
          "Operazioni disponibili",
          "DB Node disponibili",
          "SQLite Connector Node",
          "DB Writer Node",
          "DB Table Selector Node",
          "DB Reader Node",
          "Database connectors (parte 1)",
          "Database connectors (parte 2)",
          "Registrare un driver JDBC",
          "DB GroupBy Node",
          "DB Joiner Node",
          "DB Row Filter Node",
          "DB Sorter Node",
          "DB Query Node",
          "DB Connection Table Writer Node",
          "DB Update Node",
          "DB Insert Node",
          "DB Delete (Filter) Node",
          "DB Delete (Table) Node",
          "DB SQL Executor",
          "DB Query Extractor e Injector"
        ],
        "Machine Learning, aspetti avanzati": [
          "Introduzione",
          "Metodi Ensemble",
          "Bagging e Boosting",
          "Come funziona l'Ensemble?",
          "Bagging",
          "Bootstrap",
          "Boosting",
          "Decision Tree",
          "Out of bag",
          "Random Forest",
          "Random Forest Learner e Prediction Node (parte 1)",
          "Random Forest Learner e Prediction Node (parte 2)",
          "Random Forest Learner e Prediction Node (parte 3)",
          "Tree Ensemble Learner e Predictors Nodes",
          "Gradient Boosted Trees Learner e Prediction Nodes (parte 1)",
          "Gradient Boosted Trees Learner e Prediction (parte 2)",
          "Confronto delle performance degli algoritmi di Ensemble",
          "Parameter Optimization (parte 1)",
          "Parameter Optimization (parte 2)",
          "Cross Validation",
          "X-Partitioner e X-Aggregator Nodes",
          "Binary Classification Inspector Node (parte 1)",
          "Binary Classification Inspector Node (parte 2)",
          "Binary Classification Inspector Node (parte 3)",
          "H20 Integration in KNIME (parte 1)",
          "H2O Integration in KNIME (parte 2)",
          "Deep Learning in KNIME (parte 1)",
          "Deep Learning in KNIME (parte 2)",
          "Deep Learning in KNIME (parte 3)",
          "Deep Learning in KNIME (parte 4)",
          "Deep Learning in KNIME (parte 5)",
          "Deep Learning in KNIME (parte 6)",
          "Deep Learning in KNIME (parte 7)",
          "Sentiment Analysis utilizzando Keras (parte 1)",
          "Sentiment Analysis utilizzando Keras (parte 2)",
          "Sentiment Analysis utilizzando Keras (parte 3)",
          "Sentiment Analysis utilizzando Keras (parte 4)",
          "Sentiment Analysis utilizzando Keras (e la GPU)",
          "Deploying del modello di Sentiment Analysis con Keras"
        ],
        "Component": [
          "Cos'è un Component?",
          "Creare un Component",
          "Configurare un Component",
          "Aggiungere o modificare le porte in un Component",
          "Condividere un Component (parte 1)",
          "Condividere un Component (parte 2)",
          "Condividere un Component (parte 3)",
          "Esportare una variabile locale all'esterno del Component",
          "Come utilizzare un Component condiviso",
          "Configuration Dialog Layout",
          "Pubblicare un workflow su KNIME Server",
          "Widget nodes"
        ],
        "Visualizzazione dei dati": [
          "Introduzione",
          "JavaScript, JFreeChart, KNIME Labs Views",
          "Scatter Plot con JavaScript",
          "Scatter Plot con KNIME Views Labs (parte 1)",
          "Scatter Plot con KNIME Views Labs (parte 2)",
          "Stacked Area Chart Node (parte 1)",
          "Stacked Area Chart Node (parte 2) e Composite Layout",
          "Histogram Node",
          "Line Plot Node",
          "Heathmap Node",
          "Column Resorter Node",
          "Table View Node e interazione con Scatter Plot",
          "Creare una dashboard interattiva: Netflix dataset"
        ],
        "Esportare i dati": [
          "Salvare un dataset sul proprio Workspace Power BI"
        ],
        "Una soluzione RAG con un LLM locale": [
          "Utilizzare un LLM all'interno di KNIME",
          "RNN, LSTM e Transformer",
          "RNN (Recurrent Neural Network)",
          "LSTM (Long Short Term Memory)",
          "La rete Neurale Transformer",
          "Panoramica sui LLM",
          "Cos'è un LLM?",
          "LLM per la IA generativa",
          "Come viene addestrato un LM",
          "ChatGPT",
          "ChatGPT: pro e contro",
          "Altre abilità dei LLM",
          "Perché dovrei utilizzare un LLM locale?",
          "GPT4ALL",
          "GPT4ALL: installazione",
          "GPT4ALL: come funziona?",
          "Come utilizzare GPT4ALL in KNIME",
          "Hugging Face",
          "Accedere ad un LLM su Hugging Face",
          "LLM locali: pro e contro",
          "Embedding",
          "Creare un Embedder con KNIME",
          "Criterio di ricerca nel DB Vettoriale",
          "RAG (Retrieval Augmented Generation)",
          "KNIME for generative AI",
          "Prompt Engineering",
          "Creiamo un Embedding con KNIME",
          "Prompt utilizzando la funzione RAG",
          "Parametri di un LLM",
          "RAG con KNIME",
          "La cattedrale e il bazaar",
          "Creiamo il nostro Workflow di RAG",
          "La fase di generation (prima parte)",
          "La fase di generation (seconda parte)"
        ]
      },
      "requirements": [
        "Avere un computer con almeno uno dei seguenti sistemi operativi: Windows, macOS, Linux; potrebbe essere necessario un profilo di amministratore, per l'installazione",
        "Conoscenza delle principali tecniche utilizzate nel machine learning: Supervised e Unsupervised Classification, Clustering",
        "Non serve conoscere alcun linguaggio di programmazione",
        "Avere seguito il corso KNIME Analytics Platform per Data Scientists, corso base o conoscere le basi di KNIME"
      ],
      "description": "Questo corso di rivolge alle persone che manipolano i dati per la loro attività (studenti, professionisti) e vorrebbero utilizzare gli algoritmi di machine learning per il data mining ma non hanno voglia o tempo di imparare un linguaggio di programmazione, come R o Python.\nFortunatamente ci sono strumenti che permettono di raggiungere gli stessi obiettivi, senza utilizzare una riga di codice (a meno che non si voglia proprio farlo).\nTra questi, sicuramente, KNIME Analytics Platform, o più semplicemente KNIME® è il più conosciuto e utilizzato in questo ambito.\nKNIME® è un ambiente completo e Open Source per l'analisi dei dati e il machine learning, che permette l'uso degli algoritmi di data mining più diffusi all'interno di un Workbench visuale, grazie all'utilizzo di componenti software, detti nodi, che combinati in maniera opportuna, permettono di elaborare qualsiasi base di dati.\nIl corso, di livello intermedio, copre aspetti avanzati di KNIME, che consentono di migliorare l'esperienza di uso e di utilizzo e si compone di 10 sezioni:\n\n\nFlow variables: consentono di rendere dinamica l'esecuzione di un workflow sulla base di come evolvono certe informazioni al suo interno\nGestione di Date & Time: verranno trattati i nodi che permettono la manipolazione di data e ora, il calcolo di durate, la gestione dei dati su medie mobili\nControllo del flusso: Loop, If, Case, Break, gestione degli errori.\nGestire i dati in un DBMS: collegarsi ad un DBMS, creare o modificare tabelle, inserire, modificare, cancellare righe su una tabella tutto senza conoscere l'SQL (ma anche conoscendolo)\nAspetti avanzati di Machine Learning: Ensemble (Bagging, Bootstrap, Boosting), Cross Validation), Reti Neurali FFNN, CNN, RNN\nComponent: creare ed adoperare un Component all'interno di KNIME Server\nVisualizzazione dei dati: utilizzare i nodi di visualizzazione Plotly e KNIME views (Labs)\nEsportare i dati: inviare i dati di un'elaborazione da KNIME a Power BI®\nImplementare una soluzione RAG (Retrieval Augmented Generation), con un LLM (Large Language Model) locale, all'interno di KNIME\nKNIME Community Hub, lo spazio dove diversi utenti di KNIME possono collaborare ad un progetto di data analysis, eseguire i propri workflow e pubblicare i risultati\nIn tutte le sezioni si utilizzerà prevalentemente KNIME®, mostrando alcune implementazioni di data mining con dati pubblici e fornendo numerosi esempi tratti dal sito Hub di KNIME.\n\n\nNOTE dell'autore:\nKNIME® è un marchio registrato e il logo e il marchio OPEN FOR INNOVATION® sono utilizzati da KNIME AG su licenza di KNIME GmbH e sono registrati negli Stati Uniti. KNIME® è anche registrato in Germania.\nL'autore non é collegato in alcun modo all'azienda.\nIl corso è stato sviluppato sulle tracce dei corsi self paced [L2-DS] KNIME Analytics Platform for Data Scientists: Advanced e [L2-DW] KNIME Analytics Platform for Data Wranglers: Advanced, disponibili, in lingua inglese, sul sito di KNIME.\nGli esempi mostrati durante le lezioni sono per lo più disponibili sul sito KNIME Hub, cui si rimanda nelle risorse presenti alla fine di ogni lezione del corso.\nLa main version di KNIME utilizzata negli esempi è la 4.6 o 4.7 , le lezioni della sezione che spiega come implementare soluzioni che utilizzano i LLM, in KNIME, sono state create utilizzando la versione 5.2.3, che implementa i nodi per l'AI generativa. La versione di KNIME Community Hub utilizzata nella sezione relativa è la 1.12.\nGli esempi che utilizzano KNIME Server sono stati creati con la versione 4.5",
      "target_audience": [
        "Studenti di Ingegneria, Statistica, Matematica",
        "Professionisti che nel lavoro hanno a che fare con i dati e che finora hanno utilizzato Excel o Microsoft Access per le loro analisi",
        "Curiosi e appassionati di data mining, che non vogliono imparare un linguaggio di programmazione per usare le tecniche di machine learning.",
        "Chi pensa di poter usare un LLM locale nel proprio lavoro, o nella propria attività, senza accedere alle soluzioni proprietarie (e a pagamento)"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第三部 - 製作能進化的AI",
      "url": "https://www.udemy.com/course/self-improved-ai/",
      "bio": "關於Generative AI, langchain, SEO, Self-Improved AI，Improving Code Generation",
      "objectives": [
        "掌握AI程序能夠自我進化，自我完善的核心和技術",
        "讓AI程式自主分析編程和修正錯誤，提高編程質量",
        "通過編寫優化貪吃蛇遊戲，掌握AI在遊戲開發中的應用",
        "學習如何利用AI技術自動生成SEO優化的標題和內容，提供網站在搜索引擎上的可見性和流量"
      ],
      "course_content": {
        "介紹": [
          "自我完善AI",
          "課程工具準備"
        ],
        "讓AI為我們編寫第一段代碼": [
          "讓AI為我門編寫一段貪吃蛇代碼",
          "如何突破Prompt裡面有{}的難題"
        ],
        "規則運算式": [
          "規則運算式的用法",
          "規則運算式進階"
        ],
        "AI編寫程式第二段": [
          "如何移除AI編寫代碼中不需要的說明部分",
          "如何保存AI編寫好的代碼",
          "如何用Python安裝AI程式的依賴",
          "如何用執行AI程序以及之間的邏輯",
          "如何實現AI程序的錯誤自我修復",
          "如何獲得新功能建議以對應新代碼",
          "測試AI生成代碼"
        ],
        "AI SEO程式製作": [
          "AI SEO程式製作",
          "如何獲得SEO優化提示詞",
          "如何處理API的JSON數據",
          "如何處理新舊最佳標題的交接",
          "如何處理LLM不需要的輸出內容",
          "如何處理評價與優化原來的最佳標題",
          "如何處理分割標題成關鍵詞",
          "如何處理格式化LLM輸出內容",
          "如何將Python變數傳遞到HTML"
        ]
      },
      "requirements": [
        "一台電腦",
        "學習過Generative AI 第1-2部"
      ],
      "description": "你是否对人工智能的未来充满好奇？你是否渴望掌握让AI不断进化、自我升级的核心技术？在我们最新的课程中，我们将揭示这一切的奥秘，带你踏上探索AI智能进化的奇妙旅程！这门课程将带您深入了解如何制作自我升级、自我完善、自我更新的人工智能程序。\n课程亮点\nAI自动生成游戏内容： 学习如何使用AI技术来生成经典贪吃蛇等游戏的内容，实现游戏元素的自动生成，让游戏体验更加丰富多样。\n自我修改与完善： 掌握如何让AI程序具有自我修改和完善的能力，不断优化游戏体验和性能。\n自动生成SEO优化内容： 学习如何利用AI自动生成SEO标题和贴文，提高网站在搜索引擎上的曝光度和流量。\n课程内容\n第一步：创建与挑战 我们将从创建一个基本的人工智能开始，编写一个经典的贪吃蛇游戏。最初，AI可能会面临各种编译失败和程序错误，但这正是学习的起点。\n第二步：分析与改进 通过让AI自主分析错误信息并改进代码，我们会逐步见证AI的成长。AI将一次次地尝试，直至成功编写出可运行的程序。\n第三步：自主创新 当AI掌握了基本编程技能后，我们将挑战它提出有趣的改进建议，并按照这些建议更新和发展代码。最终，AI将超越原有的编程模式，创造出更加精彩的内容。\n你将学会如何：\n掌握进化式人工智能的核心技术\n让AI自主分析和改进代码\n逐步提升AI的创新能力\n克服AI在编程中的各种短板\n利用AI技术进行SEO优化，提高网络流量\n最重要的是，你将亲自参与到这一切中，感受AI进化的每一个惊喜时刻！\n探索未来，从这里开始！\n加入我们的课程，开启你的AI探索之旅，见证AI如何从零开始，逐步进化，最终实现自主编程和自我改进。掌握这些技能，不仅可以提升你的职业竞争力，更能让你在AI领域中脱颖而出。立即报名，成为AI革命的一部分！",
      "target_audience": [
        "想學習自動代碼生成的學員",
        "想學習代碼錯誤檢驗和自我修復的學員",
        "想學習AI自我升級和改進的學員"
      ]
    },
    {
      "title": "[TR] Ollama ile Yapay Zeka: Llama, Deepseek, Mistral, QwQ",
      "url": "https://www.udemy.com/course/ollama-ile-yapay-zeka-llama-deepseek-mistral-qwq/",
      "bio": "Açık kaynak modellerle AI uygulamaları oluştur: NLP, sohbet botu, kod, özetleme, otomasyon ve daha fazlası.",
      "objectives": [
        "Yapay zeka modellerini kur ve çalıştır: Ollama ile modeli yerel olarak çalıştır.",
        "Gerçek dünya AI uygulamaları geliştir: LLaMA 3, Mistral, Mixtral ve diğerleriyle.",
        "NLP görevleri yap: metin özetle, içerik üret, belge düzelt ve bilgi çıkar.",
        "AI destekli asistanlar yap: chatbot, müşteri destek ve kişisel asistan.",
        "Kod üret ve hata ayıkla: CodeLlama ile kodlama sürecini hızlandır.",
        "Web uygulamalarına AI entegre et: FastAPI ve etkileşimli arayüz kullan.",
        "İş otomasyonu yap: e-posta yanıtları, toplantı özetleri ve özgeçmişler.",
        "Gerçek veri ve API’lerle çalışarak AI ile analizler yap.",
        "Model performansını optimize et: prompt ayarı ve yanıt doğruluğunu artır."
      ],
      "course_content": {
        "Hızlı Başlangıç – Ollama Kurulumu ve İlk AI Modeli": [
          "Bölüm Tanıtımı: Hızlı Başlangıç – Ollama Kurulumu ve İlk AI Modeli",
          "Ollama’ya Giriş",
          "Ollama’yı Kurma ve Yapılandırma",
          "Python ile Başlangıç",
          "İlk AI Projeniz: Sohbet Asistanı"
        ],
        "Metin İşleme için AI (NLP ve LLM’ler)": [
          "Bölüm Tanıtımı: Metin İşleme için AI (NLP ve LLM’ler)",
          "Mistral AI Modeli ile Metin Özetleyici",
          "LLaMA 3 ile Blog ve İçerik Yazarı",
          "CodeLlama ile Kod Üretici ve Hata Ayıklayıcı",
          "Deepseek R1 ile Dilbilgisi ve Yazım Denetleyici",
          "Phi-2 ile Hukuki Belge Analizörü",
          "Qwen 2.5 ile Gerçek Zamanlı Haber Özetleyici"
        ],
        "Sohbet Botları ve Asistanlar için AI": [
          "Bölüm Tanıtımı: Sohbet Botları ve Asistanlar için AI",
          "Müşteri Destek Chatbotu – QWQ AI Modeli",
          "Müşteri Destek Chatbotu – QWQ AI Modeli",
          "Tıbbi Belirti Kontrolü – MedLLaMA 2",
          "E-Ticaret Ürün Önerici – Granite 3.2"
        ]
      },
      "requirements": [
        "Temel Programlama Bilgisi – Python hakkında temel bilgilere sahip olmak faydalıdır ancak zorunlu değildir. Kodlama sürecinde sizi adım adım yönlendireceğiz.",
        "Yapay Zeka ve Makine Öğrenimine Temel Düzeyde Hakimiyet – Derin ML bilgisi gerekmez, ancak AI modellerinin nasıl çalıştığını bilmek yararlı olur.",
        "Web Geliştirme Bilgisi (Opsiyonel) – HTML, JavaScript ve FastAPI konularında deneyim, AI tabanlı web uygulamaları geliştirirken yardımcı olur.",
        "Bu kurs, yapay zeka geliştirmeye yeni başlayanları yönlendirmek için yapılandırılmıştır ve deneyimli geliştiriciler için gelişmiş projeler sunar.",
        "Yapay zekaya yeniyseniz her şeyi adım adım anlatacağız. Deneyimliyseniz, Ollama'nın en iyi AI modelleriyle gerçek dünya uygulamaları geliştirme pratiği kazanacaksınız."
      ],
      "description": "Ollama ile Tam Yığın Yapay Zeka: Llama, DeepSeek, Mistral, QwQ, Phi-2, MedLlama2, Granite3.2, en yeni açık kaynak yapay zeka modelleriyle gerçek dünya uygulamaları geliştirmeyi ve dağıtmayı öğreten nihai uygulamalı yapay zeka geliştirme kursudur. İster yapay zekayı yeni keşfeden bir acemi, ister deneyimli bir geliştirici olun, bu kurs size büyük dil modellerini (LLM’ler) web uygulamalarına, otomasyon araçlarına ve ileri düzey çözümlere entegre etmenizi sağlayacak pratik projeler sunar.\nKurs boyunca, güçlü yapay zeka modellerini pahalı bulut API'larına ihtiyaç duymadan yerel olarak çalıştırmak için Ollama’yı nasıl kuracağınızı, yapılandıracağınızı ve kullanacağınızı öğreneceksiniz. LLaMA 3, DeepSeek, Mistral, Mixtral, QwQ, Phi-2, MedLlama2, Granite3.2 ve CodeLlama ile çalışacak; doğal dil işleme (NLP), metin üretimi, kod tamamlama, hata ayıklama, belge analizi, duygu analizi ve yapay zeka destekli otomasyon konularında uzmanlık kazanacaksınız.\nKurs, gerçek dünya uygulamalarıyla doludur. Yapay zeka destekli bir haber özetleyici geliştirecek, metin düzeltme aracı oluşturacak, müşteri destek sohbet botu yazacak ve iş otomasyonu için akıllı bir asistan kuracaksınız. Her proje, FastAPI, Python, Ollama ve REST API’leri ile tam yığın geliştirme becerileri kazanmanıza yardımcı olacak.\nAyrıca, gerçek zamanlı verileri API'lar aracılığıyla almayı ve işlemeyi öğreneceksiniz. Bu sayede haber özetleme aracı, finansal rapor analizörü ve işe alım süreçlerini otomatikleştiren başvuru filtreleyici gibi uygulamalar geliştireceksiniz.\nKursun sonunda, metin işleme, doğal dil anlama, chatbot geliştirme, otomasyon ve LLM tabanlı uygulamaları kapsayan projeler geliştirmiş olacaksınız. Yapay zeka modellerini başarıyla dağıtacak ve bunları üretime hazır uygulamalara entegre etme konusunda kendinize güven duyacaksınız.\nİster geliştirici, veri bilimci, girişimci, araştırmacı olun, ister yalnızca yapay zekaya ilgi duyun; bu kurs, yapay zeka modellerini etkili biçimde uygulamak için gereken tüm pratik becerileri size kazandıracaktır.\nYapay zeka geliştirme yeteneklerinizi bir üst seviyeye taşımaya hazırsanız, bu kurs tam size göre!",
      "target_audience": [
        "Öğrenciler ve Kendi Kendine Öğrenenler – Yapay zekaya ilginiz var ama nereden başlayacağınızı bilmiyor musunuz? Bu kurs, önceki deneyim gerektirmeden sizi adım adım yönlendirir.",
        "Girişimciler ve Yapay Zeka Yenilikçileri – Yapay zeka destekli ürünler mi geliştirmek istiyorsunuz? Bu kurs, AI yolculuğunuza başlamanız için uygulamalı projeler sunar.",
        "Veri Analistleri ve Araştırmacılar – Hukuki belgeler, haberler veya müşteri yorumlarından bilgi mi çıkarmak istiyorsunuz? Büyük metin verilerini AI ile analiz etmeyi öğrenin.",
        "Teknoloji Girişimcileri ve Ürün Yöneticileri – İşletmeniz için AI tabanlı uygulamalar mı geliştirmek istiyorsunuz? Chatbotlar, içerik üreticiler ve otomasyon araçları oluşturmayı öğrenin.",
        "AI ve Makine Öğrenimi Meraklıları – LLaMA 3, Mistral, Mixtral, CodeLlama ve DeepSeek-R1 gibi modellerle çalışmak mı istiyorsunuz? Model dağıtımı konusunda pratik kazanacaksınız.",
        "Geliştiriciler ve Programcılar – AI modellerini web uygulamalarına veya yazılım projelerine entegre etmek mi istiyorsunuz? Python ve FastAPI ile tam yığın AI uygulamaları yapmayı öğrenin."
      ]
    },
    {
      "title": "파이썬으로 배우는 크롤링 실습",
      "url": "https://www.udemy.com/course/webcrolling-suminw/",
      "bio": "파이썬 기본 문법, 자료구조, 판다스와 함께 배우는 웹 크롤링(with beautifulsoup)",
      "objectives": [
        "크롤링 개념에 대해서 배우게 됩니다.",
        "파이썬 문법을 이해하고 프로그래밍을 시작할 수 있습니다.",
        "데이터를 저장하고 관리하는 방법을 배우게 됩니다.",
        "BeatifulSoup를 활용한 웹 스크레핑을 배우게 됩니다.",
        "HTML의 구조에 대해 이해할 수 있습니다."
      ],
      "course_content": {
        "강의 및 파이썬 소개": [
          "인트로",
          "강의 목표",
          "파이썬 소개 및 환경 셋팅"
        ],
        "파이썬 문법 - 자료형 및 제어문": [
          "1. 자료구조란? / 숫자, 문자 자료형",
          "2. 리스트, 튜플 자료형",
          "3. 딕셔너리, 집합, 불 자료형",
          "4. if문",
          "5. for문",
          "6. while문"
        ],
        "웹 크롤링 소개": [
          "크롤링이란?",
          "주의 사항",
          "활용 라이브러리 소개"
        ],
        "데이터 다루기": [
          "데이터 관리 및 저장",
          "판다스 기본 사용법 - 1",
          "판다스 기본 사용법 - 2"
        ],
        "크롤링 실습": [
          "HTML 개념 설명",
          "웹 크롤링 기본 실습",
          "뉴스 기사 크롤링 실습(using 다음 뉴스)",
          "안티 크롤링 소개",
          "안티 크롤링을 활용한 실습(using 네이버 뉴스)",
          "금융 데이터 크롤링 실습(using 네이버 금융)"
        ],
        "데이터 분석 강의 소개 (강의 추천)": [
          "데이터 분석 강의 소개 (강의 추천)"
        ]
      },
      "requirements": [
        "프로그래밍 경험이 필요하지 않습니다. 시작부터 배우게 됩니다.",
        "인터넷을 실행할 수 있는 컴퓨터만 있으면 됩니다."
      ],
      "description": "데이터가 중요하다고 하여 관심이 있지만, 나도 수집해서 무언인가를 하고 싶다면 어떻게 해야 할까요?\n\n\n아래의 질문들은 데이터 업무를 하면 항상 주변에서 듣고 있는 질문입니다.\nㄴ 나도 코딩을 통해서 데이터를 수집할 수 있을까?\nㄴ 비전공자인데, 시작하기에는 어렵지 않을까요?\nㄴ 개발을 전혀 모르는데 괜찮을까요?\n\n\n\"우선 시작해봐요!\"라고 말할 수 있을 만큼 누구나 쉽게 따라 할 수 있습니다.\n\n\n<학습 내용>\n1. 파이썬 기초 문법 학습 : 직접 실습하면서 프로그래밍을 이해\n2. 웹 크롤링 : 크롤링을 하기 위한 개념부터 실습을 위한 라이브러리 학습 및 실제 홈페이지를 활용한 실습\n+ 수업 자료를 통해서 스스로 학습하고 복습할 수 있는 자신감\n+ 업무에 자주 쓰이는 함수를 배우고 소소한 꿀팁 공유\n\n\n<수강 후 기대효과>\n1. 파이썬 문법을 이해하고 다른 언어도 자신감있게 배울 수 있습니다.\n2. 기본적인 크롤링 방법을 활용해 실무에서 활용하는 데이터 생성이 가능합니다.\n3. 오픈소스나 다른 사람들의 코드를 보고 이해하며 나에게 맞는 코드로 적용할 수 있습니다.\n\n\n<주의 사항>\n강의에서는 크롤링의 단어를 많이 언급하고 있습니다.\n실제로 이 강의는 데이터를 수집하는 강의(웹 스크랩핑)입니다.",
      "target_audience": [
        "데이터 수집이 필요한 모든 사람(기획자, 마케터, 실무자)",
        "다양한 IT 분야에 도전하고 싶은 분",
        "불필요한 시간을 제거하고 싶은 분"
      ]
    },
    {
      "title": "Aceleração de Aplicações de IA com RAPIDS para GPU",
      "url": "https://www.udemy.com/course/aceleracao-de-aplicacoes-de-ia-com-nvidia-rapids/",
      "bio": "Computação com GPU e CUDA de alta velocidade e desempenho! Crie pipelines de Ciência de Dados 50 vezes mais rápidos!",
      "objectives": [
        "Entender as diferenças entre processamento utilizando CPU e GPU",
        "Utilizar o cuDF como substituto ao pandas para processamento acelerado na GPU",
        "Implementar códigos utilizando o cuDF para manipulação de DataFrames",
        "Utilizar o cuPy como substituto ao numpy para processamento acelerado na GPU",
        "Utilizar o cuML como substituo ao scikit-learn para processamento acelerado na GPU",
        "Implementar projetos de machine learning completos utilizando o cuDF e cuML",
        "Comparar o desempenho das bibliotecas clássicas do Python que são executadas na CPU com as bibliotecas do RAPIDS executadas na GPU",
        "Implementar projetos com o DASK para processamento paralelo e distribuído",
        "Integrar o DASK com o cuDF e cuML para desempenho superior na GPU"
      ],
      "course_content": {
        "Introdução": [
          "Conteúdo do curso",
          "Mais sobre Inteligência Artificial",
          "CPU x GPU",
          "GPU e CUDA",
          "Rapids",
          "Recursos para download"
        ],
        "cuDF": [
          "cuDF - intuição",
          "Instalação",
          "Pandas e cuDF",
          "Comandos básicos 1",
          "Comandos básicos 2",
          "Comandos básicos 3",
          "Comandos básicos 4",
          "Interoperabilidade com o cuPy",
          "Outras conversões de dados",
          "Funções definidas pelo usuário 1",
          "Funções definidas pelo usuário 2",
          "Comparativo de desempenho 1",
          "Comparativo de desempenho 2",
          "Comparativo de desempenho 3"
        ],
        "cuML": [
          "cuML - intuição",
          "Iniciando o ambiente",
          "Regressão com scikit-learn",
          "Regressão com cuML",
          "Ridge regression",
          "Tuning dos parâmetros",
          "Comparativo de desempenho 1",
          "Comparativo de desempenho 2",
          "Comparativo de desempenho 3"
        ],
        "Projeto no ecossistema Rapids": [
          "Instalação e bibliotecas",
          "Base de dados do censo",
          "Atributos categóricos 1",
          "Atributos categóricos 2",
          "Tratamento da base de dados",
          "Regressão logística e kNN",
          "Random Forest e SVM",
          "EXERCÍCIO",
          "Solução para o exercício 1",
          "Solução para o exercício 2"
        ],
        "Dask": [
          "Dask - intuição",
          "Criação de cluster local",
          "Arrays de GPUs distribuídas",
          "Dask e cuDF",
          "Dask e cuML 1",
          "Dask e cuML 2"
        ],
        "Considerações finais": [
          "Considerações finais",
          "BONUS"
        ]
      },
      "requirements": [
        "Lógica de programação",
        "Programação básica em Python",
        "Machine learning: entendimento básico do processo de treinamento de algoritmos, bem como as técnicas de classificação e regressão"
      ],
      "description": "Este curso foi desenvolvido de forma independente e não é afiliado ou patrocinado pela NVIDIA Corporation. RAPIDS é um projeto de código aberto originalmente desenvolvido pela NVIDIA.\nA ciência de dados e o machine learning representam os maiores setores computacionais do mundo, onde melhorias modestas na precisão dos modelos analíticos podem se traduzir em bilhões de impacto no resultado final. Os cientistas de dados constantemente se esforçam para treinar, avaliar, iterar e otimizar modelos para alcançar resultados altamente precisos e um desempenho excepcional. Com a poderosa plataforma RAPIDS da NVIDIA, o que antes levava dias agora pode ser realizado em questão de minutos, tornando a construção e implantação de modelos de alto valor mais fácil e ágil. Em Data Science, a capacidade computacional adicional significa insights mais rápidos e eficazes. RAPIDS aproveita o poder do NVIDIA CUDA para acelerar todo o fluxo de trabalho de treinamento de modelos de ciência de dados, executando-o em unidades de processamento gráfico (GPUs).\nNeste curso, você vai aprender tudo o que precisa saber para levar suas aplicações de aprendizado de máquina para outro nível! Confira abaixo alguns dos tópicos que serão abordados:\n\nUtilizar as bibliotecas cuDF, cuPy e cuML ao invés do Pandas, Numpy e scikit-learn; o que garante que os dados sejam processados e algoritmos de machine learning executados com alto desempenho na GPU\nComparar o desempenho das bibliotecas clássicas do Python com o RAPIDS. Em alguns experimentos executados durante as aulas, conseguimos taxas de aceleração superiores a 900x. Isso indica que em determinadas bases de dados e com determinados algoritmos, o RAPIDS consegue ser 900 vezes mais rápido!\nCriar um projeto completo e passo a passo de machine learning utilizando o RAPIDS, desde o carregamento dos dados até as previsões\nUtilizar o DASK para paralelismo de tarefas em múltiplas GPUs ou CPUs; integrado com o RAPIDS para um desempenho superior\nDurante o curso vamos utilizar a linguagem de programação Python e o Google Colab on-line. Desta forma, você não precisa possuir uma GPU local para acompanhar as aulas, pois utilizaremos o hardware gratuito disponibilizado pelo Google.",
      "target_audience": [
        "Cientistas de Dados e profissionais de Inteligência Artificial que querem aumentar o desempenho de suas aplicações",
        "Profissionais que trabalham ou desejam trabalhar na área de ciência de dados, especialmente aqueles que desejam melhorar suas habilidades em treinamento de modelos de machine learning e análise de dados",
        "Qualquer pessoa que tenha interesse em aprender sobre machine learning, especialmente com foco em implementações de alto desempenho usando GPUs",
        "Profissionais que estão envolvidos no desenvolvimento e implementação de modelos de machine learning",
        "Alunos de graduação e pós-graduação que estão estudando disciplinas ligadas a área de Inteligência Artificial"
      ]
    },
    {
      "title": "[ES] IA Full-Stack con Ollama: Llama, Deepseek, Mistral, QwQ",
      "url": "https://www.udemy.com/course/ia-full-stack-con-ollama-llama-deepseek-mistral-qwq/",
      "bio": "Crea apps de IA con modelos open-source: NLP, chatbots, generación de código, resúmenes y más.",
      "objectives": [
        "Aprende a implementar modelos de IA: instala, configura y ejecuta modelos con Ollama.",
        "Crea apps con IA: usa modelos como LLaMA 3, Mistral, CodeLlama, Mixtral y DeepSeek-R1.",
        "Realiza tareas de PLN: resume textos, genera contenido y extrae datos clave.",
        "Desarrolla asistentes con IA: crea chatbots y asistentes personales inteligentes.",
        "Genera y depura código con CodeLlama para mejorar tu flujo de desarrollo.",
        "Integra IA en apps web con FastAPI y una interfaz interactiva en tiempo real.",
        "Automatiza tareas: respuestas a emails, resúmenes de reuniones y CVs.",
        "Usa datos reales y APIs para obtener insights con modelos de IA.",
        "Mejora el rendimiento de IA ajustando prompts y optimizando respuestas."
      ],
      "course_content": {
        "Inicio Rápido - Configuración de Ollama y Primer Modelo de IA": [
          "Intro: Inicio Rápido - Configuración de Ollama y Primer Modelo IA",
          "Introducción a Ollama",
          "Instalación y Configuración de Ollama",
          "Funcionando con Python",
          "Tu Primer Proyecto de IA: Asistente de Chat con IA"
        ],
        "IA para Procesamiento de Texto (NLP y LLMs)": [
          "Introducción a la Sección: IA para Procesamiento de Texto (NLP y LLMs)",
          "Resumen de Texto con IA usando el Modelo Mistral",
          "Redactor de Blogs y Contenido con IA usando LLaMA 3",
          "Generador y Depurador de Código con IA usando CodeLlama",
          "Corrector Gramatical y Ortográfico con IA usando Deepseek R1",
          "Analizador de Documentos Legales con IA usando Phi-2",
          "Resumen de Noticias en Tiempo Real con IA usando Qwen 2.5"
        ],
        "IA para Bots Conversacionales y Asistentes": [
          "Introducción a la Sección: IA para Bots Conversacionales y Asistentes",
          "Chatbot de Atención al Cliente - Modelo QWQ",
          "Asistente Virtual con IA - LLaMA 2",
          "Verificador de Síntomas Médicos con IA - MedLLaMA 2",
          "Recomendador de Productos para E-commerce con IA - Granite 3.2"
        ]
      },
      "requirements": [
        "Conocimientos básicos de programación – Tener algo de experiencia con Python es útil, pero no obligatorio. Te guiaremos en el proceso de codificación.",
        "Comprensión básica de IA y aprendizaje automático – No se requiere conocimiento profundo de ML, pero entender cómo funcionan los modelos de IA será beneficioso.",
        "Familiaridad con desarrollo web (opcional) – Algo de experiencia con HTML, JavaScript y FastAPI te ayudará a crear aplicaciones web impulsadas por IA.",
        "Este curso está diseñado para guiar a principiantes absolutos en el desarrollo de IA, y también ofrece proyectos avanzados para desarrolladores experimentados.",
        "Si eres nuevo en IA, cubriremos todo paso a paso. Si ya tienes experiencia, obtendrás práctica en la construcción y despliegue de aplicaciones reales con modelos de Ollama."
      ],
      "description": "IA Full-Stack con Ollama: Llama, DeepSeek, Mistral, QwQ, Phi-2, MedLlama2, Granite3.2 es el curso práctico definitivo de desarrollo en IA. Aprenderás a construir y desplegar aplicaciones de IA reales utilizando los modelos de código abierto más recientes. Ya seas principiante o desarrollador con experiencia, este curso te proporcionará proyectos prácticos para integrar modelos de lenguaje (LLMs) en aplicaciones web, herramientas de automatización y soluciones avanzadas impulsadas por IA.\nA lo largo del curso, aprenderás a instalar, configurar y usar Ollama para ejecutar potentes modelos de IA localmente, sin depender de APIs en la nube costosas. Trabajarás con LLaMA 3, DeepSeek, Mistral, Mixtral, QwQ, Phi-2, MedLlama2, Granite3.2 y CodeLlama, adquiriendo experiencia en procesamiento de lenguaje natural (NLP), generación de texto, completado de código, depuración, análisis de documentos, análisis de sentimientos y automatización con IA.\nEl curso está repleto de proyectos reales. Desarrollarás un resumidor de noticias con IA, una herramienta de corrección de textos, un chatbot de atención al cliente y un asistente inteligente para automatización empresarial. Cada proyecto incluye experiencia práctica con FastAPI, Python, Ollama y REST APIs, asegurando que adquieras habilidades full-stack en integración de IA.\nTambién aprenderás a obtener y procesar datos en tiempo real mediante APIs, ideal para quienes desean construir aplicaciones que analicen información actualizada. Crearás un resumidor de noticias en tiempo real, un analizador de informes financieros con IA y un evaluador automatizado de candidaturas laborales.\nAl finalizar, habrás desarrollado proyectos completos de IA abarcando desarrollo full-stack, procesamiento de texto, comprensión de lenguaje natural, automatización y aplicaciones basadas en LLMs. Estarás preparado para implementar modelos de IA en aplicaciones listas para producción.\nEste curso es ideal para desarrolladores, científicos de datos, emprendedores, investigadores y entusiastas de la IA. Aprenderás a crear aplicaciones web con IA, integrar modelos NLP y automatizar tareas usando herramientas impulsadas por IA.\n¡Si estás listo para llevar tus habilidades de desarrollo en IA al siguiente nivel, este curso es para ti!",
      "target_audience": [
        "Estudiantes y autodidactas – ¿Te interesa la IA pero no sabes por dónde empezar? Este curso no requiere experiencia previa y te guiará paso a paso para construir aplicaciones prácticas.",
        "Fundadores de startups e innovadores en IA – Si deseas crear productos impulsados por IA, este curso ofrece proyectos prácticos para comenzar tu camino en IA.",
        "Analistas de datos e investigadores – ¿Quieres obtener información de documentos legales, artículos de noticias o reseñas de clientes? Aprende a usar IA para analizar textos masivos.",
        "Emprendedores tecnológicos y gerentes de producto – ¿Necesitas desarrollar aplicaciones con IA para tu negocio? Aprende a crear chatbots, generadores de contenido y herramientas de automatización.",
        "Entusiastas de la IA y el aprendizaje automático – ¿Te interesa trabajar con modelos como LLaMA 3, Mistral, Mixtral, CodeLlama y DeepSeek-R1? Adquirirás experiencia práctica en implementación de modelos.",
        "Desarrolladores y programadores – ¿Quieres integrar modelos de IA en apps web o herramientas de automatización? Aprende a crear aplicaciones IA full-stack con Python y FastAPI."
      ]
    },
    {
      "title": "Machine Learning: Regressão com Linguagem Python",
      "url": "https://www.udemy.com/course/machine-learning-regressao-com-linguagem-python/",
      "bio": "Aprenda sobre Regressão Linear, Ridge, Lasso e Elastic Net com um PROJETO do zero!",
      "objectives": [
        "O que é Machine Learning",
        "O que é Regressão",
        "O que é e como funciona o Jupyter Notebook",
        "Conhecer os principais tipos de aprendizado (supervisionado, não supervisionado e por esforço)",
        "Executar códigos em Python, célula por célula",
        "Entender quando aplicar transformação nos dados",
        "Aprender um pouco mais sobre o pandas",
        "Aprender sobre algoritmos de Regressão",
        "Aprender sobre Regressão Linear",
        "Aprender sobre Regressão Lasso, Ridge e Elastic Net",
        "Aprender a realizar otimização de hiper-parâmetros",
        "Aprender o que é cross-validation",
        "Aprender o que é Data Leakage",
        "Entender quando normalizar ou padronizar seus dados",
        "Aprender a treinar e testar seus modelos",
        "Aprender sobre MSE, RMSE, MAE, R quadrado"
      ],
      "course_content": {
        "Apresentação": [
          "Apresentação do Instrutor",
          "Módulos",
          "Ferramentas, Dicas e Contato"
        ],
        "Machine Learning (módulo comum a todos os cursos de Machine Learning)": [
          "Definição",
          "Tipos de Aprendizado",
          "Aprendizado Supervisionado",
          "Aprendizado Não Supervisionado",
          "Aprendizado por Reforço",
          "Exemplos de Projetos"
        ],
        "Regressão": [
          "Definição",
          "Algoritmos",
          "Regressão Linear Múltipla",
          "Regressão Ridge",
          "Regressão Lasso",
          "Regressão Elastic Net",
          "Pipeline"
        ],
        "Projeto": [
          "Etapa 1: Coleta",
          "Etapa 2: Análise Exploratória e Limpeza dos Dados (PARTE I)",
          "Etapa 2: Análise Exploratória e Limpeza dos Dados (PARTE II)",
          "Etapa 2: Análise Exploratória e Limpeza dos Dados (PARTE III)",
          "Etapa 2: Análise Exploratória e Limpeza dos Dados (PARTE IV)",
          "Etapa 3: Machine Learning (PARTE I)",
          "Etapa 3: Machine Learning (PARTE II)",
          "Etapa 3: Machine Learning (PARTE III)"
        ]
      },
      "requirements": [
        "Conhecimento básico de Python",
        "Muita vontade de aprender e de se destacar no mercado",
        "Você não precisa ser da área de exatas para realizar esse curso"
      ],
      "description": "COM PROJETO\nTEORIA E PRÁTICA COMBINADAS\n\n\nSOBRE O CURSO\nEsse NÃO é mais um curso complicado, sem explicações claras ou exemplos práticos para o mercado de trabalho.\n\n\nEsse curso É um jeito simples de você aprender Machine Learning em projetos de Regressão (Aprendizado Supervisionado), dos primeiros conceitos até os mais avançados.\n\n\nVocê não precisa ter experiência na área de Dados ou exatas para acompanhar todo o curso, que foi pensado com didática simples e módulos progressivos para você avançar com segurança! E para auxiliá-lo na evolução ao longo do curso, temos um projeto do início ao fim! Porém, é interessante que você tenha já um conhecimento básico em Python.\n\n\nComece hoje a explorar a área de Ciência de Dados com tranquilidade. Mesmo que já esteja na área, essa é a oportunidade para você melhorar suas habilidades com um conhecimento novo. Machine Learning traz MUITO VALOR para o negócio com análises preditivas, que preparam a empresa para o que pode ocorrer no futuro, diferentemente do BI tradicional que trabalha com análises descritivas sobre o passado.\n\n\nCada vez mais o mercado de trabalho exige de vários profissionais o conhecimento sobre Machine Learning!\n\n\nAbaixo a trilha ideal em direção ao sucesso na área de dados:\nLinguagem SQL para Análise de Dados.\nCriação com Dashboards com Looker Studio.\nPython: Manipulação de Dados com Pandas.\nMachine Learning: Clusterização com Linguagem Python.\nMachine Learning: Classificação com Linguagem Python.\nMachine Learning: Regressão com Linguagem Python (esse que você está vendo).\n\n\nSOBRE O INSTRUTOR\nMe chamo Caio Avelino, e o conhecimento que vou dividir com você nesse curso foi adquirido, principalmente, com minha experiência no mercado de trabalho. Atuo nas áreas de Business Intelligence, Ciência de Dados e Inteligência Artificial há anos e tive a oportunidade de desenvolver minhas habilidades em diversas startups.\n\n\nAté mais!",
      "target_audience": [
        "Iniciantes e Curiosos sobre Machine Learning",
        "Alunos de Ciência de Dados que gostariam de aprender Classificação e Aprendizado Supervisionado",
        "Analistas de BI com interesse em utilizar Python para Machine Learning para se destacarem no mercado",
        "Qualquer pessoa, de qualquer área que deseja entrar no Mundo de Dados"
      ]
    },
    {
      "title": "【한글자막】 OpenAI 파이썬 API 부트캠프: AI 와 GPT, 파이썬으로 AI 애플리케이션 개발하기!",
      "url": "https://www.udemy.com/course/openai-python-api-bootcamp-ai-gpt3-korean/",
      "bio": "프로젝트를 통해 OpenAI 의 강력한 API 로 텍스트와 이미지를 생성하는 인공지능의 힘을 경험, OpenAI 가 제공하는 Python API 활용하기, OpenAI API 를 활용해 파이썬으로 AI 앱 개발하기!",
      "objectives": [
        "Open API 기초 배우기",
        "API 이용을 위한 OpenAI 계정 만들기",
        "Python으로 OpenAI API 이용해 보기",
        "10+ 프로젝트를 통해 스타트업 아이디어 떠올리기",
        "기존 Python 애플리케이션에 OpenAI API를 연동해 AI 기능 활용하기",
        "OpenAI로 텍스트 임베딩해 보기",
        "OpenAI의 GPT 모델 적용하기",
        "DALLE-2 API로 이미지 생성하기",
        "사용자를 위해 콘텐츠와 이미지 자동 생성하기"
      ],
      "course_content": {
        "강좌 소개": [
          "FAQ, 과정 파일, 도움받는 법",
          "개발자의 날 이후 업데이트된 Zip 파일",
          "준비됐는지 확인하기",
          "강좌 커리큘럼 개요",
          "OpenAI 개요",
          "집중 강의: GPT 작동 원리",
          "집중 강의: DALL-E 작동 원리",
          "프리티어 변경 사항 관련",
          "OpenAI 계정 설정",
          "AI 안전 및 정렬",
          "OpenAI 최신 소식!",
          "OpenAI 가격"
        ],
        "자연어를 SQL로": [
          "code-davinci 모델 업데이트",
          "OpenAI 업데이트 확인하기",
          "NLP to SQL - 프로젝트 개요",
          "표 데이터 설정",
          "자연어 요청",
          "텍스트 완성 API - 매개변수 개요",
          "OpenAI API 호출 및 요청 처리"
        ],
        "자동 시험 생성기": [
          "자동 시험 생성 - 프로젝트 개요",
          "프롬프트 설계",
          "프롬프트로 시험 생성하기",
          "질문 및 답변 추출",
          "시험 시뮬레이션",
          "파이썬 스크립트 살펴보기"
        ],
        "자동 레시피 및 요리 이미지 생성기": [
          "레시피 프로젝트 개요",
          "Completion API로 레시피 텍스트 생성하기",
          "Image API(DALLE)로 요리 이미지 생성하기",
          "전체 파이썬 스크립트 살펴보기"
        ],
        "자동 블로그 게시물 생성기": [
          "자동 블로그 게시물 생성 프로젝트 개요",
          "GitHub 페이지 설정하기",
          "GitPython - 자동 업데이트 함수 – 파트 1",
          "GitPython - 자동 업데이트 함수 – 파트 2",
          "OpenAI API 호출",
          "파이썬 스크립트 살펴보기"
        ],
        "GPT 감정 분석": [
          "Reddit 감정 분석 소개",
          "Reddit API 설정하기",
          "Reddit 코멘트와 제목",
          "OpenAI로 감정 분석하기"
        ],
        "코드 설명 자동화 - Docstring": [
          "Code Explainer 소개",
          "OpenAI API 호출 - Docstring",
          "파이썬 함수와 Docstring 병합",
          "파이썬 스크립트 살펴보기"
        ],
        "번역 프로젝트": [
          "뉴스 번역 요약 프로젝트 소개",
          "국제 신문 스크랩",
          "OpenAI 번역과 요약"
        ],
        "미세 조정과 챗봇": [
          "미세 조정과 챗봇",
          "데이터 설정",
          "미세 조정 과정과 비용 추정",
          "미세 조정 모델 사용"
        ],
        "질문과 답변 텍스트 임베딩": [
          "텍스트 임베딩 소개",
          "모델 환각",
          "문구 데이터",
          "토큰 개수 및 비용",
          "문구 유사도 및 컨텍스트 주입"
        ]
      },
      "requirements": [
        "파이썬 프로그래밍 경험",
        "파이썬 데이터 에코시스템 (Panda, NumPy 등)에 대한 경험 권장됨",
        "OpenAI API 사용을 위해 신용카드를 등록할 수 있어야 함",
        "pip install 명령어로 라이브러리 설치하기"
      ],
      "description": "이 강의는 유데미 강좌 중 \"OpenAI Python API Bootcamp: Learn to use AI, GPT, and more!\"와 동일한 강의이며, 한국어 자막이 기존 강의와 다르게 전문 한글 자막이 제공됩니다. 또한 강의 내용에 대한 질문은 Q&A에 영어로 남겨주시면 오리지널 강사님으로부터 답변을 받으실 수 있습니다. 강의 내용 외의 문의는 한국어로 남겨주셔도 되며, 웅진씽크빅 글로벌에서 매일 확인하여 답변드리고 있으니 편하게 질문해주세요! :)\n\n\nOpenAI 가 제공하는 Python API 를 최대치로 이용하는 방법을 알려드리는 과정에 오신 것을 환영합니다!\n\n\n본 과정에서 여러분은 OpenAI 의 강점을 이용해, 파이썬으로 지능 AI 애플리케이션과 솔루션을 만드는 방법을 배울 수 있습니다. OpenAI API 는 자연어 처리, 컴퓨터 비전 등을 위한 기능을 제공하는 뛰어난 AI 플랫폼 중 하나입니다. OpenAI API 를 이용해 사람의 언어를 이해하고 그에 반응할 수 있는 AI 애플리케이션을 만들어 텍스트를 생성하고 감정 분석을 수행하는 등 다양한 기능을 구현할 수 있습니다.\n\n\n본 과정은 프로젝트를 기반으로 설명을 제공하며, 각 섹션마다 제공하는 프로젝트는 스타트업에 적합한 아이디어를 토대로 합니다. 섹션을 하나씩 따라가면서 다양한 프로젝트를 통해 새 기술을 익혀나가실 수 있습니다.\n\n\n이 과정을 수료하시면 파이썬으로 OpenAI API 를 활용하는 방법을 익힐 수 있으며 여러분의 개인 프로젝트에도 AI 를 적용할 수 있게 됩니다. OpenAI API 를 사용하기 위해 인증하는 법, API 를 호출하는 법, 그리고 결과를 분석하는 방법을 배웁니다. 또한 텍스트 생성과 질의 응답 등 NLP 작업을 수행하는 방법 및 API 를 이용해 AI 기반 솔루션 을 만드는 방법도 배우게 됩니다.\n\n\nAI의 강점이 적용된 실질적인 프로젝트:\n자연어를 SQL 쿼리로 만들기\n자동 블로그 포스트 생성하기\n재료를 가지고 자동으로 요리법 만들기\n국제 신문을 번역하고 요약하기\n자동으로 파이썬 docstring 생성하기\n내 상황에 맞게 챗봇 튜닝하기\nReddit 글을 감정 분석하기\n기업 문서를 벡터 임베딩 처리하기\n\n\n과정 전체를 거치며 실제 사례와 실습을 통해 실전 경험을 쌓을 수 있으며 새롭게 배운 내용을 바로 적용할 수 있습니다. 또한 에러를 처리하거나 성능을 최적화하는 등 OpenAI API 를 효율적으로 처리한 사례들도 배웁니다.\n\n\n여러분이 소프트웨어 개발자이든 데이터 사이언티스트이든 혹은 단순히 AI 를 배우고 싶은 분이든, 이 과정은 여러분을 위한 과정입니다. 과정을 수료하시면 OpenAI API 를 활용하는 데 필요한 기반을 쌓아 스스로 AI 솔루션 을 개발할 수 있게 됩니다.\n\n\n자, 이제 스킬업 하시고 AI 세계에 빠져들 준비가 되셨다면 지금 수강신청하셔서\nOpenAI API 를 활용해 파이썬으로 AI 애플리케이션을 개발해보는 놀라운 경험을 해보세요!",
      "target_audience": [
        "OpenAI가 제공하는 최신 Python API의 가능성을 파악하고 싶은 파이썬 개발자"
      ]
    },
    {
      "title": "Data Science от Проблемы до Решения",
      "url": "https://www.udemy.com/course/data-science-ew/",
      "bio": "Учись работать на Python решая проекты в области Data Science. От анализа данных до построения рекомендательной системы",
      "objectives": [
        "Научишься обрабатывать данные на примере реальных проектов используя язык Python",
        "Узнаешь про различные библиотеки Python, используемые для обработки данных и построения моделей машинного обучения",
        "Сможешь визуализировать данные используя язык Python",
        "На практике применишь алгоритмы supervised learning: logistic regression, linear regression, decision tree, random forest, SVM",
        "Также научишься применять алгоритмы для сегментации такие как K-means clustering (unsupervised learning)",
        "Научишься использовать Time Series models (временные ряды)",
        "Применишь ensemble learning: AdaBoost, Gradient Boosting, XGBoost",
        "Попробуешь построить систему рекоммендаций и посмотреть что такое text mining",
        "Узнаешь про deep leaning используя Tensor flow и Keras: Convolutional Networks, Recurrent Neural Networks"
      ],
      "course_content": {
        "Введение": [
          "Что это за курс?",
          "Настройка среды для обучения",
          "Настройка среды для обучения - Установка Anaconda"
        ],
        "Проект 1: Проанализировать данные Федерального управления гражданской авиации с": [
          "Проблема",
          "Решение"
        ],
        "Проект 2: Проанализировать данные пожарной части Нью-Йорка": [
          "Проблема",
          "Решение"
        ],
        "Проект 3: Построить ML: прогноз продаж в зависимости от рекламного бюджета": [
          "Проблема",
          "Решение"
        ],
        "Проект 4: Посторить ML - прогноз заболевания диабетом на основе мед показателей": [
          "Проблема",
          "Решение"
        ],
        "Проект 5: Проанализировать данные по автомобилям и построить графики зависимости": [
          "Проблема",
          "Решение"
        ],
        "Проект 6: Посторить ML модель, прогнозирующую объемы продаж в магазине.": [
          "Проблема",
          "Решение"
        ]
      },
      "requirements": [
        "Интерес к решению практических задач. Навыки программирования не обязательны. Основы выучите на примерах."
      ],
      "description": "Данный курс содержит разбор проблем/задач, которые встречаются в работе у Data Scientist-ов. Так как этот курс для начинающих, то мы подробно проходим некоторые детали, которые могут быть скучны для тех, кто уже работает в этой роли или в смежной специальности. Сложность задач идет по нарастающей.\nЦель курса это показать природу задач в области Data Science и чтобы учащийся смог быстро определить для себя, подходит ли эта дисциплина для дальнейшего своего развития или же это не то, что изначально думал учащийся про Data Science. Также этот курс поможет учащимся быстро влиться в эту область, решая различные проблемы.\nЯ постарался сделать курс более реалистичным/живым т.е. так, что если бы ко мне подсадили новенького и я бы объяснял ему/ей на рабочем месте, как и что делать. Это может выглядеть совсем как нестандартный курс, но я бы хотел поэкпериментировать и получить ваш фидбэк. Сам курс на русском языке, но он также содержит лексику на английском языке, чтобы вы знали хотя бы технический английский.\nЕсли такой формат понравится, то я добавлю еще модули, в которых мы дойдем до построения рекомендательной системы.\nНа данном курсе мы будем использовать Jupyter Notebook. Данные для анализа будут предоставлены в файлах, которые необходимо будет прочитать с помощью языка Python.",
      "target_audience": [
        "Данный курс предназначен для тех, кто хочет узнать, что такое Data Science на практике. Это курс для начинающих, кто никогда не программировал на Python для решения задач в области Data Science. Сложность задач идет с возрастанием. Первая половина курса не будет интересна тем, кто решал хоть какие-то задачи на Python в облати анализа данных и построения моделей."
      ]
    },
    {
      "title": "Do 0 ao Data Warehouse com Erathos, dbt e BigQuery",
      "url": "https://www.udemy.com/course/do-0-ao-data-warehouse-com-erathos-dbt-e-bigquery/",
      "bio": "Como construir um DW medalhão com a Erathos, BigQuery e dbt",
      "objectives": [
        "Como criar e orquestrar data pipelines em minutos com a Erathos",
        "O que é um data warehouse medalhão",
        "Desenvolvimento de models e documentação no dbt",
        "Usando BigQuery como ambiente analítico"
      ],
      "course_content": {
        "Introdução": [
          "Boas vindas!",
          "Desafios de ser data-driven",
          "Ferramentas utilizadas nesse curso"
        ],
        "Criação de data pipelines e o ambiente analítico": [
          "Criando sua conta na Erathos",
          "Conectando com sua fontes de dados",
          "Desenvolvimento e orquestração do pipeline de dados",
          "O que é e como conectar o ambiente analítico com a Erathos"
        ],
        "Arquitetura medalhão e modelos de dados": [
          "Introdução ao Módulo",
          "Introdução e configuração do dbt",
          "Arquitetura Medalhão",
          "Views vs Tabelas vs Views Materializadas",
          "Criando a camada Processed",
          "Criando a camada Trusted",
          "dbt como ferramenta de documentação"
        ],
        "Usando BigQuery e LookerStudio + conectando a Erathos a APIs": [
          "Conectando APIs com a Erathos + BigQuery com Looker Studio e Google Sheets"
        ],
        "Conclusão": [
          "Agradecimentos e resumo"
        ]
      },
      "requirements": [
        "Conhecimento básico de SQL vai facilitar a compreensão"
      ],
      "description": "Aprenda a gerir e executar a jornada completa do 0 ao data warehouse na estrutura medalhão, desde a extração de dados de suas data sources e ingestão no ambiente analítico utilizando a Erathos como plataforma de movimentação de dados. A criação de modelagem avançadas e documentação desses modelos no dbt. Tudo isso em conjunto com a solução BigQuery como data warehouse.\n\n\nEm resumo, como criar um data warehouse, pipelines e modelagem avançadas com a plataforma de movimentação de dados Erathos, BigQuery e o dbt.\n\n\nEste curso é 100% financiado pela Erathos.\n\n\nO que você aprenderá:\nOs Desafios de se tornar Data-driven:\nDescubra quais as principais barreiras para uma empresa se tornar data-driven e como superá-las.\n\nConceitos de Data Warehouse e Pipelines de Dados:\nEntenda a estrutura e o funcionamento dos data warehouses e como são feitos processos de ELT (extração, carregamento e transformação) de dados.\n\n\nModelagem de Dados com dbt\nAprenda a modelar dados de maneira eficaz com uma estrutura medalhão (Data warehouse Medallion) utilizando o dbt.\n\n\nBigQuery\nDomine o uso do BigQuery, desde suas conexões com bancos de dados até a configuração e execução de queries complexas em conjunto com a Erathos e dbt.\n\n\nVisualizações de Dados\nConecte seus dados ao Looker Studio e crie visualizações impactantes que ajudam na tomada de decisões.\n\n\n\n\nEste curso inclui\n1h45m de vídeo sob demanda\nAcesso no disponível móvel\n1 mês de acesso a Erathos\nCertificado de conclusão",
      "target_audience": [
        "Engenheiros de dados",
        "Analistas de dados",
        "Business Intelligence Developer",
        "Estudantes de Engenharia e TI",
        "Entusiastas na área de dados"
      ]
    },
    {
      "title": "Curso python para ciencia de datos",
      "url": "https://www.udemy.com/course/curso-python-para-ciencia-de-datos/",
      "bio": "Programar en Python para ciencia de datos",
      "objectives": [
        "Como descargar e instalar Python",
        "Tipos de datos y operaciones en python. Operaciones aritméticas, lógicas y con texto. Tipos de objetos en python. Listas, Diccionarios, Data Frames.",
        "Bucles de programación en Python. For, If, while. Funciones y construcción de funciones. Carga de librerías de terceros.",
        "Importación de datos de excel y csv. Uso de la librería Pandas. Filtrado, construcción de tablas de contingencia y unión de tablas.",
        "Uso de gráficas con pyplot. Histograma, gráfico de líneas, gráfico de puntos. Análisis de distribuciones.",
        "Machine learning. Problemas de regresión y clasificación. Uso de train test split. Entrenamiento, evaluación y ajuste de modelos. Predicción."
      ],
      "course_content": {
        "Introducción": [
          "Descargar e instalar python y pycharm",
          "Operaciones aritméticas en Python",
          "Tipos de datos en Python",
          "Operaciones lógicas",
          "Operaciones lógicas 2"
        ],
        "Tipos de objetos en Python": [
          "Listas",
          "Methods de las listas",
          "Diccionarios",
          "Anidación de objetos",
          "Anidación de objetos continuación"
        ],
        "Loops en Python": [
          "Foor loop",
          "Foor en diccionarios",
          "Foor anidado",
          "If",
          "Combinando if y for",
          "While loop"
        ],
        "Librerías y funciones en Python": [
          "Funciones",
          "Librerías",
          "Librerías continuación",
          "Construcción de funciones"
        ],
        "Las Data Frames en Python": [
          "La librería pandas",
          "Las Data Frames",
          "Importar archivos csv",
          "Importar archivos de excel"
        ],
        "Preparación de datos": [
          "Estadísticos descriptivos",
          "Selección de filas",
          "Índices",
          "Selección de filas usando iloc",
          "Comparaciones",
          "Copy",
          "Valores perdidos",
          "Valores perdidos continuación",
          "Agrupamiento y tablas de contingencia",
          "Exportación de tablas"
        ],
        "Unión de tablas": [
          "Left join",
          "Right Join",
          "Inner Join",
          "Outer Join"
        ],
        "Graficando con Python": [
          "Gráfica con plot",
          "Histogramas",
          "Distribuciones de probabilidad",
          "Gráfico de líneas",
          "Gráfico de puntos",
          "Gráfico de regresión lineal"
        ],
        "Clasificación con Machine Learning": [
          "Machine Learning",
          "Clasificación por edad",
          "Selección de variables",
          "Bases de entrenamiento y de prueba",
          "Entrenamiento del modelo",
          "Evaluación del modelo",
          "Árboles de decisión",
          "52. Complejidad del modelo",
          "Agregando más variables",
          "Agregando más variables continuación"
        ],
        "Machine learning en problemas de regresión": [
          "Base de precios",
          "Selección de variables para regresión",
          "Transformando variables",
          "Creación de tabla para el modelo",
          "Selección de variable objetivo",
          "Entrenamiento",
          "Predicción"
        ]
      },
      "requirements": [
        "No se requieren conocimientos previos de programación ni de estadística."
      ],
      "description": "En este curso aprenderás los principales fundamentos de python para emplearlo en el análisis y ciencia de datos.\nNo necesitas conocimientos previos. Comenzaremos desde qué es python, como instalarlo y cómo trabajar con un entorno de desarrollo (IDE). Abarcaremos temas intermedios como los fundamentos de machine learnig tanto para clasificación como para regresión. Lo cuál te permitirá conseguir tu primer empleo como analista de datos empleando python.\nAprenderás a declarar variables en python, que son uno de los principales objetos y que te permitirán realizar múltiples operaciones con datos. Aprenderás sobre los ciclos como for y while, que te permitirán automatizar todo tipo de tareas. El curso también explica los tipos de datos, las tablas o data frames y las operaciones aritméticas como la potencia, y el manejo de operadores.\nTambién aprenderás a manipular y arreglar datos, a crear tablas de contingencia para el análisis. A unir tablas, lo cual es una habilidad fundamental cuando tienes datos que provienen de distintas fuentes.\nEl curso también incluye cómo crear gráficas de diferentes tipos como los gráficos de barras, histogramas para analizar distribuciones, loas gráficos de línea y los diagramas de dispersión o gráficos de puntos.\nEl curso también te enseñará a realizar modelos predictivos para tomar desiciones con base en los datos empleando técnicas de machine learning.",
      "target_audience": [
        "Curso para personas interesadas en aprender el lenguaje python para ciencia de datos partiendo desde cero."
      ]
    },
    {
      "title": "SQL과 Tableau로 만들어보는 애자일한 AB 테스트 분석 시스템",
      "url": "https://www.udemy.com/course/sql-tableau-abtest/",
      "bio": "가상 A/B 테스트 데이터를 DuckDB 기반 SQL로 처리하여 Tableau상에서 분석 대시보드를 만들어봅니다. 이 과정에서 관련 통계도 다루고 A/B 테스트가 무엇인지 소개하고 왜 애자일해야하는지 설명합니다",
      "objectives": [
        "AB 테스트가 무엇이며 무엇이 아닌지 이해한다",
        "AB 테스트 구현 방식이 무엇인지 하이레벨로 이해한다",
        "AB 테스트 분석과 관련 통계에 대해 배운다",
        "AB 테스트 분석을 데이터를 바탕으로 해본다",
        "태블로 사용법을 배운다",
        "태블로를 통해 AB 테스트 분석을 시각화해본다"
      ],
      "course_content": {
        "강의 소개": [
          "강사 소개",
          "강의 소개"
        ],
        "A/B 테스트 소개": [
          "A/B 테스트 소개",
          "데이터팀의 미션과 발전 단계: A/B 테스트는 데이터 기반 의사 결정의 한가지",
          "왜 A/B 테스트는 애자일해야 하는가?",
          "전체적인 A/B 테스트 프로세스",
          "A/B 테스트 분석 소개",
          "A/B 테스트 관련 어려움들"
        ],
        "A/B 테스트 시스템 구성 이해": [
          "A/B 테스트 시스템 구성과 전체 과정",
          "유데미 추천엔진 AB 테스트 과정",
          "사용자를 A/B로 나누는 방법 이해하기",
          "사용자를 A/B로 나누는 방법 실습하기",
          "A/B 테스트 결과 분석의 어려움"
        ],
        "A/B 테스트 관련 통계 이해": [
          "AB 테스트 관련 기본 통계 리뷰",
          "중심극한정리 소개와 실습",
          "A/B 테스트 트래픽 크기 비교",
          "A/B 테스트 트래픽 크기 비교 실습"
        ],
        "손과 코딩으로 해보는 가상 데이터 기반 A/B 테스트 분석": [
          "A/B 테스트 분석용 가상 데이터 소개",
          "Two-Sample t-test 리뷰와 실습",
          "Impression/Click/Purchase/Amount 비교",
          "A/B 테스트 분석은 대시보드에서 어떻게 구현되나?"
        ],
        "Tableau 기반의 A/B 테스트 분석 시각화": [
          "다양한 시각화 툴 소개",
          "좋은 지표란?",
          "Tableau 용어 설명",
          "OLAP 큐브 생성 실습",
          "Tableau Public 다운로드와 시작",
          "대시보드 구현 방식 큰 그림과 Z-Score 계산방법 리뷰",
          "Traffic Trend Chart 모양 만들기",
          "Traffic Trend Chart z-score 계산해보기",
          "Impression Chart와 Filter 추가하기",
          "Click/Purchase/Revenue Charts 추가하고 최종 마무리하기"
        ],
        "보너스 강의": [
          "강의 질문 리뷰"
        ],
        "최종 요약": [
          "마무리와 다음 스텝"
        ]
      },
      "requirements": [
        "SQL 기본적인 문법에 대한 이해",
        "태블로 퍼블릭을 다운로드 받아 사용할 수 있는 윈도우나 맥 환경",
        "통계 중에서도 귀무 가설에 대한 기본적인 이해 혹은 이해가 없어도 배우겠다는 의지",
        "파이썬의 기본적인 문법에 대한 이해. 사용자를 A와 B로 나누는 로직과 중심극한 정리를 이해할 때 쓰는 정도로 아주 중요하지 않습니다."
      ],
      "description": "이 강의에서는 가상 A/B 테스트 데이터를 DuckDB라는 데이터베이스 위에서 SQL로 처리하여 Tableau Public상에서 A/B 테스트 분석을 위한 대시보드로 만들어봅니다. 유데미, 야후 등의 실리콘밸리 회사에서 A/B 테스트에 관여했던 경험을 바탕으로 만든 강의라 실용적인 팁과 모범 사례들도 배울 수 있는 유용한 강의가 되리라 믿습니다.\n\n\n이번 강의는 다음과 같은 섹션으로 구성되어 있습니다.\n\n\n강의 소개\nA/B 테스트 소개\nA/B 테스트 시스템 구성 이해\nA/B 테스트 관련 통계 이해\n손과 코딩으로 해보는 가상 데이터 기반 A/B 테스트 분석\nTableau 기반의 A/B 테스트 분석 시각화\n최종 요약과 다음 스텝\n모든 실습에는 Google Colab과 Tableau Public을 사용합니다.",
      "target_audience": [
        "AB 테스트가 무엇인지 이해하고 싶은 사람",
        "업무적으로 AB 테스트에 간접적으로 관여해야 하는 사람",
        "AB 테스트를 직접적으로 설계하고 분석해야하는 사람"
      ]
    },
    {
      "title": "Kaggle - Veri Bilimi Alanında Daha İyi Bir Profil Oluşturun",
      "url": "https://www.udemy.com/course/kaggle-veri-bilimi-alannda-daha-iyi-bir-profil-olusturun/",
      "bio": "Kaggle Makine Öğrenimi ve Veri Bilimi topluluğudur. Kaggle ile Veri Bilimi, Makine Öğrenimi ve Python'da CV'ni Güçlendir",
      "objectives": [
        "İster makine öğrenimi, ister finans alanında çalışıyor olun, ister web geliştirme veya veri bilimi alanında kariyer yapıyor olun.",
        "Python öğrenebileceğiniz en önemli becerilerden biridir. Python'un basit sözdizimi özellikle masaüstü, web ve iş uygulamaları için uygundur.",
        "Udemy'deki OAK Academy Python eğitmenleri, yazılım geliştirmeden veri analizine kadar uzmandırlar ve öğrencilere yönelik etkili eğitimleriyle bilinirler.",
        "Kaggle Nedir?",
        "Kaggle’a Kaydolma ve Üye Giriş Prosedürleri",
        "Kaggle Ana sayfasını Tanıma",
        "Kaggle da Yarışmalar",
        "Kaggle’da Veri Setleri",
        "Kaggle’da Kod Bölümünü İnceleme",
        "Kaggle’da Tartışma Nedir?",
        "Kaggle’da Kurslar",
        "Kaggle’da Kullanıcılar Arasındaki Sıralama",
        "Blog ve Dökümantasyon Bölümleri",
        "Kaggle’da Kullanıcı Sayfası İnceleme",
        "Kaggle İçerisindeki Hazine",
        "Kaggle Üzerinde Notebook Yayınlama",
        "Kaggle'da Başarıya Ulaşmak İçin Neler Yapılmalıdır?"
      ],
      "course_content": {
        "Kaggle ile İlk Temas": [
          "Kaggle Nedir?",
          "Kaggle hakkında sık sorulan sorulan",
          "Kaggle’a Kaydolma ve Üye Giriş Prosedürleri",
          "Kaggle Ana sayfasını Tanıma",
          "Quiz"
        ],
        "Kaggle’da Yarışmalar": [
          "Kaggle’da Yarışmalar: Ders 1",
          "Kaggle’da Yarışmalar: Ders 2",
          "Quiz"
        ],
        "Kaggle’da Veri Setleri": [
          "Kaggle’da Veri Setleri",
          "Quiz"
        ],
        "Kaggle’da Kod Bölümü": [
          "Kaggle’da Kod Bölümünü İnceleme: Ders 1",
          "Kaggle’da Kod Bölümünü İnceleme: Ders 2",
          "Kaggle’da Kod Bölümünü İnceleme: Ders 3",
          "Quiz"
        ],
        "Kaggle’da Tartışma Bölümü": [
          "Kaggle’da Tartışma Nedir?",
          "Quiz"
        ],
        "Kaggle’da En Çok Kullanılan Diğer Seçenekler": [
          "Kaggle’da Kurslar",
          "Kaggle’da Kullanıcılar Arasındaki Sıralama",
          "Blog Ve Dökümantasyon Bölümleri",
          "Quiz"
        ],
        "Kaggle’da Ayrıntıya İnme": [
          "Kaggle’da Kullanıcı Sayfası İnceleme",
          "Kaggle İçerisindeki Hazine",
          "Kaggle Üzerinde Notebook Yayınlama",
          "Kaggle'da Başarıya Ulaşmak İçin Neler Yapılmalıdı",
          "Quiz"
        ],
        "Extra": [
          "Kaggle - Veri Bilimi Alanında Daha İyi Bir Profil Oluşturun"
        ]
      },
      "requirements": [
        "Kaggle ile Veri Bilimi, Makine Öğrenimi, Python Portföyünü geliştirme arzusu",
        "Kaggle hakkında bilgi edinme arzusu",
        "Ders videolarını eksiksiz ve sırayla izlenmesi",
        "İnternet bağlantısı.",
        "Dersi izleyebileceğiniz cep telefonu, bilgisayar veya tablet gibi herhangi bir cihaz."
      ],
      "description": "Veri bilimi, makine öğrenimi, Python, İstatistik, R, Makine Öğrenmesi, Derin Öğrenme, Django\n\n\nMerhaba,\n“Kaggle - Veri Bilimi Alanında Daha İyi Bir Profil Oluşturun” kursumuza hoş geldiniz.\nKaggle Makine Öğrenimi ve Veri Bilimi topluluğudur. Kaggle ile Veri Bilimi, Makine Öğrenimi ve Python'da CV'ni Güçlendir\n\n\nGoogle’ın bir yan kuruluşu olan Kaggle, çevrimiçi veri bilimciler ve makine öğrenimi uygulayıcıları topluluğudur.\nKaggle, kullanıcıların veri kümelerini bulmasına ve yayınlamasına, web tabanlı bir veri bilimi ortamında modelleri keşfetmesine ve oluşturmasına, diğer veri bilimciler ve makine öğrenimi mühendisleriyle çalışmasına ve veri bilimi zorluklarını çözmek için yarışmalara katılmasına olanak tanır.\nMakine öğrenimi, yalnızca tahmine dayalı mesajlaşma veya akıllı telefon ses tanıma için kullanışlı değildir. Makine öğrenimi sürekli olarak yeni endüstrilere ve yeni sorunlara uygulanmaktadır. İster pazarlamacı ister video oyunu tasarımcısı veya programcı olun, Oak Academy'de makine öğreniminde işinize ve uygulamanıza yardımcı olacak kurslar mevcuttur. Makine öğrenimi olmadan hayatımızı hayal etmek zordur. Tahmini mesajlaşma, e-posta filtreleme ve Amazon'un Alexa'sı ve iPhone'un Siri'si gibi sanal kişisel asistanların tümü, makine öğrenimi algoritmalarına ve matematiksel modellere dayalı olarak çalışan teknolojilerdir.\nVeri bilimi uygulaması, finans, ulaşım, eğitim, üretim, insan kaynakları ve bankacılık dahil olmak üzere dünya çapında birçok sektörde talep gören bir beceridir. Bilginizi artırmak için Python, istatistik, makine öğrenimi ve daha fazlasını içeren veri bilimi kurslarını keşfedin. Araştırma, istatistik ve analitik ile ilgileniyorsanız veri bilimi eğitimi almalısınız.\nİster makine öğrenimi, ister finans alanında çalışıyor olun, ister web geliştirme veya veri bilimi alanında kariyer yapıyor olun, Python öğrenebileceğiniz en önemli becerilerden biridir. Python'un basit sözdizimi özellikle masaüstü, web ve iş uygulamaları için uygundur.\nKaggle, kurulum gerektirmeyen, özelleştirilebilir bir Jupyter Notebooks ortamı sunar.\nKaggle ile ücretsiz GPU' lara ve topluluk tarafından yayınlanan büyük bir veri ve kod deposuna erişebilirsiniz.\nKaggle, veri bilimcilerinin makine öğrenimi zorluklarında rekabet edebilecekleri bir platformdur. Bu zorluklar, konut fiyatlarını tahmin etmekten kanser hücrelerini tespit etmeye kadar her şey olabilir.\nKaggle, veri bilimi sorunları konusunda başkalarına her zaman yardım etmeye istekli olan büyük bir veri bilimcisi topluluğuna sahiptir.\nKaggle, yarışmalara ek olarak, makine öğrenimine başlamanıza yardımcı olabilecek birçok öğreticiye ve kaynağa da sahiptir.\nKendini geliştirmeye çalışan bir veri bilimcisi iseniz, Kaggle başlamak için en iyi yoldur. Birçok şirket, yarışmalarında yüksek derecelere sahip olanlara teklifler vermektedir. Aslında, yüksek sıralamalarından birine ulaşabilirseniz, Kaggle tam zamanlı işiniz olabilir.\nKaggle'ın içinde, veri bilimi çalışmanızı yapmak için ihtiyacınız olan tüm kod ve verileri bulacaksınız. Herhangi bir analizi anında kullanmak için 50.000'den fazla genel veri kümesi ve 400.000 genel not defterini bu platformda bulabilirsiniz.\nEvet şimdi gelelim kursumuza..\nHiçbir platformda Kaggle ile ilgili bu kadar detaylı bir kurs olmadığını biliyor muydunuz?\nVe veri bilimi ihtiyaçlarının 2026 yılına kadar 11,5 milyon iş fırsatı yaratacağını biliyor musunuz?\nVeri bilimi kariyerleri için ortalama maaşın 100.000 dolar olduğunu biliyor musunuz?\n\n\nVERİ BİLİMİ KARİYERLERİ GELECEĞİ ŞEKİLLENDİRİYOR\nBU KARİYER KAGGLE PLATFORMU İLE CANLANDIRIN\n\n\nPeki, Veri Bilimi neden bu kadar önemli bir alan? Gelin birlikte inceleyelim.\nDevlet güvenliğinden flört uygulamalarına kadar hemen hemen her alanda veri bilimi uzmanlarına ihtiyaç vardır. Milyonlarca işletme ve devlet dairesi, başarılı olmak ve müşterilerine daha iyi hizmet vermek için büyük verilere güveniyor. Bu nedenle, veri bilimi kariyerleri yüksek talep görmektedir.\nİşverenin en çok talep ettiği becerilerden birini öğrenmek istiyorsanız?\nVeri Bilimini merak ediyor ve Python ile veri dünyasına kendi kendine öğrenme yolculuğunuza başlamak istiyorsanız?\nDeneyimli bir geliştiriciyseniz ve Veri Biliminde bir iş arıyorsanız!\nHer durumda, doğru yerdesiniz!\n\n\nVeri biliminde CV'nizi geliştirmek için süper bir kurs olan “Kaggle - Veri Bilimi Alanında Daha İyi Bir Profil Oluşturun” kursumuzu sizin için tasarladık.\nKursta, her bölümü ayrıntılı olarak inceleyeceksiniz. Bu kurs ile Kaggle platformunu adım adım tanıyacaksınız.\n\n\nKaggle hakkında Sık Sorulan Sorular:\n\n\nKaggle nedir?\nGoogle LLC'nin bir yan kuruluşu olan Kaggle, çevrimiçi bir veri bilimcileri ve makine öğrenimi uygulayıcıları topluluğudur.\nKaggle, kurulum gerektirmeyen, özelleştirilebilir bir Jupyter Notebooks ortamı sunar. Ücretsiz GPU'lara ve topluluk tarafından yayınlanan büyük bir veri ve kod deposuna erişin.\nKaggle, veri bilimcilerinin makine öğrenimi zorluklarında rekabet edebilecekleri bir platformdur. Bu zorluklar, konut fiyatlarını tahmin etmekten kanser hücrelerini tespit etmeye kadar her şey olabilir. Kaggle, veri bilimi sorunları konusunda başkalarına her zaman yardım etmeye istekli olan büyük bir veri bilimcisi topluluğuna sahiptir. Kaggle, yarışmalara ek olarak, makine öğrenimine başlamanıza yardımcı olabilecek birçok öğretici ve kaynağa da sahiptir.\nKalkınan bir veri bilimcisi iseniz, Kaggle başlamak için en iyi yoldur. Birçok şirket, yarışmalarında yüksek derecelere sahip olanlara teklifler verecektir. Aslında, yüksek sıralamalarından birine ulaşabilirseniz, Kaggle tam zamanlı işiniz olabilir.\nMakine Öğrenmesi nedir?\nMakine öğrenmesi, gerçek dünya verileri üzerinde eğitilmiş bir model kullanarak tahminlerde bulunan sistemleri tanımlar. Örneğin, bir kedinin resimde olup olmadığını tespit edebilen bir sistem kurmak istediğimizi varsayalım. Makine öğrenimi modelimizi eğitmek için önce birçok resmi bir araya getiriyoruz. Bu eğitim aşamasında, bir kedi içerip içermediğine dair bilgilerle birlikte resimleri modele besliyoruz.\nBöylece Eğitim sırasında model, kedilerle en yakından ilişkili olan görüntülerdeki kalıpları öğrenir. Bu model daha sonra, beslediği yeni görüntülerin bir kedi içerip içermediğini tahmin etmek için eğitim sırasında öğrenilen kalıpları kullanabilir.\nBu özel örnekte, bu kalıpları öğrenmek için bir sinir ağı kullanabiliriz, ancak makine öğrenimi bundan çok daha basit olabilir. Bir dizi gözlemlenen veri noktasına bir çizgi uydurmak ve bu çizgiyi yeni tahminler yapmak için kullanmak bile bir makine öğrenimi modeli olarak sayılır.\nVeri bilimi nedir?\nHer zamankinden daha fazla veriye sahibiz. Ancak veriler tek başına çevremizdeki dünya hakkında bize pek bir şey söyleyemez. Bilgileri yorumlamamız ve gizli kalıpları keşfetmemiz gerekiyor.\nİşte burada veri bilimi devreye girer. Veri bilimi, ham verileri anlamak için algoritmalar kullanır. Veri bilimi ile geleneksel veri analizi arasındaki temel fark, tahmine odaklanmasıdır. Veri bilimi, verilerdeki kalıpları bulmaya ve bu kalıpları gelecekteki verileri tahmin etmek için kullanmaya çalışır. Büyük miktarda veriyi işlemek, kalıpları keşfetmek ve eğilimleri tahmin etmek için makine öğreniminden yararlanır.\nVeri bilimi, verilerin hazırlanmasını, analiz edilmesini ve işlenmesini içerir. Birçok bilimsel alandan yararlanır ve bir bilim olarak verileri analiz etmek ve mevcut yöntemleri doğrulamak için yeni algoritmalar oluşturarak ilerler.\nKaggle ne için kullanılır?\nKaggle, fikirleri paylaşmak, ilham almak, diğer veri bilimcileriyle rekabet etmek, yeni bilgiler ve kodlama hileleri öğrenmek ve ayrıca gerçek dünyadaki veri bilimi uygulamalarının çeşitli örneklerini görmek için kullanılan bir web sitesidir.\nKaggle'ı kullanmak ücretsiz mi?\nKaggle Hizmetleri ücretsiz olarak sunulabilir ancak bazı Hizmetleri kullanmak için parasal bir ücret talep edilebilir.\nKaggle için tipik kullanım durumları nelerdir?\nKaggle, analiz edilmesi gerektiğini düşündükleri verilere sahip işletmeler için en iyisidir. Kaggle'ın en önemli yararı, bu şirketlerin verileriyle nasıl çalışacağını bilen birini kolayca bulabilmeleridir; bu, sorunu çözmeyi, sistemlerinde neyin yanlış olduğunu bulmaya çalışmaktan çok daha kolay hale getirir.\nKaggle'daki popüler yarışmalar nelerdir?\nKaggle'da birçok farklı yarışma türü mevcuttur. Mikroskop görüntülerinde kanser hücrelerini tahmin etmekten, herhangi bir gündeki fazla mesai değişiklikleri için uydu görüntülerini analiz etmeye kadar her konuda bir yarışmaya katılabilirsiniz.\nÖrnekler şunları içerir:\nBeygir gücü ve kat edilen mesafe gibi özelliklere göre araba fiyatlarını tahmin etme\nEyaletlere göre oylama modellerini tahmin etme\nHangi ülkelerde en fazla ormansızlaşmaya sahip olduğunu görmek için uydu görüntülerini analiz etme\nKaggle yeni başlayanlar için iyi bir platform mu?\nKaggle ve tipik veri bilimi arasındaki farklılıklara rağmen, Kaggle yeni başlayanlar için harika bir öğrenme aracı olabilir. Her yarışma bağımsızdır. Veri toplamanıza gerek yoktur, bu da sizi diğer becerilere odaklanmak için serbest bırakır.\nKaggle nasıl çalışır?\nKaggle'daki her yarışmanın kendisiyle ilişkili bir veri seti ve ulaşmanız gereken bir hedefi vardır (örneğin, konut fiyatlarını tahmin edin veya kanser hücrelerini tespit edin).\nVerilere mümkün olduğunca sık erişebilir ve tahmin modelinizi oluşturabilirsiniz. Yine de çözümünüzü bir kez gönderdikten sonra, onu gelecekteki gönderimler için kullanamazsınız.\nBu, herkesin birbiriyle rekabet ederken aynı noktadan başlamasını sağlar, bu nedenle, sorunu çözmeye çalışan diğerlerinden daha fazla hesaplama gücüne sahip olanlara verilen hiçbir avantaj yoktur.\nYarışmalar, karmaşıklık düzeyine, ne kadar sürdüğüne, para ödülü olup olmamasına vb. bağlı olarak farklı kategorilere ayrılır, böylece farklı deneyim seviyelerine sahip kullanıcılar aynı arenada birbirleriyle rekabet edebilir.\nKaggle'da bir yarışmaya nasıl girilir?\nBir yarışmaya girmek için kayıt süreci çok basittir: Çoğu yarışma, yarışmacılardan her mücadelenin sonunda belirli kriterleri karşılayan kodu göndermelerini ister. Ancak, rakiplerin hangi algoritmaları kullandıklarını açıklamalarını veya işlerin nasıl yürüdüğü hakkında girdi sağlamalarını istedikleri zamanlar olabilir.\nKaggle yarışmaları nasıl para kazandırır?\nKaggle'daki birçok şirket çözüm arıyor, bu nedenle bazı yarışmalarda ödül vardır. Çözümünüz yeterince güçlüyse, çok para kazanabilirsiniz!\nBu yarışmalardan bazıları sadece eğlence veya öğrenme amaçlıdır, ancak yine de kazananları nakit veya ticari ödüllerle ödüllendirir.\nKaggle'da rekabet etmek için hangi araçları kullanmalıyım?\nRakiplerin güvendiği en önemli araç Python programlama dilidir. Tüm veri bilimcilerinin %60'ından fazlası tarafından kullanılmaktadır, bu nedenle arkasında son derece büyük bir topluluğa sahiptir. Ayrıca son derece sağlamdır ve başlamanıza yardımcı olacak veri işleme, ön işleme ve keşif için birçok farklı pakete sahiptir.\nTensorFlow, makine öğrenimi meraklılarının Kaggle yarışmalarını çözmek için kullandığı bir başka popüler araçtır. Mümkün olan en iyi sonuçları elde etmek için modellerin hızlı prototiplenmesine olanak tanır.\nPython ve Tensorflow'a ek olarak R (istatistiksel bir programlama dili), Git (sürüm kontrolü) ve Bash (komut satırı arayüzü) gibi başka araçlar da kullanılır.\nSorunları çözmek için Kaggle kullanmanın temel faydası nedir?\nKaggle, size birinci sınıf bir veri bilimcisi olmanız için gerekli araçları sağlamayı amaçlamaktadır. Size gerçek zamanlı olarak gerçek verilere erişim sağlarlar, böylece dünya çapında şirketlerin karşılaştığı sorunlara benzer sorunları çözme alıştırması yapabilirsiniz.\nKaggle, en güncel bilgilere sahip olmanız için sitelerini sürekli güncellemektedir.\nYeni başlayanlar Kaggle’dan nasıl faydalanabilir?\nKaggle, yeni başlayanlara makine öğrenimi hakkında daha fazla bilgi edinmenin bir yolunu sunar ve nerede olurlarsa olsunlar becerilerini kullanmalarına olanak tanır.\nKaggle'ı kullanmak, yeni başlayanların sektörde neler olup bittiğini görmelerine, trendleri takip etmelerine ve işler değiştikçe araçları konusunda uzman olmalarına olanak tanır.\nAyrıca, yeni başlayanlar veya belirli kavramlar hakkında tazeleme kursu isteyenler veya başlamak için yardıma ihtiyaç duyanlar için ücretsiz eğitim materyali sunar.\nKaggle'ı kullanmakla kim ilgilenir?\nHazır birçok öğretici ve veri seti ile Makine Öğrenimi meraklıları Kaggle ile çok ilgilenecektir.\nKaggle, Makine öğrenimi hakkında daha fazla bilgi edinmek, öğrendiklerini uygulamak ve diğer veri bilimcilerle rekabet etmek için mükemmel bir yerdir. Bu onların zanaatlarında daha iyi olmalarına yardımcı olacaktır.\nÇalışmalarında makine öğrenimini kullanmak isteyen veri analistleri, satış rakamlarını tahmin etme veya müşteri davranışlarını tahmin etme gibi işle ilgili görevlerin performansını iyileştirmek için araçlar seçerken Kaggle'a başvurabilir.\nEk olarak, üçüncü taraf çözümler arayan işletmeler, Kaggle'ın ihtiyaç duydukları hizmeti sunan kapsamlı şirketler listesinden yararlanabilir.\nKaggle hala popüler mi?\nKaggle, Muhteşem makine öğrenimi modelleri oluşturmak için diğer veri bilimcileriyle etkileşim kurmak, bağlantı kurmak ve işbirliği yapmak için harika bir ekosistem.\nYıllar içinde Kaggle, eğlenceli beyin egzersizlerinden para ödülleri veren ve katılımcıları sıralayan ticari yarışmalara kadar uzanan yarışmalar düzenleyerek popülerlik kazanan bir platform oldu ve günümüzde de bu popülerliğini hala sürdürmektedir.\n\n\nBu kurs herkes içindir!\n\"Kaggle - Veri Bilimi Alanında Daha İyi Bir Profil Oluşturun\" kursu herkes içindir!\nDaha önce deneyiminiz yoksa sorun değil! Bu kurs, yeni başlayanlardan profesyonellere kadar (tazeleme amaçlı) herkese öğretmek için ustalıkla tasarlanmıştır.\n\n\nBu kurs ile Ne öğreneceksin?\nBu kursta en baştan başlayıp örneklerle \"Kaggle\" ın sonuna kadar gideceğiz.\nKurs sırasında aşağıdaki konuları göreceksiniz:\n\n\nKaggle Nedir?\nKaggle’a Kaydolma ve Üye Giriş Prosedürleri\nKaggle Ana sayfasını Tanıma\nKaggle’da Yarışmalar\nKaggle’da Veri Setleri\nKaggle’da Kod Bölümünü İnceleme\nKaggle’da Tartışma Nedir?\nKaggle’da Kurslar\nKaggle’da Kullanıcılar Arasındaki Sıralama\nBlog Ve Dökümantasyon Bölümleri\nKaggle’da Kullanıcı Sayfası İnceleme\nKaggle İçerisindeki Hazine\nKaggle Üzerinde Notebook Yayınlama\nKaggle'da Başarıya Ulaşmak İçin Neler Yapılmalıdır?\nGüncel kursum ile kendinizi güncel tutma şansınız olacak. Ayrıca, öğrenmenizi desteklemek ve sorularınızı yanıtlamak için sürekli olarak hazır olacağımı söylemekten mutluluk duyuyorum.\n\n\nNeden bu kursu almalısınız?\nCevabımız basit: Öğretimin kalitesi.\nKaydolduğunuzda, OAK Academy'nin deneyimli geliştiricilerin uzmanlığını hissedeceksiniz.\n\n\nVideo ve Ses Üretim Kalitesi\nTüm videolarımız, size en iyi öğrenme deneyimini sağlamak için yüksek kaliteli video ve ses olarak oluşturulur/üretilir.\nBu kurs ile aşağıdakileri elde edeceksiniz:\nKursa Ömür Boyu Erişim\nSoru-Cevap bölümünde Hızlı ve Kolay Destek\nİndirilmeye Hazır Udemy Bitirme Sertifikası\nHer türlü soruyu yanıtlayarak tam destek sunuyoruz.\nÖğrenmeye hazırsanız:\n\"Kaggle - Veri Bilimi Alanında Daha İyi Bir Profil Oluşturun\nKaggle Makine Öğrenimi ve Veri Bilimi topluluğudur. Kaggle ile Veri Bilimi, Makine Öğrenimi ve Python'da CV'ni Güçlendir \" isimli,\nKursumuzda görüşürüz!",
      "target_audience": [
        "Veri kümelerini bulmak ve yayınlamak, web tabanlı bir veri bilimi ortamında modelleri keşfetmek ve oluşturmak, diğer veri bilimciler ve makine öğrenimi mühendisleriyle çalışmak ve veri bilimi zorluklarını çözmek için yarışmalara katılmak isteyen herkes.",
        "Veri bilimi ve makine öğrenmesi alanında rekabet etmek isteyenler",
        "Kaggle öğrenmek isteyenler",
        "Kaggle ile Veri Bilimi, Makine Öğrenimi, Python konularında özgeçmişini geliştirmek isteyenler",
        "Yapay Zeka, Makine Öğrenimi, Derin Öğrenme, kısaca Veri Bilimi ile ilgilenen herkes"
      ]
    },
    {
      "title": "Machine learning : modèles génératifs (GANs) avec PyTorch",
      "url": "https://www.udemy.com/course/machine-learning-gans-pytorch/",
      "bio": "Apprenez Pytorch par la pratique en implémentant un réseau antagoniste génératif",
      "objectives": [
        "Les opportunités qu'offrent les modèles génératifs",
        "Le fonctionnement des GANs (intuitif)",
        "Le fonctionnement mathématique des GANs",
        "L'implémentation et l'optimisation de réseaux de neurones avec PyTorch",
        "L'implémentation d'un GAN en partant de zéros",
        "La génération d'images synthétiques"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Application des modèles génératifs"
        ],
        "GANs : explications": [
          "Explication intuitive",
          "Explication mathématique"
        ],
        "Implémentation": [
          "Google colab",
          "Modules & dataset",
          "Helpers",
          "Générateur",
          "Discriminateur",
          "Training loop",
          "Binary cross entropy loss",
          "Entraînement des réseaux de neurones",
          "Analyse des résultats"
        ]
      },
      "requirements": [
        "Savoir programmer avec Python",
        "Connaissances basiques du Machine Learning (réseaux de neurones & classificateurs)"
      ],
      "description": "Dans ce cours accéléré, nous allons aborder les opportunités qu'offrent les modèles génératifs et ensuite, nous nous intéresserons plus particulièrement aux Generative Adversarial Networks (GANs).\n\n\nJe vais vous expliquer le fonctionnement des GANs de manière intuitive et ensuite, nous nous plongerons dans l'article qui les a introduit en 2014 (Ian J. Goodfellow et al.). Je vous expliquerai donc de manière mathématique le fonctionnement des GANs, ce qui vous permettra d'avoir les bases nécessaires pour implémenter votre premier GAN en partant de zéro.\n\n\nNous implémenterons en approximativement 100 lignes de code un générateur, un discriminateur et le pseudo-code décrit dans l'article afin d'entraîner ces derniers. Nous utiliserons le langage de programmation Python et le framework PyTorch. Après entraînement, le générateur nous permettra de générer des images synthétiques.\n\n\nJ'ai la conviction qu'un concept s'apprend par la pratique et ce cours accéléré a pour objectif de vous donner les bases nécessaires afin de continuer votre apprentissage du Machine Learning, de PyTorch et des modèles génératifs (GANS, Variational Autoencoders, Normalizing Flows, ...).\n\n\nÀ l'issue de ce cours, le participant aura la possibilité d'utiliser Python (et plus particulièrement le framework PyTorch) afin d'implémenter des articles scientifiques et des solutions d'intelligence artificielle. Ce cours a également pour objectif d'être un tremplin dans votre apprentissage des modèles génératifs.\n\n\nAu-delà des GANs, ce cours est également une introduction générale au framework PyTorch et un cours de Machine learning de niveau intermédiaire .\n\n\nConcepts abordés:\nLe framework PyTorch afin d'implémenter et d'optimiser des réseaux de neurones.\nLe framework Keras afin de charger un ensemble de données.\nGoogle colab.\nL'utilisation des modèles génératifs dans le monde de la recherche et industriel.\nLes GANs de manière intuitive.\nLes GANs de manière mathématique.\nLa génération de données synthétiques.\nL'implémentation d'un article scientifique.\n\n\nN'attendez plus avant de vous lancer dans le monde des modèles génératifs!",
      "target_audience": [
        "Toute personne intéressée par le machine learning & l'intelligence artificielle.",
        "Toute personne intéressée par les modèles génératifs et par les GANs.",
        "Toute personne qui aimerait apprendre PyTorch par la pratique.",
        "Les professionnels qui veulent utiliser les opportunités qu'offrent les modèles génératifs."
      ]
    },
    {
      "title": "图神经网络实战",
      "url": "https://www.udemy.com/course/graphgcn/",
      "bio": "图神经网络",
      "objectives": [
        "图神经网络核心算法",
        "图神经网络建模工具",
        "图神经网络最新论文",
        "图神经网络应用项目"
      ],
      "course_content": {
        "图神经网络基础": [
          "1-图神经网络应用领域分析",
          "2-图基本模块定义",
          "3-邻接矩阵的定义",
          "4-GNN中常见任务",
          "5-消息传递计算方法",
          "6-多层GCN的作用",
          "课件与代码下载"
        ],
        "图卷积GCN模型": [
          "1-GCN基本模型概述",
          "2-图卷积的基本计算方法",
          "3-邻接的矩阵的变换",
          "4-GCN变换原理解读"
        ],
        "图模型必备神器PyTorch Geometric安装与使用": [
          "1-PyTorch Geometric工具包安装与配置方法",
          "2-数据集与邻接矩阵格式",
          "3-模型定义与训练方法",
          "4-文献引用数据集分类案例实战"
        ],
        "使用PyTorch Geometric构建自己的图数据集": [
          "1-构建数据集基本方法",
          "2-数据集与任务背景概述",
          "3-数据集基本预处理",
          "4-用户行为图结构创建",
          "5-数据集创建函数介绍",
          "6-网络结构定义模块",
          "7-TopkPooling进行下采样任务",
          "8-获取全局特征",
          "9-模型训练与总结"
        ],
        "图注意力机制与序列图模型": [
          "1-图注意力机制的作用与方法",
          "2-邻接矩阵计算图Attention",
          "3-序列图神经网络TGCN应用",
          "4-序列图神经网络细节"
        ],
        "图相似度论文解读": [
          "1-要完成的任务分析",
          "2-基本方法概述解读",
          "3-图模型提取全局与局部特征",
          "4-NTN模块的作用与效果",
          "5-点之间的对应关系计算",
          "6-结果输出与总结"
        ],
        "图相似度计算实战": [
          "1-数据集与任务概述",
          "2-图卷积特征提取模块",
          "3-分别计算不同Batch点的分布",
          "4-获得直方图特征结果",
          "5-图的全局特征构建",
          "6-NTN图相似特征提取‘",
          "7-预测得到相似度结果"
        ],
        "基于图模型的轨迹估计": [
          "1-数据集与标注信息解读",
          "2-整体三大模块分析",
          "3-特征工程的作用与效果",
          "4-传统方法与现在向量空间对比",
          "5-输入细节分析",
          "6-子图模块构建方法",
          "7-特征融合模块分析",
          "8-VectorNet输出层分析"
        ],
        "图模型轨迹估计实战": [
          "1-数据与环境配置",
          "2-训练数据准备",
          "3-Agent特征提取方法",
          "4-DataLoader构建图结构",
          "5-SubGraph与Attention模型流程"
        ],
        "图神经网络结合时间序列": [
          "1-业务应用场景介绍与分析",
          "2-图神经网络建模解决思路与方法",
          "3-传感器节点特征提取方法",
          "4-图模型结合时间序列",
          "5-ICU传感器数据集介绍"
        ]
      },
      "requirements": [
        "熟悉深度学习"
      ],
      "description": "图神经⽹络模块课程旨在帮助同学们快速掌握深度学习在图模型领域算法及其应⽤项⽬。内容主要包括三个模块：1.图神经⽹络经典算法解读，详细解读GNN,GCN,注意⼒机制图模型等算法 ；2 . 图神经⽹络框架PyTorch-Geometric，全程实战解读图神经⽹络框架应⽤⽅法；3 .图神经⽹络项⽬实战，基于真实数据集与实际项⽬展开图数据集构建与模型训练并应⽤到实际场景中。实战项目中主要包括图神经网络在各大场景中的应用，首先讲解应用算法，并切结合具体论文展开分析，源码讲解全部基于真实数据集与实际任务需求进行展开。适合AI方向准备搞研究生的同学们，也适合这个方向准备针对自己的数据进行建模与应用分析的。",
      "target_audience": [
        "AI方向准备搞研究课题和实际项目"
      ]
    },
    {
      "title": "Introduction à Nerf (Neural Radiance Fields)",
      "url": "https://www.udemy.com/course/nerf-neural-radiance-fields-fr/",
      "bio": "Introduction à NeRF et à la reconstruction 3D",
      "objectives": [
        "Introduction à la reconstruction",
        "Introduction à la reconstruction 3D",
        "Introduction à NeRF (Neural Radiance Fields)",
        "Génération de nouveaux points de vues avec NeRF",
        "Reconstruction 3D avec NeRF (extraction de mesh)",
        "Introduction à la génération de rendus 3D"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "Introduction à la reconstruction"
        ],
        "Reconstruction 3D": [
          "Caméra",
          "Camera: visualisation",
          "Génération de rendus",
          "Images de profondeur",
          "Génération de rendus différentiable & Optimisation",
          "Caméra: matrice de rotation"
        ],
        "Modules de reconstruction 3D": [
          "Camera et DataLoader - partie 1",
          "Camera et DataLoader - partie 2",
          "Génération de rendus différentiable - partie 1",
          "Génération de rendus différentiable - partie 2",
          "Génération de rendus différentiable - partie 3",
          "Génération de rendus différentiable - partie 4",
          "Modèle: Voxels - partie 1",
          "Modèle: Voxels - partie 2",
          "Algorithme d'optimisation",
          "Fond blanc: regularisation - partie 1",
          "Fond blanc: regularisation - partie 2"
        ],
        "Synthèse de nouvelles vues": [
          "Synthèse de nouvelles vues",
          "Génération d'une vidéo sur base d'une trajectoire de caméra - partie1",
          "Génération d'une vidéo sur base d'une trajectoire de caméra - partie2"
        ],
        "Reconstruction 3D: extraction de mesh": [
          "Reconstruction 3D: extraction de mesh - partie1",
          "Reconstruction 3D: extraction de mesh - partie2",
          "Reconstruction 3D: extraction de couleurs"
        ],
        "Nettoyage du code": [
          "Nettoyage du code"
        ],
        "Nerf (Neural Radiance Fields )": [
          "Nerf: Introduction",
          "Nerf: Implémentation",
          "Données synthétiques: problème de mode collapse et solution",
          "Analyse des résultats",
          "Coarse and fine networks",
          "Encoding positionnel: plus de détails"
        ],
        "Aller plus loin: articles scientifiques": [
          "Structure de cette Section",
          "Learned Initializations for Optimizing Coordinate-Based Neural Representations",
          "GRF: Learning a General Radiance Field for 3D Representation and Rendering",
          "pixelNeRF: Neural Radiance Fields from One or Few Images",
          "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
          "iNeRF: Inverting Neural Radiance Fields for Pose Estimation",
          "NeRF−−: Neural Radiance Fields Without Known Camera Parameters",
          "AutoInt: Automatic Integration for Fast Neural Volume Rendering",
          "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
          "Light Field Networks: Neural Scene Representations with Single-Evaluation Render",
          "PlenOctrees for Real-time Rendering of Neural Radiance Fields",
          "FastNeRF: High-Fidelity Neural Rendering at 200FPS",
          "SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference",
          "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
          "NeuS: Learning Neural Implicit Surfaces by Volume Rendering",
          "Plenoxels: Radiance Fields without Neural Networks",
          "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",
          "State of the Art on Neural Rendering",
          "Advances in Neural Rendering"
        ],
        "Outils et implémentations open source": [
          "Luma AI",
          "nerf_pl",
          "NerfStudio",
          "SDF Studio",
          "Tiny CUDA Neural Networks",
          "NerfAcc",
          "Instant Neural Graphics Primitives",
          "ngp_pl"
        ],
        "Conclusion": [
          "Conclusion"
        ]
      },
      "requirements": [
        "Connaissances basiques en programmation",
        "Connaissances basiques en machine learning"
      ],
      "description": "Bienvenue dans ce cours à propos de Nerf (neural radiance fields)!\n\n\nLes champs de radiance neuronaux (Nerf) sont une technologie innovante qui suscite un vif intérêt dans le monde de la vision par ordinateur. Ils permettent notamment de générer de nouveaux points de vue, ainsi que de reconstruire des objets 3D. Depuis leurs apparitions il y a deux ans, de nombreuses startups ont vu le jour, et comme le suggèrent les offres d'emplois, les grandes sociétés technologiques (Meta, Apple, Google, Amazon, ...) les utilisent.\n\n\nDans ce cours en ligne, vous allez découvrir :\nComment fonctionnent les modèles Nerf et comment ils peuvent être utilisés dans diverses applications\nComment entraîner et évaluer un modèle de Nerf\nComment intégrer Nerf dans vos projets de vision par ordinateur\nDes exemples de cas d'utilisation réels de Nerf dans l'industrie, y compris chez les grandes entreprises technologiques\n\n\nNotre cours est conçu pour les développeurs et les scientifiques qui veulent se familiariser avec Nerf et les utiliser dans leurs projets. Nous couvrons tous les aspects de la mise en place et de l'utilisation de Nerf, du début à la fin.\n\n\nInscrivez-vous maintenant pour accéder à notre cours en ligne complet sur les modèles de type Nerf et découvrez comment cette technologie peut améliorer vos projets de vision par ordinateur.\n\n\nNe manquez pas cette occasion de vous perfectionner et de découvrir les dernières avancées en matière de vision par ordinateur grâce aux Nerf!",
      "target_audience": [
        "Aux ingénieurs et programmeurs.",
        "Aux entrepreneurs.",
        "Aux étudiants et chercheurs."
      ]
    },
    {
      "title": "빅데이터분석기사 완전정복 필기편 : Part.2 빅데이터 탐색(1)",
      "url": "https://www.udemy.com/course/part2-1-c/",
      "bio": "빅데이터 탐색하기! 데이터 전처리부터 고급 데이터 탐색까지 함께 배워봅시다.",
      "objectives": [
        "빅데이터 탐색",
        "데이터 전처리",
        "데이터 정제",
        "분석 변수 처리",
        "데이터 탐색의 기초",
        "예상문제 풀이",
        "고급 데이터 탐색"
      ],
      "course_content": {
        "빅데이터 탐색 : 데이터 전처리(데이터 정제, 분석 변수 처리)": [
          "1 데이터 정제(데이터 전처리 개념, 특징, 과정, 데이터 정제 개념, 과정, 정제 방법, 일관성 유지 정제 방법)",
          "데이터 정제(데이터 결측값 종류 3가지, 결측값 처리 절차, 결측값 처리 방법)",
          "데이터 정제(데이터 이상값 발생원인 7가지, 이상값 검출 방법, 통계기법 이용, 시각화 이용)",
          "데이터 정제(데이터 이상값 처리 방법, 삭제, 대체, 변환)",
          "분석 변수 처리(변수의 개념, 유형 6가지, 변수 선택, 변수 선택 기법, 필터 기법, 래퍼 기법)",
          "분석 변수 처리(임베디드 기법 개념, 라쏘, 릿지, 엘라스틱넷, Select From Model 사례)",
          "분석 변수 처리(차원축소 개념, 차원축소 기법 6가지 개념, 특징)",
          "분석 변수 처리(파생변수 개념, 생성 방법 6가지, 인코딩 개념, 종류 4가지 설명)",
          "분석 변수 처리(변수 변환 개념, 방법 6가지 개념, 예시)",
          "분석 변수 처리(불균형 데이터 처리 개념, 처리 기법 5가지 개념 및 설명)",
          "데이터 전처리 챕터 예상문제 풀이"
        ],
        "빅데이터 탐색 : 데이터 탐색(데이터 탐색의 기초, 고급 데이터 탐색)": [
          "데이터 탐색의 기초(탐색적 데이터 분석 개념, 필요성, 분석과정 및 절차, 이상치 검출 방법 3가지)",
          "데이터 탐색의 기초(변수 간의 상관성 분석 개념, 종류, 상관분석 기본가정 4가지, 피어슨 상관계수 개념 및 데이터 산점도)",
          "데이터 탐색의 기초(피어슨, 스피어만 상관계수 차이점, 스피어만 상관계수 개념, 특징, 단조 관계 개념)",
          "데이터 탐색의 기초(중심화 경향 기초 통계량 개념, 산술, 기하, 조화 평균, 중앙값, 최빈값, 분위수 개념)",
          "데이터 탐색의 기초(산포도 개념, 분산, 표준편차, 범위, 평균 절대 편차, 사분위범위, 변동계수 개념, 공식, 예시)",
          "데이터 탐색의 기초(자료의 분포 형태, 왜도, 첨도의 개념 특징)",
          "데이터 탐색의 기초(도수분포표, 히스토그램, 막대그래프, 파이차트, 산점도, 줄기 잎 그림, 상자수염 그림 개념, 특징)",
          "고급 데이터 탐색(시공간 데이터 개념, 탐색 방법 3가지, 변량 개념, 변량 데이터 유형 3가지)",
          "고급 데이터 탐색(변량 데이터 탐색 방법 3가지, 비정형 데이터 탐색 방법, 파싱, 파서 개념)",
          "데이터 탐색 예상 문제 풀이-1",
          "데이터 탐색 예상 문제 풀이-2"
        ]
      },
      "requirements": [
        "\"한번에 합격하겠다는 의지! 데이터 통계와 데이터 보안 관련 분야 지식이 있으면 좋습니다.\""
      ],
      "description": "안녕하세요, ITGO 입니다.\n\n\n본 강의는 빅데이터분석기사 완전정복 필기편 : Part.2 빅데이터 탐색(1) 강의입니다.\n\n\n빅데이터 분석 기사는 국가 기술 자격증으로 필기와 실기 시험이 있습니다.\n\n\n본 강의는 빅데이터 분석 기사 자격증 취득을 위한 필기 시험 대비 강의입니다.\n\n\n빅데이터 분석 기사 필기 시험 핵심 개념부터 예상 문제 풀이까지 제대로 배워 한번에 합격합시다!\n\n\n\n\n누구를 위한 강의인가요?\n\n\n빅데이터 분석기사(필기) 자격증 취득을 원하시는 분들\n\n\n데이터 분석 직무로의 취업 및 이직을 준비하시는 분들\n\n\n데이터 분석에 대해 공부하고자 하시는 분들\n\n\n\n\n무엇을 배우나요?\n\n\n빅데이터 탐색\n\n\n데이터 전처리\n\n\n데이터 정제\n\n\n분석 변수 처리\n\n\n데이터 탐색의 기초\n\n\n고급 데이터 탐색\n\n\n예상문제 풀이\n\n\n\n\n빅데이터분석기사 완전정복 필기편 : Part.2 빅데이터 탐색(1) 강의에 입문해봅시다~!\n\n\n\n\n그럼 강의에서 만나요 :)",
      "target_audience": [
        "빅데이터 분석기사(필기) 자격증 취득을 원하시는 분들",
        "데이터 분석 직무로의 취업 및 이직을 준비하시는 분들",
        "데이터 분석에 대해 공부하고자 하시는 분들"
      ]
    },
    {
      "title": "Computação Quântica com Python",
      "url": "https://www.udemy.com/course/computacao-quantica-com-python/",
      "bio": "Construção de Circuitos e Algoritmos Quânticos para Simulação, Classificação, Regressão e Agrupamento",
      "objectives": [
        "Operação de Portas Quânticas Lógicas Aplicadas a Circuitos Quânticos Simulados e Reais",
        "Construção de Circuitos Quânticos para Simulação, Classificação, Regressão, Sequenciamento, Clusterização e Criptografia",
        "Predição de Dados com Análise Classificativa e Regressiva com Dados Estruturados e Não Estruturados",
        "Geração de Padrões com Base em Amostragem Estruturada e Não Estruturada"
      ],
      "course_content": {
        "Apresentação do Curso de Computação Quântica": [
          "Apresentação do Tema do Curso",
          "Apresentação do Conteúdo do Curso"
        ],
        "Introdução aos Principais Conceitos Teóricos": [
          "Introdução à História e Conceitos da Mecânica Quântica",
          "Introdução à História e Conceitos da Computação Quântica"
        ],
        "Instalação e Assinatura da Biblioteca de Códigos": [
          "Instalação e Assinatura da Biblioteca para Acesso ao Computador Quântico",
          "Instalação e Assinatura da Biblioteca pelo Repositório do Curso"
        ],
        "Portas Quânticas Lógicas": [
          "Portas Quânticas Unitárias",
          "Portas Quânticas Compostas"
        ],
        "Construção de Circuitos Quânticos": [
          "Construção e Aplicação de Circuitos Quânticos",
          "Criptografia Quântica com Portas Lógicas",
          "Clusterização por Tunelamento Quântico",
          "Fatoração de Números Primos com o Algoritmo de Shor"
        ],
        "Algoritmos de Simulação Quântica": [
          "Simulação Quântica na Classificação de Dados",
          "Simulação Quântica na Regressão de Dados",
          "Simulação Quântica no Sequenciamento de Dados",
          "Salvamento e Carregamento de Modelos Pré Treinados",
          "Previsão de Dados Climáticos com Simulação Quântica",
          "Simulação de Buracos Negros"
        ],
        "Aplicações Reais de Análise Preditiva": [
          "Classificação de Dados Reais com a Base de Dados do IRIS",
          "Regressão de Dados Reais com Previsão Climática",
          "Previsão do Valor de Ativos do Mercado Financeiro",
          "Geração de Padrões com Dados de Sorteios Reais",
          "Geração de Padrões Não Estruturados",
          "Classificação de Dados Não Estruturados"
        ]
      },
      "requirements": [
        "Conhecimentos de Nível Intermediário em Lógica de Programação e Python"
      ],
      "description": "Trabalhe  com a construção de circuitos e algoritmos quânticos com foco comercial. Aprenda a tecnologia computacional mais avançada do mundo e monte o seu próprio negócio de Computação Quântica. Você aprenderá a trabalhar com algoritmos de simulação quântica, portas e barramentos quânticos na construção circuitos para análise classificativa e regressiva, sequenciamento de dados, clusterização por tunelamento, criptografia quântica, geração de conteúdo com base em padrões de amostra, manipulação de dados estruturados e não estruturados e os principais fatores envolvidos nessa nova tecnologia que é a mais promissora da atualidade em todo o mundo.\n\n\nA Computação Quântica é atualmente a área da computação considerada a mais promissora no que diz respeito a algoritmos de predição, simulação e otimização de dados. Apontada como a principal tecnologia de ponta que está na fronteira do conhecimento das grandes potências mundiais como Estados Unidos e China.\n\n\nNão perca tempo e saia na frente dos demais com um conhecimento que é ainda pouco dominado no mundo e inédito no Brasil. Monte o seu próprio negócio na área com a biblioteca de códigos Neuraline e fature dinheiro em um setor sem concorrentes. Este é o único curso sobre o tema em língua portuguesa tratado de forma completa com exemplos de aplicações reais e projetos desenvolvidos do início ao fim de forma rápida e objetiva.",
      "target_audience": [
        "Desenvolvedores e/ou Cientistas de Dados"
      ]
    },
    {
      "title": "[FR] IA Full-Stack avec Ollama : Llama, Deepseek, Mistral",
      "url": "https://www.udemy.com/course/ia-full-stack-avec-ollama-llama-deepseek-mistral-qwq/",
      "bio": "Créez des apps IA avec des modèles open-source : NLP, chatbots, code, résumé, automatisation et plus.",
      "objectives": [
        "Déployez des modèles IA : installez, configurez et exécutez-les avec Ollama.",
        "Créez des apps IA : utilisez LLaMA 3, Mistral, CodeLlama, Mixtral et DeepSeek-R1.",
        "Faites du traitement NLP : résumez, générez et extrayez des infos de textes.",
        "Développez des assistants IA : chatbots, support client, assistants personnels.",
        "Générez et corrigez du code avec CodeLlama pour booster votre productivité.",
        "Intégrez l’IA dans des apps web avec FastAPI et une interface réactive.",
        "Automatisez les tâches : e-mails, comptes rendus et création de CV.",
        "Analysez des données en temps réel depuis APIs et modèles IA.",
        "Optimisez la performance des modèles avec prompts et réponses précises."
      ],
      "course_content": {
        "Démarrage Rapide – Configuration d’Ollama et Premier Modèle IA": [
          "Intro : Démarrage Rapide – Config Ollama & Premier Modèle IA",
          "Introduction à Ollama",
          "Installation et Configuration d’Ollama",
          "Mise en route avec Python",
          "Votre Premier Projet IA : Assistant de Chat IA"
        ],
        "IA pour le Traitement de Texte (NLP & LLMs)": [
          "Introduction à la Section : IA pour le Traitement de Texte (NLP & LLMs)",
          "Résumeur de Texte avec IA utilisant le Modèle Mistral",
          "Rédacteur de Blogs et Contenu avec LLaMA 3",
          "Générateur et Débogueur de Code avec CodeLlama",
          "Correcteur d’Orthographe et de Grammaire avec Deepseek R1",
          "Analyseur de Documents Juridiques avec Phi-2",
          "Résumeur de Nouvelles en Temps Réel avec Qwen 2.5"
        ],
        "IA pour Chatbots et Assistants Conversationnels": [
          "Introduction à la Section : IA pour Chatbots et Assistants",
          "Chatbot de Support Client – Modèle QWQ",
          "Assistant Virtuel avec IA – LLaMA 2",
          "Vérificateur de Symptômes Médicaux – MedLLaMA 2",
          "Recommandeur de Produits pour E-Commerce – Granite 3.2"
        ]
      },
      "requirements": [
        "Connaissances de base en programmation – Une certaine familiarité avec Python est utile mais non obligatoire. Nous vous guiderons dans le processus de codage.",
        "Compréhension de base de l'IA et du Machine Learning – Aucune expertise en ML n’est nécessaire, mais comprendre le fonctionnement des modèles IA est un plus.",
        "Familiarité avec le développement web (optionnel) – Une expérience en HTML, JavaScript et FastAPI facilitera la création d’applications IA.",
        "Ce cours est conçu pour guider les débutants complets dans le développement IA, tout en proposant des projets avancés aux développeurs expérimentés.",
        "Si vous débutez en IA, nous aborderons tout étape par étape. Si vous êtes expérimenté, vous apprendrez à créer et déployer des applications IA concrètes avec Ollama."
      ],
      "description": "IA Full-Stack avec Ollama : Llama, DeepSeek, Mistral, QwQ, Phi-2, MedLlama2, Granite3.2 est la formation pratique ultime pour apprendre à développer et déployer des applications d’IA concrètes en utilisant les derniers modèles open-source. Que vous soyez débutant curieux de l’intelligence artificielle ou développeur expérimenté, ce cours vous propose des projets concrets pour intégrer des modèles de langage (LLMs) dans des applications web, des outils d’automatisation et des solutions avancées pilotées par l’IA.\nTout au long du cours, vous apprendrez à installer, configurer et utiliser Ollama pour exécuter localement des modèles puissants sans dépendre des APIs cloud coûteuses. Vous travaillerez avec LLaMA 3, DeepSeek, Mistral, Mixtral, QwQ, Phi-2, MedLlama2, Granite3.2 et CodeLlama, en acquérant de l'expérience dans le traitement du langage naturel (NLP), la génération de texte, la complétion de code, le débogage, l’analyse de documents, l’analyse de sentiments et l’automatisation par IA.\nLe cours est riche en projets pratiques. Vous développerez un résumé intelligent de l’actualité, un outil de relecture assisté par IA, un chatbot de support client, ainsi qu’un assistant intelligent pour automatiser les tâches métiers. Chaque projet vous offre une expérience complète avec FastAPI, Python, Ollama et les REST APIs, pour maîtriser le développement full-stack en IA.\nVous apprendrez également à récupérer et traiter des données en temps réel via des APIs, ce qui est idéal pour créer des applications IA qui analysent des flux d’information actuels. Vous réaliserez un résuméur d’actualités en temps réel, un analyseur de rapports financiers, et un outil d’analyse automatisée de candidatures.\nÀ la fin du cours, vous aurez construit plusieurs projets d’IA couvrant le développement full-stack, le traitement de texte, la compréhension du langage naturel, les chatbots, l’automatisation par IA et les applications à base de LLMs. Vous serez prêt à déployer des modèles IA dans des applications en production et à exploiter les technologies IA les plus avancées pour créer des solutions intelligentes.\nCe cours s’adresse aux développeurs, data scientists, entrepreneurs, chercheurs et passionnés d’IA. Vous apprendrez à concevoir des applications web avec IA, intégrer des modèles NLP, et automatiser des tâches à l’aide d’outils pilotés par l’IA.\nSi vous êtes prêt à faire passer vos compétences en IA au niveau supérieur, ce cours est fait pour vous !",
      "target_audience": [
        "Étudiants et autodidactes – Intéressé par l’IA mais vous ne savez pas par où commencer ? Ce cours ne nécessite aucune expérience préalable et vous guidera pas à pas.",
        "Fondateurs de startups et innovateurs IA – Vous souhaitez créer des produits basés sur l’IA ? Ce cours vous offre des projets pratiques pour démarrer votre aventure en IA.",
        "Analystes de données et chercheurs – Vous voulez extraire des informations de documents juridiques, d’articles ou d’avis clients ? Apprenez à analyser efficacement de grands ensembles de textes.",
        "Entrepreneurs tech et chefs de produit – Besoin de développer des applications IA pour votre entreprise ? Apprenez à créer des chatbots, générateurs de contenu et outils d’automatisation.",
        "Passionnés d’IA et de machine learning – Intéressé par les modèles comme LLaMA 3, Mistral, Mixtral, CodeLlama, et DeepSeek-R1 ? Acquérez une expérience pratique du déploiement de modèles IA.",
        "Développeurs et programmeurs – Vous souhaitez intégrer l’IA à vos applis web ou projets logiciels ? Ce cours vous apprendra à créer des applications IA full-stack avec Python et FastAPI."
      ]
    },
    {
      "title": "基于时间序列算法的指标异常监控",
      "url": "https://www.udemy.com/course/lczwntgm/",
      "bio": "大数据算法实战",
      "objectives": [
        "1.了解可视化平台示意图的必要性，交互分析维度下筛选与拆分",
        "2.了解指标异常的定义与指标异常监控效果示意",
        "3.理解arima模型原理、用途及效果评估",
        "4.掌握facebook开源工具prophet，学会使用prophet评估节假日影响的方法"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "课程内容": [
          "数据可视化平台简介",
          "指标异常监控",
          "时间序列算法简介",
          "指标的节假日效应评估"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "了解大数据算法"
      ],
      "description": "在传统的监控中，我们有阈值告警、斜率告警、变化率告警等，每种告警方式都可以解决某些问题，但不可避免的在实践过程中会遇到种种问题。为了更简洁、更准确的进行告警，大家会基于时间序列算法预测实际，快来了解一下它在各种不同的场景中发挥的重大作用吧!快来学习《基于时间序列算法的指标异常监控》的课程吧！\n本次课针对该问题分成4个部分进行解答：第一部分：数据可视化平台简介，第二部分：指标异常监控，第三部分：时间序列算法简介，第四部分：指标的节假日效应评估\n\n\n本节课程是由授课老师与三节课合作制作的。在此，要特别感谢老师的辛苦付出！经历了课程立项、设计、开发中的众多环节，我们才能最终为你呈现现在的这门课程。无论是授课老师还是三节课团队，都希望这门课程能够让你有所收获，希望同学们结合个人工作情况，学以致用。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "1.企业产品、运营、数据方向从业者",
        "2.寻求通过大数据算法进行指标异常监控的企业产品、业务负责人",
        "3.对机器学习在业务中的实际应用感兴趣的学习者"
      ]
    },
    {
      "title": "手把手构建人工智能产品",
      "url": "https://www.udemy.com/course/uszeblwn/",
      "bio": "系统了解人工智能产品搭建的全流程",
      "objectives": [
        "了解人工智能产品的概念",
        "掌握人工智能产品的搭建4步骤",
        "通过案例拆解，理解人工智能产品的构成",
        "了解人工智能产品经理的发展路径和企业未来发展的驱动"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲",
          "引入讲解"
        ],
        "一、人工智能产品的概念": [
          "概念"
        ],
        "二、人工智能产品构建4步骤": [
          "业务认知与分析",
          "业务规则与数据准备、模型评价、模型构建"
        ],
        "三、人工智能产品研发案例": [
          "分子图像识别系统（上）",
          "分子图像识别系统（下）",
          "抗生素用临床辅助系统"
        ],
        "四、人工智能产品的未来": [
          "人工智能产品的未来"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "有一定产品设计经验，了解产品设计"
      ],
      "description": "随着AI技术的逐渐普及与落地，AI产品在市场上也变得分外火热。在人工智能产品构建过程分为很多阶段，从最顶层的商业思考到下层的模型研发都有着不同的思考方式。\n为此，三节课特别邀请到拥有8年+行业数字化转型与AI产品经验，著有《手把手构建人工智能产品》的高飞老师，带你了解何为人工智能以及人工智能产品的意义，为你讲解如何搭建人工智能产品，通过案例拆解让你更深入理解人工智能产品究竟是如何构成的，并进一步与你分享人工智能产品经理的能力要求与发展路径，以及人工智能产品的未来发展。相信对你的人工智能产品经理的发展规划会起到一定的帮助。\n本节课程是由授课老师与三节课合作制作的。在此，要特别感谢老师的辛苦付出！经历了课程立项、设计、开发中的众多环节，我们才能最终为你呈现现在的这门课程。无论是授课老师还是三节课团队，都希望这门课程能够让你有所收获，希望同学们结合个人工作情况，学以致用。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "有一定产品设计经验，想要入职AI产品岗位的产品新人",
        "缺少体系化的方法论，正在从事AI产品的产品经理",
        "有toC产品经验，想要转型AI产品的产品经理"
      ]
    },
    {
      "title": "Machine Learning dengan Python",
      "url": "https://www.udemy.com/course/machine-learning-dengan-python/",
      "bio": "Belajar Machine Learning menggunakan python",
      "objectives": [
        "Menscraping data dari file dari berbagai format",
        "Memproses data menjadi dataset yang baik",
        "Membuat visualisasi data",
        "Memahami model - model machine learning. Baik tipe supervised dan unsupervised",
        "mengevaluasi model machine learning dan menemukan paramter yang terbaik pada sebuah machine learning"
      ],
      "course_content": {
        "Introduction": [
          "Pendahuluan",
          "Data Preparation dan Library",
          "Flow Data Preparation",
          "Scraping PDF",
          "Scraping word dan json"
        ],
        "Scraping Data": [
          "Scraping PDF dan konversi ke excel",
          "Scraping PDF part 2",
          "Scraping Word",
          "Scraping json"
        ],
        "Data Preprocessing": [
          "Exploratory Data Analysis",
          "Deteksi dan mengatasi missing data",
          "Menangani Kategori Data",
          "Normalized dan Standardized",
          "Split Dataset"
        ],
        "Data Visualization": [
          "Pengantar Data Visualization",
          "Ticks dan Bar",
          "Bar dan Pie Chart",
          "Stack Bar dan Stack Area",
          "Histogram"
        ],
        "Data Visualization Latihan": [
          "Line Plot",
          "Bar Plot",
          "Stack Area",
          "Stack Bar"
        ],
        "Linear Regression": [
          "Pengantar Machine Learning",
          "Linear Regression",
          "Praktek Linear Regression",
          "Praktek Linear Regression 2"
        ],
        "Overfitting dan Underfitting": [
          "Pengantar Overfitting dan Underfitting",
          "Overfitting dan Underfitting"
        ],
        "Classification": [
          "Pengantar Classification",
          "Konsep Logistic Regression",
          "Konsep Decision Tree",
          "Konsep K-Nearest Neigbor (KNN)",
          "Konsep Support Vector Machine (SVM)",
          "Konsep Naive Bayes"
        ],
        "Praktek Classification": [
          "Logistic Regression",
          "Decision Tree",
          "K-Nearest Neigbor (KNN)",
          "Support Vector Machine (SVM)",
          "Naive Bayes",
          "Perceptron (Bonus)"
        ],
        "Evaluation": [
          "Pengantar Evaluation",
          "Pipeline",
          "Learning Curve",
          "Validation Curve",
          "Fine Tuning",
          "Confusion Matrix",
          "Nested Validation"
        ]
      },
      "requirements": [
        "Python level pemula"
      ],
      "description": "Machine Learning merupakan disiplin ilmu yang dirancang untuk membaca pola atau perilaku berdasarkan data. Berkembangnya machine learning ini tidak luput dari kebutuhan bisnis untuk cepat mengambil keputusan tepat. Sehingga jalan keluarnya adalah dengan mempelajari pola - pola data histori.\nUntuk itu dirancang course machine learning ini untuk dapat dipelajari bagaimana cara memprediksi sesuatu berdasarkan pola-pola data yang sudah ada. Course ini juga dirancang mulai awal mendapatkan data atau scraping data yang diperlukan, kemudian memprosesnya untuk menghasilkan sebuah dataset. Tidak mudah untuk membuat dataset yang baik ada beberapa tahapan yang harus dilakukan untuk menghasilkan dataset yang terbaik. Dataset inilah yang digunakan sebagai data untuk membuat machine learning. Ada 2 tipe machine learning yang akan dipelajari pada course ini yaitu supervised learning dan unsupervised learning. Pada course ini akan dibahas beberapa model agar anda dapat menbedakan model - model tersebut. Tidak cukup hanya sampai membuat machine learning course ini juga mempelajari bagaimana cara membuat data visualisasinya serta bagaimana mengevaluasi dan meningkatkan tingkat akurasi dari sebuah machine learning. Sehingga setelah belajar course ini diharapkan anda sudah menguasai konsep machine learning yang bisa anda gunakan pada dunia kerja.\n\n\nCakupan Materi Course yang diajarkan :\nModule 1 : Preparing Data / Menyiapkan Data\nModule 2 : Data Preprocessing\nModule 3 : Data Visualization\nModule 4 : Linear Regression\nModule 5 : Overfitting dan Underfitting\nModule 6 : Classification\nModule 7 : Evaluation dan Hyperparameter Tuning\nModule 8 : Clustering",
      "target_audience": [
        "Semua orang yang ingin mempelajari mengenai machine learning"
      ]
    },
    {
      "title": "Mastering Vector Databases for AI Applications | Arabic",
      "url": "https://www.udemy.com/course/mastering-vector-databases-for-ai-applications-arabic/",
      "bio": "Learn Vector Databases (pinecone & qdrant) in detail for image search and semantic search engines, End-to-End projects.",
      "objectives": [
        "What is vector databases, and why we use them?",
        "Review on embeddings for text data and feature extraction for images",
        "Mastering pinecone vector database via end-to-end projects",
        "Semantic Search using pinecone vector database and Deployment",
        "Image Search using pinecone vector database and Deployment",
        "Mastering qdrant vector database via end-to-end projects",
        "Semantic Search using qdrant vector database and Deployment",
        "Image Search using qdrant vector database and Deployment",
        "Libraries for image search (classical approach)",
        "Libraries for semantic search (classical approach)"
      ],
      "course_content": {
        "Introduction": [
          "Introduction"
        ],
        "Code material & Community": [
          "Code material & community"
        ],
        "Introduction to Vector Database": [
          "Introduction to Vector Database"
        ],
        "Semantic Search using pinecone": [
          "Intro to pinecone vector database",
          "Pinecone cloud service",
          "Preparing semantic search dataset",
          "Connecting pinecone",
          "Pinecone functions (upsert | update | fetch | delete | query)",
          "Prepare utils file for deployment",
          "Endpoint to search (semantic & pinecone)",
          "Endpoint to update (semantic & pinecone)"
        ],
        "Image Search using pinecone": [
          "Prepare Dataset and Model",
          "Upserting to pinecone",
          "Helper functions for search endpoint",
          "Endpoint to search (image& pinecone)",
          "Endpoint to update (image & pinecone)"
        ],
        "Semantic Search using qdrant": [
          "Intro to qdrant, prepare dataset and model",
          "Upserting to qdrant",
          "Endpoint to search (semantic & qdrant)",
          "Endpoint to update (semantic & qdrant)"
        ],
        "Image Search using qdrant": [
          "Upserting to qdrant",
          "Endpoint to search (image & qdrant)",
          "Endpoint to update (image & qdrant)"
        ],
        "Open-Source Libraries": [
          "Open-Source Libraries"
        ]
      },
      "requirements": [
        "Python programming language",
        "Basic knowledge of machine and deep learning",
        "Basic knowledge of deployment (optional)"
      ],
      "description": "Welcome to this course\nThis course is Mastering Vector Databases for AI Applications in Arabic by Eng/Mohammed Agoor.\n\n\nIn this course, we are going on a journey to discover vector databases like (pinecone, Qdrant) and how to get the best benefit from them via end-to-end projects for building semantic search engines and image search engines from scratch up to deployment and testing.\n\n\nWhat we learn here is a lot, We will start by discussing the vector databases and their benefits, review embeddings for text data and feature extraction for images, then we will dive into the pinecone vector database and use it for building an end-to-end semantic search engine and an end-to-end image search engine. Then we will dive into another great vector database (Qdrant) and use it also for the same projects.\n\n\nAre you ready to take your AI skills to the next level? This course is designed to equip you with the knowledge and hands-on experience needed to build semantic search engines and image search engines from scratch to deployment and testing.\n\n\nWhat You'll Learn:\nThrough a series of engaging modules, you'll explore a wealth of concepts and practical techniques:\nVector databases, and their benefits\nReview embeddings for text data and feature extraction for images\nPinecone vector database and its cloud service\nPinecone pricing estimator, and (project, index, collection, pod size, pod type) definitions\nConnecting pinecone\nSemantic search project using pinecone\nPinecone functions (upsert, delete, update, fetch, query)\nDeployment of semantic search with pinecone project using FastAPI\nEndpoint to search for semantic search with pinecone project\nEndpoint to update (index, delete) for semantic search with pinecone project\nImage search project using pinecone\nDeployment of image search with pinecone project using FastAPI\nEndpoint to search for image search with pinecone project\nEndpoint to update (index, delete) for image search with pinecone project\nQdrant vector database, (cluster, collection) definitions\nQdrant dashboard\nConnecting qdrant\nSemantic search project using qdrant\nDeployment of semantic search with qdrant project using FastAPI\nEndpoint to search for semantic search with qdrant project\nEndpoint to update (index, delete) for semantic search with qdrant project\nImage search project using qdrant\nDeployment of image search with qdrant project using FastAPI\nEndpoint to search for image search with qdrant project\nEndpoint to update (index, delete) for semantic search with qdrant project\nWhether you're an AI enthusiast, developer, or data scientist, this course will empower you with the knowledge and practical skills necessary to excel in utilizing vector databases for AI applications.\nJoin us now and embark on an enriching learning journey that will set you on the path to mastering vector databases for cutting-edge AI projects.\nEnroll today!",
      "target_audience": [
        "Artificial Intelligence Engineers",
        "Generative AI Engineers",
        "Data Scientists/ Analysts",
        "Students or statisticians",
        "Anyone with interest in AI field"
      ]
    },
    {
      "title": "실전 프로젝트: 네이버 카페 데이터 수집 및 활용",
      "url": "https://www.udemy.com/course/navercafe-crawling-and-use/",
      "bio": "Google Apps Script로 데이터 수집부터, 웹 크롤링, 그리고 Dify로 QnA 봇 생성까지",
      "objectives": [
        "Google Apps Script를 활용해 네이버 카페 게시글을 자동 수집하는 방법",
        "OpenAI API를 활용하여 데이터를 분석하고 의미 있는 정보를 필터링하는 방법",
        "Selenium을 이용해 웹 페이지의 데이터를 크롤링하는 방법",
        "Dify를 사용하여 AI Q&A 봇을 개발하고 이를 통해 지식 기반 서비스를 구축하는 방법"
      ],
      "course_content": {
        "강의 소개": [
          "강의 소개",
          "Google Apps Script 소개",
          "문제 상황 설정"
        ],
        "Google Apps Script를 활용한 게시글 자동 수집": [
          "네이버 카페글 검색 API",
          "네이버 오픈 API 이용 신청",
          "나의 첫 자바스크립트 코딩",
          "[코드 제공] 나의 첫 자바스크립트 코딩",
          "Google Apps Script와 Spreadsheet 연동",
          "[코드 제공] Google Apps Script와 Spreadsheet 연동",
          "검색 API를 통해 네이버 카페글 가져오기",
          "[코드 제공] 검색 API를 통해 네이버 카페글 가져오기",
          "특정 카페글만 필터링하여 가져오기",
          "[코드 제공] 특정 카페글만 필터링하여 가져오기",
          "불러온 카페글을 구글 시트로 저장",
          "[코드 제공] 불러온 카페글을 구글 시트로 저장",
          "Trigger를 이용해 코드 자동 실행"
        ],
        "OpenAI API를 통해 게시글의 유의미성 판단": [
          "두 번째 과제 소개",
          "OpenAI API 키 발급",
          "OpenAI API를 통해 게시글의 유의미성 판단",
          "[코드 제공] OpenAI API를 통해 게시글의 유의미성 판단"
        ],
        "Selenium을 이용해 게시글 전체 내용을 크롤링": [
          "세 번째 과제 소개",
          "구글 서비스 계정 생성 및 권한 설정",
          "구글 드라이브 및 시트 API 활성화",
          "구글 스프레드 시트 데이터 불러오기",
          "[코드 제공] 구글 스프레드 시트 데이터 불러오기",
          "크롤링의 법적 위험성",
          "개발자 도구를 이용해 HTML 구조를 살펴보기",
          "Selenium을 이용해 게시글 전체 내용을 크롤링",
          "[코드 제공] Selenium을 이용해 게시글 전체 내용을 크롤링"
        ],
        "Dify를 이용해 Q&A 봇 생성": [
          "네 번째 과제 소개",
          "Dify 둘러보기",
          "텍스트 임베딩 및 지식 생성",
          "지식 기반 Q&A 봇 생성",
          "앱 배포 및 실행"
        ]
      },
      "requirements": [
        "프로그래밍 경험: 이 강의에서는 JavaScript와 Python을 사용한 크롤링 및 데이터 처리를 다루므로, 어떠한 프로그래밍 언어에서든 기본적인 프로그래밍 경험이 필요합니다. 변수 선언, 조건문, 함수 작성과 같은 프로그래밍의 기본 개념을 알고 있어야 원활한 수강이 가능합니다.",
        "Jupyter Notebook 사용 경험: 실습에서는 Jupyter Notebook을 활용하여 Python 코드를 실행하므로, Jupyter Notebook에서 코드를 작성하고 실행하는 방법을 알고 있어야 합니다."
      ],
      "description": "이 강의는 Google Apps Script를 이용한 데이터 수집 자동화부터 Dify를 활용한 AI Q&A 봇 개발까지 실습하는 실전 프로젝트입니다. 네이버 카페에서 수백 개의 게시글을 수동으로 확인하는 대신, 데이터를 효율적으로 수집하고 AI를 통해 유의미한 정보를 선별하는 과정을 다룹니다.\n강의는 다음과 같은 핵심 단계로 진행됩니다:\nGoogle Apps Script를 사용하여 네이버 카페 게시글 링크와 요약 내용을 자동으로 수집합니다.\nOpenAI API를 활용해 수집된 게시글의 유의미성을 판단하고, 필요한 정보를 필터링합니다.\nSelenium으로 크롤링하여 게시글의 전체 내용을 자동으로 추출합니다.\nDify 플랫폼을 통해 수집된 데이터를 바탕으로 Q&A 봇을 만들어 실제로 질문에 답할 수 있는 시스템을 구축합니다.\n이 강의는 커뮤니티에서 필요한 정보를 효율적으로 수집하고, AI를 통해 인사이트를 빠르게 도출해내는 것을 목표로 합니다. 사업 준비, 취업, 여행 계획 등 여러 상황에서 유용한 정보를 자동으로 모으고 활용하는 방법을 배우게 될 것입니다.",
      "target_audience": [
        "데이터 수집 및 처리 자동화에 관심이 있는 개발자/비개발자: 웹 데이터를 수집하고 이를 기반으로 자동화 도구를 개발하고자 하는 분들",
        "LLM 및 AI 앱 개발에 관심이 있는 AI/ML 연구자: LLM 학습에 필요한 데이터 수집뿐 아니라, Dify를 사용해 AI 앱을 빠르게 프로토타이핑하고 싶어하는 분들"
      ]
    },
    {
      "title": "Introdução à Machine Learning em Python com scikit-learn",
      "url": "https://www.udemy.com/course/mlcompython/",
      "bio": "Aprenda a criar modelos para predição e categorização!",
      "objectives": [
        "Transforme dados em previsões poderosas com Machine Learning",
        "Domine o Scikit-learn para criar modelos inteligentes em minutos",
        "Utilize pandas, numpy e matplotlib para explorar e visualizar dados como um profissional",
        "Aprenda os algoritmos essenciais de Machine Learning e quando usá-los",
        "Melhores práticas para validar e otimizar seus modelos",
        "Entenda os bastidores dos algoritmos de Machine Learning e como ajustá-los"
      ],
      "course_content": {
        "Classificação": [
          "O que é machine learning e conhecendo o scikit-learn",
          "Responda as perguntas a seguir sobre a aula anterior",
          "Classificação utilizando vizinhanças",
          "Classificador por vizinhanças",
          "Medindo a performance do modelo",
          "Performance do modelo",
          "Visualizando a complexidade do modelo"
        ],
        "Regressão": [
          "Introdução à regressão",
          "Regressão",
          "Fundamentos da Regressão Linear",
          "Regressão Linear",
          "Validação cruzada",
          "Validação cruzada",
          "Regressão regularizada",
          "Regressão de Lasso"
        ],
        "Ajuste fino (fine-tuning) do seu modelo": [
          "Seu modelo é realmente bom?",
          "Regressão logística e a curva ROC",
          "Ajuste de hiperparâmetro"
        ],
        "AutoML com Pycaret": [
          "Avaliando diversos modelos ao mesmo tempo"
        ]
      },
      "requirements": [
        "Conhecimento básico de pyhton"
      ],
      "description": "Curso de Introdução à Machine Learning com Python no Scikit-Learn\nBem-vindo ao seu guia completo para começar a jornada na ciência de dados e machine learning! Este curso abrange os fundamentos essenciais do machine learning usando Python e a biblioteca Scikit-Learn, um dos conjuntos de ferramentas mais poderosos e acessíveis para desenvolver modelos de aprendizado de máquina.\nO que você aprenderá:\nClassificação: Descubra como construir modelos de classificação precisos para prever categorias e tomar decisões com base em dados históricos.\nRegressão: Aprenda a aplicar técnicas de regressão para prever valores contínuos e entender a relação entre variáveis.\nFine Tuning: Explore estratégias avançadas de ajuste de hiperparâmetros para otimizar o desempenho dos seus modelos e evitar overfitting.\nAvaliando diversos modelos ao mesmo tempo: Domine as melhores práticas de AutoML com pycaret para treinar e testar diversos modelos ao mesmo tempo, analisar qualidade das métricas de saída e tunar parâmetros específicos.\nPor que este curso é único:\nAbordagem Prática: Aprenda fazendo, com projetos práticos e exercícios que consolidam seu conhecimento.\nInstrutor Especializado: Orientação de um instrutor experiente com passagem por startups dentro e fora do Brasil, que simplifica conceitos complexos e torna o aprendizado acessível a todos os níveis de habilidade.\nCertificado de Conclusão: Receba um certificado de conclusão ao finalizar o curso, comprovando suas habilidades em machine learning com Python e Scikit-Learn.\nEste curso é ideal para estudantes, profissionais de TI, cientistas de dados aspirantes e qualquer pessoa interessada em explorar o vasto campo da inteligência artificial e do aprendizado de máquina. Não importa se você é um iniciante absoluto ou tem alguma experiência prévia, este curso fornecerá as bases sólidas necessárias para começar sua jornada de aprendizado de machine learning com confiança e habilidade.",
      "target_audience": [
        "Desenvolvedores iniciantes de Python curiosos para aprender machine learning e ciência de dados"
      ]
    },
    {
      "title": "Use Python in Power BI in Arabic شرح بالعربي للبايثون",
      "url": "https://www.udemy.com/course/use-python-in-power-bi-in-arabic/",
      "bio": "استفيد من مزايا كل من البايثون و الباور بي أي في نفس المشروع",
      "objectives": [
        "Learn how to install Python and its Libraries",
        "How to use Python in Power BI to get data",
        "How to use Python in Power BI to Transform Data",
        "How to use Python in Power BI to add new Visuals",
        "Integration between Power BI & Jupyter notebook"
      ],
      "course_content": {
        "Introduction to Python in Power BIمقدمة عن فوائد استخدام البايثون في باور بي أي": [
          "Introduction to Python in Power BI",
          "Power BI in Jupyter"
        ],
        "Use Python to Get Data in Power BI استخدام البايثون لإدخال بيانات في الباور بي أ": [
          "Install Python and Python Libraries",
          "Insert Data Table into Power BI Using Python إدخال جدول بيانات باستخدام البايثون",
          "Insert Python Libraries Data in Power BI",
          "Solution for Python installation Path"
        ],
        "Use Python to Transform Data in Power BI": [
          "Fill Null by Average Data Using One Line Code",
          "Get Statistics & Correlations for the Whole Dataset",
          "Export Data from Power BI to CSV",
          "Export Data from Power BI to Excel"
        ],
        "Use Python For Power BI Visualizations": [
          "Use Python to add Histograms for all columns using one code",
          "Add Hexbin Visual to PBI using Python and Comparison with Scatter Visual",
          "Add Heatmap to check correlations among columns",
          "Add PairPlot to check relationships among columns",
          "Add Stripplot to PBI and its benefits compared to Column Visual",
          "Add Violin & Box Plots to Check Distributions",
          "Use Different Palettes for Different Colors in Python Visuals",
          "Save Python Visuals as Images",
          "Important References"
        ],
        "Power BI Integration with Jupyter June-2023 Updates": [
          "Power BI Library Installation Using Anaconda",
          "Insert Power BI in Jupyter"
        ]
      },
      "requirements": [
        "Level 1 in Power BI",
        "Python Knowledge"
      ],
      "description": "Python has benefits that don't exist in Power BI. This Course will teach you how to get additional benefits from Python that does not exist in Power BI\nكل من الباور بي أي و البايثون له مميزات مختلفة عن الآخر. في هذا الكورس ، سوف تتعلم كيفية الاستفادة من مميزات البايثون و الغير موجودة في الباور بي أي، للاستفادة من مميزات كلاهما في نفس المشروع\n1. Install Python Software and Install Python Librariesتنصيب برنامج البايثون و المكتبات الخاصة به لاستخدامها في الباور بي أي\n2. Get Data from Python Libraries as Seaborn and from different tables ادخال بينات في الباور بي أي باستخدام البايثون والاستفادة من البيانات المتواجدة في مكتبات البايثون\n3. Additional Data Transformation Operations using Python, as Correlations, Describe, Export to Excel , Export to CSV…etc عمليات جديدة و إضافية بتعديل و تظبيط البيانات باستخدام البايثون قبل استخدامها في التقرير ، مثل العلاقات بين البيانات و الاحصائيات الخاصة بيها ة استخراجها في اكسيل أو غيره\n4. Multiple New Python Visuals (Histogram, Hexbin, Heatmap, Stripplot, PairPlot , ViolinPlot, BoxPlot…..etc) هناك العديد من الأشكال الجديدة في البايثون و الغير موجودة في الباور بي أي بالرغم من أهميتها البالغة في التقارير، ستتعلم كيفية اضافتها بسهولة باستخدام البايثون\n5. Format Python Visual in Power BI using different Palettes. تظبيط الألوال في البايثون\n6. Save Python Visuals as Images حفظ الصور في البايثون\n7. Integration between Power BI & Jupyter notebook",
      "target_audience": [
        "All People interested in developing reports and dashboards",
        "Data Scientists",
        "Power BI Developers",
        "Python Developers"
      ]
    },
    {
      "title": "Pythonによるビジネスに役立つデータ分析・可視化入門（Pandas・Plotly・Streamlit編）",
      "url": "https://www.udemy.com/course/data-analysis-visualization/",
      "bio": "初心者向けのデータ分析・可視化のコースです。Pythonのライブラリ（Pandas・Matplotlib・Plotly・Streamlit）を使い、具体例を交えながら、データの処理・分析から可視化までを丁寧に解説します。",
      "objectives": [
        "Pandasを用いて、効率的にデータを分析・可視化できるようになります。",
        "Matplotlib・Plotlyを用いて、効果的にデータを可視化できるようになります。",
        "Streamlitを用いてデータに関するWebアプリを作成し、効果的に情報を共有できるようになります。",
        "Pythonの文法を理解し、基本的なプログラミング技術を習得できます。",
        "実践的な演習問題を通じてデータ分析・可視化の理解を深めることができます。"
      ],
      "course_content": {
        "はじめに": [
          "このコースで学べるトピック",
          "【重要】本コースのソースコード・資料"
        ],
        "環境構築と使い方": [
          "このセクションで学べるトピック",
          "Anacondaのインストール",
          "仮想環境の作成とライブラリのインストール",
          "JupyterLabの使い方"
        ],
        "Python超入門": [
          "このセクションで学べるトピック",
          "データ型と変数",
          "数値計算",
          "print関数",
          "リスト",
          "タプル",
          "辞書 (Dictionary)",
          "演習１【課題】",
          "演習１【解答】",
          "条件分岐",
          "繰り返し処理：for",
          "繰り返し処理：while",
          "関数.",
          "ライブラリ",
          "クラス：定義",
          "クラス：継承",
          "演習２【課題】",
          "演習２【解答】"
        ],
        "Numpy・Pandas入門": [
          "このセクションで学べるトピック",
          "Numpy超入門：ベクトル、行列の作成",
          "Numpy超入門：連続した配列の自動作成",
          "Numpy超入門：乱数の自動生成",
          "【課題】Numpy・Pandas入門 演習１",
          "【解答】Numpy・Pandas入門 演習１",
          "Seriesの基本：Pandas、Seriesとは",
          "Seriesの基本：Seriesの作成",
          "Seriesの基本：統計情報の取得・データ抽出",
          "DataFrameの基本：DataFrameとは",
          "DataFrameの基本：DataFrameの作成",
          "DataFrameの基本：インデックス設定・変更・解除",
          "DataFrameからデータの抽出：基本的な情報の確認",
          "DataFrameからデータの抽出：指定行・指定列の抽出",
          "DataFrameからデータの抽出：条件指定での抽出",
          "DataFrameからデータの抽出：複数条件/範囲指定での抽出",
          "DataFrameのソート",
          "Excel、CSVファイルからの読み込み",
          "Excel、CSVファイルへの書き込み",
          "欠損値の対応：欠損値／非欠損値の抽出",
          "欠損値の対応：欠損値の対応",
          "【課題】Numpy・Pandas入門 演習２",
          "【解答】Numpy・Pandas入門 演習２：Pandas Seriesの基本",
          "【解答】Numpy・Pandas入門 演習２：Pandas DataFrameの基本",
          "【解答】Numpy・Pandas入門 演習２：DataFrameからデータの抽出",
          "【解答】Numpy・Pandas入門 演習２：DataFrameのソート",
          "【解答】Numpy・Pandas入門 演習２：Excel、CSVファイルへの書き込み",
          "【解答】Numpy・Pandas入門 演習２：欠損値の対応"
        ],
        "Pandasによるデータ処理・分析": [
          "このセクションで学べるトピック",
          "Groupbyによる集計",
          "DataFrameの連結・結合：連結",
          "DataFrameの連結・結合：内部結合",
          "DataFrameの連結・結合：外部結合",
          "ピボットテーブルの作成：データの準備",
          "ピボットテーブルの作成：ピボットテーブル作成",
          "時系列データの集計：日付型インデックスの設定",
          "時系列データの集計：年・週・月毎の集計",
          "時系列データの集計：shiftによる日付操作",
          "時系列データの集計：会計年度の集計",
          "時系列データの集計：移動平均の算出",
          "【課題】Pandasによるデータ処理・分析 演習",
          "【解答】Pandasによるデータ処理・分析 演習：Pandas DataFrameの連結・結合",
          "【解答】Pandasによるデータ処理・分析 演習：Pandas Groupbyによる集計",
          "【解答】Pandasによるデータ処理・分析 演習：ピボットテーブルの作成"
        ],
        "Matplotlib・Pandasによるデータ可視化": [
          "このセクションで学べるトピック",
          "Matplotlibの基本：基本的な使い方",
          "Matplotlibの基本：グラフの装飾",
          "Matplotlibの基本：複数系列データの表示",
          "Matplotlibの基本：グラフの保存",
          "Matplotlibの基本：オブジェクト指向の方法",
          "Matplotlib 棒グラフ：グラフ作成の基本",
          "Matplotlib 棒グラフ：積み上げ棒グラフ",
          "Matplotlib 棒グラフ：積み上げ棒グラフ（【参考】コード短縮化）",
          "Matplotlib 円グラフ：グラフ作成の基本",
          "Matplotlib 円グラフ：グラフの装飾",
          "Matplotlib 散布図：グラフ作成の基本",
          "Matplotlib 散布図：グラフの装飾",
          "Matplotlib 散布図：複数系列データの表示",
          "Matplotlib 日本語表示の方法",
          "Matplotlib ヒストグラム：グラフ作成の基本",
          "Matplotlib ヒストグラム：グラフの装飾",
          "Matplotlib ヒストグラム：複数系列データの表示",
          "Matplotlib 箱ひげ図：グラフ作成の基本",
          "Matplotlib 箱ひげ図：グラフの装飾",
          "Matplotlib 箱ひげ図：複数系列データの表示",
          "Matplotlib 複数のグラフの描画",
          "Pandasのデータ可視化機能：折れ線グラフ、棒グラフ、積み上げ棒グラフ、円グラフ",
          "Pandasのデータ可視化機能：散布図、ヒストグラム、箱ひげ図"
        ],
        "Plotlyによるインタラクティブなグラフの作成": [
          "このセクションで学べるトピック",
          "Plotlyの基本：使い方",
          "Plotlyの基本：グラフの装飾",
          "Plotlyの基本：レイアウト調整",
          "Plotlyの基本：ファイル出力",
          "散布図",
          "散布図行列",
          "折れ線グラフ",
          "棒グラフ・積み上げ棒グラフ",
          "円グラフ",
          "ヒストグラム",
          "箱ひげ図"
        ],
        "Streamlitによるデータ分析・可視化用Webアプリの作成": [
          "このセクションで学べるトピック",
          "VS Codeと拡張機能のインストール",
          "このセクションで利用するフォルダ",
          "Streamlitの基本：公式ページ・デモアプリ",
          "Streamlitの基本：DataFrame・テーブルの表示",
          "Streamlitの基本：折れ線グラフの表示・保存",
          "Streamlitによるグラフの表示：標準グラフ",
          "Streamlitによるグラフの表示：外部ライブラリ",
          "Streamlitで利用できる部品：ボタン・チェックボックス・ラジオボタン・セレクトボックス・スライダー",
          "Streamlitで利用できる部品：テキスト・数値・日付・時間・ファイルアップロード",
          "Streamlitでのレイアウト調整：カラム",
          "Streamlitでのレイアウト調整：サイドバー",
          "StreamlitでのWebアプリの作成：サイドバーの作成",
          "StreamlitでのWebアプリの作成：DataFrameの抽出、表・グラフの表示",
          "【課題】広告効果の可視化・分析アプリ",
          "【解答】広告効果の可視化・分析アプリ：サイドバーの作成",
          "【解答】広告効果の可視化・分析アプリ：DataFrameの抽出",
          "【解答】広告効果の可視化・分析アプリ：散布図の表示",
          "【解答】広告効果の可視化・分析アプリ：データ分析",
          "【補講】デプロイメント：全体の流れ",
          "【補講】デプロイメント：事前準備",
          "【補講】デプロイメント：デプロイ作業"
        ]
      },
      "requirements": [
        "プログラミングやPythonは未経験でも問題ありません。Pythonについて、このコースで必要な知識は学べるよう、初心者向けの補講のレクチャーも用意しております。",
        "講師はWindowsの環境で解説しておりますが、Mac（M1～M4を除く）でも同様に進めていくことができます。",
        "講師はAnacondaでのPython3環境を構築し、JupyterLabやVS Codeを元に解説を進めておりますが、別のPython3環境でも進めていくことができます。",
        "AnacondaでのPython3の環境構築、JupyterLabやVS Codeの使い方についての講義も提供しております。",
        "なおQ&Aフォームでは、コースで取り扱っていないトピックについてはお答えできませんので、ご理解賜りますようお願い申し上げます。"
      ],
      "description": "近年、ビジネスでプログラムを使ったデータ分析と可視化のスキルがますます重要になっています。例えば、顧客満足度の分析を行うことで、顧客のフィードバックを基にサービス改善点を特定することができます。また、売上トレンドを把握するために、季節ごとの売上変動をグラフ化して予測することが可能です。さらに、マーケティングキャンペーンの効果を可視化することで、どの施策が最も効果的だったかを一目で確認できるようになります。\n\n\nしかし、このスキルを持つ人はまだ少なく、手作業やExcelでのデータ分析や可視化は時間がかかり、ミスもしやすくて、効果的に活用するのが難しいことが多いです。さらに、データ分析と可視化のプログラミングは難しそうなイメージがあり、多忙なビジネスパーソンにとっては学ぶ時間を確保するのも困難です。\n\n\nそこでこのコースでは、初心者でも安心して学べる「Python」を使って、ビジネスに役立つデータ分析と可視化の方法を基礎から解説します。\n\n\n本コースの特徴:\n初心者向けに徹底解説: Pythonの基本を丁寧に説明し、最小限のトピックで最速の習得を目指します。プログラミング未経験の方でもすぐに始められる内容です。\nビジネスに応用可能なスキル: 実際のビジネスにおいて、分析・可視化したいデータや目的は、様々なものがあります。様々なデータや目的に柔軟に対応できるよう、ビジネスでのデータ分析と可視化に必要なポイントと対応方法を幅広く解説します。\n実用的な事例をもとに学習: 実際のビジネスシーンでのデータ分析、可視化の活用例を取り上げ、学んだスキルがどのように役立つかを具体的にイメージできるようにします。\n本コースを通じて、データ分析・可視化にかかる時間を大幅に短縮し、ビジネスの意思決定や成長に必要なデータを効率よく分析・可視化する方法を学ぶことができます。今すぐ始めて、データを活用したビジネスの成長をサポートするスキルを手に入れましょう！\n\n\n【このコースで扱うトピック】\n1. Python超入門\nPython 初心者でもこのコースの内容が理解できるよう、Pythonの基本から丁寧に解説を進めていきます。できるだけ短い時間で理解できるよう、後のデータ分析・可視化に関する内容を理解する上で必要最低限のトピックに絞っています。\n\n\n2. Numpy・Pandas入門\nNumpyやPandasにより、データを処理・集計し、分析するのに必要な基本について学びます。通常、データは、直ぐに分析できる状態にないことが多いです。ここでは、特にデータを処理し、分析できる状態にする方法について学んでいく予定です。\n\n\n3. Pandasによるデータ処理・分析\nそしてデータ分析の基本を学んだ後は、次により実践的なトピックに入っていきます。データをグループ化し集計する方法を学び、次に、複数のデータを結合して分析に利用する方法、ピボットテーブルを用いた分析や時系列データの分析方法についても確認していきます。\n\n\n4. Matplotlib・Pandasによるデータ可視化\nそして、データ分析の次は可視化に入っていきます。データ集計や分析には、グラフによる可視化が不可欠です。生のデータはただの数字の羅列に過ぎず、一目で必要な情報を伝えることは難しいです。しかし、グラフにまとめたり、ダッシュボードにすることで、分析結果が一目で理解でき、必要な情報をすぐに伝えることができます。Pythonのグラフの描画に欠かせないのが、Matplotlibになります。Matplotlibは、Pythonのグラフ描画用ライブラリで、様々なグラフを作成し、データを可視化することができます。\n\n\n5. Plotlyによるインタラクティブなグラフの作成\nPythonでのデータ可視化にはmatplotlibが一般的ですが、見栄えがあまり良くありません。部署を超えて共有したり顧客に見せる場合には、Plotlyのインタラクティブなグラフが適しています。ここでは、Plotlyを使って様々なグラフの作成方法を学びます。\n\n\n6. Streamlitによるデータ分析・可視化用Webアプリの作成\nStreamlitは、データに関するWebアプリを簡単に作成できるフレームワークです。MatplotlibやPlotlyと連携し、高度なグラフを表示し、インタラクティブなレポートを作成できます。これにより、ステークホルダーと効果的に情報を共有し、データ分析の結果をすぐにウェブアプリとして公開できるため、コラボレーションがスムーズに行えます。このコースでは、各支店の売上状況が確認できるWebアプリを作成したり、ある企業の広告効果を可視化・分析するWebアプリを作成します。\n\n\nこのコースでは、これらのデータ分析と可視化に欠かせないトピックをカバーすることで、ビジネスのさまざまな場面で応用できるスキルを身につけられるようデザインされています。",
      "target_audience": [
        "プログラミングによるデータ分析や可視化をビジネスに活用されたい方",
        "データ分析や可視化に興味があるが、始め方がわからない方"
      ]
    },
    {
      "title": "Azure Machine Learning: Da Teoria à Prática",
      "url": "https://www.udemy.com/course/inteligencia-artificial-com-azure-machine-learn/",
      "bio": "Desenvolva suas habilidades para criar e disponibilizar seus próprios modelos de Inteligência Artificial.",
      "objectives": [
        "Aplicar conceitos de aprendizado de máquina usando a plataforma Azure Machine Learning",
        "Aprenderá a criar e treinar modelos de aprendizado de máquina para analisar dados financeiros e fazer previsões.",
        "Explorar técnicas avançadas de aprendizado de máquina, como regressão, classificação, e entenderá como aplicá-las em diferentes cenários.",
        "Estará equipado com habilidades práticas para aplicar aprendizado de máquina em finanças usando a plataforma Azure Machine Learning.",
        "Buscar o preço real de qualquer ativo/ação baseado em inteligência artificial",
        "Criar sua própria IA e disponibilizar o acesso na internet",
        "Aprender como funciona um projeto em C# para tratamento dos dados.",
        "Criar uma API em net core para consulta da Inteligência Artificial."
      ],
      "course_content": {
        "Introdução": [
          "Introdução"
        ],
        "Teoria sobre o aprendizado de máquina": [
          "Aprendizado de maquina",
          "Aprendizado supervisionado",
          "Exemplos práticos",
          "Análise de uma Classificação",
          "Análise de uma Predição",
          "Modelo de dados"
        ],
        "Aprenda a operar com o TradingView: da leitura de gráficos ao Pine Script": [
          "Introdução",
          "Indicadores",
          "Estratégias",
          "Entendendo o Pine Script",
          "Programação com Pine Script",
          "Criação do modelo de Regressão",
          "Criação do modelo de Classificação"
        ],
        "Aprenda a manipular dados com C#: do tratamento à análise": [
          "Verificação do modelo de regressão",
          "Configuração do Ambinente",
          "Criação do Projeto Console",
          "Algoritmo do Modelo de Regressão",
          "Verificação do Modelo de Classificação",
          "Algoritmo do Modelo de Classificação",
          "Enviando Projeto para o Git"
        ],
        "Domine a inteligência artificial na nuvem com o Azure Machine Learning": [
          "Navegando no Azure",
          "Criando Módulo do Azure Machine Learn",
          "Navegando no Azure Machine Learn",
          "Criando a Computação",
          "Executando um Pipeline",
          "Selecionando e Categorizando Colunas",
          "Separando Treinamento / Teste",
          "Treinando Modelo de Classificação",
          "Avaliando Resultado da Classificação",
          "Avaliando Modelo de Classificação",
          "Enviando Nova Versão do Modelo",
          "Publicando Pipeline da Classificação",
          "Criando Pipeline da Regressão",
          "Avaliando Resultado Regressão",
          "Adicionando Indicadores no Modelo",
          "Avaliando Resultado da Regressão",
          "Publicando a Regressão",
          "Modelo de Classificação Automatizado"
        ],
        "Consumindo a IA com .Net Core [Bônus]": [
          "Consumindo a IA pelo Swagger",
          "Revisando o Projeto da API .NET Core"
        ]
      },
      "requirements": [
        "Vontade de aprender e explorar recursos tecnológicos.",
        "Vontade de conhecer como funciona na prática uma Inteligência Artificial para precificar o mercado financeiro",
        "Vontade de conhecer a nuvem da Microsoft"
      ],
      "description": "Este curso completo de Machine Learning para o mercado financeiro inclui um módulo exclusivo sobre como trabalhar com Trading View para análise técnica, incluindo gráficos, indicadores, estratégias e muito mais.\nAo combinar suas habilidades em Machine Learning com o uso do Trading View, você poderá realizar análises de mercado mais precisas e avançadas, identificar tendências e oportunidades de investimento com a ajuda de uma Inteligência artificial.\nPara isso, será demonstrado um cenário de mercado financeiro, onde utilizaremos o Trading View para coletar dados de mercado e, em seguida, utilizar o Azure Machine Learning para criar modelos de previsão de mercado (Classificação e Regressão). Além disso, trataremos os dados usando conceitos básicos de programação em C#, como a leitura de arquivos, a manipulação de dados além da publicação da IA como serviço web.\nMinha experiência como CTO em uma startup, combinada com meus conhecimentos no Azure Machine Learning, me permitem trazer uma perspectiva única e prática para esse curso. Trarei muitos insights e conhecimentos aprendidos nesses anos de experiência.\nInscreva-se agora neste curso e comece a desenvolver suas habilidades para se destacar no mercado onde estará capacitado a criar seus próprios modelos de Machine Learning usando o Azure, utilizando dados do mercado financeiro e aplicando conceitos básicos de programação em C#. Isso permitirá aplicar esses conhecimentos em suas próprias carreiras e projetos, e até mesmo operar no mercado financeiro com mais confiança e habilidade.\nAo final do curso, os alunos estarão capacitados a criar seus próprios modelos de Machine Learning usando o Azure, utilizando dados de diferentes tipos de cenários, não se limitando apenas ao mercado financeiro. Isso permitirá que sejam aplicados esses conhecimentos em diversas áreas de atuação, como saúde, logística, varejo, entre outras, e desenvolvam soluções inovadoras.",
      "target_audience": [
        "Iniciantes que desejam aprender mais sobre finanças e técnicas de aprendizado de máquina.",
        "Desenvolvedores de Pine Script que desejam aprimorar suas estratégias.",
        "Entusiastas de Inteligência Artificial que gostariam de criar modelos para aprendizado de máquina.",
        "Pessoas que desejem conhecer como funciona na prática inteligência artificial da Microsoft.",
        "Realizar investimentos embasados na inteligência artificial."
      ]
    },
    {
      "title": "301A - LTM F5 CERTIFICATION EXAM",
      "url": "https://www.udemy.com/course/301a-ltm-f5-certification-exam/",
      "bio": "301A - LTM F5 CERTIFICATION EXAM",
      "objectives": [],
      "course_content": {
        "Practice Tests": []
      },
      "requirements": [],
      "description": "Certificaction exams helps you develop career-advancement technical skills and a broad understanding of F5 products and solutions, including options emphasizing Management, Sales, Product Specializations, and Solutions, including Cloud and Security. The program is progressive, with high-level certifications based on the skills and knowledge demonstrated in previous certifications.\n\n\nThis course is for people who want to learn about F5 and improve their skills on platforms like BIG-IP, VIPRION and VELOS. During these preparation exams, students will learn in detail how the F5 platform is structured\n\n\nThis practice exam ensures that candidates have the skills and understanding necessary for day-to-day management of Application Delivery Networks (ADNs) and verifies the candidate has the necessary foundation to pursue all further certification tracks.\n\n\nPassing this exam shows independence in performing day-to-day operations and basic troubleshooting of TMOS-based devices in various application environments after it has been installed, configured, and implemented.\n\n\nIndividuals preparing for F5 have knowledge and understanding of technology concepts and standards applicable to the day-to-day operations of F5 products in various environments. These individuals have a basic understanding of the fundamental concepts, configurations, and processes required to maintain a healthy facility, identify problems, and provide basic fault isolation to help senior engineers correct faults.",
      "target_audience": [
        "All students who need to take the LTM certification exam"
      ]
    },
    {
      "title": "ETRA Marathon TWO ماراثون تحليل البيانات",
      "url": "https://www.udemy.com/course/etra-marathon-two/",
      "bio": "Statistics, Study Designs, and R",
      "objectives": [
        "مفاهيم إحصائية متعلقة بالعينة (المتوسط الحسابي, الانحراف المعياري, الوسيط, Q1,Q3)",
        "مفاهيم إحصائية متعلقة بالمجتمع (الفرضية الصفرية والفرضية البديلة, P value, 95% CI)",
        "خارطة طريق لاختيار الاختيار الإحصائي المناسب للعينة التي لديك",
        "أنواع الدراسات العلمية الدراسات الوصفية وأنواعها",
        "Case-Control Study Cohort Study Cross-Sectional Study 2x2 table Odds Ratio Relative Risk",
        "مشاكل الدراسات الوصفية: Chance and errors Bias Confounders",
        "واجهة برنامج R, المكتبات في R, كيفية بناء متغير, كيفية بناء بيانات",
        "تجهيز البانات وتنظيفها الإحصاء الوصفي الرسوم البيانية",
        "الإحصاء الاستنباطي: t test ANOVA Correlation Chi-square OR RR Linear Regression Logistic Regression"
      ],
      "course_content": {
        "الدورة الثانية - الإحصاء للباحثين والأكاديميين - Statistics": [
          "الإحصاء - مصطلحات ومفاهيم",
          "الإحصاء الاستنباطي (الاستدلالي) Inferential Statistics 1",
          "الإحصاء الاستنباطي (الاستدلالي) Inferential Statistics 2",
          "كيف تختار الاختبار الإحصائي المناسب Statistical Tests"
        ],
        "الدورة الثالثة: الدراسات الوصفية في البحوث العلمية - Study Designs": [
          "مصطلحات ومفاهيم - Case Control Study",
          "مصطلحات ومفاهيم - Cohort Study",
          "Cross-sectional + Chance and Error",
          "Biases",
          "Confounders + Causality"
        ],
        "Workshop (R) - الدورة الرابعة: التطبيق العملي": [
          "واجهة البرنامج وصنع المتغيرات",
          "صنع البيانات-الرياضيات-أنواع المتغيرات",
          "تجهيز البيانات وتنظيفها",
          "الإحصاء الوصفي والرسوم البيانية",
          "t.test-ANOVA-Correlation-X2-OR-RR",
          "Linear and Logistic Regression"
        ]
      },
      "requirements": [
        "None"
      ],
      "description": "الماراثون يحتوي التالي:\n- دورة البيانات والرسوم البيانية\n- دورة الإحصاء\n- دورة تصميم الدراسات الوصفية Study Designs\n-ورشة عمل R\n\n\nمحتويات دورة البيانات والرسوم البيانية:\n\n\nماهي البيانات\nماهي المتغيرات وأهميتها\nأنواع المتغيرات\nتجهيز البيانات (إدخال البيانات, تمثيلها واختبار متغيراتها وحفظها)\nماهي الرسوم البيانية\nBar plot\nBox plot\nHistogram\nScatter plot\nتطبيق عملي في R\n\n\nمحتويات الدورة الثانية:\n\n\nهذه الدورة تعتبر حجر أساس لكل باحث ومهتم بالبيانات وتحليلها\n\n\nمحتويات دورة الإحصاء:\n\n\nماهو الإحصاء\n\n\nمفاهيم إحصائية متعلقة بالعينة (المتوسط الحسابي, الانحراف المعياري, الوسيط, Q1,Q3)\n\n\nمفاهيم إحصائية متعلقة بالمجتمع (الفرضية الصفرية والفرضية البديلة, P value, 95% CI)\n\n\nخارطة طريق لاختيار الاختيار الإحصائي المناسب للعينة التي لديك (t.test, ANOVA, Correlation, Chi-sqaure , Regression)\n\n\nمحتويات الدورة الثالثة:\n\n\nهذه الدورة تشرح مفاهيم الدراسات الوصفية في البحوث العلمية\n\n\nمحتويات الدورة:\n\n\nأنواع الدراسات العلمية\nالدراسات الوصفية وأنواعها\nCase-Control Study\nCohort Study\nCross-Sectional Study\n2x2 table\nOdds Ratio\nRelative Risk\nمشاكل الدراسات الوصفية:\nChance and errors\nBias\nConfounders\n\n\nمحتويات الدورة الرابعة:\n\n\nهذه الدورة الرابعة من ماراثون إترا وهي عبارة عن تطبيق عملي في R\n\n\nمحتويات الدورة:\n\n\nواجهة البرنامج\nالمكتبات في البرنامج\nكيفية بناء متغير\nكيفية بناء بيانات\nتجهيز البانات وتنظيفها\nالإحصاء الوصفي\nالرسوم البيانية\nالإحصاء الاستنباطي:\nt.test\nANOVA\nCorrelation\nChi-square\nOR\nRR\nLinear Regression\nLogistic Regression\n\n\n--------------------\nThis course is considered a cornerstone for researchers and those interested in data analysis. This course is the first for the Data Analysis cohort. The Data Analysis cohort includes:\n\n\nData and plots course\nStatistics course\nDescriptive Studies Design course\nR workshop\n* Contents of the data and plots course include the following:\n\n\nWhat are data\nWhat are variables and their importance\nTypes of variables\nData preparation (entering, representing and testing variables, and saving them)\nWhat are graphic representations\nBar plot\nBox plot\nHistogram\nScatter plot\nPractical application in R\n\n\n* Contents of the statistics course:\n\n\nWhat is statistics\nSample-related statistical concepts (computational average, standard deviation, median, Q1, Q3)\nPopulation-related statistical concepts (null hypothesis and alternative hypothesis, P value, 95% CI)\nMap of how to choose appropriate statistical methods for the sample you have (t.test, ANOVA, Correlation, Chi-sqaure , Regression)\n* Contents of the study design course:\n\n\nTypes of scientific studies\nDescriptive studies and their types\nCase-Control Study\nCohort Study\nCross-Sectional Study\n2x2 table\nOdds Ratio\nRelative Risk\nProblems of descriptive studies:\nChance and errors\nBias\nConfounders\n* Contents of R workshop course:\n\n\nThe interface of the program\nLibraries in the program\nHow to build a variable\nHow to build data\nData preparation and cleaning\nDescriptive statistics\nGraphic representation\nInferential statistics: t.test ANOVA Correlation Chi-square OR RR Linear Regression Logistic Regression\nThis is an applied course, where you will practice using the R programming language to perform data analysis tasks and apply statistical methods, including descriptive statistics, graphic representation, and inferential statistics, using R packages and libraries.",
      "target_audience": [
        "للباحثين والأكاديميين والمهتمين بتحليل البيانات والبحث العلمي"
      ]
    },
    {
      "title": "국가공인데이터분석 ADP 자격증 합격하기! 인공지능 데이터분석",
      "url": "https://www.udemy.com/course/adp-ai-e/",
      "bio": "다양한 머신러닝 이론과 실습을 동시에 배우는 최적의 인공지능 코스",
      "objectives": [
        "ADP 자격증 취득과 관련된 내용 전반을 배울 수 있다.",
        "AI 개발 및 분석 실무 노하우를 배울 수 있다.",
        "다양한 머신러닝 이론과 실무 활용할 수 있다.",
        "컴퓨터비전을 배울 수 있다."
      ],
      "course_content": {
        "중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.1": [
          "지도, 비지도, 준지도, 강화",
          "인공지능의 주도권 전환",
          "영상처리 개요",
          "머신러닝 시스템의 종류",
          "머신 러닝의 주요 도전 과제",
          "영상의 획득과 표현 방법",
          "회귀 기반 머신러닝 기초",
          "러닝 기초 데이터 이해를 위한 탐색과 회귀 기반 머신시각화",
          "이미지 프로세싱 기초",
          "컬러 스페이스"
        ],
        "중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.2": [
          "영상의 이진화",
          "적응형 이진화",
          "이미지 연산",
          "영상의 히스토그램",
          "머신러닝을 위한 회귀 개념 및 수학 기초",
          "머신러닝을 위한 기초",
          "수치 예측 머신러닝 시각화",
          "기계학습을 위한 회귀 최종정리와 비정형 데이터마이닝 실전",
          "확률과 베이즈통계학 정리",
          "딥러닝 동작원리"
        ],
        "중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.3": [
          "로지스틱 회귀",
          "KNN",
          "모델 진단(rmse)",
          "CI",
          "의사 결정 나무",
          "로지스틱 회귀",
          "분류모델 평가1",
          "분류모델 평가2",
          "분류모델 평가3",
          "텐서플로 설치",
          "다층퍼셉트론",
          "활성화 함수"
        ],
        "중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.4": [
          "openCV를 위한 넘파이",
          "신경망(퍼셉트론)",
          "활성화함수",
          "ROI",
          "케라스로 함수형 API 사용하여 복잡한 모델 만들기",
          "서브클래스 API로 동적 모델 만들기",
          "가중치 초기화",
          "사전 훈련된 층 재상용하기",
          "텐서플로 활용",
          "텐서플로 데이터 적재와 전처리",
          "언어 모델1",
          "언어 모델2",
          "언어 모델3"
        ]
      },
      "requirements": [
        "데이터과학을 위한 기초 수학과 통계 개념을 알고 계시면 좋습니다."
      ],
      "description": "현직 데이터사이언티스트 전문가가 전하는 AI 개발 및 분석 실무 노하우 및 체계적인 교육을 제공합니다. 회사내에서 머신러닝 및 딥러닝 개발 및 분석 업무로 보직 변경을 하고자 하는 분들은 많은 도움을 받으실 수 있습니다. 다양한 머신러닝 이론을 실습을 통해 체계적으로 학습해가시기 바랍니다. 해당 과정은 아래와 같은 커리큘럼으로 구성되어 있습니다.\n\n\n<교육 내용>\n[중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.1]\n지도, 비지도, 준지도, 강화\n인공지능의 주도권 전환\n영상처리 개요\n머신러닝 시스템의 종류\n머신 러닝의 주요 도전 과제\n영상의 획득과 표현 방법\n회귀 기반 머신러닝 기초\n회귀 기반 머신러닝 기초 데이터 이해를 위한 탐색과 시각화\n이미지 프로세싱 기초\n컬러 스페이스\n\n\n[중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.2]\n영상의 이진화\n적응형 이진화\n이미지 연산\n영상의 히스토그램\n머신러닝을 위한 회귀 개념 및 수학 기초\n머신러닝을 위한 기초\n수치 예측 머신러닝 시각화\n기계학습을 위한 회귀 최종정리와 비정형 데이터마이닝 실전\n확률과 베이즈통계학 정리\n딥러닝 동작원리\n\n\n[중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.3]\n로지스틱 회귀\nKNN\n모델 진단(rmse)\nCI\n의사 결정 나무\n로지스틱 회귀\n분류모델 평가1\n분류모델 평가2\n분류모델 평가3\n텐서플로 설치\n다층퍼셉트론\n활성화 함수\n\n\n[중급개발자를 위한 인공지능 기반 데이터분석 및 컴퓨터비전 기술 (ADP시험대비) Part.4]\nopenCV를 위한 넘파이\n신경망(퍼셉트론)\n활성화함수\nROI\n케라스로 함수형 API 사용하여 복잡한 모델 만들기\n서브클래스 API로 동적 모델 만들기\n가중치 초기화\n사전 훈련된 층 재상용하기\n텐서플로 활용\n텐서플로 데이터 적재와 전처리\n언어 모델1\n언어 모델2\n언어 모델3",
      "target_audience": [
        "데이터분석 관련 분야 취업 및 ADP 자격 취득 희망자"
      ]
    },
    {
      "title": "Practical Data Science made Simple",
      "url": "https://www.udemy.com/course/practical-data-science-made-simple/",
      "bio": "Data science, data",
      "objectives": [
        "Practical of Data Science in Python",
        "Working with Cassandra Database",
        "Working with Power BI",
        "Installation of Anaconda and Jyputer"
      ],
      "course_content": {
        "Practical Data Science": [
          "Overview of Data Science Practical",
          "Prerequisite - Software & VKHG DataSet",
          "Python & Anaconda Installation",
          "Creating Data Model Using Cassandra",
          "TEXT Delimited CSV Format - HORUS",
          "XML to HORUS",
          "JSON to HORUS",
          "Database to HORUS",
          "Image to HORUS",
          "Video to HORUS",
          "Audio to HORUS",
          "Fixer Utilities - Utilities and Auditing",
          "Data Binning or Bucketing - Utilities and Auditing",
          "Averaging Data - Utilities and Auditing",
          "Outlier Detection - Utilities and Auditing",
          "Audit Logging - Utilities and Auditing",
          "Retrieve Attributes - Retrieving Data",
          "Loading IP_DATA_ALL - Retrieving Data",
          "Loading Vermeulen PLC - Retrieving Data",
          "Scheduling Of Jobs - Retrieving Data",
          "Krennwallner AG - Retrieving Data",
          "Online Visitor Data - Retrieving Data",
          "XML Processing - Retrieving Data",
          "Hillman Ltd Warehouse Rules - Retrieving Data",
          "Drop Columns _ All Elements Missing Values - Assessing Data",
          "Drop Columns _ Any Elements Missing Values - Assessing Data",
          "Drop Rows _ Missing 3 or More Values - Assessing Data",
          "Network Routing Diagram in R - Assessing Data",
          "Forecasting - Processing Data",
          "Linear Regression - Transforming Data",
          "Power BI Installation - Power BI",
          "Connecting Excel & Overview - Power BI",
          "Import Data From OData Feed - Power BI",
          "Manage Relationship & Generating Reports - Power BI",
          "Sample Slip1",
          "Sample Slip 2"
        ]
      },
      "requirements": [
        "No Programming experience required"
      ],
      "description": "The fundamentals of data science, exploratory data analysis, statistical methods, the role of data, the Python programming language, the difficulties of bias, variance, and overfitting, selecting the appropriate performance metrics, model evaluation techniques, model optimization using hyperparameter tuning and grid search cross validation techniques, etc. are all covered in this course.\nIn-depth data analysis utilizing Python, statistical methods, exploratory data analysis, and a variety of predictive modeling techniques—including a variety of classification algorithms, regression models, and clustering models—will all be covered in this course. The use cases and situations for implementing predictive models will be covered.\nFor anyone new to Python, this course is a must-have. It goes over Python for Data Science and Machine Learning in great detail.\nWith fully developed projects and examples that walk you through the approaches of exploratory data analysis, model construction, model optimization, and model evaluation, the majority of this course is hands-on.\nThis course goes into great detail on how to teach exploratory data analysis using the Numpy and Pandas libraries. It also covers the Seaborn and Marplotlib Libraries for Visualization creation.\nA lecture on Deep Neural Networks is also included, which includes a worked-out example of Image Classification using TensorFlow and Keras.",
      "target_audience": [
        "Python developer curious about Data Science"
      ]
    },
    {
      "title": "数据挖掘基础实践教程",
      "url": "https://www.udemy.com/course/lmxtbzhp/",
      "bio": "深度剖析数据挖掘方法体系，系统性掌握数据挖掘的关键要领",
      "objectives": [
        "了解数据挖掘的关键技术与应用场景",
        "通过不同的案例讲解，深度学习数据挖掘的建模模型",
        "掌握不同的数据算法，为后续数据建模打下坚实基础",
        "提升数据挖掘的应用认知，实战掌握数据挖掘多样化的业务应用"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "什么是数据挖掘": [
          "为什么会出现数据挖掘技术",
          "数据挖掘存在的价值",
          "数据挖掘究竟是什么",
          "数据挖掘考虑解决的问题",
          "对数据挖掘的常见误解",
          "什么是大数据"
        ],
        "数据挖掘方法论：CRISP-DM": [
          "CRISP-DM概述",
          "CRISP-DM商业理解",
          "CRISP-DM其他理解与准备"
        ],
        "数据挖掘方法体系入门": [
          "统计模型概述",
          "统计模型分类",
          "别忘了统计描述也是战斗力"
        ],
        "数据挖掘方法体系介绍": [
          "回归类模型概述",
          "回归类模型的方法框架",
          "类别预测模型及实现原理",
          "类别预测模型的方法框架",
          "聚类分析概述",
          "聚类模型的方法框架",
          "主成分分析与因子分析",
          "关联分析与序列分析"
        ],
        "文本挖掘入门": [
          "文本挖掘概述",
          "文本挖掘工具",
          "TM工具：R",
          "TM工具：Python"
        ],
        "数据挖掘项目的综合评估": [
          "数据挖掘项目效果的评估",
          "类别预测模型的效果评价",
          "聚类模型的评价",
          "如何改进模型结果",
          "数据挖掘项目失败的原因"
        ],
        "数据挖掘中的软件工具": [
          "数据挖掘软件概述",
          "SAS_EM简介",
          "SAS_EM操作入门",
          "Modeler简介",
          "Modeler操作入门",
          "编程工具简介",
          "R数据挖掘操作入门",
          "sklearn操作入门"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "0基础学员适用"
      ],
      "description": "数据挖掘来源于数据分析，但又完全不同于数据分析，那么他们之间究竟有怎样的联系与区别，数据挖掘和当下很热的大数据之间又有着怎样的联系呢？\n数据挖掘在企业中的实际应用是怎样的？又出现过哪些让人哭笑不得的案例呢？\nCRISP-DM作为最成功的数据挖掘方法论，如何应用呢？\n如何评估一个数据挖掘项目的实施效果，作为动辄数万数十万投入的项目，又有哪些原因可能导致数据挖掘项目最终失败呢？\n本课程为数据挖掘的方法论和分析理念学习，可适用于依赖任何数据挖掘工具的人员\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。\n未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "适合中高级的业务数据分析师进阶学习",
        "适合初中级Python数据分析师树立数据挖掘意识，提升数据挖掘能力",
        "适合数据挖掘领域的初学者或小白入门学习",
        "适合人工智能领域，智能推荐领域的算法工程师建立数据挖掘的方法体系"
      ]
    },
    {
      "title": "Devenez Expert dans la programmation Python",
      "url": "https://www.udemy.com/course/devenez-expert-dans-la-programmation-python/",
      "bio": "Débuter et apprendre a programmer en Python progressivement sans aucun prérequis.",
      "objectives": [
        "Les fondamentaux du langage Python",
        "Créer, gérer ou télécharger des fichiers",
        "Créer vos propres modules et packages",
        "Manipuler les tuples, listes et dictionnaires",
        "Manipuler les chaines de caractères",
        "Utiliser des fonctions de la librairie standard de Python",
        "Apprendre la programmation orientée objet (POO)",
        "L'autonomie en utilisant les ressources destinés au Python"
      ],
      "course_content": {},
      "requirements": [
        "Aucun prérequis"
      ],
      "description": "La maîtrise des fondamentaux de Python est une porte d'entrée incontournable pour tous ceux qui souhaitent explorer le monde de la programmation. Cette formation complète et accessible est conçue pour les débutants absolus et ceux qui cherchent à consolider leurs connaissances en Python.\nCe que vous apprendrez :\nDans cette formation, vous découvrirez pas à pas les bases du langage Python, des concepts élémentaires aux structures de contrôle plus avancées. Vous explorerez les variables, les types de données, les boucles, les fonctions, ainsi que la manipulation de listes, de chaînes de caractères et de dictionnaires.\nEn suivant des exemples concrets et des exercices pratiques, vous apprendrez à résoudre des problèmes et à mettre en pratique vos connaissances fraîchement acquises. De plus, vous comprendrez l'importance des bonnes pratiques de programmation pour écrire un code clair, efficace et lisible.\nPourquoi choisir cette formation :\nPédagogie progressive : Les leçons sont organisées de manière progressive, avec des explications simples et des exemples pratiques pour favoriser une compréhension approfondie des concepts.\nExercices interactifs : Des exercices intégrés vous permettent de mettre immédiatement en pratique ce que vous apprenez, renforçant ainsi votre compréhension et vos compétences en Python.\nSupport personnalisé : Vous bénéficierez d'un support dédié pour répondre à vos questions et clarifier les points difficiles.\nÀ la fin de cette formation, vous serez équipé des connaissances de base nécessaires pour aborder des projets de programmation plus avancés en Python. Prêt à embarquer dans ce voyage captivant vers la maîtrise des bases fondamentales du langage Python ?",
      "target_audience": [
        "Toutes personnes désireuses d'apprendre la programmation"
      ]
    },
    {
      "title": "数据科学和商业分析中的统计学基础",
      "url": "https://www.udemy.com/course/dluvhqmr/",
      "bio": "工作中真正用得到的统计学：描述统计学、推理统计学、假设检验、回归分析",
      "objectives": [
        "理解统计学的基本原理",
        "学习如何使用不同类型的数据",
        "掌握绘制不同类型的数据",
        "掌握集中趋势、不对称性和变异性量度的计算方法",
        "计算相关性和协方差",
        "区分和使用不同类型的分布",
        "估计置信区间",
        "进行假设检验",
        "数据驱动的决策",
        "理解回归分析的机制",
        "进行回归分析",
        "使用和理解虚拟变量",
        "基于Python和R来理解数据科学的相关概念！"
      ],
      "course_content": {},
      "requirements": [
        "不需要任何先学经验，从零基础开始，我们帮您逐步建立相关知识体系。一切都在课程中。",
        "有学习和实践的意愿和热情"
      ],
      "description": "想成为市场分析师、商业智能分析师、数据分析师或数据科学家吗？\n想获得此类工作所需的量化技能吗？\n没错，您来对了！\n《数据科学和商业分析中的统计学基础》正是为此而打造！（还附赠Excel模板哦）\n从这里开始吧，这将是一个完美的起点！\n相信用不了多久，您就能掌握这些基本技能，理解这些复杂的统计分析逻辑。我们打造这门课程的目标是：\n简单易懂\n综合全面\n实际可用\n切中要害\n包含大量练习和资源\n数据驱动\n会向您介绍统计科学术语\n会教你数据可视化\n向您展示量化研究的主要支柱\n本课所包含的大多数主题互联网上都已有很多解释，这不是什么秘密。但是，我们几乎不可能找到一个成体系的内容，给您讲清楚为什么某些统计测试会被如此频繁的使用。现代编程语言和相关程序包正在自动化大多数这些功能，但本课程会为您提供更有价值的东西——批判性思维能力。计算机和编程语言就像海上的船。它们是很好的船只，可以将您带到所需的目的地，但您，作为有远见的数据科学家或商业智能分析师，可以为它们指明正确的前进方向。\n\n\n我们对教学充满热情\n我们为了创建最好的统计学课程，连续工作了几个月，这将为您带来最大的价值。我们希望您取得成功，这就是本课为什么要制作的非常有趣的原因。高质量的动画、一流的课程材料、测验问题、讲义和课程笔记，以及包含学习会涉及到的新术语的词汇表，而且这也只是您加入本课程后的一部分好处。\n\n\n这门课程与其他统计学课程有何不同？\n高品质的制作——高清视频和动画（这不是无聊讲座的集合！）\n知识渊博的老师（一位曾参加过国际竞赛水平的数学家和统计学家）\n完整的课程结构——我们将涵盖成为市场分析师、商业智能分析师、数据分析师或数据科学家所需的所有主要统计学主题和技能\n广泛的案例研究将帮助您巩固所学的一切\n出色的支持 - 如果您对某个概念不了解或只是单纯想给我们留个言，我们将会在1个工作日内给您答复\n动态的教学节奏 - 不想浪费时间？讲师已在整个课程中设定了非常好的节奏\n\n\n为什么需要这些技能？\n薪水/收入 —— 数据科学领域的职业是当今最受企业欢迎的职业之一。大多数企业开始意识到可支配数据能带来的竞争优势，而且这种趋势会持续增长。\n升职——如果您建立了对统计学的良好理解，您就能够用量化的证据来支撑自己的商业创意，毋庸置疑这将会是一条升职捷径。\n有保障的未来——如上所言，对理解数字和数据并能够解释它的人，企业的需求正在呈指数增长；您可能还听说过即将被自动化浪潮所取代的工作，对吧？是的，数据科学职业是那些设计自动化的人，而不是被自动化所取代的人。\n成长——这不是一份无聊的工作。每天，您都会面临不同的挑战，这些挑战将考验您现有的技能，并要求您学习新的东西。\n请记住，该课程附带Udemy的30天无条件退款保证。为什么我们敢作出这样的承诺呢？因为我们坚信这门课程将为您提供巨大的价值。\n\n\n点击“立即购买”，让我们今天就开始学习吧！",
      "target_audience": [
        "想要从事数据科学职业方向的人",
        "想要从事商业智能职业方向的人",
        "业务分析师",
        "业务主管",
        "对数字和量化分析有兴趣的人",
        "想了解统计学的微妙之处以及它在商业世界中的使用方式的人",
        "想系统学习统计学的人",
        "想掌握统计学基础的人"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第18部 如何訓練DeepSeek大語言模型",
      "url": "https://www.udemy.com/course/generative_ai_18/",
      "bio": "關於DeepSeek，RoPE，MoE，MLA，MHA，Transformer，Attention，Positional Embedding，SoftMax，Logits",
      "objectives": [
        "DeepSeek的發展與技術特點",
        "如何使用 Python 編寫MLA & RoPE",
        "如何用Python編寫MLA的各層和結構",
        "如何用Python編寫混合專家架構",
        "如何實現訓練和編寫DeepSeekModel"
      ],
      "course_content": {},
      "requirements": [
        "一台電腦"
      ],
      "description": "掌握AI未來：用Python代碼拆解DeepSeek-R1，開啓低成本大模型訓練之旅——從理論到實踐，手把手教你成為下一代AI開發者\nDeepSeek-R1：一場全球AI競賽的顛覆性突破\n2025年初，中國AI企業DeepSeek憑借開源模型DeepSeek-R1震撼全球，其性能與OpenAI的頂級推理模型o1旗鼓相當，但成本僅為後者的1/40！這一突破不僅讓亞馬遜、微軟等科技巨頭爭相接入，更引發美國政界與科技界的雙重震動\n\n\n課程包括內容：\n\n\nDeepSeek的發展與技術特點(DeepSeek V1, DeepSeek V2, DeepSeek V3, DeepSeek R1-Zero, DeepSeek R1, )\n神經網絡Neural Networks的組成和原理\n前饋神經網絡的原理&數據加載(輸入層，隱藏層， Softmax，Logits，激活函數，輸出層)\n如何用Python訓練前饋神經網絡(tensor, epoch)\n什麼是Transformer & Attention & MHA & GQA & MQA(Transformer, Attention, Positional Embedding)\n什麼是MLA & RoPE & MoE\n用Python編寫MLA的各層和結構\n如何添加RotaryEmbedding到MLA\n如何用Python編寫混合專家架構\n如何實現訓練和編寫DeepSeekModel\n如何實現 Transformer(Encoder, Decoder)",
      "target_audience": [
        "對資料科學感興趣的Python 開發人員",
        "對 Deepseek 感興趣的學員"
      ]
    },
    {
      "title": "[FR] Certificat d’Explorateur en Ingénierie de l’IA",
      "url": "https://www.udemy.com/course/fr-certificat-dexplorateur-en-ingenierie-de-lia/",
      "bio": "Construisez vos bases en IA avec Python, Data Science, Mathématiques et les fondamentaux du Machine Learning",
      "objectives": [
        "Écrire du code Python propre pour des applications d'IA en utilisant des variables, des boucles, des fonctions et la POO",
        "Analyser et manipuler des données avec Pandas et NumPy",
        "Visualiser des ensembles de données avec Matplotlib et Seaborn",
        "Comprendre les concepts mathématiques fondamentaux comme l’algèbre linéaire et le calcul pour l’IA",
        "Appliquer la théorie des probabilités et les statistiques à la résolution de problèmes d’IA",
        "Expliquer le fonctionnement et l'entraînement des modèles d'apprentissage automatique",
        "Construire et évaluer des modèles de ML de base avec Scikit-learn",
        "Développer une base solide pour aborder des sujets avancés en IA et ML"
      ],
      "course_content": {
        "Introduction au cours et à l’instructeur": [
          "Ce que vous apprendrez dans le Certificat d’Explorateur en Ingénierie de l’IA"
        ],
        "Bases de la programmation Python pour l’intelligence artificielle": [
          "Jour 1 : Introduction à Python et configuration de l’environnement",
          "Jour 2 : Structures de contrôle en Python",
          "Jour 3 : Fonctions et modules",
          "Jour 4 : Structures de données (Listes, Tuples, Dictionnaires, Ensembles)",
          "Jour 5 : Manipulation des chaînes de caractères",
          "Jour 6 : Gestion des fichiers",
          "Jour 7 : Code pythonique et projet pratique",
          "Ressources pour les projets"
        ],
        "Fondamentaux de la science des données pour l’intelligence artificielle": [
          "Jour 1 : Introduction à NumPy pour le calcul numérique",
          "Jour 2 : Opérations avancées avec NumPy",
          "Jour 3 : Introduction à Pandas pour la manipulation des données",
          "Jour 4 : Nettoyage et préparation des données avec Pandas",
          "Jour 5 : Agrégation et regroupement des données avec Pandas",
          "Jour 6 : Visualisation de données avec Matplotlib et Seaborn",
          "Jour 7 : Projet d’analyse exploratoire des données (EDA)",
          "Ressources pour les projets"
        ],
        "Mathématiques pour le Machine Learning et l’intelligence artificielle": [
          "Jour 1 : Fondamentaux de l’algèbre linéaire",
          "Jour 2 : Concepts avancés en algèbre linéaire",
          "Jour 3 : Calcul différentiel pour le Machine Learning (Dérivées)",
          "Jour 4 : Calcul intégral et optimisation pour le ML",
          "Jour 5 : Théorie des probabilités et distributions",
          "Jour 6 : Fondamentaux en statistiques",
          "Jour 7 : Mini-projet mathématique – Régression linéaire de zéro",
          "Ressources pour les projets"
        ],
        "Probabilités et statistiques pour le Machine Learning et l’IA": [
          "Jour 1 : Théorie des probabilités et variables aléatoires",
          "Jour 2 : Distributions de probabilité en Machine Learning",
          "Jour 3 : Inférence statistique – Estimation et intervalles de confiance",
          "Jour 4 : Tests d’hypothèse et valeurs p",
          "Jour 5 : Types de tests d’hypothèse",
          "Jour 6 : Corrélation et analyse de régression",
          "Jour 7 : Projet d’analyse statistique – Analyse de données réelles",
          "Ressources pour les projets"
        ],
        "Introduction au Machine Learning": [
          "Jour 1 : Bases et terminologie du Machine Learning",
          "Jour 2 : Introduction à l’apprentissage supervisé et aux modèles de régression",
          "Jour 3 : Modèles de régression avancés – Régression polynomiale et régularisatio",
          "Jour 4 : Introduction à la classification et régression logistique",
          "Jour 5 : Évaluation des modèles et validation croisée",
          "Jour 6 : Algorithme des k plus proches voisins (k-NN)",
          "Jour 7 : Mini-projet en apprentissage supervisé",
          "Ressources pour les projets"
        ],
        "Quiz final et félicitations": [
          "Quiz final",
          "Félicitations et bonne continuation"
        ]
      },
      "requirements": [
        "Aucune expérience préalable en programmation ou en IA n’est requise — ce cours est adapté aux débutants",
        "Un ordinateur (Windows, macOS ou Linux) avec un accès à Internet",
        "Envie d’apprendre et d’expérimenter de nouveaux concepts",
        "Connaissances de base en mathématiques de niveau lycée (l’algèbre et l’arithmétique sont utiles mais pas obligatoires)",
        "Capacité à installer des logiciels comme Python, Jupyter Notebook et les bibliothèques nécessaires (nous vous guiderons étape par étape)",
        "Curiosité pour le fonctionnement de l’IA et passion pour la résolution de problèmes",
        "Engagement à suivre les leçons et à réaliser les exercices pratiques",
        "Facultatif : Un cahier ou un outil de prise de notes numérique pour noter les idées clés et les formules"
      ],
      "description": "Prêt à faire vos premiers pas dans l’intelligence artificielle et à devenir ingénieur en IA ?\nLe Certificat d’Explorateur en Ingénierie de l’IA est votre porte d’entrée vers le monde passionnant et en pleine croissance de l’IA, du machine learning et de la science des données. Conçu pour les débutants, ce cours pratique vous donne les compétences fondamentales nécessaires pour commencer votre parcours en tant que développeur IA ou concepteur de produits IA.\nDans ce cours, vous commencerez par les bases de la programmation Python pour l’IA. Python est aujourd’hui le langage le plus populaire dans le domaine de l’IA. Vous apprendrez à écrire un code propre, à comprendre les variables, les boucles, les fonctions et la programmation orientée objet — posant ainsi les bases pour créer de vraies applications d’IA.\nEnsuite, vous explorerez les fondamentaux de la science des données pour l’IA, avec des modules sur le prétraitement des données, la visualisation et l’analyse exploratoire des données (EDA) à l’aide de Pandas, NumPy et Matplotlib. Maîtriser les données est essentiel en IA, et cette section vous assure une expérience pratique et prête à l’emploi.\nPuis, vous maîtriserez les mathématiques pour le machine learning et l’IA — un pilier central pour tout professionnel sérieux de l’IA. Nous abordons les bases de l’algèbre linéaire, du calcul différentiel et des opérations matricielles de manière intuitive et orientée vers l’application, pour développer une pensée analytique solide.\nVous étudierez également les probabilités et statistiques pour le machine learning, un domaine crucial pour comprendre comment les modèles d’IA apprennent à partir des données. Vous découvrirez le théorème de Bayes, les distributions, l’écart type, les intervalles de confiance et les tests d’hypothèses — le tout illustré par des exemples centrés sur l’IA pour faciliter la compréhension.\nEnfin, vous ferez vos premiers pas dans le monde du machine learning. Dans l’introduction au machine learning, vous apprendrez comment fonctionnent les algorithmes comme la régression linéaire, la classification et le clustering. Vous utiliserez Scikit-learn pour entraîner et évaluer des modèles simples, acquérant une expérience concrète de la création de pipelines de machine learning.\nÀ la fin du Certificat d’Explorateur en Ingénierie de l’IA, vous aurez une compréhension solide des concepts clés de l’IA et serez prêt à aborder des sujets plus avancés comme le deep learning, le traitement du langage naturel (NLP) et le développement de produits IA. Que vous soyez étudiant, développeur, en reconversion ou passionné de tech, ce cours vous offre un parcours structuré et accessible pour construire vos bases en IA.\n- Aucune expérience préalable requise\n- Projets pratiques inclus\n- Certificat de réussite\n- Parfait pour les débutants en IA, les aspirants data scientists et les futurs chefs de produit IA\nFaites le premier pas vers l’avenir — rejoignez des milliers d’apprenants et commencez votre parcours pour devenir ingénieur en IA certifié dès aujourd’hui.",
      "target_audience": [
        "Futurs ingénieurs en IA et data scientists débutant de zéro",
        "Chefs de produit et responsables techniques souhaitant comprendre les bases de l’IA",
        "Étudiants se préparant à des programmes avancés en IA ou en apprentissage automatique",
        "Professionnels en reconversion vers des rôles axés sur l’IA",
        "Toute personne souhaitant explorer le monde de l’IA sans expérience préalable en programmation ou en science des données"
      ]
    },
    {
      "title": "AI 商业设计实战：搞定高单价的主题系列作品",
      "url": "https://www.udemy.com/course/ai-stvvg/",
      "bio": "本课程将深入剖析Stable Diffusion的精髓，从基本原理到模型架构，再到训练方法，为学员呈现一个完整的学习路径。",
      "objectives": [
        "学员将掌握AI在商业设计中的应用技巧",
        "掌握主题系列作品的设计原理",
        "将了解当前市场上高单价主题系列作品的流行风格和受众喜好",
        "将学习到如何通过实战演练来提升自己的设计水平"
      ],
      "course_content": {
        "产品概述": [
          "产品概述：AI 绘画主流产品比较",
          "产品概述：本地环境搭建",
          "环境搭建： S D云端环境搭建",
          "界面认识：全面认识界面功能介绍",
          "常用插件：常用重要插件安装指南",
          "产品概述：底层技术原理"
        ],
        "基础功能介绍": [
          "模型认识：大模型和VAE模型",
          "模型认识：如何提高模型性能和表达能力",
          "模型认识：如何用图像训练模型？",
          "提示词基础：数量与权重",
          "提示词进阶：语法与案例",
          "图像去燥原理：采样方法和采样步数",
          "图像去燥实践：面部修复和高清修复",
          "参数设置：如何调整图像生成的差异度？",
          "基础功能：如何根据信息反向生成图片？",
          "提示词矩阵：精准输入提高图像生成质量",
          "ControlNet模型：实现固定画面的构图",
          "ControlNet模型：界面功能大介绍",
          "ControlNet模型：标志化命名规则",
          "基础功能：如何使用深度信息控制出图结果？",
          "基础功能：如何通过上传图片来增加出图的可控性？",
          "基础功能：如何完成图像局部重绘？",
          "基础功能：如何设置图像局部重绘参数？"
        ],
        "商业实战案例": [
          "商业化案例：如何将老照片修复成高清照片？",
          "商业化案例：盲盒IP设计实战",
          "商业化案例：如何设计一款运动鞋？",
          "商业化案例：电商产品场景图实战",
          "商业化案例：如何生成不同风格、精度的图标？",
          "商业化案例：如何对图片进行局部调整？",
          "商业化案例：如何实现一键批量自动抠图？",
          "商业化案例：如何拓展图片尺寸并添加新的内容？",
          "商业化案例：如何设计一个角色的正背侧三视图？",
          "商业化案例：如何输出无版权风险高相似度的图片？",
          "商业化案例：如何设计完整的游戏卡牌？",
          "商业化案例：如何生成不同状态的表情包？",
          "商业化案例：如何制作极具艺术感的海报字体？"
        ],
        "最新功能": [
          "最新功能：XL1.0模型前瞻",
          "最新功能：XL1.0模型初体验",
          "最新功能：XL1.0模型风格讲解",
          "最新功能：XL1.0模型本地部署"
        ]
      },
      "requirements": [
        "无需经验"
      ],
      "description": "本课程将深入剖析Stable Diffusion的精髓，从基本原理到模型架构，再到训练方法，为学员呈现一个完整的学习路径。通过大量的实际案例和项目练习，学员将能够亲自动手，用简单的文本描述或少量参考图像，生成令人惊叹的三维场景和精细细节。这不仅仅是一次技术的学习，更是一次创意的释放。学员将掌握图像生成、风格转换、图像修复和超分辨率等商业创作任务的核心技能，为未来的职业发展奠定坚实基础。此外，课程还将分享众多成功的商业案例和创意应用，让学员领略Stable Diffusion在品牌广告设计、插画创作、时尚产业、电影特效、电商数字应用等领域的广阔前景。",
      "target_audience": [
        "职业设计师和创意工作者",
        "设计爱好者与转行人员"
      ]
    },
    {
      "title": "اساسيات الذكاء الاصطناعي من الصفر",
      "url": "https://www.udemy.com/course/aibasics/",
      "bio": "خذ مكان الالة قبل ان تأخذ مكانك وابدء من الان",
      "objectives": [
        "كورس اساسيات تعلم الذكاء الاصطناعي من الصفر",
        "لابد من تعلم الذكاء الاصطناعي",
        "يا تاخد مكان المكنة او المكنة تاخد مكانك",
        "ابدء طريقك نحو تعلم شيئ عظيم",
        "فقط تحتاج ان تبدء"
      ],
      "course_content": {
        "البداية": [
          "ما هو الذكاء الاصطناعي",
          "ما هو الذكاء الاصطناعي",
          "خريطة الكورس",
          "الذكاء الاصطناعي"
        ],
        "تعلم الالة": [
          "ما هو تعلم الالة",
          "لماذا تعلم الالة مهم ؟"
        ],
        "انواع تعلم الالة": [
          "ما هم الانواع ؟",
          "اول نوع من تعلم الالة",
          "الفرق بين Linear - Nonlinear",
          "اول مفهوم في التعلم باشراف",
          "ثاني مفهوم في التعلم باشراف",
          "ثاني نوع من تعلم الالة",
          "اول مفهوم في التعلم بدون اشراف",
          "ثاني مفهوم في التعلم بدون اشراف",
          "ثالث نوع من تعلم الالة",
          "رابع نوع من تعلم الالة"
        ],
        "التعلم العميق": [
          "ما هو التعلم العميق",
          "انواع التعلم العميق",
          "الخلية العصبية",
          "ماهم وظائف التنشيط",
          "تطبيقات التعلم العميق",
          "اول نوع من التعلم العميق",
          "ثاني نوع من التعلم العميق"
        ],
        "اللغة الطبيعية": [
          "ما هي اللغة الطبيعية",
          "الخطوات الاساسية في اللغة الطبيعية",
          "اول خطوة",
          "الفرق بين البرمجة التقليدية و البرمجة باللغة الطبيعية لفصل الكلمات",
          "ثاني خطوة",
          "ثالث خطوة",
          "رابع خطوة",
          "خامس واخر خطوة"
        ],
        "مكتبة CV": [
          "Computer Vision ماهو",
          "Part 2 for Computer Vision"
        ],
        "نهاية الكورس": [
          "الخاتمة",
          "AI question",
          "branch of AI",
          "AI component"
        ],
        "رائيك": [
          "ما رائيك في الدورة"
        ]
      },
      "requirements": [
        "لا تحتاج لاي شيئ, فقط ابدء الان",
        "في هذا الكورس ابدء معك من الصفر بالفعل ولا تحتاج لاي كورس لكي تبدء معي"
      ],
      "description": "اهلا بك في كورس اساسيات الذكاء الاصطناعي من الصفر\nتريد ان تبدء في مجال من مجالات الذكاء الاصطناعي ولم تعرف اي بداية مناسبة لك\nالكورسات بتدخل علي شرح الكود علطول\nولم تجد كورس يبدء من الصفر بالاساسيات بدون كود\nاذا انت في المكان الصحيح .\nخذ مكان الالة قبل ان تأخذ مكانك وابدء من الان\nهذا الكورس يبدء من الصفر لفهم مجالات الذكاء الاصطناعي ويضم مفاهيم اساسية ضرورية في سوق العمل\nالكورس مختصر بدون حشو, تاخذ خلاصة الموضوع بالصور والامثلة الحياتيه في الحياه اليوميةالشرح هيكون بدون كود لسهولة الفهم ولعدم التعقيد لانك في بداية تعلمك لهذا المجال\nالشرح هيكون بالعربي وجزء بسيط بالانجليزية لضمان وصول المعني والمقصود من شرح جزء معين\nأن تبدأ متأخرا خير من ألا تبدأ\nفقط تحتاج ان تاخذ الخطوة وتبدء التعلم\nجاءت لك الفرصة لاتقوم بتضيعها\nانتظرك باذن الله تعالي\nنحن في عالم الناس يفقدون وظائفهم بسبب التطورات التي تحدث في العالم كله من تطورات مثل شات بوت شات جي بي تي وميد جيرني وغيره من التطورات\nChat GPT - Mid journey\nالغرض من انك تطور نفسك لكي تكون انت من تبرمج الاله لكي يكون لك شأنك في هذه التطورات في شتي المجالات\nومن المجالات التي سوف تقل بنسبه كبيرة وتنعدم هي الرد علي العملاء , ف هحاول تذاكر وتشتغل علي نفسك بحيث يكون لك مكان",
      "target_audience": [
        "هذا الكورس لاي شخص يريد ان يكون لديه اساس قوي في مجالات الذكاء الاصطناعي",
        "بداية في عالم الذكاء الاصطناعي من لاشيئ الي معرفه اشياء كثيرة"
      ]
    },
    {
      "title": "Регулярные выражения в Python",
      "url": "https://www.udemy.com/course/ittensive-python-regular-expressions/",
      "bio": "Шаблоны символов и позиций, квантификаторы и группы, re.match, finditer, split и sub",
      "objectives": [
        "Структура регулярного выражения",
        "Шаблоны символов и позиций",
        "\"Жадные\" и \"ленивые\" квантификаторы",
        "Скобочные группы и перечисления",
        "Модуль re: match, fiinditer, split, sub",
        "Сложные шаблоны позиций",
        "Аббревиатуры и повторы",
        "Конечные автоматы"
      ],
      "course_content": {
        "Синтаксис": [
          "Структура регулярного выражения",
          "Шаблоны символов и диапазонов",
          "Квантификаторы",
          "«Жадные» алгоритмы",
          "Простые регулярные выражения",
          "Скобочные группы и перечисления",
          "Шаблоны позиции",
          "Сложные шаблоны позиции",
          "Порядок правителей России",
          "Скобочные группы",
          "Разбор автомобильных номеров"
        ],
        "Регулярные выражения в Python": [
          "Модуль re",
          "Нахождение подстрок и групп",
          "Аббревиатуры",
          "Модуль re",
          "findall и split",
          "Группы при заменах",
          "Удаление повторов",
          "Группировка",
          "Форматирование номера телефона"
        ],
        "Заключение": [
          "Конечные автоматы",
          "Поиск e-mail в тексте"
        ]
      },
      "requirements": [
        "Базовый Python"
      ],
      "description": "На этом курсе вы освоите работу со строками при помощи регулярных выражений - формального языка работы с подстроками в тексте на основе метасимволов. В курсе изучается синтаксис регулярных выражений и базовые конструкции для поиска и замены повторяющихся в строках выражений, используя шаблоны.\nБольшое внимание уделено практической отработке навыков на примере модуля re в Python и его методов - match, finditer, split и sub.",
      "target_audience": [
        "Программисты на Python",
        "Программисты на других языках, интересующиеся работой с текстом",
        "Бизнес-аналитики, автоматизирующие процессы"
      ]
    },
    {
      "title": "Sabermetric: Data science en Baseball con Rstudio y Power BI",
      "url": "https://www.udemy.com/course/sabermetric-data-science-en-baseball-con-rstudio-y-power-bi/",
      "bio": "Aprende a aplicar análisis de datos del Baseball y Sabermetría con R y elabora informes avanzados del MLB con Power BI.",
      "objectives": [
        "De sabermetría en el Baseball.",
        "Acerca de la sintaxis de Scripts en R.",
        "A realizar análisis exploratorios de datos en R.",
        "Creación de Slides usando Rmarkdown.",
        "A modelar datos en Power Query.",
        "A crear informes avanzados en Microsoft Power BI."
      ],
      "course_content": {
        "Introducción": [
          "Presentación del Curso",
          "¡No califiques el curso antes de tiempo!"
        ],
        "Análisis Exploratorio de Datos del Baseball con RStudio": [
          "Script Rmarkdown",
          "Slides Iniciales",
          "Slides equipos de la Liga Nacional",
          "Slides equipos de la Liga Americana",
          "Slides Series Mundiales",
          "Slides indicadores Ganados y Perdidos",
          "Slides expectativa triunfos vs triunfos",
          "Slides Premios a Jugadores",
          "Slides Premios MVP y TC",
          "Principales Indicadores Sabermétricos"
        ],
        "Modelado de los Datos en Power Query de Power BI": [
          "Conexión a los Datos",
          "Transformación de los Datos",
          "Tabla Temporadas en Power Query",
          "Tabla Imágenes de los Logos de los Equipos de MBL"
        ],
        "Resumen Estadístico por Equipos del Baseball": [
          "Tabla Box Score de los Equipos",
          "Cálculo de Slugging por Equipo",
          "Cálculo OBP y AVG por Equipos",
          "Filtros por Temporadas y Título dinámico",
          "Tarjeta de indicadores",
          "Tabla Ligas y Series Mundiales Ganadas",
          "Series Mundiales Ganadas por Liga"
        ],
        "Resumen Estadístico a la Ofensiva (Batting)": [
          "Box Score Batting - Slugging",
          "Batting BABIP - OPS - BB%",
          "Batting - Tarjetas de Indicadores generales",
          "Premios a Jugadores"
        ],
        "Resumen Estadístico a la Defensiva (Fielding)": [
          "Box Score Fielding",
          "Gráficos Fielding"
        ],
        "Resumen Estadístico de Pitcheo (Pitching)": [
          "Box Score Pitching",
          "Tarjeta de indicadores de Pitching",
          "Gráficas con datos de Pitching"
        ],
        "Publicación del Informe al Servicio Web de Power BI": [
          "Publicación del informe al Servicio de Power BI"
        ]
      },
      "requirements": [
        "Ayuda mucho el conocimiento previo de Baseball, R y Power BI."
      ],
      "description": "La Sabermetría ha cambiado la forma de entender el Baseball, llevando el análisis mucho más allá que el tradicional análisis estadístico que se realizaba, creando nuevas métricas que facilitan el contexto para responder muchas interrogantes acerca de los resultados del Baseball e intentar predecir lo que puede suceder en el futuro.\nLa explosión en el uso de la Sabermetría llegó con el libro “Moneyball: El arte de ganar un juego injusto” del año 2003 cuyo éxito quedó demostrado con la filmación de la exitosa película del mismo nombre en el 2011. En la actualidad todos los equipos de las grandes ligas del Baseball cuentan con equipos de analistas sabermétricos en su staff, situación que era un sueño apenas para Bill James su creador.\nEl avance tecnológico informático y el desarrollo del Data Science y del Business Intelligence, ha facilitado que pueda ponerse en práctica la Sabermetría de forma rápida y dinámica, en este caso en particular vamos a utilizar el software estadístico R con su IDE RStudio para explorar los datos del paquete Lahman que contiene los datos históricos del mejor Baseball del mundo.\nTambién vamos a traer esos datos al poderoso Microsoft Power BI para crear un avanzado informe analítico para mostrar indicadores sabermétricos y mucha más información en grandes Dashboards de información.\nSe trata de un interesante curso para aquellas personas que trabajan con modelado y análisis de datos y especialmente con datos deportivos, también orientados en aquellos Data Scientist y usuarios de Power BI.",
      "target_audience": [
        "Apasionados del análisis de datos deportivos.",
        "Interesados en Ciencia de Datos.",
        "Interesados en modelado de datos con Power Query.",
        "Interesados en aprender Microsoft Power BI.",
        "Investigadores y analistas deportivos."
      ]
    },
    {
      "title": "SQL- Formation complète de A à Z sous MySQL",
      "url": "https://www.udemy.com/course/sql-formation-complete-sous-mysql/",
      "bio": "Formation complète avec des explications approfondies sur les notions de bases du SQL",
      "objectives": [
        "Les bases du DDL : Création et gestion des objets au niveau d'une base de données MySQL",
        "Installation et configuration de MySQL",
        "Les bases du DML : INSERT, DELETE et UPDATE",
        "Requêtes SQL pour interroger une base de données (SELECT)",
        "Jointure des tables"
      ],
      "course_content": {
        "Introduction": [
          "Introduction",
          "SGBDR",
          "Terminologies",
          "Fonctions SQL",
          "Fonctions SQL : DDL et DML",
          "MySQL",
          "Installation MySQL"
        ],
        "Les types de données": [
          "Introduction",
          "Type numérique",
          "Type Date",
          "Type Caractères"
        ],
        "Les Opérateurs": [
          "Introduction",
          "Opérateurs arithmétiques",
          "Opérateurs de comparaisons",
          "Opérateurs logiques"
        ],
        "DDL": [
          "Introduction",
          "DDL - Base de données",
          "DDL - Tables",
          "DDL -> Tables -> Création (CREATE)",
          "DDL -> Tables -> Création - Exemple",
          "DDL -> Tables -> Création - Exemple (Violation contrainte)",
          "DDL -> Tables -> Suppression (DROP)",
          "DDL -> Tables -> Modification (ALTER)",
          "DDL -> Tables -> Modification -> Ajout",
          "DDL -> Tables -> Modification -> Ajout - Exemple 1",
          "DDL -> Tables -> Modification -> Ajout - Exemple 2",
          "DDL -> Tables -> Modification -> Suppression",
          "DDL -> Tables -> Modification -> Suppression - Exemple 1",
          "DDL -> Tables -> Modification -> Suppression - Exemple 2",
          "DDL -> Tables -> Modification -> Modification",
          "DDL -> Tables -> Modification -> Modification - Exemple 1",
          "DDL -> Tables -> Modification -> Modification - Exemple 2",
          "DDL -> Tables -> Modification -> Modification - Exemple 3",
          "DDL - Contraintes - NOT NULL",
          "DDL - Contraintes - UNIQUE",
          "DDL - Contraintes - CHECK",
          "DDL - Contraintes - DEFAULT",
          "DDL - Contraintes - Primary key",
          "DDL - Contraintes - Foreign key",
          "DDL - Contraintes - Foreign key - Exemple 1",
          "DDL - Contraintes - Foreign key - Exemple 2",
          "DDL - Contraintes - Foreign key - Exemple 3"
        ],
        "DML": [
          "Introduction",
          "DML - INSERT",
          "DML - INSERT - EXEMPLE 1",
          "DML - INSERT - EXEMPLE 2",
          "DML - INSERT - EXEMPLE 3",
          "DML - UPDATE",
          "DML - UPDATE - EXEMPLE 1",
          "DML - DELETE",
          "DML - DELETE - EXEMPLE 1",
          "DML - SELECT",
          "DML - SELECT - Mise en place de la BDD",
          "DML - SELECT - EXEMPLE",
          "DML - SELECT -> DISTINCT",
          "DML - SELECT -> DISTINCT - Exemple",
          "DML - SELECT -> CREATE Table",
          "DML - SELECT -> CREATE Table - EXEMPLE",
          "DML - SELECT -> GROUP BY",
          "DML - SELECT -> GROUP BY EXAMPLE 1",
          "DML - SELECT -> HAVING",
          "DML - SELECT -> ORDER BY",
          "DML - SELECT -> ORDER BY - EXEMPLE",
          "DML - SELECT -> WHERE",
          "DML - SELECT - WHERE -> AND, OR et NOT",
          "DML - SELECT - WHERE -> AND, OR et NOT - EXEMPLE 1",
          "DML - SELECT - WHERE -> AND, OR et NOT - EXEMPLE 2",
          "DML - SELECT - WHERE -> IN et NOT IN",
          "DML - SELECT - WHERE -> IN et NOT IN - EXEMPLE",
          "DML - SELECT - WHERE -> BETWEEN et NOT BETWEEN",
          "DML - SELECT - WHERE -> BETWEEN et NOT BETWEEN - EXEMPLE",
          "DML - SELECT - WHERE -> LIKE et NOT LIKE",
          "DML - SELECT - WHERE -> LIKE et NOT LIKE - EXEMPLE",
          "DML - SELECT - LIMIT",
          "DML - SELECT - LIMIT -EXEMPLE",
          "DML - Sous-requête - dans le WHERE",
          "DML - Sous-requête - dans le WHERE - EXEMPLE",
          "DML - Sous-requête -> Retourne un seul résultat",
          "DML - Sous-requête -> Retourne un seul résultat - EXEMPLE",
          "DML - Sous-requête -> Retourne une colonne",
          "DML - Sous-requête -> Retourne une colonne - EXEMPLE",
          "DML - Sous-requête - dans le FROM",
          "DML - Sous-requête - dans le FROM - EXEMPLE"
        ],
        "Jointures": [
          "Introduction",
          "INNER JOIN",
          "INNER JOIN - EXEMPLE",
          "LEFT/RIGHT JOIN",
          "EXEMPLE LEFT JOIN"
        ],
        "FONCTIONS": [
          "Agrégation",
          "Agrégation - EXEMPLE",
          "Chaînes de caractères",
          "Chaînes de caractères - EXEMPLE",
          "Dates",
          "Dates - EXEMPLE"
        ]
      },
      "requirements": [
        "Connaissance MERISE est souhaitable mais pas fondamentale",
        "Aucune connaissance en SQL où MySQL n'est requise"
      ],
      "description": "Cette formation est destinée aux apprentis qui n’ont pas d’expérience où une première expérience en base de données et SQL et qui cherchent à développer une maîtrise des requêtes SQL. Grâce aux différentes sections, on couvrira des sujets tels que les bases SQL, les bases des instructions de définitions de données (Création, suppression des bdd et tables et différentes contraintes) et finalement les instructions de manipulation de données ((SELECT, INSERT, UPDATE et DELETE).\nVous en apprendrez plus sur les requêtes de tables individuelles, ainsi que des requêtes sur plusieurs tables, des clés étrangères et l’opération JOIN.\nLe langage de définition des données (DDL) permet :\n· Création des bases de données,\n· Modification des bases de données,\n· Création, Suppression et Mise a jour des tables,\n· Définition des contraintes au niveau des tables (clef primaire, clef étrangère ….)\nLe langage de manipulation des données (DML) permet quant à lui\n· D’interroger les différentes tables de la base de données\n· Insérer de nouvelles données dans les tables\n· Modifier les données qui ont été insérées\n· Supprimer les données depuis des tables\nObjectifs pédagogiques :\n· Comprendre les bases du DDL\n· Comprendre les bases du DML\nCette formation bénéficie d’une alternance de présentation d'exemples de requêtes, de démonstrations et de mises en pratique, de chaque notion expliquée afin de permettre à l’apprenti une compréhension encore plus approfondie",
      "target_audience": [
        "Etudiants post-bac",
        "Personnes désirant apprendre le SQL",
        "Utilisateurs professionnels et utilisateurs finaux non technique"
      ]
    },
    {
      "title": "MLops + ORANGE completo: modelos machine learning",
      "url": "https://www.udemy.com/course/mlops-orange-completo-modelos-machine-learning/",
      "bio": "Análise de modelos matemáticos e estatísticos de forma intensa",
      "objectives": [
        "Exploração de Dados",
        "Visualização de Dados",
        "Machine Learning",
        "Agrupamento, descoberta de grupos em dados",
        "Classificação e modelagem preditiva",
        "Algoritmos de Mineração",
        "Análise Estatística",
        "Trabalhando Widget: Color, Distributions, Pivot Table",
        "Trabalhando Widget: Feature Statistics, Data Sample",
        "Trabalhando com Widget: Paint Data",
        "Trabalhando com Widget: Outliers ,Scatter Plot",
        "Trabalhando com: Create Class",
        "Trabalhando com: Select By data index",
        "Trabalhando com: Edit Domain",
        "Trabalhando com: Freeviz",
        "Trabalhando com: Árvore de Decisão",
        "Trabalhando com: Cluster - Imagens",
        "Trabalhando com: Correlação",
        "Trabalhando com: Cluster – K-means",
        "Trabalhando com: Cluster - Imagens",
        "Trabalhando com Widget Predictions (realizando previsões)",
        "Trabalhando com Widget Confusion Matrix (analisando matriz de score)",
        "Trabalhando com Widget Test and Score (avaliando modelos)",
        "Criando um modelo estatístico",
        "Estimando pelo modelo estatístico",
        "Salvando modelos estatísticos em python e executando em bases de teste para previsões",
        "Trabalhando com o algoritmo de associação APRIORI",
        "Trabalhando com Widget MDS",
        "Trabalhando com Widget Mosaic Display",
        "Trabalhando Widget CN2 Rules",
        "Trabalhando Widget Box Plot",
        "Criando modelos por Redes Neurais"
      ],
      "course_content": {
        "MLops": [
          "Instruções",
          "O que é MLOps",
          "Material para Download",
          "O Problema",
          "DevOps vs MLOps",
          "MLOps"
        ],
        "MLFlow": [
          "MLFLow",
          "Pré-Requisitos"
        ],
        "Configuração do Ambiente Python": [
          "Apresentação",
          "Instalação"
        ],
        "Criando e Registrando Modelos": [
          "NaiveBayes",
          "MLFlow com NaiveBayes",
          "MLFlow com NaiveBayes Parte II",
          "MLFlow com Random Forest",
          "MLFlow com Keras",
          "MLFLow com Keras Parte II",
          "MLFLow com Keras Parte III",
          "MLFlow com Keras Parte IV"
        ],
        "Implantação de Modelos": [
          "Servindo Modelos",
          "Preparando o Serviço",
          "Consumindo o Serviço"
        ],
        "ORANGE Data Science - 100% visual - Parte01": [
          "Entendendo o funcionamento da IDE do ORANGE",
          "Instalação do ORANGE",
          "Trabalhando com arquivos e utilizando: DATA TABLE, SELECT ROWS, SELECT COLUMNS",
          "Trabalhando Widget: Color, Distributions, Pivot Table, Feature Statistics, Data",
          "Trabalhando com Widget::Paint Data , Outliers ,Scatter Plot",
          "Trabalhando com: Create Class,Select By data index,Edit Domain",
          "Trabalhando com: Freeviz",
          "Trabalhando com Árvore de Decisão",
          "Trabalhando com: Cluster - Imagens",
          "Trabalhando com: Correlação",
          "Trabalhando com: Cluster – K-means",
          "Trabalhando com: Cluster - K-mens - Parte 02"
        ],
        "ORANGE Data Science - 100% visual - Parte02": [
          "Trabalhando com Widget Predictions (realizando previsões)",
          "Trabalhando com Widget Confusion Matrix e Test and Score",
          "Criando um modelo estatístico, estimando dados e salvando modelo",
          "Trabalhando com o algoritmo de associação APRIORI",
          "Trabalhando com Widget MDS",
          "Trabalhando com Widget Mosaic Display",
          "Trabalhando Widget CN2 Rules",
          "Trabalhando Widget Box Plot",
          "Trabalhando Widget Rede Neural",
          "Trabalhando Análise de Componentes Principais",
          "Mineração de Texto",
          "Análise Curva ROC",
          "Fim do curso ORANGE Data Science - 100% VISUAL",
          "Responda a nossa pergunta"
        ],
        "Aula Bônus": [
          "Aula Bônus"
        ]
      },
      "requirements": [
        "Não há pré-requisitos, apenas conhecimento básico matemática, estatística"
      ],
      "description": "Este é um daqueles cursos que você precisa trabalhar para entender como funciona os modelos matemáticos e estatísticos e como podem ser utilizados de forma prática, no seu dia a dia, com uma fundamentação teórica rica e grandes insights pelos professores.\nMLops significa (Machine Learning Operations), baseado em princípios semelhantes ao DevOps, que da mesma forma busca integrar e automatizar o desenvolvimento e a operação. É a operação de modelos de Machine Learning.\nVeremos como trabalha em MLops:\ncriar e manter modelos de Machine Learning pode apresentar vários desafios:\nModelos são temporais, precisam, eventualmente, serem atualizados\nModelos não dependem apenas de código, mas de dados\nModelos devem ser modulares\nModelos devem ser versionados, assim como qualquer outro tipo de programa\nDados podem mudar, e isso pode afetar de forma positiva o modelo, mas também pode simplesmente para-lo\nModelos precisam ser testados\nModelos precisam ser monitorados, pois sua performance pode se degradar\nModelos precisam ser implantados\nJá no ORANGE que é uma das poucas ferramentas de mercado, totalmente construída em python, que o analista de dados  pode trabalhar de forma totalmente visual, com um amplo aspecto de atendimento a diversas demandas na área de mineração de dados por meio do uso de  algoritmos de Machine Learning.\nASPECTOS PRESENTES:\nAprendizado de máquina de código aberto e visualização de dados para iniciantes e especialistas. Fluxos de trabalho de análise de dados interativos com uma grande caixa de ferramentas\nexecute análise de dados simples com visualização inteligente de dados. Explore distribuições estatísticas, gráficos de dispersão ou mergulhe mais fundo com árvores de decisão, agrupamentos hierárquicos. Até seus dados multidimensionais podem se tornar sensíveis em 2D, especialmente com classificações e seleções inteligentes de atributos.\nExploração interativa de dados para análise qualitativa rápida com visualizações limpas. A interface gráfica do usuário permite que você se concentre na análise exploratória de dados em vez codificação, enquanto padrões inteligentes tornam extremamente fácil a criação rápida de protótipos de um fluxo de trabalho de análise de dados. Coloque widgets na tela, conecte-os, carregue seus conjuntos de dados e colete os insights!\nUse vários complementos disponíveis no Orange para extrair dados de fontes de dados externas.\n\n\nEste curso comtempla o ROADMAP ONE, que contempla a base para iniciar os seus projetos de mineração de dados e o ROADMAP TWO, que contempla a parte final dando ênfase nos algoritmos de Machine Learning mais complexo.\nSerá um dos cursos mais incríveis sobre Machine Learning que você aprenderá.\nVenha e comece hoje mesmo.",
      "target_audience": [
        "Estudantes de BI, Estatística, Computação, Informática, Gestores de Empresas (RH, Administração, Economia, dentre outros)"
      ]
    },
    {
      "title": "Python数据分析行业案例：推荐系统",
      "url": "https://www.udemy.com/course/python-pd/",
      "bio": "帮助学员学习在具体互联网业务背景下推荐系统的实战操作",
      "objectives": [
        "帮助学员快速掌握互联网背景推荐系统的实战操作",
        "帮助学员学会熟练应用Pandas、sklearn等接口编程",
        "快速掌握推荐系统应用中的各类算法",
        "帮助学员产出实际可应用的业务分析模板，为后续工作带来便捷"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "推荐系统综述": [
          "课程介绍",
          "为什么会出现推荐系统",
          "推荐系统的常见形式",
          "推荐系统算法的基本思路",
          "推荐系统评测的三大步骤",
          "怎样才能算一个好的推荐系统？",
          "推荐系统模型评估：准确率",
          "推荐系统模型评估：其他指标",
          "推荐系统常用的相似度指标",
          "推荐系统中的一些基础模型",
          "电影评分数据集简介",
          "网易云音乐数据集简介",
          "云音乐数据集的预处理"
        ],
        "Suprise包使用入门": [
          "suprise包简介",
          "suprise包实战：读取数据",
          "suprise包实战：数据拆分",
          "suprise包实战：模型的拟合与评估",
          "suprise包实战：模型的拟合与评估将模型结果用于推荐"
        ],
        "协同过滤": [
          "协同过滤概述",
          "ItemCF方法",
          "UserCF方法",
          "ml100k案例：筛选算法框架",
          "ml100k案例：模型参数调优",
          "ml100k案例：将模型结果用于推荐"
        ],
        "矩阵分解": [
          "SVD的基本原理",
          "如何将SVD用于推荐系统",
          "VDpp与NMF",
          "SVD案例"
        ],
        "基于内容的推荐算法": [
          "CB方法的基本原理",
          "词袋模型",
          "用sklearn生成文档-词条矩阵",
          "ml案例：基于词频矩阵",
          "关键词提取的基本思路",
          "TF- IDF算法",
          "ml案例：基于TF-IDF实现"
        ],
        "结合文本挖掘进行推荐": [
          "如何将文本挖掘技术和内容推荐相结合",
          "分词原理讲解",
          "结巴分词的基本用法",
          "使用自定义词典和搜狗细胞词库",
          "去除停用词",
          "云音乐案例：基于词袋模型进行推荐",
          "从词袋模型到N-gram模型",
          "文本信息的分布式表示",
          "共现矩阵",
          "NNLM模型的突破",
          "Word2vec一出，满座皆惊",
          "文档相似度的doc2vec实现",
          "云音乐案例：基于词向量模型进行推荐"
        ],
        "基于列表序列进行推荐": [
          "如何基于列表序列进行推荐",
          "云音乐案例：基于关联分析进行推荐",
          "云音乐案例：基于Word2vec进行推荐"
        ],
        "聚类方法在推荐系统中的应用": [
          "聚类分析概述",
          "聚类分析的方法分类",
          "BIRCH聚类",
          "聚类分析在推荐系统中的应用思路",
          "云音乐案例：聚类分析的数据准备",
          "云音乐案例：具体建模操作"
        ],
        "冷启动问题": [
          "冷启动概述",
          "用户冷启动的实现案例",
          "物品冷启动的实现案例"
        ]
      },
      "requirements": [
        "具备Python基础知识"
      ],
      "description": "推荐系统在当今的互联网行业中正在起到不可或缺的作用，本课程基于实际案例，由推荐系统的概念、框架、评估体系等入手，完整实现了推荐系统中应用的各类算法，包括协同过滤、矩阵分解、基于内容的推荐算法、结合文本挖掘（词频矩阵、TF-IDF、word2vec）的推荐算法、关联分析、聚类分析在推荐算法中的应用方式等，相关代码可作为分析模板供学员在工作中直接套用。\n【课程特色】\n可作为业务分析模板：课程内容完全基于真实业务分析场景构建，提供全部编写的函数工具和源代码，可直接作为同类业务场景中的业务分析模板加以使用。 双案例课程结构：充分考虑到案例代表性和分析需求上的差异化，精选电影评分和云音乐歌单数据这两个业务案例，分别代表rating和non-rating这两类推荐系统将会面对的典型数据类型，更有利于拓展学员的分析能力。\n本课程版权归三节课所有，未经书面同意私自录制、转载等行为均属侵权行为。课程内的所有内容，包括但不限于视频、文字、图片均由三节课公司或其他权利人依法拥有其知识产权，包括但不限于著作权、商标权、专利权等。\n未经三节课公司的明确书面特别授权，任何人不得为任何目的使用以及向任何自然人或单位提供出售、营销、出版或利用三节课官网上提供的任何内容或服务。如有违反，您将承担由此给三节课公司或其他权利人造成的一切损失。",
      "target_audience": [
        "适合企业内中高级数据分析师进一步提升自身实战能力",
        "适合精通行业分析应用的相关研究人员或工程师",
        "适合中高级Python数据分析师掌握推荐系统的关键技术"
      ]
    },
    {
      "title": "Derinlemesine Python 5 : AI Computer Vision",
      "url": "https://www.udemy.com/course/derinlemesine-python-ai-computer-vision/",
      "bio": "Artificial Intelligence, OpenCV, Image & Video Processing, Pillow, Image Features, Machine Learning, Sklearn, TensorFlow",
      "objectives": [
        "Yapay Zeka ve Bilgisayar Görümü (Computer Vision), Görsel Verilerle Veri Bilimi (Data Science) öğrenmek isteyenler"
      ],
      "course_content": {
        "OpenCV - Tabanlılar (Basics)": [
          "İmge (Image)",
          "Bozölçek (Grayscale)",
          "Çizim (Draw)",
          "Olay (Event) - Açar (Key) & Fare (Mouse)"
        ],
        "OpenCV - İmge İşlemleri (Image Operations)": [
          "İmge Ekleme (Image Adding)",
          "İmge Dönüştürme (Image Transformation)",
          "Aşındırma (Erosion) & İrileştirme (Dilation)"
        ],
        "OpenCV - İmge Süreçleme (Image Processing)": [
          "Ayrım (Difference)",
          "Eşyükselti (Contour)",
          "Ayrım (Difference) & Eşyükselti (Contour)",
          "Gürültüsüzleme (Denoising)",
          "İç-boyama (Inpainting)"
        ],
        "OpenCV - Nesne Sezimleme (Object Detection)": [
          "Ayrıt Sezimleme (Edge Detection)",
          "Yüz Sezimleme (Face Detection)",
          "Göz Sezimleme (Eye Detection)",
          "Kesimlik Eşleşme (Template Matching)",
          "Köşe / Bucak (Corner)",
          "Anahtar Noktalar / Açar Doğrultuları (Key Points)"
        ],
        "OpenCV - Görüntü (Video)": [
          "Görüntü Ele-geçirme (Video Capture)",
          "Görüntü Saklama (Video Saving)",
          "Görüntüde Çizim (Drawing On Video)",
          "Dosyadan Görüntü (Video From File)",
          "Çatımlar (Frames) & Gürültüsüzleme (Denoising)"
        ],
        "OpenCV - Hareket / Devinim (Motion)": [
          "Eşleşme (Match)",
          "Ayrım (Difference)",
          "Bayağı Kaydırma (Mean-Shift)",
          "Işıklı Akış (Optical Flow)",
          "Artalan Çıkartma (Background Subtraction)"
        ],
        "Pillow - PIL (Python Image Library - Piton İmge Betikliği)": [
          "İmge (Image)",
          "İşlemler (Operations)",
          "İmge Üzerinde Değişiklikler (Changes on Image)",
          "Renk / Tüs (Color)",
          "Çizim (Draw)"
        ],
        "Sklearn - İmge Özellikleri (Image Features)": [
          "İmge (Image)",
          "Ayrıtlar (Edges)",
          "HOG",
          "SLIC",
          "Evrişim (Convolution)"
        ],
        "Sklearn - Sınıflandırma / Kökleşileme (Classification)": [
          "Destek Yöneyi Kökleşileme (Support Vector Classification)",
          "İlkeci Bileşen Çözümleme (Principal Component Analysis) - PCA"
        ],
        "Tensorflow & Keras - Evrişimli Sinir Ağları (Convolutional Neural Network)": [
          "Veri İndirme (Data Download) & Çiçekler (Flowers) Örneği",
          "Geçmiş (History) & CNN - CIFAR Örneği",
          "Uyarlanmış Taslam (Customized Model) - MNIST Sayamaklar (Digits) Örneği",
          "Öngörü Çözümleme (Prediction Analysis) & MNIST Moda (Fashion)"
        ]
      },
      "requirements": [
        "Temel düzeyde Python dili, Veri Bilimi (Data Science) gerekiyor. Ancak imge özellikleriyle makine öğrenmesi yapanlar için Machine Learning (Düzenek Öğrenmesi) de gerekli."
      ],
      "description": "Bu eğitim Python ile Articial Intelligence & Computer Vision konusu işlenmektedir. Object, Edge, Video Detection ile tanıma işlemleri gösterilmektedir. Image & Video verileriyle çalışma ve Feature Extraction açıklanmaktadır. OpenCV dışında; PIL/Pillow ile image processing; Skit-Learn, TensorFlow & Keras ile Machine Learning gösterilmektedir.\nEğitim, imge süreçleme işlevlerini kapsadığı gibi görsel türde verilerin yapay zeka için hazırlanması anlamına gelen imge özellik sökme (image feature extraction) konularını da içermektedir. Bir yönüyle yapay zekanın yazı türü veriler için uygulanması anlamında ileri bir konusu olarak işlev görmektedir.\nYapay us / düzenek öğrenmesi konularıyla çalışmayan; buna karşın imge (image), görüntü (video) gibi görsel konularla, dirileştirme (animation) ya da görsel geliştirme işlemleriyle ilgilenenlere de bu kurs önerilmektedir. Çünkü bilgisayar görümü için iyi bilinmeleri gerektiğinden imge türü verilerle ilgili hemen her konu burada anlatılmaktadır. Başka bir deyişle bu eğitim, Python dilinin görsel konularını kapsayan bir ileri kursu olarak işlev görmektedir.\nEğitim tümüyle uygulamaları olarak yapılmaktadır. Kuramsal/Teorik konular yeri geldikçe, özet olarak verilmektedir. Python dilindeki betikliklerle yazı türü verilerle yapay zeka uygulamalarının nasıl geliştirildiği anlatılmakta; kullanılan algoritmaların nasıl geliştirildiği ve nasıl çalıştığı konusu, yalnızca onları kullanabilmek için gerektiği ölçüde anlatılmaktadır. Her yöntemin ya da algoritmanın çalışması kendi başına, bilgisayar bilimi ve matematiğin konusu olduğu için burada çok ayrıntılı olarak gösterilmemektedir.\nEğitimde Python dili ve Veri Bilimi (Data Science) en başından anlatılmamakta, temel düzeyde bilindiği var sayılmaktadır. Öte yandan bu konular yeri geldikçe uygulama olarak gösterildiği için, bu eğitimle Python ve Veri Bilimi bilgilerinin ilerletilmesi sağlanmaktadır. Bu nedenle, yapay zekayla ilgilenmeyenleri de Python dilini geliştirmek için bu eğitimi almalarını öneriyoruz. Başlardaki bilgisayar görümü (computer vision) konuları için yapay zeka ya da makine öğrenmesi bilgileri gerekmese de son bölümde bu konulara girildiği için Machine Learning konusu, an azından temel düzeyde bilinmesi yararlı olur.\n\n\nEğitimdeki örnekleri GitHub sitesinde godoro-education kullanıcısı altında python-computer-vision adlı depoya katıldıktan sonra görebilirsiniz.",
      "target_audience": [
        "Veri Bilimi ve Yapay Zeka konusunda görüntü ve imge türü verilerle çalışmak isteyenler. Makine Öğrenmesi bilgisini bu alanlarda geliştirmek isteyenler."
      ]
    },
    {
      "title": "ＡＩ機械翻訳（付録：日本語対応ソースコード付）：RNNでディープラーニング：自然言語処理入門編",
      "url": "https://www.udemy.com/course/rnn-seq2seq/",
      "bio": "Encoder とDecoder + RNN(GRU)を使って Sequence to Sequence モデルを作成しよう",
      "objectives": [
        "機械翻訳の仕組み",
        "Sequence to Sequenceモデル",
        "RNN->LSTM->GRU の基礎知識",
        "Google Colaboratoryの使い方"
      ],
      "course_content": {
        "はじめに": [
          "はじめに",
          "受講対象者"
        ],
        "機械翻訳の仕組み": [
          "Sequence to Sequence",
          "RNN",
          "LSTM",
          "GRU"
        ],
        "環境設定": [
          "環境設定の全体図",
          "Google Colabolatryのインストール",
          "ドライブのマウント",
          "ディレクトリの操作について",
          "データの保存について",
          "まとめ",
          "教材コードのダウンロード"
        ],
        "事前設定": [
          "事前設定",
          "Langクラスの解説",
          "Language クラスの作成１",
          "Language クラスの作成２",
          "Normalize",
          "Normalizeの処理",
          "文章データの読み込み処理について",
          "readLangs 文章データの読み込み",
          "学習データのフィルタリング",
          "FilterPairs 言語対のフィルタリング",
          "prepareData データの準備",
          "ファイルのアップロード",
          "ファイルのアップロード（Safari）"
        ],
        "Sequence to Sequenceモデルの定義": [
          "Sequence to Sequenceモデルの定義",
          "Encoderについて",
          "EncoderRNNクラスの作成",
          "Decoderについて",
          "DecoderRNNクラスの作成１",
          "DecoderRNNクラスの作成２"
        ],
        "学習処理の作成": [
          "学習処理の作成",
          "学習データの準備について",
          "翻訳05-15-学習データの準備",
          "train関数について",
          "翻訳05-16-train-1",
          "翻訳05-17-train-2-out",
          "翻訳05-18-time_helper 1",
          "翻訳05-18-time_helper 2",
          "翻訳05-19-trainIters_1",
          "翻訳05-20-trainIters_2"
        ],
        "可視化設定": [
          "翻訳05-21-showPlot",
          "翻訳05-22-evaluate_1",
          "翻訳05-23-evaluate_2",
          "翻訳05-24-evaluateRandomely"
        ],
        "実行コード": [
          "翻訳05-25-instance",
          "翻訳05-26-Run_and_evaluate"
        ],
        "付録：日本語対応ソース": [
          "日本語対応ソースのダウンロード",
          "Attention",
          "ソースコードの解説"
        ]
      },
      "requirements": [
        "Pythonの基礎知識 (ソースコードがあるので他の言語を知っていれば可)",
        "プログラミングの基礎知識",
        "ニューラルネットワークの知識があれば理解が深まります。"
      ],
      "description": "初心者にも扱いやすいPyTorchフレームワークを使って、Python コードを一行づつ解説していきます。\nRNNの仕組み利用して、Encoder Decoderクラスを作成し、翻訳の基本的な仕組みを学びます。\nフレームワークのRNNには LSTMの発展形であるGRUを使用し、EncoderとDecoderという仕組みを利用して機械的に翻訳を学習させて行きます。\n機械学習では文法を考えずに大量のデータを読み込んで学習していきます。以前の翻訳はルールベースで翻訳作業を行ってきました。しかし、ビッグデータにより受け入れる情報量と処理能力が向上すると、いかに文法を学ぶということが、コストのかかる作業であったかということがわかってきます。\nこのレクチャーはとコーディングの解説が長く続く部分もありますので、何の説明をしているのか意味がわからなくなった場合などは、Q&Aにてご質問ください。\nPythonコードと、PyTorchフレームワークで解説していきます。\n（英語から日本語へ翻訳するAttentionのコードも付属）",
      "target_audience": [
        "人工知能に興味を持つ初級のPython学習者",
        "ディープラーニング初心者",
        "機械翻訳の初心者"
      ]
    },
    {
      "title": "Machine Learning 1x1 - inklusive Python Demos",
      "url": "https://www.udemy.com/course/machine-learning-ready-1/",
      "bio": "Lerne alle wichtigen Grundlagen des Machine Learning kennen und verstehe, wie Maschinen lernen",
      "objectives": [
        "Du verstehst, wie sich das Machine Learning in den Kontext der Künstlichen Intelligenz einordnet",
        "Du kannst beurteilen, wann der Einsatz von Machine Learning sinnvoll und zielführend ist",
        "Du kennst alle relevanten Machine Learning Tasks und Algorithmen im Bereich des Supervised Learning",
        "Du verstehst, WAS Machine Learning Algorithmen lernen - und WIE sie es lernen",
        "Du weißt alles über den größten Feind eines jeden Data Scientists: das OVERFITTING - wodurch es entsteht, wie man es erkennen und verhindern kann",
        "Du entwickelst im Rahmen einer Code Challenge Deine erste eigene Machine Learning-Anwendung in PYTHON"
      ],
      "course_content": {},
      "requirements": [
        "Keine Vorkenntnisse notwendig.",
        "Gute Mathematikkenntnisse und erste Programmiererfahrung von Vorteil."
      ],
      "description": "Herzlich willkommen in meinem Kurs:\nMachine Learning 1x1\n\n\nIn diesem Kurs lernst Du alle wichtigen Grundlagen des Machine Learning. Du erfährst, wie sich das Machine Learning in den Kontext der Künstlichen Intelligenz einordnet und erhältst einen Überblick über die verschiedenen Machine Learning-Tasks und -Algorithmen im Bereich des Supervised Learning. Du lernst, was Machine Learning-Algorithmen eigentlich genau lernen und wie sie es lernen. Außerdem stelle ich Dir den größten Feind eines jeden Data Scientists vor: das Overfitting.\n\n\nAlle Konzepte besprechen wir nicht nur in der Theorie, sondern machen sie im Rahmen von Code Demos direkt anfassbar. Alle Code-Beispiele sind in Python programmiert - wir arbeiten mit einschlägigen ML Libraries wie Pandas, NumPy und Scikit-learn.\n\n\nAlle Materialien, die wir im Kurs verwenden (Power Point Slides, Python Notebooks, usw.), stehen Dir zeitlich unbegrenzt auch zum Download zur Verfügung. Wenn Du Inhalte aus diesem Kurs für eigene Präsentationen / Veröffentlichungen verwenden möchtest, freue ich mich darüber - bitte Dich aber gleichzeitig darum, auf mich als Urheber zu referenzieren.\n\n\nSolltest Du während des Kurses Fragen oder Anmerkungen haben, kannst Du mich jederzeit gerne kontaktieren - ich freue mich über Deine Nachricht und Dein Feedback.\n\n\nDieser Kurs ist ein Teil meiner Wissens-Plattform MACHINE LEARNING-READY. Alle Informationen zu den weiteren Wissensangeboten rund um das Thema Machine Learning findest Du auf meiner Website. Dort hast Du auch die Möglichkeit, Dich für meinen Newsletter anzumelden und von besonderen Angeboten zu profitieren. Den Link zur Website findest Du in meinem Profil auf LinkedIn. Dorthin gelangst Du über meine Udemy Dozentenseite oder direkt über Google.\n\n\nViel Spaß beim Lernen!",
      "target_audience": [
        "Alle, die verstehen wollen, wie Maschinen aus Daten lernen.",
        "Alle, die eine Karriere im Bereich im Bereich Data Science anstreben.",
        "z.B. als Data Scientist, ML Engineer oder Data Science Manager"
      ]
    },
    {
      "title": "Metodología de Data Science y Python Básico.",
      "url": "https://www.udemy.com/course/metodologia-de-data-science-y-python-basico/",
      "bio": "Una guía para entender tu problema y resolverlo con datos!!",
      "objectives": [
        "Obtendrás una visión general de lo que es la ciencia de datos (Data Science) y de como esta nos ayuda a resolver problemas.",
        "Para no estar perdidos en el camino y maximizar nuestros resultados, es necesario conocer la Metodología que rige el Data Science, para así tener una idea solida de como usar los datos en cada etapa y resolver un problema en cuestión.",
        "Comenzaremos desde el entendimiento del negocio, la recopilación y análisis de datos, la construcción de un modelo, hasta la implementación, entendiendo que se hace en cada una de las etapas para tener éxito.",
        "Aprenderás lo básico de Yython y programación, ademas del uso de Numpy y Pandas las cuales son algunas de las librerías mas usadas en Data Science!!",
        "Ciclo de vida de un proyecto en Data Science."
      ],
      "course_content": {},
      "requirements": [
        "Ningúno específicamente."
      ],
      "description": "La Ciencia de datos esta creciendo muy rápidamente  en estos últimos años, ya que en todas partes encontramos datos los cuales nos podrían ayudar a mejorar nuestros procesos, aumentar nuestras  ventas, entender que esta pasando con nuestra empresa, dar respuestas a preguntas, predecir enfermedades, detectar cáncer, y muchas aplicaciones mas.\nAhora el verdadero problema radica en que faltan Científicos de datos (Data Scientist). Python es uno de los programas que mas es usado en este mundo de los datos, por lo que empezar a usarlo es necesario, pero que es todo este conocimiento sin un mapa que nos guié?, para eso esta la Metodología de Ciencia de los datos (Data Science), la cual nos llevara de la mano desde que definimos nuestro problema, hasta la resolución del mismo, todo esto en un solo Curso!!.",
      "target_audience": [
        "Desarrolladores de Python con interés en la Ciencia de Datos",
        "Personas con experiencia en Python que no tengan una metodología a seguir."
      ]
    },
    {
      "title": "Power bi - Essencial",
      "url": "https://www.udemy.com/course/power-bi-essencial/",
      "bio": "Transforme Dados em Decisões Estratégicas: Domine Power Query e Power BI para Impulsionar Resultados de Negócios",
      "objectives": [
        "Desenvolver Prossifionais com capacidade de análise de dados",
        "Dashboard em power bi",
        "Análista de dados",
        "Profissionais de todas as áreas."
      ],
      "course_content": {
        "Power bi Básico": [
          "Bem vindo",
          "Instalando Power bi",
          "Panorama Geral - Power bi",
          "Criando um Cartão",
          "Criando um Cartão - Parte 2",
          "Criando um Filtro e um cabeçalho",
          "Grafico de Linhas",
          "Gráfico de Barras",
          "Criando uma tabela",
          "Gráfico de Rosca",
          "PROJETO DASHBOARD MARKETING - parte 1",
          "PROJETO DASHBOARD MARKETING - Parte 2"
        ],
        "Modulo 2 - Criando Medidas Agregadoras": [
          "Primeira Função Agregadora",
          "Medida de Soma",
          "Média",
          "Como dividir ?",
          "Organizando Medidas",
          "Realizando medida com filtro - função Calculate",
          "Máximo e Mínimo",
          "Ajustando Tabela Power Query",
          "Contangem e Contagem Distinta",
          "Finalizando Modulo"
        ],
        "Modulo 3 - Avançando no conceito e criando novo dashboard": [
          "Introdução Modulo 3",
          "Importando dados",
          "Relacionando tabelas",
          "Criando tabela dCalendario",
          "Criando medidas parte 1",
          "Criando Medidas Parte 2",
          "Criando Medidas Parte 3",
          "Criando Medidas Parte 4",
          "Montando Dashboard - Filtro",
          "Dashboard - Cards",
          "Dashboard - Gráfico de Linhas",
          "Dashboard - Gráfico de Mapas",
          "Dashboard - Tabela e Gráfico de Barras",
          "Dashboard - Importando Visual",
          "Dashboard - Tooltip",
          "Dashboard Finalizando Primeira Aba",
          "Dashboard - Novas Medidas",
          "Dashboard - Criando Medidas com Tabela Virtual",
          "Dashboard - Medidas com Calculate",
          "Dashboard - Finalizando medidas Calculate",
          "Dashboard - Medidas Switch",
          "Dashboard - Medidas Switch parte 2",
          "Dashboard - Top Lucro",
          "Dashboard - Top Lucro parte 2",
          "Dashboard - Finalizando 2 aba",
          "Dashboard Aba 3 - Parte 1",
          "Variação da Meta",
          "Como calcular com Ano anterior",
          "Função userelationship",
          "Montando Parte 3",
          "Finalizando Dashboard - Modulo 3"
        ],
        "Power Query - Tratamento de dados": [
          "Licenças power bi",
          "Tratando tabelas desconfiguradas",
          "Conceito de tabela e intervalo de dados",
          "Tabela dCalendario",
          "Importando dados da Web",
          "Pasta como fonte de dados",
          "Tratando tabelas mescladas",
          "Agradecimentos"
        ]
      },
      "requirements": [
        "Não há necessidade de experiência.",
        "Aqui você aprenderá do basico ao avançado em power bi e análise de dados"
      ],
      "description": "Este curso de Power BI é ideal para quem deseja dominar a criação de dashboards e relatórios interativos, desde o nível básico até o avançado. Ao longo das aulas, você aprenderá a transformar dados brutos em insights valiosos, abordando conceitos essenciais que garantem o sucesso no desenvolvimento de projetos.\nIniciaremos com os fundamentos do Power BI, explorando suas principais ferramentas e funcionalidades. À medida que avançamos, você será guiado através de temas importantes, como modelagem de dados, criação de medidas DAX, e design visual de dashboards. Também serão abordados aspectos críticos como a integração com diversas fontes de dados, a automação de processos e as melhores práticas de governança de dados.\n\nTambém iremos explorar o Power Query, onde você aprenderá a importar, limpar e transformar dados de diversas fontes de maneira eficiente. Com o domínio dessa ferramenta, você será capaz de automatizar processos e preparar seus dados para análises detalhadas no Power BI.\nAo final deste curso, você estará preparado para desenvolver qualquer tipo de dashboard, aplicando os conhecimentos adquiridos para resolver problemas reais e agregar valor ao seu trabalho ou negócio. Com uma abordagem prática e exemplos do mundo real, este curso concentra tudo o que você precisa para se tornar um especialista em Power BI, em um único lugar. Junte-se a nós e dê o próximo passo na sua jornada de aprendizado em Power BI!",
      "target_audience": [
        "Profissionais da área de TI",
        "Profissionais da área de RH",
        "Profissionais da área administrativa",
        "Profissionais da área de marketing",
        "Profissionais da área financeira"
      ]
    },
    {
      "title": "(Ken Cen出品)Generative AI第10部 LLM網絡爬蟲 AI Web Scrape",
      "url": "https://www.udemy.com/course/generative_ai_10/",
      "bio": "關於ScrapeGraphAI、Jina Reader API與FireCrawl，LLM Agent，Tool Call,AgentQL",
      "objectives": [
        "掌握如何運用 ScrapeGraphAI 快速抓取數據",
        "掌握如何通過強大的圖形化分析將結果轉化為有價值的市場洞見",
        "掌握如何輕鬆監控市場趨勢、競爭對手動態及消費者行為，實現業務的快速決策與創新",
        "掌握如何將大規模數據轉換成簡明易懂的圖表與報告"
      ],
      "course_content": {
        "課程準備": [
          "課程軟體安裝",
          "如何使用Poetry"
        ],
        "如何製作我們第一個AI網絡爬蟲": [
          "如何安裝ScrapeGraphAI以及VSCode設定",
          "如何編寫我們第一個ScrapeGraphAI網絡爬取數據程式"
        ],
        "ScrapeGraphAI各種延伸功能": [
          "如何使用SmartScraperMultiGraph與SearchGraph",
          "如何使用ScriptCreatorGraph"
        ],
        "如何使用Reader API": [
          "如何使用Jina的Reader&Search&Grounding",
          "如何使用Jina的Embedding model",
          "如何在LlamaIndex當中使用Jina Embedding Model",
          "如何在LlamaIndex當中使用Jina Reranker"
        ],
        "如何使用FireCrawl作為網絡爬蟲": [
          "如何在本地部署免費FireCrawl網絡爬蟲工具"
        ],
        "如何將AI Agent智能體與網絡爬取工具連結在一起": [
          "如何定義Agent使用網站爬取工具",
          "如何讓Agent智能體找到運行的工具",
          "如何把Tool Call結果添加到Agent中進行處理",
          "如何處理過長Messages或者Prompts的上下文長度."
        ],
        "如何繞過網站攔截爬取網站數據": [
          "如何做Web Scraping繞開網站攔截",
          "如何使用AgentQL搜索Youtube數據",
          "如何用AgentQL獲得視頻描述與用戶留言"
        ]
      },
      "requirements": [
        "一台電腦"
      ],
      "description": "你想學習如何快速而精確地從網絡獲取數據，突破傳統爬蟲技術的侷限？本課程將帶你深入了解最新一代的AI數據爬取工具，這些工具不僅能幫助你高效地爬取網絡數據，還讓你在無需技術門檻的情況下，自由地與網頁互動，快速填寫表單或抓取數據。以下是你無法錯過這門課程的幾大亮點：\n全面介紹ScrapeGraphAI、Jina Reader API與FireCrawl等尖端工具：這些強大的AI工具不僅高效而且靈活，遠遠超越傳統爬蟲工具，讓你能以AI的速度和精度抓取所需的資料，並且操作直觀，無需複雜編碼。\nLLM Agent加持，實現智能問答與自動化表單填充：不僅僅是爬取數據，你將學會如何運用LLM Agent將數據轉化為實用的信息，實現智能問答和自動填充表單的功能，將繁瑣的工作變得輕鬆自如，無需人工干預。\nAgentQL技術解密，無需Selector即可互動：學習如何使用AgentQL繞過網站攔截，實現和網頁上的各個組件或元素直接互動。不再需要手動配置Selector，直接操作更靈活更高效，打破傳統網頁解析的限制。\n適合各類型學員，輕鬆上手高效收穫：無論你是資料科學家、SEO專家，還是新手工程師，本課程會用簡單的步驟和詳細的案例，讓你快速掌握如何運用這些AI工具來抓取、分析並應用網頁數據，提升工作效率。\n在本課程結束時，你將能完全掌握這些工具，並運用它們進行數據抓取、信息提取和自動化操作。立即加入我們的課程，學會利用AI技術抓取精準數據，讓你在數據爬取與處理上無往不利！",
      "target_audience": [
        "希望提升抓取和分析數據效率的數據分析師",
        "需要時刻跟蹤競爭對手價格、產品趨勢和消費者評價的電商業者",
        "SEO專家與行銷人員",
        "程式開發者"
      ]
    },
    {
      "title": "深度学习-语音识别实战（基于PyTorch）",
      "url": "https://www.udemy.com/course/pytorch-x/",
      "bio": "基于深度学习的语音识别算法（论文）实战",
      "objectives": [
        "掌握语音经典算法及其应用领域",
        "熟练使用PyTorch框架构建语音识别模型",
        "掌握语音识别领域经典论文算法",
        "熟悉语音数据预处理方法",
        "熟练使用深度学习框架进行语音识别任务开发",
        "熟悉语音分离技术落地方案",
        "熟练使用PyTorch框架构建语音分离模型",
        "掌握语音合成最新论文及其算法思想",
        "熟练使用PyTorch构建语音合成模型",
        "掌握变声器构建原理及其应用",
        "熟练使用PyTorch框架构建变声器模型"
      ],
      "course_content": {},
      "requirements": [
        "熟悉Python与深度学习基本算法"
      ],
      "description": "基于深度学习的语音识别实战课程主要包括三部分内容：1.经典论文算法讲解；2.算法源码解读；3.项目实战；通俗讲解语音识别领域当下经典论文思想，详细解读源码中每一核心模块并基于真实数据集展开项目实战。整体课程覆盖语音识别领域四大核心主题：语音识别，语音分离，语音转换，语音合成；每一主题均按照论文思想解读，源码分析，项目实战顺序进行讲解。提供课程所需全部数据集，代码，PPT课件。",
      "target_audience": [
        "人工智能，深度学习方向的同学们"
      ]
    },
    {
      "title": "【Azureで始める】AI開発のためのPython入門",
      "url": "https://www.udemy.com/course/python4ai/",
      "bio": "AI開発の新定番。最速でABC人材を目指そう",
      "objectives": [
        "AI開発の流れや必要なスキルセットを理解できる",
        "AI開発に必要なPythonスキル",
        "AI開発で求められるソフトウェアエンジニアリングスキル",
        "AI開発で必要なデータサイエンスのライブラリ"
      ],
      "course_content": {
        "はじめに": [
          "講座概要"
        ],
        "AI開発": [
          "AI開発演習(概要)",
          "AI開発演習(実践)",
          "Scikit learn part1",
          "Scikit-learn part2",
          "Scikit-learn part3"
        ],
        "Microsoft Azure": [
          "アカウント作成",
          "work spaceの作成",
          "Computingの作成",
          "データセットのアップロード",
          "ターミナル",
          "notebookの作成"
        ],
        "Python基礎": [
          "四則演算",
          "変数の作成",
          "変数の型",
          "変数の名前",
          "文字列",
          "format",
          "リスト(list)",
          "タプル(tuple)",
          "辞書(dict)",
          "集合(set)",
          "print関数",
          "forループ",
          "forループ zip",
          "forループ enumerate",
          "if文",
          "ファイルの読み込み・作成",
          "関数の基礎",
          "デフォルト引数(default args)",
          "キーワード引数(kwargs)",
          "lambda関数",
          "try_except",
          "リスト内包表記",
          "ライブラリのimport",
          "os_",
          "tqdm",
          "glob",
          "continue"
        ],
        "Numpy": [
          "1d array",
          "2d array",
          "zeros / ones",
          "reshape",
          "random",
          "linspace",
          "統計量",
          "where",
          "nanの取り扱い",
          "stack / np.c_",
          "行列演算"
        ],
        "Pandas": [
          "Series",
          "DataFrame",
          "read_csv",
          "read_excel",
          "read_sql",
          "データの選択",
          "データの抽出",
          "データ型",
          "統計量",
          "drop / fill",
          "concat",
          "index",
          "rename",
          "ビン(bin)",
          "merge",
          "copy",
          "apply",
          "iterrows",
          "groupby",
          "pivot_table",
          "styleの適用"
        ],
        "Matplotlib": [
          "Scatter plot",
          "Line Plot",
          "ヒストグラム",
          "figureの設定",
          "subplots"
        ],
        "Seaborn": [
          "Scatterplot",
          "Lineplot",
          "Histogram",
          "Boxplot",
          "Heatmap",
          "テーマ・コンテキスト"
        ],
        "Python応用": [
          "クラスの基礎",
          "クラスの継承",
          "property",
          "raise error",
          "docstrings",
          "デバッガの使い方(VS Code) part1",
          "デバッガの使い方(VS Code) part2",
          "単体テスト part1",
          "単体テスト part2",
          "コードスタイル",
          "pipの使い方",
          "API referenceの読み方"
        ]
      },
      "requirements": [
        "Pythonについて学んだことがある"
      ],
      "description": "【本講座の概要】\n本講座はAI開発のために必要なPythonスキルを身に付けるための講座です。\n近年、AIが産業構造や働き方など、社会全体に影響を及ぼしていることもあり、AI開発に携わりたい、データサイエンティストを目指したいと考えている人は少なくありません。\n\n\n一方で、こんな悩みは無いでしょうか？\n・実際にどこまでのレベルに達すればAI開発の現場で通用するのか？\n・そもそもデータサイエンティストになるにあたってスキルセットとして何が必要なのか？\n・AI開発とはどんな風に進むのか？\n\n\n本講座はそんな多くの人が抱えがちな悩みに応えるために作成されました。\nこの講座はAI開発のシステムや開発の流れなど全体像から学習をスタートします。その後、全体像からデータサイエンティストに必要なスキルセットをバックキャストし、この中でも重要なPythonのスキルとして何が必要なのかを整理、そして実際のPythonについて学習をスタートします。\n\n\nAI開発に必要なPythonスキルを一通り学ぶため、前半はPythonの基本的な内容となっていますが、徐々に難易度が上がっていき、より実践的な内容となってきます。VS codeによるデバッグ、単体テスト、API referenceの読み方,  docstringの書き方など、実際の実務で重要な内容が組み込まれています。\n\n\nAI開発で必須のライブラリ(Numpy, Pandas, Matplotlib, Seaborn, Scikit-learn)についても実際によく使う内容を中心に一通り学びます。\n\n\nこの講座を受講する事でAI開発の全体像を正しく理解した上で、納得感を持ってAI開発に必要なPythonスキルセットを身に付けることができます。また、実際のAI開発で必要なスキルを学ぶため、闇雲に色々な教材に手を出す必要がなくなります。\n\n\n\n\n【人事の方/マネージャークラスの方へ】\n本コースは次のような使い方が可能です。\n・社内で実践的なAI開発スキルを持ったAI人材を育成したい\n・効率よくプロレベルのデジタル人材を育成したい\n\n\n本コースは現場でAI開発に携わるためのPythonスキルを一通り学ぶことができます。本コースを受講いただくことで、コースの内容は実務で応用可能であり、部署全体のAI開発スキルセットを底上げする事ができます。\n\n\n【対象者とゴール】\nこのコースは初級レベルであり、Pythonについて学んだことがある入門レベルのPython学習者がAI開発の文脈で必要なスキルセットを身に付けるためのコースです。\n\n\n本コースのゴールは3つあります。\n1. AI開発の流れを正しく把握し、何がどこまで必要かを理解する\n2. AI開発に必要なPythonのスキルを一通り身に付ける\n3. ソフトウェア開発・ソフトウェアエンジニアリングで必要なスキル(debug, 単体テスト, ドキュメントの読み方など)を身に付ける",
      "target_audience": [
        "AI開発に関心のあるPython初級者"
      ]
    },
    {
      "title": "R言語によるデータサイエンス【データ操作, 最短習得コース】 (2021, tidyverse+α, 基礎+応用)",
      "url": "https://www.udemy.com/course/r-datascience-2021-tidyverse/",
      "bio": "AI開発の前に知るべき操作(読込, 前処理, 加工, レポート化)と、実用的なパッケージや関数を抽出し、最小限の時間でR言語を活用するデータ分析者のレベルに成長できるようコース内容を調整しております。",
      "objectives": [
        "データ分析の初歩であるデータ操作方法",
        "R言語の使用方法とパッケージの機能紹介",
        "最新のパッケージの利用方法"
      ],
      "course_content": {
        "00-ソフトウェアのインストール": [
          "R言語のインストール",
          "Rstudioのインストール",
          "minicondaのインストール",
          "Rstudioの初期設定",
          "おまけ：gitのインストール"
        ],
        "01-R言語とデータサイエンスの関係": [
          "本コースの説明",
          "データ分析に関わる言語",
          "今までのR言語と最近のR言語の比較",
          "Rで機械学習や深層学習を行うには",
          "R言語の歴史と統合開発環境",
          "R言語のコミュニティについて"
        ],
        "02-R言語を使ってみよう": [
          "データとコードのダウンロード",
          "四則演算",
          "変数への代入",
          "ベクトルの概念",
          "listとdata frame",
          "データの抽出とmatrix",
          "tidyverse「tibble」とdata frameやmatrixの違い",
          "for, if, function　の説明",
          "for, if, function　をコーディング",
          "tidyverse「purrr」パッケージの紹介",
          "基本機能のplot関数について",
          "plot関数　コーディング",
          "plotに類似した関数とtidyverse「ggplot2」の紹介",
          "グローバル設定操作の「withr」パッケージ",
          "R言語のmethodsとは",
          "generic関数とmethods",
          "R言語とデータセット",
          "データセットの検索方法"
        ],
        "03-Rstudioの紹介": [
          "Rstudioとは",
          "Rstudioについて補足",
          "RstudioとbaseRの比較",
          "Rstudioに備わっている機能の紹介",
          "Rで分析projectを管理する",
          "プロジェクトテンプレートの利用"
        ],
        "04-パッケージを調べ、インストールする": [
          "インストールされているパッケージを確認する方法",
          "CRANからのパッケージインストール",
          "scatter3Dパッケージを実際にインストールして使う",
          "読込方法であるlibraryとrequirについて",
          "パッケージのインストールを楽にする「pacman」パッケージ",
          "インストール時のエラー対応",
          "パッケージ検索方法",
          "テーマを変更するための「rsthemes」パッケージ"
        ],
        "05-tidyverseの紹介": [
          "tidyverseとdata tiding",
          "tidyverse コアパッケージ",
          "コアパッケージを確認する",
          "tidy data とは何か",
          "tidy data であるメリット",
          "理想のデータ形式とは１",
          "理想のデータ形式とは２",
          "tidy data をつくる「tidyr」パッケージの紹介",
          "データの「縦(long)」「横(wide)」",
          "tidy な形でなくていい場合"
        ],
        "06-パイプ機能とtibbleについて": [
          "パイプ機能を提供する「magriter」パッケージ",
          "代入演算子について",
          "右矢印での代入演算子について",
          "パイプ機能を文章として読む",
          "パイプの便利さをコーディングを通して理解する",
          "パイプを使うべきでない場面"
        ],
        "07-データ前処理のはじめ": [
          "「tibble」パッケージの紹介",
          "tibbleの性質とdplyrの導入",
          "データ前処理の機能を持つ「janitor」パッケージ",
          "データ全体を俯瞰する「skimr」パッケージ"
        ],
        "08-dplyrパッケージの紹介と、関係するパッケージ": [
          "データ操作の大部分で利用できる「dplyr」パッケージ",
          "select　関数",
          "renameとrelocate　関数",
          "mutate　関数",
          "mutate　関数と「lubridate」パッケージ１",
          "mutate　関数と「lubridate」パッケージ２",
          "mutate　関数と「lubridate」パッケージ３",
          "mutate　関数と「slider」パッケージ",
          "mutate　関数と「forcats」パッケージ１",
          "mutate　関数と「forcats」パッケージ２",
          "mutate　関数と「stringr」パッケージ",
          "dplyrの紹介 - 続き",
          "countとsummarizeとtally　関数",
          "addcount　関数",
          "across　関数とsummarizeでの利用例",
          "filter　関数",
          "arrange　関数",
          "bind　関数",
          "topn　関数",
          "slice　関数",
          "distinct　関数",
          "join系の関数",
          "joinの関数でコーディング",
          "dplyrの並列化「multplyr」パッケージ",
          "「tidyr」パッケージの紹介",
          "tidyrでデータをlong形式へ変換",
          "tidyrでデータをwide形式へ変換",
          "complete　関数",
          "expand_grid　関数",
          "separateとunite　関数",
          "extract　関数",
          "「glue」パッケージの紹介とsummarizeやtidyrでの利用方法",
          "nest　関数",
          "「broom」パッケージの機能紹介とnest、lm　関数の組み合わせ"
        ],
        "09-データベース接続、様々なファイルの読み込み": [
          "「fs」パッケージとディレクトリ操作",
          "「readr」パッケージによるデータ読み込み",
          "「data.table」パッケージによるデータ読み込みと利用",
          "「dtplyr」パッケージによるdata.tableの操作",
          "「readxl」パッケージによるデータ読み込み",
          "「vroom」パッケージによるデータ読み込み",
          "同一フォルダ内のファイルを結合しながら読み込む方法",
          "「haven, jsonlite」パッケージによるSAS, SPSS, STATA, など特殊ファイルの読み込み",
          "「DBI, SQLite」パッケージによるデータベースファイルへの接続",
          "「RODBC」パッケージによるデータベースへの接続",
          "「dbplyr」パッケージによるデータベース内のdplyr操作"
        ]
      },
      "requirements": [
        "基本的なITの知識(ITパスポートレベルがあると望ましい)",
        "興味のある事柄を自ら調査するスキル"
      ],
      "description": "データサイエンスの実務において、機械学習モデルの構築は全体の約2割に過ぎません。\n残りの8割は、データの収集、クレンジング、探索的分析、可視化、レポーティングが占めています。\n本コースでは、この「見えない8割」の作業を効率的に行うためのR言語の活用法を体系的に学びます。\n\n\nR言語を知らない方にも「R言語を起点にデータサイエンスへ入門しよう」と思っていただけるよう、最先端の機能だけでなく従来まで使われてきた機能も振り返りながら紹介しています。\n\n■使用環境：\nWindows 10\nR 4.0.5\nRStudio 1.4.1717\n\n\n利用パッケージ例：\nnycflights13,palmerpenguins,devtools,remotes,usethis,vctrs,httr,tidyverse,tidyselect,here,janitor,skimr,lubridate,hms,slider,glue,broom,fs,data.table,dtplyr,tictoc,readxl,vroom,DBI,RSQLite,reprex,odbc,RODBC,reticulate,gapminder,dbplyr,prettycode,styler,testthat,bench,parallelly,parallel,furrr,multidplyr,rsthemes,scales,cowplot,patchwork,withr,ProjectTemplate,scatterplot3d,haven,kableExtra,knitr,xaringan,revealjs\n\n\n■得られる知識：\n・「RやRstudio」を使うと、どのようなことが実現出来るのかを知ることができる\n・データサイエンスに利用する上で最低限どんな知識が必要かを知ることができる\n・tidyverseをはじめとしたパッケージや、便利な機能を持った関数群をインストール方法から紹介する\n・便利な機能や実践的な使用例をかみ砕いて紹介する\n・様々なユーザーパターンを想定し、幅広いデータファイルへの接続方法や操作方法を紹介する\n・スライドショーや報告レポート化する機能を使い、低ストレスで分析内容を資料化する方法を紹介する",
      "target_audience": [
        "データサイエンスに興味を持っている方",
        "excelだけでないデータ分析をめざす方",
        "Tidyverseは知ってるが、まだ使ったことが無い方",
        "R言語は少しだけ触ったことがあるレベルの方",
        "データ分析言語についてまったく知らない方"
      ]
    },
    {
      "title": "Domine a Manipulação de Tensores com PyTorch 2.+",
      "url": "https://www.udemy.com/course/domine-a-manipulacao-de-tensores-com-pytorch-2/",
      "bio": "Transforme sua habilidade em ciência de dados e programação explorando o poder dos tensores com PyTorch 2.+!",
      "objectives": [
        "Entender como redimensionar, transpor, concatenar e dividir tensores",
        "Operações de redução e agregação",
        "Diferenciar escalares, vetores, matrizes e tensores.",
        "Aprender a mover tensores entre a GPU e a CPU para acelerar cálculos.",
        "Técnicas de indexação de tensores para acessar elementos específicos.",
        "Executar produto escalar, multiplicação de matrizes e outras operações avançadas com tensores.",
        "Compreender e aplicar o conceito de broadcasting para operações entre tensores de diferentes tamanhos."
      ],
      "course_content": {
        "Introdução": [
          "Introdução",
          "Origem do pytorch",
          "Acesso ao colab do curso"
        ],
        "Iniciando com pytorch": [
          "Instalação e arquitetura do pytorch",
          "Escalar, Vetor, Matriz e Tensores",
          "Escalar",
          "Vetor",
          "Matriz",
          "Tensores",
          "Executando operações na GPU e CPU",
          "Acessando os elementos do tensor via index",
          "Criação e Inicialização de Tensores"
        ],
        "Funções de redução": [
          "torch.sum",
          "torch.mean",
          "torch.max",
          "torch.min",
          "torch.argmax e torch.argmin"
        ],
        "Transposição, Produto Escalar e Broadcasting": [
          "Várias formas de transposição entre tensores",
          "Várias funções de produto escalar entre tensores",
          "Broadcasting"
        ],
        "Manipulação e Transformação de Tensores": [
          "torch.reshape e torch.view",
          "torch.squeeze e torch.unsqueeze",
          "torch.cat e torch.stack",
          "torch.chunk",
          "torch.split",
          "torch.tril"
        ],
        "Salvar e carregar tensores": [
          "torch.save e torch.load"
        ]
      },
      "requirements": [
        "Ter conhecimento da linguagem de programação Python"
      ],
      "description": "Bem-vindo ao curso \"Domine a Manipulação de Tensores com PyTorch 2.+\", uma jornada completa destinada a capacitá-lo com as habilidades necessárias para manipular tensores de forma eficaz.\nPyTorch é uma das bibliotecas mais populares e poderosas para computação científica e este curso é projetado para levar você do básico ao avançado na manipulação de tensores, a espinha dorsal de operações em ciência de dados e inteligência artificial, sem a necessidade de se aprofundar em conceitos de machine learning ou redes neurais.\n\n\nO que você vai aprender:\nFundamentos e instalação do PyTorch, configurando o cenário para manipulação avançada de tensores.\nConceitos essenciais de escalares, vetores, matrizes e tensores, e como eles são utilizados em computação científica.\nExecutar operações complexas com tensores em CPUs e GPUs, maximizando a eficiência e desempenho.\nTécnicas de indexação, slicing, e manipulação de tensores.\nDominar funções de redução, transposição, e operações matemáticas essenciais com tensores.\nPráticas avançadas de manipulação, incluindo broadcasting, reshaping, e manipulações estruturais.\nComo salvar e carregar tensores, facilitando a reutilização de dados e modelos em projetos futuros.\nExperiência prática através de exercícios de codificação hands-on para consolidar sua aprendizagem.\nPara quem é este curso:\nEste curso é perfeito para estudantes, profissionais de ciência de dados, engenheiros de software, analistas de dados, e qualquer pessoa entusiasmada em aprofundar seus conhecimentos em manipulação de tensores. Se você tem uma base em programação Python e deseja explorar como manipular dados em um nível avançado, este curso é para você.",
      "target_audience": [
        "Destinado a uma ampla gama de público interessado em aprender a manipular tensores com o pytorch",
        "Estudantes e Pesquisadores que desejam entender e aplicar técnicas de manipulação de tensores em seus projetos, teses ou pesquisas.",
        "Desenvolvedores de Software que desejam adentrar no campo do aprendizado de máquina e inteligência artificial",
        "Analistas de Dados que desejam aprimorar suas habilidades de manipulação de tensores"
      ]
    },
    {
      "title": "Pelatihan Data Science dan Machine Learning Dengan R",
      "url": "https://www.udemy.com/course/datasciencer/",
      "bio": "Meningkatkan kemampuan data analitik melalui data science dan machine learning dengan R",
      "objectives": [
        "Mempelajari pengolahan data dan analisa data",
        "Visualisasi data",
        "Visualisasi data dengan ggplot2",
        "Dataset, Pra-Proses dan Pengurangan Dimensi Feature (Dimensionality Reduction)",
        "Ekplorasi beberapa algoritma pada data science dan machine learning",
        "Pratikum studi kasus data science dan machine learning",
        "Hyperparameter Tuning Untuk Model Machine Learning",
        "Ensemble Methods"
      ],
      "course_content": {},
      "requirements": [
        "Komputer dengan sistem operasi Windows, macOS atau Linux",
        "Akses internet",
        "Tidak wajib menguasai bahasa pemrograman R"
      ],
      "description": "Selamat datang di program Pelatihan Data Science dan Machine Learning Dengan R!\nPelatihan ini diperuntukan untuk rekan - rekan ingin belajar data science dan machine learning dari sudut terapan dengan memanfaatkan R.\nBagi rekan - rekan yang belum menguasai pemrograman R, pelatihan juga memberikan konten pemrograman dasar untuk  Rsehingga rekan - rekan dapat mengikuti pelatihan ini dengan baik. Bagi yang sudah bisa pemrograman R, rekan - rekan dapat melanjutkan di topik berikutnya.\nSeluruh konten didalam pelatihan ini dilaksanakan secara step - by - step (langkah demi langkah) dan berurutan sehingga ini diharapkan semua peserta dapat dengan mudah mengikuti semua praktikum yang diberikan didalam pelatihan ini. Diharapkan semua peserta dapat mengikuti konten pelatihan ini secara berurutan ;).\nBerikut ini konten yang akan diberikan pada pelatihan ini.\nPersiapan pelatihan\nPemrograman R\nPengenalan tool dan editor seperti RStudio, Jupyter Notebook / JupyterLab, Jupyter / Notebook Dengan Anaconda, dan Google Colab\nVisualisasi Data\nVisualisasi Data dengan ggplot2\nDataset, Pra-Proses dan Pengurangan Dimensi Feature\nManipulasi dan Analisa data\nEksplorasi data science dan machine learning\nPermasalahan dan Penyelesaian Kasus Linear Regression\nPermasalahan dan Penyelesaian Kasus Klasifikasi (Classification)\nPermasalahan dan Penyelesaian Kasus Kekelompokkan (Clustering)\nEnsemble Methods\nHyperparameter Tuning Untuk Model Machine Learning\nKumpulan Studi Kasus\nJika ada hal - hal yang ingin ditanyakan mengenai topik diatas, rekan - rekan dapat langsung ditulisnya di ruang diskusi pada web ini sehingga rekan-rekan lainnya dapat mengetahui dan ikut terlibat diskusinya.",
      "target_audience": [
        "Pelajar dan mahasiswa yang ingin mempelajari data science dan machine learning",
        "Professional yang ingin mempelajari data science dan machine learning",
        "Pengajar dan peneliti yang ingin mempelajari data science dan machine learning"
      ]
    },
    {
      "title": "Convolutional Neural Networks - Hinter den Kulissen",
      "url": "https://www.udemy.com/course/convolutional-neural-networks-hinter-den-kulissen/",
      "bio": "Spezialkurs zu Convolutional Neural Networks und Residual Nets - Deep Learning Architekturen für die Bildverarbeitung",
      "objectives": [
        "Du kennst Convolutional Neural Networks. Nun lernst du, wie diese Layer aufgebaut sind und funktioneren!",
        "Wir bauen gemeinsam einen Convolutional Filter theoretisch und praktisch auf.",
        "Wozu dienen Paddings und Strides in einer Convolution?",
        "Wie ist eine Convolution im fastai Framework durch den CNN-Learner umgesetzt?",
        "Welche Techniken zur Optimierung eines CNNs sind in der Trainingsmethode des fastai Frameworks gekapselt?",
        "Wozu brauche ich einen Batch-Normalization Layer?",
        "Was ist ein Residual Block und wie wird daraus ein Residual Net (ResNet)",
        "Was bedeutet Average Pooling (im Unterschied zu MaxPooling)? Wie implementiere ich einen Average Pooling Layer?",
        "Was sind Skip-Connections? Welche Bedeutet haben Skip-Connections in der ResNet Architektur und wie kann ich diese implementieren?"
      ],
      "course_content": {
        "Einleitung": [
          "Unsere ai-academy",
          "Einleitung",
          "Convolutions - Codebeispiel",
          "Convolutions - Der Conv2d Layer",
          "Paddings und Strides",
          "Die Convolution Gleichung",
          "Der fastai CNN-Learner",
          "Convolutions - Filter Arithmetik",
          "Verarbeitung von farbigen Bildern mit Convolutions",
          "Optimierung des Trainingsprozesses für CNNs",
          "Die 1CycleTraining-Methode aus dem fastai Framework",
          "Batch Normalization",
          "Batch Normalization Codebeispiel"
        ],
        "Residual Nets (ResNet)": [
          "Einführung Residual Nets",
          "Average Pooling Layer",
          "Training unseres ersten Basis Modells",
          "Was sind Skip-Connections?",
          "Skip Connections Codebeispiel",
          "Optimierung unseres Modells mit Skip Connections",
          "Bottlenecks"
        ],
        "CAM - Class Activation Map": [
          "CAM Intro",
          "CAM Implementierung mithilfe PyTorch Hook",
          "Gradient CAM",
          "CAM Implementierung Outro"
        ]
      },
      "requirements": [
        "Du solltest bereits Programmierkenntnisse in Python haben.",
        "Ein Überblick zum Thema Bildverarbeitung ist von Vorteil.",
        "Basiskenntnisse im Aufbau von neuronalen Netzen und deep learning sind hilfreich."
      ],
      "description": "Toll, dass du dich für Convolutional Neural Networks interessierst!\nWir freuen uns, dich auf unserer Kursseite begrüßen zu dürfen! Convolutional Neural Networks (CNNs) - eine Spezialform von neuronalen Netzen) stellen heute den Goldstandard für die Verarbeitung von Bilddaten im Bereich der künstlichen Intelligenz dar!\nIn diesem Spezialkurs gehen wir in der Tiefe auf die verschiedenen Elemente und LayerTypen in Convolutional Neural Networks ein. Dafür setzten wir die Werkzeuge der beiden Deep Learning Frameworks PyTorch und fastai ein und implementieren die einzelnen Layer selbst mithilfe von purem Python Code nach.\nInsbesondere behandeln wir folgende Themenbereiche:\nAufbau von Convolutional Neural Networks in der Theorie und Praxis\nDie Convolution: Wir programmieren einen Convolutional Pooling Layer (Conv2d)\nBerechnung von Paddings und Strides bei Convolutions\nDie mathematischen Gleichungen hinter Convolutions\nWie ist der CNN-Learner aus dem fastai Framework aufgebaut?\nWelche Theorien und Funktionalitäten sind in der 1cycleTraining Methode vom fastai Framework enthalten?\nWarum hilft das Konzept der Batch Normalization einem Convolutional Neural Network, das Training zu beschleunigen?\nWas ist eine Residual Network Architektur (ResNet) und wofür kann ich diese einsetzen?\nWir implementieren unser eigenes Resdiual Net Model sowohl mithilfe von PyTorch wie auch mit reinem Python Code.\nSkip Connections ist ein Konzept, das Residual Networks optimieren kann. Wir analysieren, was Skip Connections darstellen und wie wir diese in unserem Modell implementieren können.\nInsbesondere im Bereich der Analyse und Verarbeitung von Bilddaten konnte künstliche Intelligenz in den vergangenen Jahren enorme Erfolge verbuchen. Besonders gestützt wurde diese Entwicklung durch die Entwicklungen im Bereich neuronaler Netze hin zu Convolutional Neural Nets. CNNs trugen massiv dazu bei, dass die Lücke in der Bildverarbeitung (Computer Vision) zwischen den kognitiven Fähigkeiten von Mensch und Maschine zunehmend verschwindet.\nDas Ziel für künstliche Intelligenz Algorithmen ist es, dass Maschinen die Welt genauso sehen können wie wir Menschen auch und dieses Wissen für eine Vielzahl von Aufgaben wie Bilderkennung, Bildanalyse, Bildklassifizierung, etc. nützen können. Die dafür schlagende technologische Entwicklung im Bereich von deep learning stellen neuronale Netze - und zwar Convolutional Neural Nets - dar.\nEin Convolutional Neural Net ist ein deep Learning Algorithmus der mit Bilddaten arbeitet. Der Algorithmus lernt, die Gewichte und Bias-Werte im neuronalen Netz entsprechend der Wichtigkeit der einzelnen Features anzupassen. Das Vorverarbeiten der Bilddaten ist im Vergleich zu anderen Klassifizierungsalgorithmen durch den Einsatz des neuronalen Netzes weniger wichtig, um gute Erfolge erzielen zu können. Während in einfachen Algorithmen die anzuwendenden Filter manuell implementiert werden, lernt das Convolutional Neural Net diese Filter bzw. Eigenschaften selbst zu entwickeln.\nDie grundsätzliche Architektur eines neuronalen Netzes ist von der Funktionsweise und Vernetzung der Synapsen im menschlichen Gehirn inspiriert. Die ConvNet (Convolutional Neural Net) Architektur wiederum fand ihren Ursprung in der Organisation des visuellen Kortex. Einzelne Neuronen reagieren nur auf Stimuli in einem stark begrenzten Bereich des visuellen Umfelds - bezeichnet als rezeptives Feld (receptive field). Um das gesamte Sehfeld abzudecken, existiert eine Vielzahl solcher Bereiche, die sich gegenseitig überlappen.\nEin Bild wird auch in ein Convolutional Neural Net als Pixelmatrix eingeführt. Ein Ansatz wäre natürlich, diese Matrix in einen eindimensionalen Vektor zu transponieren und durch ein Multi-Level-Perzeptron Netz zur Klassifikation zu schleusen. Das könnte für sehr einfache Bilder zwar durchaus funktioniert, repräsentiert aber weder die Erfolge der letzten Jahre im Deep Learning Computer Vision Bereich, noch würde es für komplexere Bilddaten ausreichend gut funktionieren.\nEin deep Convolutional Neural Net (neuronales Netz) stellt eine Möglichkeit dar, sowohl zeitliche, wie auch räumliche Abhängigkeiten mithilfe von Filtern in Bildern darzustellen. Ein ConvNet ist in der Lage, die Anzahl der Parameter im Modell zu reduzieren und die Gewichtung über die Trainingsvorgänge hinweg anzupassen. So hat ein convolutional neural net die Aufgabe, die Dimension von Bilddaten ohne Inhaltsverlust zu reduzieren. Auf diese Weise erhalten wir ein Modell, das nicht nur mit einer hohen Genauigkeit Bilder klassifizieren kann, sondern auch sehr gut skalierbar ist, bei einer hohen Datenmenge.\nDie wichtigsten Varianten bzw. konkreten Ausprägungen von convolutional neural network Architekturmodellen sind: LeNet, AlexNet, VGGNet (VGG16, und andere), GooLeNet, ResNet, ZFNet.\nIm zweiten Teil des Kurses widmen wir uns dem Thema Residual Nets (ResNets). Es war im Jahr 2012, dass Krizhevsky et al. den Weg für deep convolutional networks frei machten. Die Tiefe des neuronalen Netzes betrug damals ganze acht Layer - davon fünf Convolutions und drei lineare Layer (fully connected layer). In dem Paper zeigten die Autoren, dass allein mit der Erhöhung der Anzahl an Layer ab einer gewissen Gesamtanzahl keine nennenswerten Verbesserungen im Trainingsergebnis des tiefen neuronalen Netzes mehr erreichbar sind. Die Evidenz zeigt, dass die optimale Gesamtanzahl von Layern für Modellen, die auf dem ImageNet Modell aufbauen zwischen 16 und 30 liegt.\nDie Schwierigkeit, tiefe neuronale Netze zu trainieren wurde durch das Einführen des Konzepts eines residual Blocks entschärft. Der residual Block ist mithilfe eines Konzepts mit der Bezeichnung \"Skip Connection\" in der Lage, die Ausgabewerte eines Layers mit jenen des nächsten Layers zu verknüpfen und bestimmte \"Kurzschlüsse\" innerhalb des convolutional Networks zu schaffen. Die Autoren des entsprechenden Papers, in dem residual blocks mit skip connections vorgestellt wurden, zeigten, dass sie so in der Lage waren, neuronale Netze mit 100 bis 1000 Layer erfolgreich zu trainieren. Auch der Test auf dem ImangeNet Datenset zeigte, dass das ResNet Modell mit 152 Layern weniger trainierbare Parameter als das VGG Modell aufweist.\nSkip Connections stellen also eine sehr interessante Erweiterung von deep convolutional networks dar. Die Autoren haben den Erfolg der ResNet Modellarchitektur hinsichtlich der Klassifizierung von Bilddaten ausdrücklich dargelegt.",
      "target_audience": [
        "Python-Entwickler mit Interesse an deep learning"
      ]
    },
    {
      "title": "ChatGPT提示词训练入门课",
      "url": "https://www.udemy.com/course/chatgpt-v/",
      "bio": "ChatGPT高质量提示词的底层方法论",
      "objectives": [
        "掌握正确向ChatGPT提问的方法",
        "撰写ChatGPT高质量提示词的底层方法论",
        "应用ChatGPT写朋友圈文案",
        "根据输出内容优化调试方法"
      ],
      "course_content": {
        "课程导读": [
          "课程简介",
          "讲师介绍",
          "课程大纲"
        ],
        "第一章 ChatGPT先导": [
          "ChatGPT的原理",
          "如何正确地向ChatGPT提问",
          "ChatGPT prompt 生产的应用和变现"
        ],
        "第二章 撰写ChatGPT高质量提示词的底层方法论挖掘机大法": [
          "写提示词前，90%的人会进入这2种混沌状态",
          "挖掘机大法帮你写好提示词"
        ],
        "第三章 ChatGPT挖掘机大法案例—朋友圈文案": [
          "ChatGPT挖掘机大法案例—朋友圈文案"
        ],
        "第四章 根据输出内容，如何写调整命令，让结果更精准？ 共3节 | 22分钟": [
          "补全被遗漏的信息",
          "给它指引一条正确的路",
          "让它出多个参考答案，你来做选择"
        ],
        "课程回顾": [
          "课后寄语"
        ]
      },
      "requirements": [
        "对ChatGPT应用感兴趣"
      ],
      "description": "ChatGPT在文本编辑方面表现优异，但不同的人使用的效果却大不相同，关键点在于ChatGPT提示词训练能力。掌握提示词训练的底层方法论，就相当于有了将ChatGPT转化为生产力的密码。 为此，三节课邀请互联网内容运营实战专家菜菜老师带来本次课程。 这门课首先介绍了ChatGPT提示词的应用及变现场景，然后系统讲解了撰写ChatGPT高质量提示词的底层方法论，课程中称之为“挖掘机大法”，通过五个步骤写好提示词。通过朋友圈文案撰写的实战案例，介绍该方法的具体应用。最后提供了调试输出内容的方法和思路，让ChatGPT在你手上发挥越来越大的价值。",
      "target_audience": [
        "想要从事小红书运营的新手小白",
        "想借助ChatGPT提升效率的运营工作者",
        "想了解ChatGPT内容生成的小白"
      ]
    }
  ]
}